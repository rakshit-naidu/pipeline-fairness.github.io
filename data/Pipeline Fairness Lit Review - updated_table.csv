Paper Title,Authors,Description,Tags/Comments,Conference Venue,Year,Paper link,Additional resources
The Fallacy of AI Functionality,"[Inioluwa Deborah Raji, I. Elizabeth Kumar, Aaron Horowitz, Andrew D. Selbst]",N/A,"Viability Assessments-General, Problem Identification",FAccT,2022,https://dl.acm.org/doi/10.1145/3531146.3533158,N/A
"When not to design, build, or deploy","[Solon Barocas, Biega, Asia J., Biega, Benjamin Fish, Jundefineddrzej Niklas, Luke Stark]",N/A,"Viability Assessments-General, Problem Identification",FAccT,2020,https://dl.acm.org/doi/abs/10.1145/3351095.3375691,N/A
Against Predictive Optimization: On the Legitimacy of Decision-Making Algorithms that Optimize Predictive Accuracy,"[Angelina Wang, Sayash Kapoor, Solon Barocas, Arvind Narayanan]",N/A,"Viability Assessments-General, Measurement",SSRN,2022,https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4238015,N/A
A validity perspective on evaluating the justified use of data-driven decision-making algorithms,"[Amanda Coston, Anna Kawakami, Haiyi Zhu, Ken Holstein, Hoda Heidari]",N/A,"Viability Assessments-General, Measurement",SATML,2023,https://arxiv.org/abs/2206.14983,N/A
Algorithmic fairness and vertical equity: Income fairness with irs tax audit models,"[Emily Black, Hadi Elzayn, Alexandra Chouldechova, Jacob Goldin, Daniel Ho]",N/A,"Problem Formulation-Prediction Target, Problem Identification (Case Study)",FAccT,2022,https://dl.acm.org/doi/10.1145/3531146.3533204,N/A
Dissecting racial bias in an algorithm used to manage the health of populations,"[Ziad Obermeyer, Brian Powers, Christine Vogeli, Sendhil Mullainathan]",N/A,"Problem Formulation-Prediction Target, Problem Identification (Case Study)",Science,2019,https://www.science.org/doi/abs/10.1126/science.aax2342,N/A
Data augmentation for fairness-aware machine learning: Preventing algorithmic bias in law enforcement systems,"[Ioannis Pastaltzidis, Nikolaos Dimitriou, Katherine Quezada-Tavarez, Stergios Aidinlis, Thomas Marquenie, Agata Gurzawska, Dimitrios Tzovaras]",N/A,"Problem Formulation-Prediction Target, Problem Identification (Case Study)",FAccT,2022,https://dl.acm.org/doi/10.1145/3531146.3534644,N/A
"The distributive effects of risk prediction in environmental compliance: Algorithmic design, environmental justice, and public policy","[Elinor Benami, Reid Whitaker, Vincent La, Hongjin Lin, Brandon R Anderson, Daniel E Ho]",N/A,"Problem Formulation-Prediction Target, Mitigation",FAccT,2021,https://dl.acm.org/doi/10.1145/3442188.3445873,N/A
Beyond distributive fairness in algorithmic decision making: Feature selection for procedurally fair learning,"[Nina Grgić-Hlača, Muhammad Bilal Zafar, Krishna P Gummadi, Adrian Weller]",N/A,"Problem Formulation-Predictive Attributes, Problem Identification",AAAI,2018,https://ojs.aaai.org/index.php/AAAI/article/view/11296,N/A
"Modeling assumptions clash with the real world: Transparency, equity, and community challenges for student assignment algorithms.","[Samantha Robertson, Tonya Nguyen, Niloufar Salehi.]",N/A,"Problem Formulation-General, Problem Identification (Case study)",CHI,2021,https://dl.acm.org/doi/10.1145/3411764.3445748,N/A
Algorithmic management reimagined for workers and by workers: Centering worker well-being in gig work,"[Angie Zhang, Alexander Boltz, Chun Wei Wang, Min Kyung Lee]",N/A,"Problem Formulation-General, Problem Identification (Case study)",CHI,2022,https://dl.acm.org/doi/10.1145/3491102.3501866,N/A
Re-imagining systems in the realm of immigration in higher education through participatory design.,[Maria Conchita A. Navarro and Orit Shaer],N/A,"Problem Formulation-General, Problem Identification (Case study)",CSCW,2022,https://dl.acm.org/doi/10.1145/3500868.3559457,N/A
"""I feel like I need to split myself in half"": Using Role Theory to Design for Parents as Caregiving Teams in the Children's Hospital","[Sarah Nikkhah, Akash Uday Rode, Priyanjali Mittal, Neha K. Kulkarni, Salonee Nadkarni, Emily L. Mueller, and Andrew D. Miller]",N/A,"Problem Formulation-General, Problem Identification (Case study)",CSCW,2022,https://dl.acm.org/doi/10.1145/3500868.3559466,N/A
Problem formulation and fairness,"[Samir Passi, Solon Barocas]",N/A,"Problem Formulation-General, Problem Identification",FAccT,2019,https://dl.acm.org/doi/10.1145/3287560.3287567,N/A
A case study of algorithm-assisted decision making in child maltreatment hotline screening decisions,"[Alexandra Chouldechova, Diana Benavides-Prado, Oleksandr Fialko, and Rhema Vaithianathan]",N/A,"Problem Formulation-General, Problem Identification (Case study)",FAccT,2018,https://proceedings.mlr.press/v81/chouldechova18a.html,N/A
Measurement and Fairness,[Abigail Z. Jacobs and Hanna Wallach],N/A,"Problem Formulation-General, Problem Identification",FAccT,2021,https://dl.acm.org/doi/10.1145/3442188.3445901,N/A
Soliciting stakeholders’ fairness notions in child maltreatment predictive systems,"[Hao-Fei Cheng, Logan Stapleton, Ruiqi Wang, Paige Bullock, Alexandra Chouldechova, Zhiwei Steven Wu, and Haiyi Zhu]",N/A,"Problem Formulation-General, Problem Identification",CHI,2021,https://dl.acm.org/doi/10.1145/3411764.3445308,N/A
Multi-category fairness in sponsored search auctions,"[Christina Ilvento, Meena Jagadeesan, and Shuchi Chawla]",N/A,"Problem Formulation-General, Problem Identification (Case study)",FAccT,2020,https://dl.acm.org/doi/10.1145/3351095.3372848,N/A
To Predict and Serve?,"[Kristian Lum, William Isaac]",N/A,"Problem Formulation-General, Problem Identification",Significance,2016,https://academic.oup.com/jrssig/article/13/5/14/7029190,N/A
Gender shades: Intersectional accuracy disparities in commercial gender classification,"[Joy Buolamwini, Timnit Gebru]",N/A,"Data Collection-Sampling, Problem Identification (Case Study)",FAccT,2018,https://proceedings.mlr.press/v81/buolamwini18a.html,N/A
Censorship of online encyclopedias: Implications for nlp models ,"[Eddie Yang, Margaret E. Roberts]",N/A,"Data Collection-Sampling, Problem Identification (Case Study)",FAccT,2021,https://dl.acm.org/doi/10.1145/3442188.3445916,N/A
Towards intersectional feminist and participatory ml: A case study in supporting feminicide counterdata collection,"[Harini Suresh, Rajiv Movva, Amelia Lee Dogan, Rahul Bhargava, Isadora Cruxen, Angeles Martinez Cuba, Guilia Taurino, Wonyoung So, Catherine D’Ignazio]",N/A,"Data Collection-Sampling, Problem Identification (Case Study)",FAccT,2022,https://dl.acm.org/doi/10.1145/3531146.3533132,N/A
"Female, white, 27? Bias evaluation on data and algorithms for affect recognition in faces","[Jaspar Pahl, Ines Rieger, Anna Möller, Thomas Wittenberg, Ute Schmid]",N/A,"Data Collection-Sampling, Problem Identification (Case Study)",FAccT,2022,https://dl.acm.org/doi/10.1145/3531146.3533159,N/A
Adaptive sampling strategies to construct equitable training datasets,"[William Cai, Ro Encarnacion, Bobbie Chern, Sam Corbett-Davies, Miranda Bogen, Stevie Bergman, Sharad Goel]",N/A,"Data Collection-Sampling, Problem Identification",FAccT,2022,https://arxiv.org/abs/2202.01327,N/A
Modeling risk and achieving algorithmic fairness using potential outcomes,[Alan Mishler],N/A,"Data Collection-Sampling, Measurement",AIES,2019,https://dl.acm.org/doi/10.1145/3306618.3314323,N/A
Identifying bias in data using two-distribution hypothesis tests,"[William Yik, Limnanthes Serafini, Timothy Lindsey, George D Montañez]",N/A,"Data Collection-Sampling, Measurement",AIES,2022,https://dl.acm.org/doi/abs/10.1145/3514094.3534169,N/A
Data augmentation for discrimination prevention and bias disambiguation,"[Shubham Sharma, Yunfeng Zhang, Jesús M Ríos Aliaga, Djallel Bouneffouf, Vinod Muthusamy, and Kush R Varshney]",N/A,"Data Collection-Sampling, Mitigation",AIES,2020,https://dl.acm.org/doi/10.1145/3375627.3375865,N/A
DECAF: Generating Fair Synthetic Data Using Causally-Aware Generative Networks,"[Boris van Breugel, Trent Kyono, Jeroen Berrevoets, and Mihaela van der Schaar]",N/A,"Data Collection-Sampling, Mitigation",NeurIPS,2019,https://openreview.net/forum?id=XN1M27T6uux,N/A
Detecting discriminatory risk through data annotation based on bayesian inferences,"[Elena Beretta, Antonio Vetrò, Bruno Lepri, and Juan Carlos De Martin]",N/A,"Data Collection-Sampling, Mitigation",FAccT,2021,https://dl.acm.org/doi/10.1145/3442188.3445940,N/A
Conditional contrastive learning with kernel,"[Yao-Hung Hubert Tsai, Tianqin Li, Martin Q. Ma, Han Zhao, Kun Zhang, Louis-Philippe Morency, and Ruslan Salakhutdinov.]",N/A,"Data Collection-Sampling, Mitigation (Traditional)",ICLR,2022,https://openreview.net/forum?id=AAJLBoGt0XM,N/A
Adaptive Sampling for Minimax Fair Classification,"[Shubhanshu Shekhar, Greg Fields, Mohammad Ghavamzadeh, and Tara Javidi]",N/A,"Data Collection-Sampling, Mitigation (Traditional)",NeurIPS,2021,https://openreview.net/forum?id=ZDMqRGSksHs,N/A
Fairbatch: Batch selection for model fairness,"[Yuji Roh, Kangwook Lee, Steven Euijong Whang, and Changho Suh]",N/A,"Data Collection-Sampling, Mitigation (Traditional)",ICLR,2021,https://openreview.net/forum?id=YNnpaAKeCfx,N/A
Learning with noisy labels revisited: A study using real-world human annotations,"[Jiaheng Wei, Zhaowei Zhu, Hao Cheng, Tongliang Liu, Gang Niu, and Yang Liu]",N/A,"Data Collection-Annotation, Problem Identification (Case Study)",ICLR,2022,https://openreview.net/forum?id=TBWA6PLJZQm,N/A
Fair classification with group-dependent label noise,"[Jialu Wang, Yang Liu, Caleb Levy]",N/A,"Data Collection-Annotation, Problem Identification",FAccT,2021,https://dl.acm.org/doi/10.1145/3442188.3445915,N/A
Assessing annotator identity sensitivity via item response theory: A case study in a hate speech corpus,"[Pratik S. Sachdeva, Renata Barreto, Claudia von Vacano, and Chris J. Kennedy]",N/A,"Data Collection-Annotation, Problem Identification",FAccT,2022,https://dl.acm.org/doi/10.1145/3531146.3533216,N/A
It’s about time: A view of crowdsourced data before and during the pandemic,"[Evgenia Christoforou, Pinar Barlas, Jahna Otterbacher]",N/A,"Data Collection-Annotation, Problem Identification",CHI,2021,https://dl.acm.org/doi/10.1145/3411764.3445317,N/A
Can less be more? when increasing-to-balancing label noise rates considered beneficial.,"[Yang Liu, Jialu Wang]",N/A,"Data Collection-Annotation, Measurement",NeurIPS,2021,https://openreview.net/forum?id=VjKhSULF7Gb,N/A
Measuring representational harms in image captioning,"[Angelina Wang, Solon Barocas, Kristen Laird, Hanna Wallach]",N/A,"Data Collection-Annotation, Measurement",FAccT,2022,https://dl.acm.org/doi/10.1145/3531146.3533099,N/A
The Values Encoded in Machine Learning Research ,"[Abeba Birhane, Pratyusha Kalluri, Dallas Card, William Agnew, Ravit Dotan, and Michelle Bao]",N/A,"Data Collection-Annotation, Measurement",FAccT,2022,https://dl.acm.org/doi/fullHtml/10.1145/3531146.3533083,N/A
Towards fairer datasets: Filtering and balancing the distribution of the people subtree in the imagenet hierarchy,"[Kaiyu Yang, Klint Qinami, Li Fei-Fei, Jia Deng, and Olga Russakovsky]",N/A,"Data Collection-Annotation, Mitigation",FAccT,2020,https://dl.acm.org/doi/10.1145/3351095.3375709,N/A
Fairness in representation for multilingual NLP: Insights from controlled experiments on conditional language modeling,[Ada Wan],N/A,"Data Collection-Feature Measurement, Problem Identification (Case Study)",ICLR,2022,https://openreview.net/forum?id=-llS6TiOew,N/A
Disparate impact of artificial intelligence bias in ridehailing economy’s price discrimination algorithms.,"[Akshat Pandey, Aylin Caliskan]",N/A,"Data Collection-Feature Measurement, Problem Identification (Case Study)",AIES,2021,https://arxiv.org/abs/2006.04599,N/A
Asymmetric shapley values: incorporating causal knowledge into model-agnostic explainability,"[Christopher Frye, Colin Rowat, Ilya Feige]",N/A,"Data Collection-Feature Measurement, Problem Identification",NeurIPS,2020,https://dl.acm.org/doi/10.5555/3495724.3495828,N/A
What are the biases in my word embedding?,"[Nathaniel Swinger, Maria De-Arteaga, Neil Thomas Heffernan IV, Mark DM Leiserson, and Adam Tauman Kalai]",N/A,"Data Collection-Feature Measurement, Measurement",AIES,2019,https://dl.acm.org/doi/10.1145/3306618.3314270,N/A
Dynamic covid risk assessment accounting for community virus exposure from a spatial-temporal transmission model,"[Yuan Chen, Wenbo Fei, Qinxia Wang, Donglin Zeng, and Yuanjia Wang]",N/A,"Data Collection-General, Problem Identification (Case Study)",NeurIPS,2021,https://proceedings.neurips.cc/paper/2021/hash/e97a4f04ef1b914f6a1698caa364f693-Abstract.html,N/A
Investigating and mitigating biases in crowdsourced data,"[Danula Hettiachchi, Mark Sanderson, Jorge Goncalves, Simo Hosio, Gabriella Kazai, Matthew Lease, Mike Schaekermann, and Emine Yilmaz]",N/A,"Data Collection-General, Problem Identification (Case Study)",CSCW,2021,https://dl.acm.org/doi/10.1145/3462204.3481729,N/A
"“Everyone wants to do the model work, not the data work”: Data Cascades in High-Stakes AI","[Nithya Sambasivan, Shivani Kapania, Hannah Highfill, Diana Akrong, Praveen Paritosh, and Lora M Aroyo]",N/A,"Data Collection-General, Problem Identification (Case Study)",CHI,2021,https://dl.acm.org/doi/10.1145/3411764.3445518,N/A
Designing an online infrastructure for collecting ai data from people with disabilities,"[Joon Sung Park, Danielle Bragg, Ece Kamar, and Meredith Ringel Morris]",N/A,"Data Collection-General, Problem Identification",FAccT,2021,https://dl.acm.org/doi/10.1145/3442188.3445870,N/A
Residual unfairness in fair machine learning from prejudiced data,"[Nathan Kallus, Angela Zhou]",N/A,"Data Collection-General, Problem Identification",ICML,2018,https://proceedings.mlr.press/v80/kallus18a.html,N/A
Data in new delhi’s predictive policing system,"[Vidushi Marda, Shivangi Narayan]",N/A,"Data Collection-General, Problem Identification",FAccT,2020,https://dl.acm.org/doi/abs/10.1145/3351095.3372865,N/A
Data-centric factors in algorithmic fairness,"[Nianyun Li, Naman Goel, and Elliott Ash]",N/A,"Data Collection-General, Measurement",AIES,2022,https://dl.acm.org/doi/10.1145/3514094.3534147,N/A
Retiring adult: New datasets for fair machine learning,"[Frances Ding, Moritz Hardt, John Miller, and Ludwig Schmidt]",N/A,"Data Collection-General, Measurement",NeurIPS,2021,https://openreview.net/forum?id=bYi_2708mKK,N/A
Assessing fairness in the presence of missing data,"[Yiliang Zhang, Qi Long]",N/A,"Data Collection-General, Measurement",NeurIPS,2021,https://openreview.net/forum?id=myJO35O7Gg,N/A
Understanding the representation and representativeness of age in ai data sets,"[Joon Sung Park, Michael S. Bernstein, Robin N. Brewer, Ece Kamar, and Meredith Ringel Morris]",N/A,"Data Collection-General, Measurement",AIES,2021,https://dl.acm.org/doi/10.1145/3461702.3462590,N/A
Can I Trust My Fairness Metric? Assessing Fairness with Unlabeled Data and Bayesian Inference,"[Disi Ji, Padhraic Smyth, and Mark Steyvers]",N/A,"Data Collection-General, Measurement",NeurIPS,2020,https://proceedings.neurips.cc/paper/2020/hash/d83de59e10227072a9c034ce10029c39-Abstract.html,N/A
Assessing algorithmic fairness with unobserved protected class using data combination,"[Nathan Kallus, Xiaojie Mao, and Angela Zhou]",N/A,"Data Collection-General, Measurement",FAccT,2020,https://dl.acm.org/doi/10.1145/3351095.3373154,N/A
The effect of differential victim crime reporting on predictive policing systems,"[Nil-Jana Akpinar, Maria De-Arteaga, and Alexandra Chouldechova]",N/A,"Data Collection-General, Measurement",FAccT,2021,https://dl.acm.org/doi/10.1145/3442188.3445877,N/A
Man is to computer programmer as woman is to homemaker? debiasing word embeddings,"[Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai]",N/A,"Data Collection-General, Measurement",NeurIPS,2016,https://dl.acm.org/doi/10.5555/3157382.3157584,N/A
Data preprocessing techniques for classification without discrimination,"[Faisal Kamiran, Toon Calders]",N/A,"Data Collection-General, Measurement",Knowledge and Information Systems,2012,https://link.springer.com/article/10.1007/s10115-011-0463-8,N/A
Why is my classifier discriminatory?,"[Irene Chen, Fredrik D Johansson, and David Sontag]",N/A,"Data Collection-General, Mitigation",NeurIPS,2018,https://dl.acm.org/doi/10.5555/3327144.3327272,N/A
Datasheets for Datasets,"[Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal III Daumé, and Kate Crawford]",N/A,"Data Collection-General, Mitigation",Communications of the ACM,2021,https://cacm.acm.org/magazines/2021/12/256932-datasheets-for-datasets/abstract,N/A
Datasheets for datasets help ml engineers notice and understand ethical issues in training data,[Karen L Boyd],N/A,"Data Collection-General, Mitigation",CHI,2021,https://dl.acm.org/doi/abs/10.1145/3479582,N/A
Automating Procedurally Fair Feature Selection in Machine Learning,"[Clara Belitz, Lan Jiang, and Nigel Bosch]",N/A,"Data Preprocessing-Feature Selection, Problem Identification/Measurement",AIES,2021,https://dl.acm.org/doi/10.1145/3461702.3462585,N/A
Hunting for Discriminatory Proxies in Linear Regression Models,"[Samuel Yeom, Anupam Datta, and Matt Fredrikson]",N/A,"Data Preprocessing-Feature Selection, Problem Identification/Mitigation",NeurIPS,2018,https://proceedings.neurips.cc/paper/2018/hash/6cd9313ed34ef58bad3fdd504355e72c-Abstract.html,N/A
FlipTest: fairness testing via optimal transport,"[Emily Black, Samuel Yeom, and Matt Fredrikson]",N/A,"Data Preprocessing-Feature Selection/Testing and Validation-Measurement, Problem Identification",FAccT,2020,https://dl.acm.org/doi/abs/10.1145/3351095.3372845,N/A
Feature-Wise Bias Amplification,"[Klas Leino, Matt Fredrikson, Emily Black, Shayak Sen, and Anupam Datta]",N/A,"Data Preprocessing-Feature Selection, Problem Identification",ICLR,2019,https://openreview.net/forum?id=S1ecm2C9K7,N/A
Assessing social and intersectional biases in contextualized word representations,"[Yi Chern Tan, L. Elisa Celis]",N/A,"Data Preprocessing-Feature Selection, Measurement",NeurIPS,2019,https://dl.acm.org/doi/10.5555/3454287.3455472,N/A
Fairness for AUC via Feature Augmentation,"[Hortense Fong, Vineet Kumar, Anay Mehrotra, and Nisheeth K. Vishnoi]",N/A,"Data Preprocessing-Feature Selection, Mitigation",FAccT,2022,https://dl.acm.org/doi/10.1145/3531146.3533126,N/A
A Geometric Solution to Fair Representations,"[Yuzi He, Keith Burghardt, and Kristina Lerman]",N/A,"Data Preprocessing-Feature Selection, Mitigation (Traditional)",AIES,2020,https://dl.acm.org/doi/10.1145/3375627.3375864,N/A
Controllable invariance through adversarial feature learning,"[Qizhe Xie, Zihang Dai, Yulun Du, Eduard Hovy, and Graham Neubig]",N/A,"Data Preprocessing-Data Cleaning (Omission), Measurement",NeurIPS,2017,https://dl.acm.org/doi/10.5555/3294771.3294827,N/A
Data Validation for Machine Learning,"[Eric Breck, Neoklis Polyzotis, Sudip Roy, Steven Whang, and Martin Zinkevich]",N/A,"Data Preprocessing-Data Cleaning (Omission), Measurement",MLSys,2019,https://mlsys.org/Conferences/2019/doc/2019/167.pdf,N/A
Optimized Pre-Processing for Discrimination Prevention,"[Flavio Calmon, Dennis Wei, Bhanukiran Vinzamuri, Karthikeyan Natesan Ramamurthy, and Kush R Varshney]",N/A,"Data Preprocessing-Data Cleaning (Omission), Mitigation",NeurIPS,2017,https://papers.nips.cc/paper_files/paper/2017/hash/9a49a25d845a483fae4be7e341368e36-Abstract.html,N/A
Towards Fair Deep Anomaly Detection,[Hongjing Zhang and Ian Davidson],N/A,"Data Preprocessing-Data Cleaning (Omission), Mitigation",FAccT,2021,https://dl.acm.org/doi/10.1145/3442188.3445878,N/A
"The Data Linter: Lightweight, Automated Sanity Checking for ML Data Sets","[Nick Hynes, D. Sculley, and Michael Terry]",N/A,"Data Preprocessing-Data Cleaning (Omission), Mitigation",NeurIPS ML Systems workshop,2017,http://learningsys.org/nips17/assets/papers/paper_19.pdf,N/A
ActiveClean: interactive data cleaning for statistical modeling,"[Sanjay Krishnan, Jiannan Wang, Eugene Wu, Michael J Franklin, and Ken Goldberg]",N/A,"Data Preprocessing-Data Cleaning (Omission), Mitigation",VLDB Endowment,2016,https://dl.acm.org/doi/10.14778/2994509.2994514,N/A
"Gender Bias in Word Embeddings: A Comprehensive Analysis of Frequency, Syntax, and Semantics","[Aylin Caliskan, Pimparkar Parth Ajay, Tessa Charlesworth, Robert Wolfe, and Mahzarin R. Banaji]",N/A,"Data Preprocessing-General, Problem Identification",AIES,2022,https://dl.acm.org/doi/abs/10.1145/3514094.3534162,N/A
Does enforcing fairness mitigate biases caused by subpopulation shift?,"[Subha Maity, Debarghya Mukherjee, Mikhail Yurochkin, and Yuekai Sun]",N/A,"Data Preprocessing-General, Problem Identification",NeurIPS,2021,https://openreview.net/forum?id=6mUrD5rg-UU,N/A
Towards Understanding and Mitigating Social Biases in Language Models,"[Paul Pu Liang, Chiyu Wu, Louis-Philippe Morency, and Ruslan Salakhutdinov]",N/A,"Data Preprocessing-General, Problem Identification",ICML,2021,https://proceedings.mlr.press/v139/liang21a.html,N/A
Fair preprocessing: towards understanding compositional fairness of data transformers in machine learning pipeline,"[Sumon Biswas, Hridesh Rajan]",N/A,"Data Preprocessing-General, Problem Identification",ESEC/FSE,2021,https://dl.acm.org/doi/10.1145/3468264.3468536,N/A
Uncertainty and the Social Planner’s Problem: Why Sample Complexity Matters,[Cyrus Cousins],N/A,"Data Preprocessing-General, Measurement",FAccT,2022,https://dl.acm.org/doi/10.1145/3531146.3533243,N/A
Promoting Fairness in Learned Models by Learning to Active Learn under Parity Constraints,"[Amr Sharaf, Hal Daume III, and Renkun Ni]",N/A,"Data Preprocessing-General, Measurement",FAccT,2022,https://dl.acm.org/doi/10.1145/3531146.3534632,N/A
Fair Generative Modeling via Weak Supervision,"[Kristy Choi, Aditya Grover, Trisha Singh, Rui Shu, and Stefano Ermon]",N/A,"Data Preprocessing-General, Measurement",ICML,2020,https://proceedings.mlr.press/v119/choi20a.html,N/A
Fair Representation Learning through Implicit Path Alignment,"[Changjian Shui, Qi Chen, Jiaqi Li, Boyu Wang, and Christian Gagné]",N/A,"Data Preprocessing-General, Measurement",ICML,2022,https://proceedings.mlr.press/v162/shui22a.html,N/A
Understanding Instance-Level Impact of Fairness Constraints,"[Jialu Wang, Xin Eric Wang, and Yang Liu]",N/A,"Data Preprocessing-General, Measurement",ICML,2022,https://proceedings.mlr.press/v162/wang22ac.html,N/A
Robin Hood and Matthew Effects: Differential Privacy Has Disparate Impact on Synthetic Data,"[Georgi Ganev, Bristena Oprisanu, and Emiliano De Cristofaro]",N/A,"Data Preprocessing-General, Measurement",ICML,2022,https://proceedings.mlr.press/v162/ganev22a.html,N/A
Beyond Parity: Fairness Objectives for Collaborative Filtering,[Sirui Yao and Bert Huang],N/A,"Data Preprocessing-General, Mitigation",NeurIPS,2017,https://papers.nips.cc/paper_files/paper/2017/hash/e6384711491713d29bc63fc5eeb5ba4f-Abstract.html,N/A
Exploiting MMD and Sinkhorn Divergences for Fair and Transferable Representation Learning,"[Luca Oneto, Michele Donini, Giulia Luise, Carlo Ciliberto, Andreas Maurer, and Massimiliano Pontil]",N/A,"Data Preprocessing-General, Mitigation",NeurIPS,2020,https://proceedings.neurips.cc/paper/2020/hash/af9c0e0c1dee63e5acad8b7ed1a5be96-Abstract.html,N/A
AutoBalance: Optimized Loss Functions for Imbalanced Data,"[Mingchen Li, Xuechen Zhang, Christos Thrampoulidis, Jiasi Chen, and Samet Oymak]",N/A,"Data Preprocessing-General, Mitigation",NeurIPS,2021,https://openreview.net/forum?id=ebQXflQre5a,N/A
Fairness via Representation Neutralization,"[Mengnan Du, Subhabrata Mukherjee, Guanchu Wang, Ruixiang Tang, Ahmed Awadallah, and Xia Hu]",N/A,"Data Preprocessing-General, Mitigation",NeurIPS,2021,https://openreview.net/forum?id=nHRGW_wETLQ,N/A
Reducing sentiment polarity for demographic attributes in word embeddings using adversarial learning,"[Chris Sweeney, Maryam Najafian]",N/A,"Data Preprocessing-General, Mitigation",FAccT,2020,https://dl.acm.org/doi/10.1145/3351095.3372837,N/A
Bias in word embeddings,"[Orestis Papakyriakopoulos, Simon Hegelich, Juan Carlos Medina Serrano, and Fabienne Marco]",N/A,"Data Preprocessing-General, Mitigation",FAccT,2020,https://dl.acm.org/doi/10.1145/3351095.3372843,N/A
Smallset Timelines: A Visual Representation of Data Preprocessing Decisions,"[Lydia R. Lucchesi, Petra M. Kuhnert, Jenny L. Davis, and Lexing Xie]",N/A,"Data Preprocessing-General, Mitigation",FAccT,2022,https://dl.acm.org/doi/abs/10.1145/3531146.3533175,N/A
Flexibly Fair Representation Learning by Disentanglement,"[Elliot Creager, David Madras, Joern-Henrik Jacobsen, Marissa Weis, Kevin Swersky, Toniann Pitassi, and Richard Zemel]",N/A,"Data Preprocessing-General, Mitigation",ICML,2019,https://proceedings.mlr.press/v97/creager19a.html,N/A
DeBayes: a Bayesian Method for Debiasing Network Embeddings,"[Maarten Buyl, Tijl De Bie]",N/A,"Data Preprocessing-General, Mitigation",ICML,2020,https://proceedings.mlr.press/v119/buyl20a.html,N/A
Data preprocessing to mitigate bias: A maximum entropy based approach,"[L. Elisa Celis, Vijay Keswani, and Nisheeth Vishnoi]",N/A,"Data Preprocessing-General, Mitigation",ICML,2020,https://proceedings.mlr.press/v119/celis20a.html,N/A
On Disentangled Representations Learned from Correlated Data,"[Frederik Träuble, Elliot Creager, Niki Kilbertus, Francesco Locatello, Andrea Dittadi, Anirudh Goyal, Bernhard Schölkopf, and Stefan Bauer]",N/A,"Data Preprocessing-General, Mitigation",ICML,2021,https://proceedings.mlr.press/v139/trauble21a.html,N/A
Learning fair representation with a parametric integral probability metric,"[Dongha Kim, Kunwoong Kim, Insung Kong, Ilsang Ohn, and Yongdai Ki]",N/A,"Data Preprocessing-General, Mitigation",ICML,2022,https://proceedings.mlr.press/v162/kim22b.html,N/A
Leave-one-out Unfairness,"[Emily Black, Matt Fredrikson]",N/A,"Statistical Modeling-Hypothesis Class, Problem Identification",FAccT,2021,https://dl.acm.org/doi/10.1145/3442188.3445894,N/A
Fairness Through Robustness: Investigating Robustness Disparity in Deep Learning,"[Vedant Nanda, Samuel Dooley, Sahil Singla, Soheil Feizi, and John P. Dickerson]",N/A,"Statistical Modeling-Hypothesis Class, Problem Identification",FAccT,2021,https://dl.acm.org/doi/10.1145/3442188.3445910,N/A
The Rich Get Richer: Disparate Impact of Semi-Supervised Learning,"[Zhaowei Zhu, Tianyi Luo, and Yang Liu]",N/A,"Statistical Modeling-Hypothesis Class, Measurement",ICLR,2022,https://openreview.net/forum?id=DXPftn5kjQK,N/A
"Towards Better Detection of Biased Language with Scarce, Noisy, and Biased Annotations","[Zhuoyan Li, Zhuoran Lu, and Ming Yin]",N/A,"Statistical Modeling-Hypothesis Class, Mitigation",AIES,2022,https://dl.acm.org/doi/10.1145/3514094.3534142,N/A
Fair decision making using privacy-protected data,"[David Pujol, Ryan McKenna, Satya Kuppam, Michael Hay, Ashwin Machanavajjhala, and Gerome Miklau]",N/A,"Statistical Modeling-Optimization Function, Problem Identification",FAccT,2020,https://dl.acm.org/doi/10.1145/3351095.3372872,N/A
Chasing Your Long Tails: Differentially Private Prediction in Health Care Settings,"[Vinith M Suriyakumar, Nicolas Papernot, Anna Goldenberg, and Marzyeh Ghassemi]",N/A,"Statistical Modeling-Optimization Function, Problem Identification",FAccT,2021,https://dl.acm.org/doi/10.1145/3442188.3445934,N/A
Fairness Without Demographics in Repeated Loss Minimization,"[Tatsunori Hashimoto, Megha Srivastava, Hongseok Namkoong, and Percy Liang]",N/A,"Statistical Modeling-Optimization Function, Problem Identification",ICML,2018,https://proceedings.mlr.press/v80/hashimoto18a.html,N/A
Distinguishing rule and exemplar-based generalization in learning systems,"[Ishita Dasgupta, Erin Grant, and Tom Griffiths]",N/A,"Statistical Modeling-Optimization Function, Problem Identification",ICML,2022,https://proceedings.mlr.press/v162/dasgupta22b.html,N/A
Underspecification Presents Challenges for Credibility in Modern Machine Learning,"[Alexander D'Amour, Katherine Heller, Dan Moldovan, Ben Adlam, Babak Alipanahi, Alex Beutel, Christina Chen, Jonathan Deaton, Jacob Eisenstein, Matthew D. Hoffman, Farhad Hormozdiari, Neil Houlsby, Shaobo Hou, Ghassen Jerfel, Alan Karthikesalingam, Mario Lucic, Yian Ma, Cory McLean, Diana Mincu, Akinori Mitani, Andrea Montanari, Zachary Nado, Vivek Natarajan, Christopher Nielson, Thomas F. Osborne, Rajiv Raman, Kim Ramasamy, Rory Sayres, Jessica Schrouff, Martin Seneviratne, Shannon Sequeira, Harini Suresh, Victor Veitch, Max Vladymyrov, Xuezhi Wang, Kellie Webster, Steve Yadlowsky, Taedong Yun, Xiaohua Zhai, D. Sculley]",N/A,"Statistical Modeling-Optimization Function, Measurement",JMLR,2022,https://www.jmlr.org/papers/volume23/20-1335/20-1335.pdf,N/A
To be Robust or to be Fair: Towards Fairness in Adversarial Training,"[Han Xu, Xiaorui Liu, Yaxin Li, Anil Jain, Jiliang Tang]",N/A,"Statistical Modeling-Optimization Function, Mitigation",ICML,2021,https://proceedings.mlr.press/v139/xu21b.html,N/A
When Worlds Collide: Integrating Different Counterfactual Assumptions in Fairness,"[Chris Russell, Matt J Kusner, Joshua Loftus, and Ricardo Silva]",N/A,"Statistical Modeling-Optimization Function, Mitigation (Traditional)",NeurIPS,2017,https://papers.nips.cc/paper_files/paper/2017/hash/1271a7029c9df08643b631b02cf9e116-Abstract.html,N/A
Loss-Aversively Fair Classification,"[Junaid Ali, Muhammad Bilal Zafar, Adish Singla, and Krishna P. Gummadi]",N/A,"Statistical Modeling-Optimization Function, Mitigation (Traditional)",AIES,2019,https://dl.acm.org/doi/10.1145/3306618.3314266,N/A
FairEGM: Fair Link Prediction and Recommendation via Emulated Graph Modification,"[Sean Current, Yuntian He, Saket Gurukar, and Srinivasan Parthasarathy]",N/A,"Statistical Modeling-Optimization Function, Mitigation (Traditional)",EAAMO,2022,https://dl.acm.org/doi/10.1145/3551624.3555287,N/A
FairBatch: Batch Selection for Model Fairness,"[Yuji Roh, Kangwook Lee, Steven Euijong Whang, Changho Suh]",N/A,"Statistical Modeling-Optimization Function, Mitigation (Traditional)",ICLR,2021,https://openreview.net/forum?id=YNnpaAKeCfx,N/A
Achieving Equalized Odds by Resampling Sensitive Attributes,"[Yaniv Romano, Stephen Bates, Emmanuel Candes]",N/A,"Statistical Modeling-Optimization Function, Mitigation (Traditional)",NeurIPS,2020,https://proceedings.neurips.cc/paper/2020/hash/03593ce517feac573fdaafa6dcedef61-Abstract.html,N/A
"Average individual fairness: algorithms, generalization and experiments","[Saeed Sharifi-Malvajerdi, Michael Kearns, and Aaron Roth]",N/A,"Statistical Modeling-Optimization Function, Mitigation (Traditional)",NeurIPS,2019,https://dl.acm.org/doi/abs/10.5555/3454287.3455027,N/A
Fair regression with wasserstein barycenters,"[Evgenii Chzhen, Christophe Denis, Mohamed Hebiri, Luca Oneto, and Massimiliano Pontil]",N/A,"Statistical Modeling-Optimization Function, Mitigation (Traditional)",NeurIPS,2020,https://dl.acm.org/doi/abs/10.5555/3495724.3496338,N/A
A Fair Classifier Using Kernel Density Estimation,"[Jaewoong Cho, Gyeongjo Hwang, and Changho Suh]",N/A,"Statistical Modeling-Optimization Function, Mitigation (Traditional)",NeurIPS,2020,https://proceedings.neurips.cc/paper/2020/hash/ac3870fcad1cfc367825cda0101eee62-Abstract.html,N/A
Sample Selection for Fair and Robust Training,"[Yuji Roh, Kangwook Lee, Steven Whang, and Changho Suh]",N/A,"Statistical Modeling-Optimization Function, Mitigation (Traditional)",NeurIPS,2021,https://openreview.net/forum?id=IZNR0RDtGp3,N/A
Learning to pivot with adversarial networks,"[Gilles Louppe, Michael Kagan, and Kyle Cranmer]",N/A,"Statistical Modeling-Optimization Function, Mitigation (Traditional)",NeurIPS,2017,https://dl.acm.org/doi/10.5555/3294771.3294865,N/A
Tilted Empirical Risk Minimization,"[Tian Li, Ahmad Beirami, Maziar Sanjabi, and Virginia Smith]",N/A,"Statistical Modeling-Optimization Function, Mitigation",ICLR,2021,https://openreview.net/forum?id=K5YasWXZT3O,N/A
Gradient Driven Rewards to Guarantee Fairness in Collaborative Machine Learning,"[Xinyi Xu, Lingjuan Lyu, Xingjun Ma, Chenglin Miao, Chuan Sheng Foo, and Bryan Kian Hsiang Low[",N/A,"Statistical Modeling-Optimization Function, Mitigation",NeurIPS,2021,https://openreview.net/forum?id=yRfsADObu18,N/A
Matching Learned Causal Effects of Neural Networks with Domain Priors,"[Sai Srinivas Kancheti, Abbavaram Gowtham Reddy, Vineeth N Balasubramanian, and Amit Sharma]",N/A,"Statistical Modeling-Regularizers, Mitigation",ICML,2022,https://proceedings.mlr.press/v162/kancheti22a.html,N/A
Selective Regression under Fairness Criteria,"[Abhin Shah, Yuheng Bu, Joshua K Lee, Subhro Das, Rameswar Panda, Prasanna Sattigeri, and Gregory W Wornell]",N/A,"Statistical Modeling-Regularizers, Mitigation",ICML,2022,https://proceedings.mlr.press/v162/shah22a.html,N/A
When Trusted Black Boxes Don't Agree: Incentivizing Iterative Improvement and Accountability in Critical Software Systems,"[Jeanna Neefe Matthews, Graham Northup, Isabella Grasso, Stephen Lorenz, Marzieh Babaeianjelodar, Hunter Bashaw, Sumona Mondal, Abigail Matthews, Mariama Njie, Jessica Goldthwaite]",N/A,"Statistical Modeling-Hyperparameters, Problem Identification",AIES,2020,https://dl.acm.org/doi/10.1145/3375627.3375807,N/A
Can Information Flows Suggest Targets for Interventions in Neural Circuits?,"[Praveen Venkatesh, Sanghamitra Dutta, Neil Mehta, and Pulkit Grover]",N/A,"Statistical Modeling-Hyperparameters, Mitigation",NeurIPS,2021,https://openreview.net/forum?id=jBQaRXpEgO,N/A
GetFair: Generalized Fairness Tuning of Classification Models,"[Sandipan Sikdar, Florian Lemmerich, and Markus Strohmaier]",N/A,"Statistical Modeling-Hyperparameters, Mitigation",FAccT,2022,https://dl.acm.org/doi/10.1145/3531146.3533094,N/A
Multi-objective multi-fidelity hyperparameter optimization with application to fairness,"[Robin Schmucker, Michele Donini, Valerio Perrone, and Cédric Archambeau]",N/A,"Statistical Modeling-Hyperparameters, Mitigation",NeurIPS workshop on Meta-learning,2020,https://www.amazon.science/publications/multi-objective-multi-fidelity-hyperparameter-optimization-with-application-to-fairness,N/A
Can We Obtain Fairness For Free?,"[Rashidul Islam, Shimei Pan, and James R Foulds]",N/A,"Statistical Modeling-Hyperparameters, Mitigation",AIES,2021,https://dl.acm.org/doi/abs/10.1145/3461702.3462614,N/A
Fair Bayesian Optimization,"[Valerio Perrone, Michele Donini, Muhammad Bilal Zafar, Robin Schmucker, Krishnaram Kenthapadi, and Cédric Archambeau]",N/A,"Statistical Modeling-Hyperparameters, Mitigation",AIES,2021,https://dl.acm.org/doi/abs/10.1145/3461702.3462629,N/A
Subgroup Generalization and Fairness of Graph Neural Networks,"[Jiaqi Ma, Junwei Deng, and Qiaozhu Mei]",N/A,"Statistical Modeling-General, Problem Identification",NeurIPS,2021,https://openreview.net/forum?id=68B1ezcffDc,N/A
Measuring Fairness of Rankings under Noisy Sensitive Information,"[Azin Ghazimatin, Matthaus Kleindessner, Chris Russell, Ziawasch Abedjan, and Jacek Golebiowski]",N/A,"Statistical Modeling-General, Problem Identification",FAccT,2022,https://dl.acm.org/doi/10.1145/3531146.3534641,N/A
Too Relaxed to Be Fair,"[Michael Lohaus, Michael Perrot, and Ulrike Von Luxburg]",N/A,"Statistical Modeling-General, Problem Identification",ICML,2020,https://proceedings.mlr.press/v119/lohaus20a.html,N/A
Characterizing Fairness Over the Set of Good Models Under Selective Labels,"[Amanda Coston, Ashesh Rambachan, and Alexandra Chouldechova]",N/A,"Statistical Modeling-General, Problem Identification",ICML,2021,https://proceedings.mlr.press/v139/coston21a.html,N/A
"Putting Fairness Principles into Practice: Challenges, Metrics, and Improvements","[Alex Beutel, Jilin Chen, Tulsee Doshi, Hai Qian, Allison Woodruff, Christine Luu, Pierre Kreitmann, Jonathan Bischof, and Ed H. Ch]",N/A,"Statistical Modeling-General, Problem Identification",AIES,2019,https://dl.acm.org/doi/10.1145/3306618.3314234,N/A
A Sandbox Tool to Bias(Stress)-Test Fairness Algorithms,"[Nil-Jana Akpinar, Manish Nagireddy, Logan Stapleton, Hao-Fei Cheng, Haiyi Zhu, Steven Wu, and Hoda Heidari]",N/A,"Statistical Modeling-General, Problem Identification",EAAMO,2022,https://arxiv.org/abs/2204.10233,N/A
Differential privacy has disparate impact on model accuracy,"[Eugene Bagdasaryan, Omid Poursaeed, and Vitaly Shmatikov]",N/A,"Statistical Modeling-General, Problem Identification",NeurIPS,2019,https://dl.acm.org/doi/abs/10.5555/3454287.3455674,N/A
"Fair, Robust, and Data-Efficient Machine Learning in Healthcare",[Harvineet Singh],N/A,"Statistical Modeling-General, Problem Identification (Case Study)",AIES,2022,https://dl.acm.org/doi/10.1145/3514094.3539552,N/A
Learning the Pareto Front with Hypernetworks,"[Aviv Navon, Aviv Shamsian, Ethan Fetaya, and Gal Chechik]",N/A,"Statistical Modeling-General, Measurement",NeurIPS,2021,https://openreview.net/forum?id=NjF772F4ZZR,N/A
Fairness Under Unawareness: Assessing Disparity When Protected Class Is Unobserved,"[Jiahao Chen, Nathan Kallus, Xiaojie Mao, Geoffry Svacha, and Madeleine Udell]",N/A,"Statistical Modeling-General, Measurement",FAccT,2019,https://dl.acm.org/doi/10.1145/3287560.3287594,N/A
POTs: protective optimization technologies,"[Bogdan Kulynych, Rebekah Overdorf, Carmela Troncoso, and Seda Gürses]",N/A,"Statistical Modeling-General, Measurement",FAccT,2020,https://dl.acm.org/doi/10.1145/3351095.3372853,N/A
Directional Bias Amplification,"[Angelina Wang, Olga Russakovsky]",N/A,"Statistical Modeling-General, Measurement",ICML,2021,https://proceedings.mlr.press/v139/wang21t.html,N/A
On the Tradeoff Between Robustness and Fairness,"[Xinsong Ma, Zekai Wang, and Weiwei Liu]",N/A,"Statistical Modeling-General, Measurement",NeurIPS,2022,https://openreview.net/forum?id=LqGA2JMLwBw,N/A
Refining Language Models with Compositional Explanations,"[Huihan Yao, Ying Chen, Qinyuan Ye, Xisen Jin, and Xiang Ren]",N/A,"Statistical Modeling-General, Mitigation",NeurIPS,2021,https://openreview.net/forum?id=dkw9OQMn1t,N/A
Noise-tolerant fair classification,"[Alex Lamy, Ziyuan Zhong, Aditya K Menon, and Nakul Verma]",N/A,"Statistical Modeling-General, Mitigation",NeurIPS,2019,https://dl.acm.org/doi/10.5555/3454287.3454314,N/A
Learning certified individually fair representations,"[Anian Ruoss, Mislav Balunovic, Marc Fischer, and Martin Vechev]",N/A,"Statistical Modeling-General, Mitigation",NeurIPS,2020,https://dl.acm.org/doi/10.5555/3495724.3496360,N/A
On Dyadic Fairness: Exploring and Mitigating Bias in Graph Connections,"[Peizhao Li, Yifei Wang, Han Zhao, Pengyu Hong, and Hongfu Liu]",N/A,"Statistical Modeling-General, Mitigation",ICLR,2021,https://openreview.net/forum?id=xgGS6PmzNq6,N/A
Fair Normalizing Flows,"[Mislav Balunovic, Anian Ruoss, and Martin Vechev]",N/A,"Statistical Modeling-General, Mitigation",ICLR,2022,https://openreview.net/forum?id=BrFIKuxrZE,N/A
Decoupled Classifiers for Group-Fair and Efficient Machine Learning,"[Cynthia Dwork, Nicole Immorlica, Adam Tauman Kalai, and Max Leiserson]",N/A,"Statistical Modeling-General, Mitigation",FAccT,2018,https://proceedings.mlr.press/v81/dwork18a.html,N/A
Recommendation Independence,"[Toshihiro Kamishima, Shotaro Akaho, Hideki Asoh, and Jun Sakuma]",N/A,"Statistical Modeling-General, Mitigation",FAccT,2018,https://proceedings.mlr.press/v81/kamishima18a.html,N/A
Balanced Neighborhoods for Multi-sided Fairness in Recommendation,"[Robin Burke, Nasim Sonboli, and Aldo Ordonez-Gauger]",N/A,"Statistical Modeling-General, Mitigation",FAccT,2018,https://proceedings.mlr.press/v81/burke18a.html,N/A
Mitigating Bias in Set Selection with Noisy Protected Attributes,"[Anay Mehrotra, L. Elisa Celis]",N/A,"Statistical Modeling-General, Mitigation",FAccT,2021,https://dl.acm.org/doi/10.1145/3442188.3445887,N/A
Achieving Fairness via Post-Processing in Web-Scale Recommender Systems,"[Preetam Nandy, Cyrus DiCiccio, Divya Venugopalan, Heloise Logan, Kinjal Basu, and Noureddine El Karoui]",N/A,"Statistical Modeling-General, Mitigation",FAccT,2022,https://dl.acm.org/doi/10.1145/3531146.3533136,N/A
Learning Optimal Fair Policies,"[Razieh Nabi, Daniel Malinsky, and Ilya Shpitser]",N/A,"Statistical Modeling-General, Mitigation",ICML,2019,https://proceedings.mlr.press/v97/nabi19a.html,N/A
"Contrastive Mixture of Posteriors for Counterfactual Inference, Data Integration and Fairness","[Adam Foster, Arpi Vezer, Craig A. Glastonbury, Paidi Creed, Samer Abujudeh, and Aaron Sim]",N/A,"Statistical Modeling-General, Mitigation",ICML,2022,https://proceedings.mlr.press/v162/foster22a.html,N/A
Mitigating Unwanted Biases with Adversarial Learning,"[Brian Hu Zhang, Blake Lemoine, and Margaret Mitchell]",N/A,"Statistical Modeling-General, Mitigation (Traditional)",AIES,2018,https://dl.acm.org/doi/10.1145/3278721.3278779,N/A
Multiaccuracy: Black-Box Post-Processing for Fairness in Classification,"[Michael P. Kim, Amirata Ghorbani, and James Zou]",N/A,"Statistical Modeling-General, Mitigation (Traditional)",AIES,2019,https://dl.acm.org/doi/10.1145/3306618.3314287,N/A
FaiR-N: Fair and Robust Neural Networks for Structured Data,"[Shubham Sharma, Alan H. Gee, David Paydarfar, and Joydeep Ghosh]",N/A,"Statistical Modeling-General, Mitigation (Traditional)",AIES,2021,https://dl.acm.org/doi/10.1145/3461702.3462559,N/A
Mitigating Racial Biases in Toxic Language Detection with an Equity-Based Ensemble Framework,"[Matan Halevy, Camille Harris, Amy Bruckman, Diyi Yang, and Ayanna Howard]",N/A,"Statistical Modeling-General, Mitigation (Traditional)",EAAMO,2021,https://dl.acm.org/doi/10.1145/3465416.3483299,N/A
Fairness without Demographics through Adversarially Reweighted Learning,"[Preethi Lahoti, Alex Beutel, Jilin Chen, Kang Lee, Flavien Prost, Nithum Thain, Xuezhi Wang, and Ed Chi]",N/A,"Statistical Modeling-General, Mitigation (Traditional)",NeurIPS,2020,https://proceedings.neurips.cc/paper/2020/hash/07fc15c9d169ee48573edd749d25945d-Abstract.html,N/A
Post-processing for Individual Fairness,"[Felix Petersen, Debarghya Mukherjee, Yuekai Sun, and Mikhail Yurochkin]",N/A,"Statistical Modeling-General, Mitigation (Traditional)",NeurIPS,2021,https://openreview.net/forum?id=qGeqg4_hA2,N/A
Conditional Learning of Fair Representations,"[Han Zhao, Amanda Coston, Tameem Adel, and Geoffrey J. Gordon]",N/A,"Statistical Modeling-General, Mitigation (Traditional)",ICLR,2020,https://openreview.net/forum?id=Hkekl0NFPr,N/A
FairCal: Fairness Calibration for Face Verification,"[Tiago Salvador, Stephanie Cairns, Vikram Voleti, Noah Marshall, and Adam M Oberman]",N/A,"Statistical Modeling-General, Mitigation (Traditional)",ICLR,2022,https://openreview.net/forum?id=nRj0NcmSuxb,N/A
Providing Item-side Individual Fairness for Deep Recommender Systems,"[Xiuling Wang, Wendy Hui Wang]",N/A,"Statistical Modeling-General, Mitigation (Traditional)",FAccT,2022,https://dl.acm.org/doi/10.1145/3531146.3533079,N/A
FADE: FAir Double Ensemble Learning for Observable and Counterfactual Outcomes,"[Alan Mishler, Edward H. Kennedy]",N/A,"Statistical Modeling-General, Mitigation (Traditional)",FAccT,2022,https://doi.org/10.1145/3531146.3533167,N/A
Fairness-aware Model-agnostic Positive and Unlabeled Learning,"[Ziwei Wu, Jingrui He]",N/A,"Statistical Modeling-General, Mitigation (Traditional)",FAccT,2022,https://dl.acm.org/doi/abs/10.1145/3531146.3533225,N/A
A Reductions Approach to Fair Classification,"[Alekh Agarwal, Alina Beygelzimer, Miroslav Dudik, John Langford, and Hanna Wallach]",N/A,"Statistical Modeling-General, Mitigation (Traditional)",ICML,2018,https://proceedings.mlr.press/v80/agarwal18a.html,N/A
Blind Justice: Fairness with Encrypted Sensitive Attributes,"[Niki Kilbertus, Adria Gascon, Matt Kusner, Michael Veale, Krishna Gummadi, and Adrian Weller]",N/A,"Statistical Modeling-General, Mitigation (Traditional)",ICML,2018,https://proceedings.mlr.press/v80/kilbertus18a.html,N/A
Learning Adversarially Fair and Transferable Representations,"[David Madras, Elliot Creager, Toniann Pitassi, and Richard Zemel]",N/A,"Statistical Modeling-General, Mitigation (Traditional)",ICML,2018,https://proceedings.mlr.press/v80/madras18a.html,N/A
Fair Mixup: Fairness via Interpolation,"[Ching-Yao Chuang, Youssef Mroueh]",N/A,"Statistical Modeling-General, Mitigation (Traditional)",ICLR,2021,https://openreview.net/forum?id=DNl5s5BXeBn,N/A
Classification with Fairness Constraints: A Meta-Algorithm with Provable Guarantees,"[L Elisa Celis, Lingxiao Huang, Vijay Keswani, and Nisheeth K Vishnoi]",N/A,"Statistical Modeling-General, Mitigation (Traditional)",FAccT,2019,https://dl.acm.org/doi/10.1145/3287560.3287586,N/A
Counterfactual Fairness in Text Classification through Robustness,"[Sahaj Garg, Vincent Perot, Nicole Limtiaco, Ankur Taly, Ed H. Chi, and Alex Beutel]",N/A,"Statistical Modeling-General, Mitigation (Traditional)",AIES,2019,https://dl.acm.org/doi/10.1145/3306618.3317950,N/A
Nonconvex Optimization for Regression with Fairness Constraints,"[Junpei Komiyama, Akiko Takeda, Junya Honda, and Hajime Shimao]",N/A,"Statistical Modeling-General, Mitigation (Traditional)",ICML,2018,https://proceedings.mlr.press/v80/komiyama18a.html,N/A
Taking Advantage of Multitask Learning for Fair Classification,"[Luca Oneto, Michele Doninini, Amon Elders, and Massimiliano Pontil]",N/A,"Statistical Modeling-General, Mitigation (Traditional)",AIES,2019,https://dl.acm.org/doi/10.1145/3306618.3314255,N/A
Accounting for Model Uncertainty in Algorithmic Discrimination,"[Junaid Ali, Preethi Lahoti, and Krishna P. Gummadi]",N/A,"Statistical Modeling-General, Mitigation (Traditional)",AIES,2021,https://dl.acm.org/doi/10.1145/3461702.3462630,N/A
Marrying Fairness and Explainability in Supervised Learning,"[Przemyslaw A. Grabowicz, Nicholas Perello, and Aarshee Mishra]",N/A,"Statistical Modeling-General, Mitigation (Traditional)",FAccT,2022,https://dl.acm.org/doi/10.1145/3531146.3533236,N/A
"It’s Not Fairness, and It’s Not Fair: The Failure of Distributional Equality and the Promise of Relational Equality in Complete-Information Hiring Games","[Benjamin Fish, Luke Stark]",N/A,"Statistical Modeling-General, Mitigation (Traditional)",EAAMO,2022,https://dl.acm.org/doi/10.1145/3551624.3555296,N/A
A Just Approach Balancing Rawlsian Leximax Fairness and Utilitarianism,"[Violet (Xinying) Chen, J. N. Hooker]",N/A,"Statistical Modeling-General, Mitigation (Traditional)",AIES,2020,https://dl.acm.org/doi/10.1145/3375627.3375844,N/A
An Algorithmic Framework for Positive Action,"[Oliver Thomas, Miri Zilka, Adrian Weller, and Novi Quadrianto]",N/A,"Statistical Modeling-General, Mitigation (Traditional)",EAAMO,2021,https://dl.acm.org/doi/10.1145/3465416.3483303,N/A
Recycling privileged learning and distribution matching for fairness,"[Novi Quadrianto, Viktoriia Sharmanska]",N/A,"Statistical Modeling-General, Mitigation (Traditional)",NeurIPS,2017,https://dl.acm.org/doi/abs/10.5555/3294771.3294836,N/A
Counterfactual Fairness,"[Matt J Kusner, Joshua Loftus, Chris Russell, and Ricardo Silva]",N/A,"Statistical Modeling-General, Mitigation (Traditional)",NeurIPS,2017,https://dl.acm.org/doi/10.5555/3294996.3295162,N/A
Empirical risk minimization under fairness constraints,"[Michele Donini, Luca Oneto, Shai Ben-David, John S Shawe-Taylor, and Massimiliano Pontil]",N/A,"Statistical Modeling-General, Mitigation (Traditional)",NeurIPS,2018,https://dl.acm.org/doi/10.5555/3327144.3327203,N/A
Fairness Behind a Veil of Ignorance: A Welfare Analysis for Automated Decision Making,"[Hoda Heidari, Claudio Ferrari, Krishna Gummadi, and Andreas Krause]",N/A,"Statistical Modeling-General, Mitigation (Traditional)",NeurIPS,2018,https://papers.nips.cc/paper_files/paper/2018/hash/be3159ad04564bfb90db9e32851ebf9c-Abstract.html,N/A
Individually Fair Rankings,"[Amanda Bower, Hamid Eftekhari, Mikhail Yurochkin, and Yuekai Sun]",N/A,"Statistical Modeling-General, Mitigation (Traditional)",ICLR,2021,https://openreview.net/forum?id=71zCSP%5FHuBN,N/A
Individually Fair Gradient Boosting,"[Alexander Vargo, Fan Zhang, Mikhail Yurochkin, and Yuekai Sun]",N/A,"Statistical Modeling-General, Mitigation (Traditional)",ICLR,2021,https://openreview.net/forum?id=JBAa9we1AL,N/A
SenSeI: Sensitive Set Invariance for Enforcing Individual Fairness,"[Mikhail Yurochkin, Yuekai Sun]",N/A,"Statistical Modeling-General, Mitigation (Traditional)",ICLR,2021,https://openreview.net/forum?id=DktZb97%5FFx,N/A
Controlling Directions Orthogonal to a Classifier,"[Yilun Xu, Hao He, Tianxiao Shen, and Tommi S. Jaakkola]",N/A,"Statistical Modeling-General, Mitigation (Traditional)",ICLR,2022,https://openreview.net/forum?id=DIjCrlsu6Z,N/A
Offline Contextual Bandits with High Probability Fairness Guarantees,"[Blossom Metevier, Stephen Giguere, Sarah Brockman, Ari Kobren, Yuriy Brun, Emma Brunskill, and Philip S. Thomas]",N/A,"Statistical Modeling-General, Mitigation (Traditional)",NeurIPS,2019,https://papers.nips.cc/paper_files/paper/2019/hash/d69768b3da745b77e82cdbddcc8bac98-Abstract.html,N/A
Approximate Heavily-Constrained Learning with Lagrange Multiplier Models,"[Harikrishna Narasimhan, Andrew Cotter, Yichen Zhou, Serena Wang, and Wenshuo Guo]",N/A,"Statistical Modeling-General, Mitigation (Traditional)",NeurIPS,2020,https://proceedings.neurips.cc/paper/2020/hash/62db9e3397c76207a687c360e0243317-Abstract.html,N/A
Group-Fair Online Allocation in Continuous Time,"[Semih Cayci, Swati Gupta, and Atilla Eryilmaz]",N/A,"Statistical Modeling-General, Mitigation (Traditional)",NeurIPS,2020,https://proceedings.neurips.cc/paper/2020/hash/9ec0cfdc84044494e10582436e013e64-Abstract.html,N/A
Fairness Violations and Mitigation under Covariate Shift,"[Harvineet Singh, Rina Singh, Vishwali Mhasawade, and Rumi Chunara]",N/A,"Statistical Modeling-General/Deployment and Integration-General, Mitigation (Traditional)",FAccT,2021,https://doi.org/10.1145/3351095.3372839,N/A
Fair Algorithms for Learning in Allocation Problems,"[Hadi Elzayn, Shahin Jabbari, Christopher Jung, Michael Kearns, Seth Neel, Aaron Roth, and Zachary Schutzman]",N/A,"Statistical Modeling-General, Mitigation (Traditional)",FAccT,2019,https://dl.acm.org/doi/10.1145/3287560.3287571,N/A
Preventing Fairness Gerrymandering: Auditing and Learning for Subgroup Fairness,"[Michael Kearns, Seth Neel, Aaron Roth, and Zhiwei Steven Wu]",N/A,"Statistical Modeling-General, Mitigation (Traditional)",ICML,2018,https://proceedings.mlr.press/v80/kearns18a.html,N/A
Probably Approximately Metric-Fair Learning,"[Gal Yona, Guy Rothblum]",N/A,"Statistical Modeling-General, Mitigation (Traditional)",ICML,2018,https://proceedings.mlr.press/v80/yona18a.html,N/A
Training Well-Generalizing Classifiers for Fairness Metrics and Other Data-Dependent Constraints,"[Andrew Cotter, Maya Gupta, Heinrich Jiang, Nathan Srebro, Karthik Sridharan, Serena Wang, Blake Woodworth, and Seungil You]",N/A,"Statistical Modeling-General, Mitigation (Traditional)",ICML,2019,https://proceedings.mlr.press/v97/cotter19b.html,N/A
Non-Discriminatory Machine Learning through Convex Fairness Criteria,"[Naman Goel, Mohammad Yaghini, and Boi Faltings]",N/A,"Statistical Modeling-General, Mitigation (Traditional)",AIES,2018,https://dl.acm.org/doi/10.1145/3278721.3278722,N/A
Fair Forests: Regularized Tree Induction to Minimize Model Bias,"[Edward Raff, Jared Sylvester, and Steven Mills]",N/A,"Statistical Modeling-General, Mitigation (Traditional)",AIES,2018,https://dl.acm.org/doi/10.1145/3278721.3278742,N/A
RAWLSNET: Altering Bayesian Networks to Encode Rawlsian Fair Equality of Opportunity,"[David Liu, Zohair Shafi, William Fleisher, Tina Eliassi-Rad, and Scott Alfeld]",N/A,"Statistical Modeling-General, Mitigation (Traditional)",AIES,2021,https://dl.acm.org/doi/10.1145/3461702.3462618,N/A
Rawlsian Fair Adaptation of Deep Learning Classifiers,"[Kulin Shah, Pooja Gupta, Amit Deshpande, and Chiranjib Bhattacharyya]",N/A,"Statistical Modeling-General, Mitigation (Traditional)",AIES,2021,https://dl.acm.org/doi/10.1145/3461702.3462592,N/A
On Human-Aligned Risk Minimization,"[Liu Leqi, Adarsh Prasad, and Pradeep K Ravikumar]",N/A,"Statistical Modeling-General, Mitigation (Traditional)",ICML,2019,https://papers.nips.cc/paper_files/paper/2019/hash/cd6b73b67c77edeaff94e24b961119dd-Abstract.html,N/A
Fair regression via plug-in estimator and recalibration with statistical guarantees,"[Evgenii Chzhen, Christophe Denis, Mohamed Hebiri, Luca Oneto, and Massimiliano Pontil]",N/A,"Statistical Modeling-General, Mitigation (Traditional)",NeurIPS,2020,https://proceedings.neurips.cc/paper/2020/hash/ddd808772c035aed516d42ad3559be5f-Abstract.html,N/A
Metric-free individual fairness in online learning,"[Yahav Bechavod, Christopher Jung, and Zhiwei Steven Wu]",N/A,"Statistical Modeling-General, Mitigation (Traditional)",NeurIPS,2020,https://dl.acm.org/doi/10.5555/3495724.3496665,N/A
Two-sided fairness in rankings via Lorenz dominance,"[Virginie Do, Sam Corbett-Davies, Jamal Atif, and Nicolas Usunier]",N/A,"Statistical Modeling-General, Mitigation (Traditional)",NeurIPS,2021,https://proceedings.neurips.cc/paper/2021/hash/48259990138bc03361556fb3f94c5d45-Abstract.html,N/A
Fairness in Risk Assessment Instruments: Post-Processing to Achieve Counterfactual Equalized Odds,"[Alan Mishler, Edward H. Kennedy, and Alexandra Chouldechova]",N/A,"Statistical Modeling-General, Mitigation (Traditional)",FAccT,2021,https://dl.acm.org/doi/10.1145/3442188.3445902,N/A
Fast online ranking with fairness of exposure,"[Nicolas Usunier, Virginie Do, and Elvis Dohmatob]",N/A,"Statistical Modeling-General, Mitigation (Traditional)",FAccT,2022,https://dl.acm.org/doi/10.1145/3531146.3534633,N/A
Multicalibration: Calibration for the (Computationally-Identifiable) Masses,"[Ursula Hebert-Johnson, Michael Kim, Omer Reingold, and Guy Rothblum]",N/A,"Statistical Modeling-General, Mitigation (Traditional)",ICML,2018,https://proceedings.mlr.press/v80/hebert-johnson18a.html,N/A
Fair Regression: Quantitative Definitions and Reduction-Based Algorithms,"[Alekh Agarwal, Miroslav Dudik, and Zhiwei Steven W]",N/A,"Statistical Modeling-General, Mitigation (Traditional)",ICML,2019,https://proceedings.mlr.press/v97/agarwal19d.html,N/A
Unlocking fairness: a trade-off revisited,"[Michael Wick, Swetasudha Panda, and Jean-Baptiste Tristan]",N/A,"Statistical Modeling-General, Mitigation",NeurIPS,2019,https://dl.acm.org/doi/10.5555/3454287.3455075,N/A
Fairness in Relational Domains,"[Golnoosh Farnadi, Behrouz Babaki, and Lise Getoor]",N/A,"Statistical Modeling-General, Mitigation (Traditional)",AIES,2018,https://dl.acm.org/doi/10.1145/3278721.3278733,N/A
Near Neighbor: Who is the Fairest of Them All?,"[Sariel Har-Peled, Sepideh Mahabadi]",N/A,"Statistical Modeling-General, Mitigation (Traditional)",NeurIPS,2019,https://papers.nips.cc/paper_files/paper/2019/hash/742141ceda6b8f6786609d31c8ef129f-Abstract.html,N/A
Rényi Fair Inference,"[Sina Baharlouei, Maher Nouiehed, Ahmad Beirami, and Meisam Razaviyayn]",N/A,"Statistical Modeling-General, Mitigation (Traditional)",ICLR,2020,https://openreview.net/forum?id=HkgsUJrtDB,N/A
Training individually fair ML models with sensitive subspace robustness,"[Mikhail Yurochkin, Amanda Bower, and Yuekai Sun]",N/A,"Statistical Modeling-General, Mitigation (Traditional)",ICLR,2020,https://openreview.net/forum?id=B1gdkxHFDH,N/A
Certifying Robustness to Programmable Data Bias in Decision Trees,"[Anna Meyer, Aws Albarghouthi, and Loris D' Antoni]",N/A,"Statistical Modeling-General, Mitigation",NeurIPS,2021,https://proceedings.neurips.cc/paper/2021/hash/dcf531edc9b229acfe0f4b87e1e278dd-Abstract.html,N/A
Fair Performance Metric Elicitation,"[Gaurush Hiranandani, Harikrishna Narasimhan, and Sanmi Koyejo]",N/A,"Testing and Validation-Evaluation Metrics, Problem Identification",NeurIPS,2020,https://proceedings.neurips.cc/paper/2020/hash/7ec2442aa04c157590b2fa1a7d093a33-Abstract.html,N/A
Is Fairness Only Metric Deep? Evaluating and Addressing Subgroup Gaps in Deep Metric Learning,"[Natalie Dullerud, Karsten Roth, Kimia Hamidieh, Nicolas Papernot, and Marzyeh Ghassemi]",N/A,"Testing and Validation-Evaluation Metrics, Problem Identification",ICLR,2022,https://openreview.net/forum?id=js62%5FxuLDDv,N/A
"Model Multiplicity: Opportunities, Concerns, and Solutions","[Emily Black, Manish Raghavan, and Solon Barocas]",N/A,"Testing and Validation-Evaluation Metrics, Problem Identification",FAccT,2022,https://dl.acm.org/doi/10.1145/3531146.3533149,N/A
Affirmative Algorithms: Relational Equality as Algorithmic Fairness,[Marilyn Zhang],N/A,"Testing and Validation-Evaluation Metrics, Problem Identification (Traditional)",FAccT,2022,https://dl.acm.org/doi/10.1145/3531146.3533115,N/A
Fair Sequential Selection Using Supervised Learning Models,"[Mohammad Mahdi Khalili, Xueru Zhang, and Mahed Abroshan]",N/A,"Testing and Validation-Evaluation Metrics, Problem Identification",NeurIPS,2021,https://proceedings.neurips.cc/paper/2021/hash/ed277964a8959e72a0d987e598dfbe72-Abstract.html,N/A
Fairness Criteria for Face Recognition Applications,[Filip Michalsky],N/A,"Testing and Validation-Evaluation Metrics, Measurement (Traditional)",AIES,2019,https://dl.acm.org/doi/10.1145/3306618.3314308,N/A
CERTIFAI: A Common Framework to Provide Explanations and Analyse the Fairness and Robustness of Black-box Models,"[Shubham Sharma, Jette Henderson, and Joydeep Ghosh]",N/A,"Testing and Validation-Evaluation Metrics, Measurement (Traditional)",AIES,2020,https://dl.acm.org/doi/10.1145/3375627.3375812,N/A
Minimax Group Fairness: Algorithms and Experiments,"[Emily Diana, Wesley Gill, Michael Kearns, Krishnaram Kenthapadi, and Aaron Roth]",N/A,"Testing and Validation-Evaluation Metrics, Measurement (Traditional)",AIES,2021,https://dl.acm.org/doi/10.1145/3461702.3462523,N/A
Fairness through computationally-bounded awareness,"[Michael Kim, Omer Reingold, and Guy Rothblum]",N/A,"Testing and Validation-Evaluation Metrics, Measurement (Traditional)",NeurIPS,2018,https://dl.acm.org/doi/10.5555/3327345.3327393,N/A
Equality of Opportunity in Classification: A Causal Approach,"[Junzhe Zhang, Elias Bareinboim]",N/A,"Testing and Validation-Evaluation Metrics, Measurement (Traditional)",NeurIPS,2018,https://papers.nips.cc/paper_files/paper/2018/hash/ff1418e8cc993fe8abcfe3ce2003e5c5-Abstract.html,N/A
The fairness of risk scores beyond classification: bipartite ranking and the xAUC metric,"[Nathan Kallus, Angela Zhou]",N/A,"Testing and Validation-Evaluation Metrics, Measurement (Traditional)",NeurIPS,2019,https://dl.acm.org/doi/10.5555/3454287.3454596,N/A
Model class reliance for random forests,"[Gavin Smith, Roberto Mansilla, and James Goulding]",N/A,"Testing and Validation-Evaluation Metrics, Measurement (Traditional)",NeurIPS,2020,https://dl.acm.org/doi/10.5555/3495724.3497594,N/A
Statistical inference for individual fairness ,"[Subha Maity, Songkai Xue, Mikhail Yurochkin, and Yuekai Sun]",N/A,"Testing and Validation-Evaluation Metrics, Measurement (Traditional)",ICLR,2021,https://openreview.net/forum?id=z9k8BWL-%5F2u,N/A
Generalized Demographic Parity for Group Fairness ,"[Zhimeng Jiang, Xiaotian Han, Chao Fan, Fan Yang, Ali Mostafavi, and Xia Hu]",N/A,"Testing and Validation-Evaluation Metrics, Measurement (Traditional)",ICLR,2022,https://openreview.net/forum?id=YigKlMJwjye,N/A
Measuring Fairness in an Unfair World,[Jonathan Herington],N/A,"Testing and Validation-Evaluation Metrics, Measurement (Traditional)",AIES,2020,https://dl.acm.org/doi/10.1145/3375627.3375854,N/A
An Axiomatic Theory of Provably-Fair Welfare-Centric Machine Learning,[Cyrus Cousins],N/A,"Testing and Validation-Evaluation Metrics, Measurement (Traditional)",NeurIPS,2021,https://proceedings.neurips.cc/paper/2021/hash/8b0bb3eff8c1e5bf7f206125959921d7-Abstract.html,N/A
"Counterfactual risk assessments, evaluation, and fairness","[Amanda Coston, Alan Mishler, Edward H. Kennedy, and Alexandra Chouldechova]",N/A,"Testing and Validation-Evaluation Metrics, Measurement (Traditional)",FAccT,2020,https://dl.acm.org/doi/10.1145/3351095.3372851,N/A
Counterfactual Fairness in Text Classification through Robustness,"[Sahaj Garg, Vincent Perot, Nicole Limtiaco, Ankur Taly, Ed H. Chi, and Alex Beutel]",N/A,"Testing and Validation-Evaluation Metrics, Measurement (Traditional)",AIES,2019,https://dl.acm.org/doi/10.1145/3306618.3317950,N/A
Contrastive Counterfactual Fairness in Algorithmic Decision-Making,"[Ece Çiğdem Mutlu, Niloofar Yousefi, and Ozlem Ozmen Garibay]",N/A,"Testing and Validation-Evaluation Metrics, Measurement (Traditional)",AIES,2022,https://dl.acm.org/doi/10.1145/3514094.3534143,N/A
Evaluating Fairness of Machine Learning Models Under Uncertain and Incomplete Information,"[Pranjal Awasthi, Alex Beutel, Matthäus Kleindessner, Jamie Morgenstern, and Xuezhi Wang]",N/A,"Testing and Validation-Evaluation Metrics, Measurement",FAccT,2021,https://dl.acm.org/doi/10.1145/3442188.3445884,N/A
A Statistical Test for Probabilistic Fairness,"[Bahar Taskesen, Jose Blanchet, Daniel Kuhn, and Viet Anh Nguyen]",N/A,"Testing and Validation-Evaluation Metrics, Measurement",FAccT,2021,https://dl.acm.org/doi/10.1145/3442188.3445927,N/A
An Outcome Test of Discrimination for Ranked Lists,"[Jonathan Roth, Guillaume Saint-Jacques, and YinYin Yu]",N/A,"Testing and Validation-Evaluation Metrics, Measurement",FAccT,2022,https://dl.acm.org/doi/10.1145/3531146.3533102,N/A
Active fairness auditing,"[Tom Yan, Chicheng Zhang]",N/A,"Testing and Validation-Evaluation Metrics, Measurement",ICML,2022,https://proceedings.mlr.press/v162/yan22c.html,N/A
Invariant Representations without Adversarial Training,"[Daniel Moyer, Shuyang Gao, Rob Brekelmans, Aram Galstyan, and Greg Ver Steeg]",N/A,"Testing and Validation-Evaluation Metrics, Mitigation",NeurIPS,2018,https://papers.nips.cc/paper_files/paper/2018/hash/415185ea244ea2b2bedeb0449b926802-Abstract.html,N/A
Causal Multi-level Fairness,"[Vishwali Mhasawade, Rumi Chunara]",N/A,"Testing and Validation-Evaluation Metrics, Mitigation (Case Study)",AIES,2021,https://dl.acm.org/doi/10.1145/3461702.3462587,N/A
From parity to preference-based notions of fairness in classification,"[Muhammad Bilal Zafar, Isabel Valera, Manuel Rodriguez, Krishna Gummadi, and Adrian Weller]",N/A,"Testing and Validation-Evaluation Metrics, Mitigation (Case Study)",NeurIPS,2017,https://dl.acm.org/doi/abs/10.5555/3294771.3294793,N/A
Avoiding discrimination through causal reasoning,"[Niki Kilbertus, Mateo Rojas Carulla, Giambattista Parascandolo, Moritz Hardt, Dominik Janzing, and Bernhard Schölkopf]",N/A,"Testing and Validation-Evaluation Metrics, Mitigation (Case Study)",NeurIPS,2017,https://dl.acm.org/doi/10.5555/3294771.3294834,N/A
From Soft Classifiers to Hard Decisions: How fair can we be?,"[Ran Canetti, Aloni Cohen, Nishanth Dikkala, Govind Ramnarayan, Sarah Scheffler, and Adam Smith]",N/A,"Testing and Validation-Evaluation Metrics, Mitigation (Case Study)",FAccT,2019,https://dl.acm.org/doi/10.1145/3287560.3287561,N/A
Leveraging Labeled and Unlabeled Data for Consistent Fair Binary Classification,"[Evgenii Chzhen, Christophe Denis, Mohamed Hebiri, Luca Oneto, and Massimiliano Pontil]",N/A,"Testing and Validation-Evaluation Metrics, Mitigation (Case Study)",NeurIPS,2019,https://proceedings.neurips.cc/paper_files/paper/2019/hash/ba51e6158bcaf80fd0d834950251e693-Abstract.html,N/A
Envy-Free Classification,"[Maria-Florina F Balcan, Travis Dick, Ritesh Noothigattu, and Ariel D Procaccia]",N/A,"Testing and Validation-Evaluation Metrics, Mitigation (Case Study)",NeurIPS,2019,https://papers.nips.cc/paper_files/paper/2019/hash/e94550c93cd70fe748e6982b3439ad3b-Abstract.html,N/A
Fairness with Overlapping Groups; a Probabilistic Perspective,"[Forest Yang, Mouhamadou Cisse, and Sanmi Koyejo]",N/A,"Testing and Validation-Evaluation Metrics, Mitigation (Case Study)",NeurIPS,2020,https://proceedings.neurips.cc/paper/2020/hash/29c0605a3bab4229e46723f89cf59d83-Abstract.html,N/A
"Classification Under Misspecification: Halfspaces, Generalized Linear Models, and Evolvability","[Sitan Chen, Frederic Koehler, Ankur Moitra, and Morris Yau]",N/A,"Testing and Validation-Evaluation Metrics, Mitigation (Case Study)",NeurIPS,2020,https://proceedings.neurips.cc/paper/2020/hash/5f8b73c0d4b1bf60dd7173b660b87c29-Abstract.html,N/A
Incorporating Interpretable Output Constraints in Bayesian Neural Networks,"[Wanqian Yang, Lars Lorch, Moritz Graule, Himabindu Lakkaraju, and Finale Doshi-Velez]",N/A,"Testing and Validation-Evaluation Metrics, Mitigation (Case Study)",NeurIPS,2020,https://proceedings.neurips.cc/paper_files/paper/2020/hash/95c7dfc5538e1ce71301cf92a9a96bd0-Abstract.html,N/A
Efficient Mirror Descent Ascent Methods for Nonsmooth Minimax Problems,"[Feihu Huang, Xidong Wu, and Heng Huang]",N/A,"Testing and Validation-Evaluation Metrics, Mitigation (Case Study)",NeurIPS,2021,https://openreview.net/forum?id=3EuMT2Lqn4q,N/A
Scalable and Stable Surrogates for Flexible Classifiers with Fairness Constraints,"[Henry C Bendekgey, Erik Sudderth]",N/A,"Testing and Validation-Evaluation Metrics, Mitigation (Case Study)",NeurIPS,2021,https://proceedings.neurips.cc/paper/2021/hash/fc2e6a440b94f64831840137698021e1-Abstract.html,N/A
Design of Experiments for Model Discrimination Hybridising Analytical and Data-Driven Approaches,"[Simon Olofsson, Marc Deisenroth, and Ruth Misener]",N/A,"Testing and Validation-General, Problem Identification (Case Study)",ICML,2018,https://proceedings.mlr.press/v80/olofsson18a.html,N/A
Testing Group Fairness via Optimal Transport Projections,"[Nian Si, Karthyek Murthy, Jose Blanchet, and Viet Anh Nguyen]",N/A,"Testing and Validation-General, Problem Identification",ICML,2021,https://proceedings.mlr.press/v139/si21a.html,N/A
De-biasing “bias” measurement,"[Kristian Lum, Yunfeng Zhang, and Amanda Bower]",N/A,"Testing and Validation-General, Measurement",FAccT,2022,https://dl.acm.org/doi/abs/10.1145/3531146.3533105,N/A
"AI Fairness 360: An Extensible Toolkit for Detecting, Understanding, and Mitigating Unwanted Algorithmic Bias","[Rachel K. E. Bellamy, Kuntal Dey, Michael Hind, Samuel C. Hoffman, Stephanie Houde, Kalapriya Kannan, Pranay Lohia, Jacquelyn Martino, Sameep Mehta, Aleksandra Mojsilovic, Seema Nagar, Karthikeyan Natesan Ramamurthy, John Richards, Diptikalyan Saha, Prasanna Sattigeri, Moninder Singh, Kush R. Varshney, Yunfeng Zhang]",N/A,"Testing and Validation-General, Measurement",N/A,2018,https://arxiv.org/abs/1810.01943,N/A
Amazon SageMaker Clarify: Machine Learning Bias Detection and Explainability in the Cloud,"[Michaela Hardt, Xiaoguang Chen, Xiaoyi Cheng, Michele Donini, Jason Gelman, Satish Gollaprolu, John He, Pedro Larroy, Xinyu Liu, Nick McCarthy, Ashish Rathi, Scott Rees, Ankit Siva, ErhYuan Tsai, Keerthan Vasist, Pinar Yilmaz, Muhammad Bilal Zafar, Sanjiv Das, Kevin Haas, Tyler Hill, Krishnaram Kenthapadi]",N/A,"Testing and Validation-General, Measurement",N/A,2021,https://arxiv.org/abs/2109.03285,N/A
Data and its (dis)contents: A survey of dataset development and use in machine learning research,"[Amandalynne Paullada, Inioluwa Deborah Raji, Emily M Bender, Emily Denton, and Alex Hanna]",N/A,"Testing and Validation-General, Measurement",Patterns,2021,https://www.sciencedirect.com/science/article/pii/S2666389921001847,N/A
Aequitas: A Bias and Fairness Audit Toolkit,"[Pedro Saleiro, Benedict Kuester, Loren Hinkson, Jesse London, Abby Stevens, Ari Anisfeld, Kit T Rodolfa, and Rayid Ghani.]",N/A,"Testing and Validation-General, Measurement",N/A,2018,https://arxiv.org/abs/1811.05577,N/A
Preference-informed fairness,"[Michael P. Kim, Aleksandra Korolova, Guy N. Rothblum, and Gal Yon]",N/A,"Testing and Validation-General, Measurement (Traditional)",FAccT,2020,https://dl.acm.org/doi/10.1145/3351095.3373155,N/A
Fair Transfer Learning with Missing Protected Attributes,"[Amanda Coston, Karthikeyan Natesan Ramamurthy, Dennis Wei, Kush R. Varshney, Skyler Speakman, Zairah Mustahsan, and Supriyo Chakraborty]",N/A,"Testing and Validation-General/Deployment and Integration-General, Mitigation",AIES,2019,https://dl.acm.org/doi/10.1145/3306618.3314236,N/A
Measuring and Mitigating Unintended Bias in Text Classification,"[Lucas Dixon, John Li, Jeffrey Sorensen, Nithum Thain, and Lucy Vasserman]",N/A,"Testing and Validation-General, Mitigation",AIES,2018,https://dl.acm.org/doi/10.1145/3278721.3278729,N/A
FairFil: Contrastive Neural Debiasing Method for Pretrained Text Encoders ,"[Pengyu Cheng, Weituo Hao, Siyang Yuan, Shijing Si, and Lawrence Carin]",N/A,"Testing and Validation-General, Mitigation",ICLR,2021,https://openreview.net/forum?id=N6JECD-PI5w,N/A
Repairing without Retraining: Avoiding Disparate Impact with Counterfactual Distributions,"[Hao Wang, Berk Ustun, and Flavio Calmon]",N/A,"Testing and Validation-General, Mitigation",ICML,2022,https://proceedings.mlr.press/v97/wang19l.html,N/A
How machine-learning recommendations influence clinician treatment selections: the example of antidepressant selection,"[Maia Jacobs, Melanie F Pradier, Thomas H McCoy Jr, Roy H Perlis, Finale Doshi-Velez, and Krzysztof Z Gajos]",N/A,"Deployment and Integration-Human/Computer Handoff, Problem Identification (Case Study)",Nature Translational Psychiatry,2021,https://www.nature.com/articles/s41398-021-01224-x,N/A
Towards Unbiased and Accurate Deferral to Multiple Experts,"[Vijay Keswani, Matthew Lease, and Krishnaram Kenthapadi]",N/A,"Deployment and Integration-Human/Computer Handoff, Problem Identification",AIES,2021,https://dl.acm.org/doi/10.1145/3461702.3462516,N/A
Disparate Interactions: An Algorithm-in-the-Loop Analysis of Fairness in Risk Assessments,"[Ben Green, Yiling Chen]",N/A,"Deployment and Integration-Human/Computer Handoff/General, Problem Identification",FAccT,2019,https://dl.acm.org/doi/10.1145/3287560.3287563,N/A
On the Fairness of Machine-Assisted Human Decisions,"[Bryce McLaughlin, Jann Spiess, and Talia Gillis]",N/A,"Deployment and Integration-Human/Computer Handoff, Problem Identification",FAccT,2022,https://dl.acm.org/doi/10.1145/3531146.3533152,N/A
Human-algorithm collaboration: Achieving complementarity and avoiding unfairness,"[Kate Donahue, Alexandra Chouldechova, and Krishnaram Kenthapadi]",N/A,"Deployment and Integration-Human/Computer Handoff, Problem Identification",FAccT,2022,https://dl.acm.org/doi/10.1145/3531146.3533221,N/A
"Race, Gender and Beauty: The Effect of Information Provision on Online Hiring Biases","[Weiwen Leung, Zheng Zhang, Daviti Jibuti, Jinhao Zhao, Maximilian Klein, Casey Pierce, Lionel Robert, and Haiyi Zhu]",N/A,"Deployment and Integration-Human/Computer Handoff, Problem Identification",CHI,2020,https://dl.acm.org/doi/10.1145/3313831.3376874,N/A
Not Just a Preference: Reducing Biased Decision-making on Dating Websites,"[Zilin Ma, Krzysztof Z. Gajos]",N/A,"Deployment and Integration-Human/Computer Handoff, Problem Identification",CHI,2022,https://dl.acm.org/doi/10.1145/3491102.3517587,N/A
"Took a Pic and Got Declined, Vexed and Perplexed: Facial Recognition in Algorithmic Management",[Elizabeth Anne Watkins],N/A,"Deployment and Integration-Human/Computer Handoff, Problem Identification",CSCW,2020,https://dl.acm.org/doi/10.1145/3406865.3418383,N/A
How Child Welfare Workers Reduce Racial Disparities in Algorithmic Decisions,"[Hao-Fei Cheng, Logan Stapleton, Anna Kawakami, Venkatesh Sivaraman, Yanghuidi Cheng, Diana Qing, Adam Perer, Kenneth Holstein, Zhiwei Steven Wu, and Haiyi Zhu]",N/A,"Deployment and Integration-Human/Computer Handoff, Measurement",CHI,2022,https://dl.acm.org/doi/10.1145/3491102.3501831,N/A
Data-Centric Explanations: Explaining Training Data of Machine Learning Systems to Promote Transparency,"[Ariful Islam Anik, Andrea Bunt]",N/A,"Deployment and Integration-Human/Computer Handoff, Mitigation",CHI,2021,https://dl.acm.org/doi/10.1145/3411764.3445736,N/A
Fairness-Aware Programming,"[Aws Albarghouthi, Samuel Vinitsky]",N/A,"Deployment and Integration-Maintenance Oversight, Measurement",FAccT,2019,https://dl.acm.org/doi/10.1145/3287560.3287588,N/A
"""The human body is a black box"": supporting clinical decision-making with deep learning","[Mark Sendak, Madeleine Clare Elish, Michael Gao, Joseph Futoma, William Ratliff, Marshall Nichols, Armando Bedoya, Suresh Balu, and Cara O’Brien]",N/A,"Deployment and Integration-General, Problem Identification (Case Study)",FAccT,2020,https://dl.acm.org/doi/10.1145/3351095.3372827,N/A
The impact of overbooking on a pre-trial risk assessment tool,"[Kristian Lum, Chesa Boudin, and Megan Price]",N/A,"Deployment and Integration-General, Problem Identification (Case Study)",FAccT,2020,https://dl.acm.org/doi/10.1145/3351095.3372846,N/A
Long-term Dynamics of Fairness Intervention in Connection Recommender Systems,"[Nil-Jana Akpinar, Cyrus DiCiccio, Preetam Nandy, and Kinjal Basu]",N/A,"Deployment and Integration-General, Problem Identification",AIES,2022,https://dl.acm.org/doi/10.1145/3514094.3534173,N/A
Designing Fair AI in Human Resource Management: Understanding Tensions Surrounding Algorithmic Evaluation and Envisioning Stakeholder-Centered Solutions,"[Hyanghee Park, Daehwan Ahn, Kartik Hosanagar, and Joonhwan Lee]",N/A,"Deployment and Integration-General, Problem Identification",CHI,2022,https://dl.acm.org/doi/10.1145/3491102.3517672,N/A
Co-Designing Checklists to Understand Organizational Challenges and Opportunities around Fairness in AI,"[Michael A Madaio, Luke Stark, Jennifer Wortman Vaughan, and Hanna Wallach]",N/A,"Deployment and Integration-General, Problem Identification",CHI,2020,https://dl.acm.org/doi/10.1145/3313831.3376445,N/A
The Principles and Limits of Algorithm-in-the-Loop Decision Making,"[Ben Green, Yiling Chen]",N/A,"Deployment and Integration-General, Problem Identification",CSCW,2019,https://dl.acm.org/doi/10.1145/3359152,N/A
How Child Welfare Workers Reduce Racial Disparities in Algorithmic Decisions,"[Hao-Fei Cheng, Logan Stapleton, Anna Kawakami, Venkatesh Sivaraman, Yanghuidi Cheng, Diana Qing, Adam Perer, Kenneth Holstein, Zhiwei Steven Wu, and Haiyi Zhu]",N/A,"Deployment and Integration-General, Problem Identification",CHI,2022,https://dl.acm.org/doi/10.1145/3491102.3501831,N/A
A Case for Humans-in-the-Loop: Decisions in the Presence of Erroneous Algorithmic Scores,"[Maria De-Arteaga, Riccardo Fogliato, and Alexandra Chouldechova]",N/A,"Deployment and Integration-General, Problem Identification",CHI,2020,https://dl.acm.org/doi/10.1145/3313831.3376638,N/A
FairCanary: Rapid Continuous Explainable Fairness,"[Avijit Ghosh, Aalok Shanbhag, and Christo Wilson]",N/A,"Deployment and Integration-General, Measurement",AIES,2022,https://dl.acm.org/doi/10.1145/3514094.3534157,N/A
Fairness Guarantees under Demographic Shift,"[Stephen Giguere, Blossom Metevier, Yuriy Brun, Philip S. Thomas, Scott Niekum, and Bruno Castro da Silva]",N/A,"Deployment and Integration-General, Measurement",ICLR,2022,https://openreview.net/forum?id=wbPObLm6ueA,N/A
Fairness warnings and fair-MAML: learning fairly with minimal data,"[Dylan Slack, Sorelle A. Friedler, and Emile Givental]",N/A,"Deployment and Integration-General, Measurement",FAccT,2020,https://dl.acm.org/doi/10.1145/3351095.3372839,N/A
Models for understanding and quantifying feedback in societal systems,"[Lydia Reader, Pegah Nokhiz, Cathleen Power, Neal Patwari, Suresh Venkatasubramanian, and Sorelle Friedler]",N/A,"Deployment and Integration-General, Measurement",FAccT,2022,https://dl.acm.org/doi/10.1145/3531146.3533230,N/A
Towards Non-Parametric Drift Detection via Dynamic Adapting Window Independence Drift Detection (DAWIDD),"[Fabian Hinder, André Artelt, and Barbara Hammer]",N/A,"Deployment and Integration-General, Measurement",ICML,2020,https://proceedings.mlr.press/v119/hinder20a.html,N/A
Ensuring Fairness under Prior Probability Shifts,"[Arpita Biswas, Suvam Mukherjee]",N/A,"Deployment and Integration-General, Measurement",AIES,2021,https://dl.acm.org/doi/abs/10.1145/3461702.3462596,N/A
Bidding strategies with gender nondiscrimination constraints for online ad auctions,"[Milad Nasr, Michael Carl Tschantz]",N/A,"Deployment and Integration-General, Mitigation",FAccT,2020,https://dl.acm.org/doi/10.1145/3351095.3375783,N/A
ABCinML: Anticipatory Bias Correction in Machine Learning Applications,"[Abdulaziz A. Almuzaini, Chidansh A. Bhatt, David M. Pennock, and Vivek K. Singh]",N/A,"Deployment and Integration-General, Mitigation",FAccT,2022,https://dl.acm.org/doi/10.1145/3531146.3533211,N/A
Active Fairness in Algorithmic Decision Making,"[Alejandro Noriega-Campero, Michiel A. Bakker, Bernardo Garcia-Bulle, and Alex ’Sandy’ Pentland.]",N/A,"Deployment and Integration-General, Mitigation",AIES,2019,https://dl.acm.org/doi/10.1145/3306618.3314277,N/A
A Framework for Understanding Sources of Harm throughout the Machine Learning Life Cycle,"[Harini Suresh, John Guttag]",N/A,"Deployment and Integration-General, Mitigation",EAAMO,2021,https://dl.acm.org/doi/fullHtml/10.1145/3465416.3483305,N/A
Predict responsibly: improving fairness and accuracy by learning to defer,"[David Madras, Toni Pitassi, and Richard Zemel]",N/A,"Deployment and Integration-General, Mitigation",NeurIPS,2018,https://dl.acm.org/doi/10.5555/3327345.3327513,N/A