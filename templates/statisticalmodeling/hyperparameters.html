<!DOCTYPE html>
<html>
<head>
	<title>STATISTICAL MODELING : HYPERPARAMETERS</title>
	<style>
		.post-container {
    margin: 20px 20px 0 0;  
    border: 5px solid #333;
    overflow: auto
}
.post-thumb {
    float: left
}
.post-thumb img {
    display: block;
    width: 200px;
    height: auto;
}
.post-content {
    margin-left: 210px
}
.post-title {
    font-weight: bold;
    font-size: 200%;
    padding: 9px;
    background: #ccc
}

.tabs {
  margin-top: 20px;
}

.tabs ul {
  list-style: none;
  margin: 0;
  padding: 0;
  display: flex;
  justify-content: space-between;
}

.tabs li {
  flex: 1;
  text-align: center;
}

.tabs li a {
  display: block;
  padding: 10px;
  background-color: #f2f2f2;
  color: #333;
  text-decoration: none;
  border: 1px solid #ccc;
  border-bottom: none;
}

.tabs li.active a {
  background-color: #fff;
  border-color: #ccc;
}

.tab-content {
  display: none;
}

.tab-content.active {
  display: block;
}

.tab-content table {
  width: 100%;
  border-collapse: collapse;
  margin-top: 1px;
}

.tab-content table th,
.tab-content table td {
  padding: 5px;
  text-align: center;
  border: 1px solid #ccc;
}
	</style>
</head>
<body>

	

  <div class="post-container">
    <h3 class="post-title">STATISTICAL MODELING : HYPERPARAMETERS</h3>
    <div class="post-thumb"><img src="/static/datapreprocess.jpeg" alt="STATISTICAL MODELING"/></div>
    <div class="post-content">
        <p>Determining what type of model will be used and how it will be trained. 
            Decisions here include: choosing the model type, learning rule and loss function, regularizers, and the values of hyper-parameters determining normalization and training procedures. 
            For example, choosing between linear, forest- based, or deep models; choosing the architecture of deep models; what constraints to add to the loss function, how to optimize that loss function (e.g. SGD or momentum), etc. 
            Each decision here can lead to downstream bias such as choose a learning rule which is biased toward a certain subset of individuals in the data or over- or under-emphasizes outliers or minority populations.</p>
    </div>
  </div>
    <div class="tabs">
      <ul>
        <li><a href="#casestudy">Case Study</a></li>
        <li><a href="#problemidentification">Problem Identification</a></li>
        <li><a href="#measurement">Measurement</a></li>
        <li><a href="#mitigation">Mitigation</a></li>
      </ul>
      <div class="tab-content" id="casestudy">
        <table>
          <thead>
            <tr>
              <th>Paper Title</th>
              <th>Authors</th>
              <th>Description</th>
              <th>Tags/Comments</th>
              <th>Conference Venue</th>
              <th>Year</th>
              <th>Paper link</th>
              <th>Additional resources</th>
            </tr>
          </thead>
          <tbody>
          </tbody>
        </table>
      </div>
      <div class="tab-content" id="problemidentification">
        <table>
          <thead>
            <tr>
              <th>Paper Title</th>
              <th>Authors</th>
              <th>Description</th>
              <th>Tags/Comments</th>
              <th>Conference Venue</th>
              <th>Year</th>
              <th>Paper link</th>
              <th>Additional resources</th>
            </tr>
          </thead>
          <tbody>
            <tr>
                <td>When Trusted Black Boxes Don't Agree: Incentivizing Iterative Improvement and Accountability in Critical Software Systems</td>
                <td>Jeanna Neefe Matthews, Graham Northup, Isabella Grasso, Stephen Lorenz, Marzieh Babaeianjelodar, Hunter Bashaw, Sumona Mondal, Abigail Matthews, Mariama Njie, Jessica Goldthwaite</td>
                <td>N/A</td>
                <td>Statistical Modeling, Hyperparameters, Problem Identification</td>
                <td>AIES</td>
                <td>2020</td>
                <td><a href="https://dl.acm.org/doi/10.1145/3375627.3375807">When Trusted Black Boxes Don't Agree</a></td>
                <td>None</td>
              </tr>
          </tbody>
        </table>
      </div>
      <div class="tab-content" id="measurement">
        <table>
          <thead>
            <tr>
              <th>Paper Title</th>
              <th>Authors</th>
              <th>Description</th>
              <th>Tags/Comments</th>
              <th>Conference Venue</th>
              <th>Year</th>
              <th>Paper link</th>
              <th>Additional resources</th>
            </tr>
          </thead>
          <tbody>
            
          </tbody>
        </table>
      </div>
      <div class="tab-content" id="mitigation">
        <table>
          <thead>
            <tr>
              <th>Paper Title</th>
              <th>Authors</th>
              <th>Description</th>
              <th>Tags/Comments</th>
              <th>Conference Venue</th>
              <th>Year</th>
              <th>Paper link</th>
              <th>Additional resources</th>
            </tr>
          </thead>
          <tbody>
            <tr>
                <td>Can We Obtain Fairness For Free?</td>
                <td>Rashidul Islam, Shimei Pan, James R. Foulds</td>
                <td>N/A</td>
                <td>Statistical Modeling, Hyperparameters, Mitigation</td>
                <td>AIES</td>
                <td>2021</td>
                <td><a href="https://dl.acm.org/doi/10.1145/3461702.3462614">Can We Obtain Fairness For Free?</a></td>
                <td>None</td>
              </tr>
              <tr>
                <td>Can Information Flows Suggest Targets for Interventions in Neural Circuits?</td>
                <td>Praveen Venkatesh, Sanghamitra Dutta, Neil Mehta, Pulkit Grover</td>
                <td>N/A</td>
                <td>Statistical Modeling, Hyperparameters, Mitigation</td>
                <td>NeurIPS</td>
                <td>2021</td>
                <td><a href="https://proceedings.neurips.cc//paper/2021/hash/18de4beb01f6a17b6e1dfb9813ba6045-Abstract.html">Can Information Flows Suggest Targets for Interventions in Neural Circuits?</a></td>
                <td>None</td>
              </tr>
              <tr>
                <td>GetFair: Generalized Fairness Tuning of Classification Models</td>
                <td>Sandipan Sikdar, Florian Lemmerich, Markus Strohmaier</td>
                <td>N/A</td>
                <td>Statistical Modeling, Hyperparameters, Mitigation</td>
                <td>FAccT</td>
                <td>2022</td>
                <td><a href="https://dl.acm.org/doi/10.1145/3531146.3533094">GetFair</a></td>
                <td>None</td>
              </tr>
            
          </tbody>
        </table>
    </div>
  </div>
    
    <!-- JavaScript for the tabs -->
    <script>
      const tabLinks = document.querySelectorAll('.tabs li a');
      const tabContents = document.querySelectorAll('.tab-content');

      tabLinks.forEach((link) => {
        link.addEventListener('click', (e) => {
          e.preventDefault();

          const targetId = e.target.getAttribute('href').substr(1);

          tabLinks.forEach((link) => {
            link.parentNode.classList.remove('active');
          });

          tabContents.forEach((content) => {
            content.classList.remove('active');
          });

          e.target.parentNode.classList.add('active');
          document.getElementById(targetId).classList.add('active');
        });
      });
    </script>
  </body>
</html>