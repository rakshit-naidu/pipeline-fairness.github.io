conference,year,title,link,authors,citations,abstract,Fairness?,,Pipeline,Pipeline2,,,,,,,,,,,,,bibtex
AIES,2018,Meritocratic Fairness for Infinite and Contextual Bandits,https://dl.acm.org/doi/10.1145/3278721.3278764,"['Matthew Joseph', 'Michael Kearns', 'Jamie Morgenstern', 'Seth Neel', 'Aaron Roth']",36,"We study fairness in linear bandit problems. Starting from the notion of meritocratic fairness introduced in~\citeJKMR16, we carry out a more refined analysis of a more general problem, achieving better performance guarantees with fewer modelling assumptions on the number and structure of available choices as well as the number selected. We also analyze the previously-unstudied question of fairness in infinite linear bandit problems, obtaining instance-dependent regret upper bounds as well as lower bounds demonstrating that this instance-dependence is necessary. The result is a framework for meritocratic fairness in an online linear setting that is substantially more powerful, general, and realistic than the current state of the art.",Maybe,,Bandit,,,,,,,,,,,,,,"@inproceedings{10.1145/3278721.3278764,
author = {Joseph, Matthew and Kearns, Michael and Morgenstern, Jamie and Neel, Seth and Roth, Aaron},
title = {Meritocratic Fairness for Infinite and Contextual Bandits},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278764},
doi = {10.1145/3278721.3278764},
abstract = {We study fairness in linear bandit problems. Starting from the notion of meritocratic fairness introduced in~citeJKMR16, we carry out a more refined analysis of a more general problem, achieving better performance guarantees with fewer modelling assumptions on the number and structure of available choices as well as the number selected. We also analyze the previously-unstudied question of fairness in infinite linear bandit problems, obtaining instance-dependent regret upper bounds as well as lower bounds demonstrating that this instance-dependence is necessary. The result is a framework for meritocratic fairness in an online linear setting that is substantially more powerful, general, and realistic than the current state of the art.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {158–163},
numpages = {6},
location = {New Orleans, LA, USA},
series = {AIES '18}
}
  
"
AIES,2020,Diversity and Inclusion Metrics in Subset Selection,https://dl.acm.org/doi/10.1145/3375627.3375832,"['Margaret Mitchell', 'Dylan Baker', 'Nyalleng Moorosi', 'Emily Denton', 'Ben Hutchinson', 'Alex Hanna', 'Timnit Gebru', 'Jamie Morgenstern']",64,"The ethical concept of fairness has recently been applied in machine learning (ML) settings to describe a wide range of constraints and objectives. When considering the relevance of ethical concepts to subset selection problems, the concepts of diversity and inclusion are additionally applicable in order to create outputs that account for social power and access differentials. We introduce metrics based on these concepts, which can be applied together, separately, and in tandem with additional fairness constraints. Results from human subject experiments lend support to the proposed criteria. Social choice methods can additionally be leveraged to aggregate and choose preferable sets, and we detail how these may be applied.",Maybe,,,,Meethods,Subset selection-- not a classification task,,,,,,,,,,,"@inproceedings{10.1145/3375627.3375832,
author = {Mitchell, Margaret and Baker, Dylan and Moorosi, Nyalleng and Denton, Emily and Hutchinson, Ben and Hanna, Alex and Gebru, Timnit and Morgenstern, Jamie},
title = {Diversity and Inclusion Metrics in Subset Selection},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375832},
doi = {10.1145/3375627.3375832},
abstract = {The ethical concept of fairness has recently been applied in machine learning (ML) settings to describe a wide range of constraints and objectives. When considering the relevance of ethical concepts to subset selection problems, the concepts of diversity and inclusion are additionally applicable in order to create outputs that account for social power and access differentials. We introduce metrics based on these concepts, which can be applied together, separately, and in tandem with additional fairness constraints. Results from human subject experiments lend support to the proposed criteria. Social choice methods can additionally be leveraged to aggregate and choose preferable sets, and we detail how these may be applied.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {117–123},
numpages = {7},
keywords = {subset selection, machine learning fairness, diversity and inclusion},
location = {New York, NY, USA},
series = {AIES '20}
}
  
"
AIES,2020,Balancing the Tradeoff between Profit and Fairness in Rideshare Platforms during High-Demand Hours,https://dl.acm.org/doi/10.1145/3375627.3375818,"['Vedant Nanda', 'Pan Xu', 'Karthik Abinav Sankararaman', 'John P. Dickerson', 'Aravind Srinivasan']",41,"Rideshare platforms, when assigning requests to drivers, tend to maximize profit for the system and/or minimize waiting time for riders. Such platforms can exacerbate biases that drivers may have over certain types of requests. We consider the case of peak hours when the demand for rides is more than the supply of drivers. Drivers are well aware of their advantage during the peak hours and can choose to be selective about which rides to accept. Moreover, if in such a scenario, the assignment of requests to drivers (by the platform) is made only to maximize profit and/or minimize wait time for riders, requests of a certain type (e.g., from a non-popular pickup location, or to a non-popular drop-off location) might never be assigned to a driver. Such a system can be highly unfair to riders. However, increasing fairness might come at a cost of the overall profit made by the rideshare platform. To balance these conflicting goals, we present a flexible, non-adaptive algorithm, NAdap, that allows the platform designer to control the profit and fairness of the system via parameters Œ± and Œ≤ respectively.We model the matching problem as an online bipartite matching where the set of drivers is offline and requests arrive online. Upon the arrival of a request, we use NAdap to assign it to a driver (the driver might then choose to accept or reject it) or reject the request. We formalize the measures of profit and fairness in our setting and show that by using NAdap, the competitive ratios for profit and fairness measures would be no worse than Œ±/e and Œ≤/e respectively. Extensive experimental results on both real-world and synthetic datasets confirm the validity of our theoretical lower bounds. Additionally, they show that NAdap under some choice of (Œ±, Œ≤) can beat two natural heuristics, Greedy and Uniform, on both fairness and profit. Code is available at: https://github.com/nvedant07/rideshare-fairness-peak/. Full paper can be found in the proceedings of AAAI 2020 and on ArXiv: http://arxiv.org/abs/1912.08388).",Maybe,,,,Methods,"Allocation task-- ensuring equitable distribution of drivers. I think this actually is pipeline fairness, but not in the traditional pipeline sense. THOUGHTS?",,,,,,,,,,,"@inproceedings{10.1145/3375627.3375818,
author = {Nanda, Vedant and Xu, Pan and Sankararaman, Karthik Abinav and Dickerson, John P. and Srinivasan, Aravind},
title = {Balancing the Tradeoff between Profit and Fairness in Rideshare Platforms during High-Demand Hours},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375818},
doi = {10.1145/3375627.3375818},
abstract = {Rideshare platforms, when assigning requests to drivers, tend to maximize profit for the system and/or minimize waiting time for riders. Such platforms can exacerbate biases that drivers may have over certain types of requests. We consider the case of peak hours when the demand for rides is more than the supply of drivers. Drivers are well aware of their advantage during the peak hours and can choose to be selective about which rides to accept. Moreover, if in such a scenario, the assignment of requests to drivers (by the platform) is made only to maximize profit and/or minimize wait time for riders, requests of a certain type (e.g., from a non-popular pickup location, or to a non-popular drop-off location) might never be assigned to a driver. Such a system can be highly unfair to riders. However, increasing fairness might come at a cost of the overall profit made by the rideshare platform. To balance these conflicting goals, we present a flexible, non-adaptive algorithm, NAdap, that allows the platform designer to control the profit and fairness of the system via parameters α and β respectively.We model the matching problem as an online bipartite matching where the set of drivers is offline and requests arrive online. Upon the arrival of a request, we use NAdap to assign it to a driver (the driver might then choose to accept or reject it) or reject the request. We formalize the measures of profit and fairness in our setting and show that by using NAdap, the competitive ratios for profit and fairness measures would be no worse than α/e and β/e respectively. Extensive experimental results on both real-world and synthetic datasets confirm the validity of our theoretical lower bounds. Additionally, they show that NAdap under some choice of (α, β) can beat two natural heuristics, Greedy and Uniform, on both fairness and profit. Code is available at: https://github.com/nvedant07/rideshare-fairness-peak/. Full paper can be found in the proceedings of AAAI 2020 and on ArXiv: http://arxiv.org/abs/1912.08388).},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {131},
numpages = {1},
keywords = {matching, rideshare, optimization, fairness},
location = {New York, NY, USA},
series = {AIES '20}
}
  
"
AIES,2020,Trade-offs in Fair Redistricting,https://dl.acm.org/doi/10.1145/3375627.3375802,['Zachary Schutzman'],6,"What constitutes a 'fair' electoral districting plan is a discussion dating back to the founding of the United States and, in light of several recent court cases, mathematical developments, and the approaching 2020 U.S. Census, is still a fiercely debated topic today. In light of the growing desire and ability to use algorithmic tools in drawing these districts, we discuss two prototypical formulations of fairness in this domain: drawing the districts by a neutral procedure or drawing them to intentionally induce an equitable electoral outcome. We then generate a large sample of districting plans for North Carolina and Pennsylvania and consider empirically how compactness and partisan symmetry, as instantiations of these frameworks, trade off with each other -- prioritizing the value of one of these necessarily comes at a cost in the other.",Maybe,,,,Measurement,"Defining and showing tradeoffs of desiderata in creating congressional districts--- again, this is close to pipeline fairness but there is no pipeline. Feels like a ""unique fairness definition"", a version of contextualized fairness. Makes its own fairness def.",,,,,,,,,,,"@inproceedings{10.1145/3375627.3375802,
author = {Schutzman, Zachary},
title = {Trade-Offs in Fair Redistricting},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375802},
doi = {10.1145/3375627.3375802},
abstract = {What constitutes a 'fair' electoral districting plan is a discussion dating back to the founding of the United States and, in light of several recent court cases, mathematical developments, and the approaching 2020 U.S. Census, is still a fiercely debated topic today. In light of the growing desire and ability to use algorithmic tools in drawing these districts, we discuss two prototypical formulations of fairness in this domain: drawing the districts by a neutral procedure or drawing them to intentionally induce an equitable electoral outcome. We then generate a large sample of districting plans for North Carolina and Pennsylvania and consider empirically how compactness and partisan symmetry, as instantiations of these frameworks, trade off with each other -- prioritizing the value of one of these necessarily comes at a cost in the other.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {159–165},
numpages = {7},
keywords = {compactness, pareto frontier, partisan symmetry, pareto-optimal, gerrymandering, fairness, redistricting, markov chain monte carlo},
location = {New York, NY, USA},
series = {AIES '20}
}
  
"
AIES,2020,A Fairness-aware Incentive Scheme for Federated Learning,https://dl.acm.org/doi/10.1145/3375627.3375840,"['Han Yu', 'Zelei Liu', 'Yang Liu', 'Tianjian Chen', 'Mingshu Cong', 'Xi Weng', 'Dusit Niyato', 'Qiang Yang']",117,"In federated learning (FL), data owners ""share"" their local data in a privacy preserving manner in order to build a federated model, which in turn, can be used to generate revenues for the participants. However, in FL involving business participants, they might incur significant costs if several competitors join the same federation. Furthermore, the training and commercialization of the models will take time, resulting in delays before the federation accumulates enough budget to pay back the participants. The issues of costs and temporary mismatch between contributions and rewards have not been addressed by existing payoff-sharing schemes. In this paper, we propose the Federated Learning Incentivizer (FLI) payoff-sharing scheme. The scheme dynamically divides a given budget in a context-aware manner among data owners in a federation by jointly maximizing the collective utility while minimizing the inequality among the data owners, in terms of the payoff gained by them and the waiting time for receiving payoff. Extensive experimental comparisons with five state-of-the-art payoff-sharing schemes show that FLI is the most attractive to high quality data owners and achieves the highest expected revenue for a data federation.",Maybe,,,,,Not prediction model,,,,,,,,,,,"@inproceedings{10.1145/3375627.3375840,
author = {Yu, Han and Liu, Zelei and Liu, Yang and Chen, Tianjian and Cong, Mingshu and Weng, Xi and Niyato, Dusit and Yang, Qiang},
title = {A Fairness-Aware Incentive Scheme for Federated Learning},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375840},
doi = {10.1145/3375627.3375840},
abstract = {In federated learning (FL), data owners ""share"" their local data in a privacy preserving manner in order to build a federated model, which in turn, can be used to generate revenues for the participants. However, in FL involving business participants, they might incur significant costs if several competitors join the same federation. Furthermore, the training and commercialization of the models will take time, resulting in delays before the federation accumulates enough budget to pay back the participants. The issues of costs and temporary mismatch between contributions and rewards have not been addressed by existing payoff-sharing schemes. In this paper, we propose the Federated Learning Incentivizer (FLI) payoff-sharing scheme. The scheme dynamically divides a given budget in a context-aware manner among data owners in a federation by jointly maximizing the collective utility while minimizing the inequality among the data owners, in terms of the payoff gained by them and the waiting time for receiving payoff. Extensive experimental comparisons with five state-of-the-art payoff-sharing schemes show that FLI is the most attractive to high quality data owners and achieves the highest expected revenue for a data federation.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {393–399},
numpages = {7},
keywords = {federated learning, incentive mechanism design},
location = {New York, NY, USA},
series = {AIES '20}
}
  
"
AIES,2020,Joint Optimization of AI Fairness and Utility: A Human-Centered Approach,https://dl.acm.org/doi/10.1145/3375627.3375862,"['Yunfeng Zhang', 'Rachel Bellamy', 'Kush Varshney']",28,"Today, AI is increasingly being used in many high-stakes decision-making applications in which fairness is an important concern. Already, there are many examples of AI being biased and making questionable and unfair decisions. The AI research community has proposed many methods to measure and mitigate unwanted biases, but few of them involve inputs from human policy makers. We argue that because different fairness criteria sometimes cannot be simultaneously satisfied, and because achieving fairness often requires sacrificing other objectives such as model accuracy, it is key to acquire and adhere to human policy makers' preferences on how to make the tradeoff among these objectives. In this paper, we propose a framework and some exemplar methods for eliciting such preferences and for optimizing an AI model according to these preferences.",Maybe,,Choosing Metrics,,,Develops a framework for including stakeholders in chosing with fairness metric is selected for a given model,,,,,,,,,,,"@inproceedings{10.1145/3375627.3375862,
author = {Zhang, Yunfeng and Bellamy, Rachel and Varshney, Kush},
title = {Joint Optimization of AI Fairness and Utility: A Human-Centered Approach},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375862},
doi = {10.1145/3375627.3375862},
abstract = {Today, AI is increasingly being used in many high-stakes decision-making applications in which fairness is an important concern. Already, there are many examples of AI being biased and making questionable and unfair decisions. The AI research community has proposed many methods to measure and mitigate unwanted biases, but few of them involve inputs from human policy makers. We argue that because different fairness criteria sometimes cannot be simultaneously satisfied, and because achieving fairness often requires sacrificing other objectives such as model accuracy, it is key to acquire and adhere to human policy makers' preferences on how to make the tradeoff among these objectives. In this paper, we propose a framework and some exemplar methods for eliciting such preferences and for optimizing an AI model according to these preferences.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {400–406},
numpages = {7},
keywords = {algorithmic fairness, policy elicitation, multi-criteria decision making},
location = {New York, NY, USA},
series = {AIES '20}
}
  
"
AIES,2020,Arbiter: A Domain-Specific Language for Ethical Machine Learning,https://dl.acm.org/doi/10.1145/3375627.3375858,"['Julian Zucker', ""Myraeka d'Leeuwen""]",10,"The widespread deployment of machine learning models in high- stakes decision making scenarios requires a code of ethics for machine learning practitioners. We identify four of the primary components required for the ethical practice of machine learn- ing: transparency, fairness, accountability, and reproducibility. We introduce Arbiter, a domain-specific programming language for machine learning practitioners that is designed for ethical machine learning. Arbiter provides a notation for recording how machine learning models will be trained, and we show how this notation can encourage the four described components of ethical machine learning.",Maybe,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3375627.3375858,
author = {Zucker, Julian and d'Leeuwen, Myraeka},
title = {Arbiter: A Domain-Specific Language for Ethical Machine Learning},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375858},
doi = {10.1145/3375627.3375858},
abstract = {The widespread deployment of machine learning models in high- stakes decision making scenarios requires a code of ethics for machine learning practitioners. We identify four of the primary components required for the ethical practice of machine learn- ing: transparency, fairness, accountability, and reproducibility. We introduce Arbiter, a domain-specific programming language for machine learning practitioners that is designed for ethical machine learning. Arbiter provides a notation for recording how machine learning models will be trained, and we show how this notation can encourage the four described components of ethical machine learning.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {421–425},
numpages = {5},
keywords = {ethical machine learning, domain-specific languages},
location = {New York, NY, USA},
series = {AIES '20}
}
  
"
AIES,2021,Fair Machine Learning Under Partial Compliance,https://dl.acm.org/doi/10.1145/3461702.3462521,"['Jessica Dai', 'Sina Fazelpour', 'Zachary Lipton']",11,"Typically, fair machine learning research focuses on a single decision maker and assumes that the underlying population is stationary. However, many of the critical domains motivating this work are characterized by competitive marketplaces with many decision makers. Realistically, we might expect only a subset of them to adopt any non-compulsory fairness-conscious policy, a situation that political philosophers call partial compliance. This possibility raises important questions: how does partial compliance and the consequent strategic behavior of decision subjects affect the allocation outcomes? If k% of employers were to voluntarily adopt a fairness-promoting intervention, should we expect k% progress (in aggregate) towards the benefits of universal adoption, or will the dynamics of partial compliance wash out the hoped-for benefits? How might adopting a global (versus local) perspective impact the conclusions of an auditor? In this paper, we propose a simple model of an employment market, leveraging simulation as a tool to explore the impact of both interaction effects and incentive effects on outcomes and auditing metrics. Our key findings are that at equilibrium: (1) partial compliance by k% of employers can result in far less than proportional (k%) progress towards the full compliance outcomes; (2) the gap is more severe when fair employers match global (vs local) statistics; (3) choices of local vs global statistics can paint dramatically different pictures of the performance vis-a-vis fairness desiderata of compliant versus non-compliant employers; (4) partial compliance based on local parity measures can induce extreme segregation. Finally, we discuss implications for auditors and insights concerning the design of regulatory frameworks.",Maybe,,,,,Not a prediction system---evaluates what happens if some actors in a system decide to implement fairness metrics and others do not,,,,,,,,,,,"@inproceedings{10.1145/3461702.3462521,
author = {Dai, Jessica and Fazelpour, Sina and Lipton, Zachary},
title = {Fair Machine Learning Under Partial Compliance},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462521},
doi = {10.1145/3461702.3462521},
abstract = {Typically, fair machine learning research focuses on a single decision maker and assumes that the underlying population is stationary. However, many of the critical domains motivating this work are characterized by competitive marketplaces with many decision makers. Realistically, we might expect only a subset of them to adopt any non-compulsory fairness-conscious policy, a situation that political philosophers call partial compliance. This possibility raises important questions: how does partial compliance and the consequent strategic behavior of decision subjects affect the allocation outcomes? If k% of employers were to voluntarily adopt a fairness-promoting intervention, should we expect k% progress (in aggregate) towards the benefits of universal adoption, or will the dynamics of partial compliance wash out the hoped-for benefits? How might adopting a global (versus local) perspective impact the conclusions of an auditor? In this paper, we propose a simple model of an employment market, leveraging simulation as a tool to explore the impact of both interaction effects and incentive effects on outcomes and auditing metrics. Our key findings are that at equilibrium: (1) partial compliance by k% of employers can result in far less than proportional (k%) progress towards the full compliance outcomes; (2) the gap is more severe when fair employers match global (vs local) statistics; (3) choices of local vs global statistics can paint dramatically different pictures of the performance vis-a-vis fairness desiderata of compliant versus non-compliant employers; (4) partial compliance based on local parity measures can induce extreme segregation. Finally, we discuss implications for auditors and insights concerning the design of regulatory frameworks.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {55–65},
numpages = {11},
keywords = {fairness, hiring, distributive justice, fair machine learning, regulation, simulations, segregation},
location = {Virtual Event, USA},
series = {AIES '21}
}
  
"
AIES,2021,FairOD: Fairness-aware Outlier Detection,https://dl.acm.org/doi/10.1145/3461702.3462517,"['Shubhranshu Shekhar', 'Neil Shah', 'Leman Akoglu']",22,"Fairness and Outlier Detection (OD) are closely related, as it is exactly the goal of OD to spot rare, minority samples in a given population. However, when being a minority (as defined by protected variables, such as race/ethnicity/sex/age) does not reflect positive-class membership (such as criminal/fraud), OD produces unjust outcomes. Surprisingly, fairness-aware OD has been almost untouched in prior work, as fair machine learning literature mainly focuses on supervised settings. Our work aims to bridge this gap. Specifically, we develop desiderata capturing well-motivated fairness criteria for OD, and systematically formalize the fair OD problem. Further, guided by our desiderata, we propose FairOD, a fairness-aware outlier detector that has the following desirable properties: FairOD (1) exhibits treatment parity at test time, (2) aims to flag equal proportions of samples from all groups (i.e. obtain group fairness, via statistical parity), and (3) strives to flag truly high-risk samples within each group. Extensive experiments on a diverse set of synthetic and real world datasets show that FairOD produces outcomes that are fair with respect to protected variables, while performing comparable to (and in some cases, even better than) fairness-agnostic detectors in terms of detection performance.",Maybe,,Algorithm,,,"Unsupervised-- outlier detection, do we care?",,,,,,,,,,,"@inproceedings{10.1145/3461702.3462517,
author = {Shekhar, Shubhranshu and Shah, Neil and Akoglu, Leman},
title = {FairOD: Fairness-Aware Outlier Detection},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462517},
doi = {10.1145/3461702.3462517},
abstract = {Fairness and Outlier Detection (OD) are closely related, as it is exactly the goal of OD to spot rare, minority samples in a given population. However, when being a minority (as defined by protected variables, such as race/ethnicity/sex/age) does not reflect positive-class membership (such as criminal/fraud), OD produces unjust outcomes. Surprisingly, fairness-aware OD has been almost untouched in prior work, as fair machine learning literature mainly focuses on supervised settings. Our work aims to bridge this gap. Specifically, we develop desiderata capturing well-motivated fairness criteria for OD, and systematically formalize the fair OD problem. Further, guided by our desiderata, we propose FairOD, a fairness-aware outlier detector that has the following desirable properties: FairOD (1) exhibits treatment parity at test time, (2) aims to flag equal proportions of samples from all groups (i.e. obtain group fairness, via statistical parity), and (3) strives to flag truly high-risk samples within each group. Extensive experiments on a diverse set of synthetic and real world datasets show that FairOD produces outcomes that are fair with respect to protected variables, while performing comparable to (and in some cases, even better than) fairness-agnostic detectors in terms of detection performance.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {210–220},
numpages = {11},
keywords = {end-to-end detector, fair outlier detection, algorithmic fairness, outlier detection, deep learning, anomaly detection},
location = {Virtual Event, USA},
series = {AIES '21}
}
  
"
AIES,2021,Beyond Reasonable Doubt: Improving Fairness in Budget-Constrained Decision Making using Confidence Thresholds,https://dl.acm.org/doi/10.1145/3461702.3462575,"['Michiel A. Bakker', 'Duy Patrick Tu', 'Krishna P. Gummadi', 'Alex Sandy Pentland', 'Kush R. Varshney', 'Adrian Weller']",3,"Prior work on fairness in machine learning has focused on settings where all the information needed about each individual is readily available. However, in many applications, further information may be acquired at a cost. For example, when assessing a customer's creditworthiness, a bank initially has access to a limited set of information but progressively improves the assessment by acquiring additional information before making a final decision. In such settings, we posit that a fair decision maker may want to ensure that decisions for all individuals are made with similar expected error rate, even if the features acquired for the individuals are different. We show that a set of carefully chosen confidence thresholds can not only effectively redistribute an information budget according to each individual's needs, but also serve to address individual and group fairness concerns simultaneously. Finally, using two public datasets, we confirm the effectiveness of our methods and investigate the limitations.",Maybe,,,,,"Another paper that gives a fairness criteria for a different setup---i.e. in this case getting more and more info over time. Is this a metrics thing, in which case we maybe don't care?",,,,,,,,,,,"@inproceedings{10.1145/3461702.3462575,
author = {Bakker, Michiel A. and Tu, Duy Patrick and Gummadi, Krishna P. and Pentland, Alex Sandy and Varshney, Kush R. and Weller, Adrian},
title = {Beyond Reasonable Doubt: Improving Fairness in Budget-Constrained Decision Making Using Confidence Thresholds},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462575},
doi = {10.1145/3461702.3462575},
abstract = {Prior work on fairness in machine learning has focused on settings where all the information needed about each individual is readily available. However, in many applications, further information may be acquired at a cost. For example, when assessing a customer's creditworthiness, a bank initially has access to a limited set of information but progressively improves the assessment by acquiring additional information before making a final decision. In such settings, we posit that a fair decision maker may want to ensure that decisions for all individuals are made with similar expected error rate, even if the features acquired for the individuals are different. We show that a set of carefully chosen confidence thresholds can not only effectively redistribute an information budget according to each individual's needs, but also serve to address individual and group fairness concerns simultaneously. Finally, using two public datasets, we confirm the effectiveness of our methods and investigate the limitations.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {346–356},
numpages = {11},
keywords = {active feature acquisition, individual fairness, fairness},
location = {Virtual Event, USA},
series = {AIES '21}
}
  
"
AIES,2021,Learning to Generate Fair Clusters from Demonstrations,https://dl.acm.org/doi/10.1145/3461702.3462558,"['Sainyam Galhotra', 'Sandhya Saisubramanian', 'Shlomo Zilberstein']",8,"Fair clustering is the process of grouping similar entities together, while satisfying a mathematically well-defined fairness metric as a constraint. Due to the practical challenges in precise model specification, the prescribed fairness constraints are often incomplete and act as proxies to the intended fairness requirement. Clustering with proxies may lead to biased outcomes when the system is deployed. We examine how to identify the intended fairness constraint for a problem based on limited demonstrations from an expert. Each demonstration is a clustering over a subset of the data. We present an algorithm to identify the fairness metric from demonstrations and generate clusters using existing off-the-shelf clustering techniques, and analyze its theoretical properties. To extend our approach to novel fairness metrics for which clustering algorithms do not currently exist, we present a greedy method for clustering. Additionally, we investigate how to generate interpretable solutions using our approach. Empirical evaluation on three real-world datasets demonstrates the effectiveness of our approach in quickly identifying the underlying fairness and interpretability constraints, which are then used to generate fair and interpretable clusters.",Maybe,,Data,,,Not prediction,,,,,,,,,,,"@inproceedings{10.1145/3461702.3462558,
author = {Galhotra, Sainyam and Saisubramanian, Sandhya and Zilberstein, Shlomo},
title = {Learning to Generate Fair Clusters from Demonstrations},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462558},
doi = {10.1145/3461702.3462558},
abstract = {Fair clustering is the process of grouping similar entities together, while satisfying a mathematically well-defined fairness metric as a constraint. Due to the practical challenges in precise model specification, the prescribed fairness constraints are often incomplete and act as proxies to the intended fairness requirement. Clustering with proxies may lead to biased outcomes when the system is deployed. We examine how to identify the intended fairness constraint for a problem based on limited demonstrations from an expert. Each demonstration is a clustering over a subset of the data. We present an algorithm to identify the fairness metric from demonstrations and generate clusters using existing off-the-shelf clustering techniques, and analyze its theoretical properties. To extend our approach to novel fairness metrics for which clustering algorithms do not currently exist, we present a greedy method for clustering. Additionally, we investigate how to generate interpretable solutions using our approach. Empirical evaluation on three real-world datasets demonstrates the effectiveness of our approach in quickly identifying the underlying fairness and interpretability constraints, which are then used to generate fair and interpretable clusters.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {491–501},
numpages = {11},
keywords = {fairness, clustering, maximum-likelihood estimation, interpretability},
location = {Virtual Event, USA},
series = {AIES '21}
}
  
"
AIES,2021,GAEA: Graph Augmentation for Equitable Access via Reinforcement Learning,https://dl.acm.org/doi/10.1145/3461702.3462615,"['Govardana Sachithanandam Ramachandran', 'Ivan Brugere', 'Lav R. Varshney', 'Caiming Xiong']",3,"Disparate access to resources by different subpopulations is a prevalent issue in societal and sociotechnical networks. For example, urban infrastructure networks may enable certain racial groups to more easily access resources such as high-quality schools, grocery stores, and polling places. Similarly, social networks within universities and organizations may enable certain groups to more easily access people with valuable information or influence. Here we introduce a new class of problems, Graph Augmentation for Equitable Access (GAEA), to enhance equity in networked systems by editing graph edges under budget constraints. We prove such problems are NP-hard, and cannot be approximated within a factor of (1-1/3e). We develop a principled, sample- and time- efficient Markov Reward Process (MRP)-based mechanism design framework for GAEA. Our algorithm outperforms baselines on a diverse set of synthetic graphs. We further demonstrate the method on real-world networks, by merging public census, school, and transportation datasets for the city of Chicago and applying our algorithm to find human-interpretable edits to the bus network that enhance equitable access to high-quality schools across racial groups. Further experiments on Facebook networks of universities yield sets of new social connections that would increase equitable access to certain attributed nodes across gender groups.",Maybe,,,,,Not prediction,Making graph data fair,,,,,,,,,,"@inproceedings{10.1145/3461702.3462615,
author = {Ramachandran, Govardana Sachithanandam and Brugere, Ivan and Varshney, Lav R. and Xiong, Caiming},
title = {GAEA: Graph Augmentation for Equitable Access via Reinforcement Learning},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462615},
doi = {10.1145/3461702.3462615},
abstract = {Disparate access to resources by different subpopulations is a prevalent issue in societal and sociotechnical networks. For example, urban infrastructure networks may enable certain racial groups to more easily access resources such as high-quality schools, grocery stores, and polling places. Similarly, social networks within universities and organizations may enable certain groups to more easily access people with valuable information or influence. Here we introduce a new class of problems, Graph Augmentation for Equitable Access (GAEA), to enhance equity in networked systems by editing graph edges under budget constraints. We prove such problems are NP-hard, and cannot be approximated within a factor of (1-1/3e). We develop a principled, sample- and time- efficient Markov Reward Process (MRP)-based mechanism design framework for GAEA. Our algorithm outperforms baselines on a diverse set of synthetic graphs. We further demonstrate the method on real-world networks, by merging public census, school, and transportation datasets for the city of Chicago and applying our algorithm to find human-interpretable edits to the bus network that enhance equitable access to high-quality schools across racial groups. Further experiments on Facebook networks of universities yield sets of new social connections that would increase equitable access to certain attributed nodes across gender groups.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {884–894},
numpages = {11},
keywords = {equity, social networks, fairness, reinforcement learning, dataset},
location = {Virtual Event, USA},
series = {AIES '21}
}
  
"
AIES,2022,A Dynamic Decision-Making Framework Promoting Long-Term Fairness,https://dl.acm.org/doi/10.1145/3514094.3534127,"['Bhagyashree Puranik', 'Upamanyu Madhow', 'Ramtin Pedarsani']",0,"With AI-based decisions playing an increasingly consequential role in our society, for example, in our financial and criminal justice systems, there is a great deal of interest in designing algorithms conforming to application-specific notions of fairness. In this work, we ask a complementary question: can AI-based decisions be designed to dynamically influence the evolution of fairness in our society over the long term? To explore this question, we propose a framework for sequential decision-making aimed at dynamically influencing long-term societal fairness, illustrated via the problem of selecting applicants from a pool consisting of two groups, one of which is under-represented. We consider a dynamic model for the composition of the applicant pool, in which admission of more applicants from a group in a given selection round positively reinforces more candidates from the group to participate in future selection rounds. Under such a model, we show the efficacy of the proposed Fair-Greedy selection policy which systematically trades the sum of the scores of the selected applicants (""greedy'') against the deviation of the proportion of selected applicants belonging to a given group from a target proportion (""fair''). In addition to experimenting on synthetic data, we adapt static real-world datasets on law school candidates and credit lending to simulate the dynamics of the composition of the applicant pool. We prove that the applicant pool composition converges to a target proportion set by the decision-maker when score distributions across the groups are identical.",Maybe,,,,,Over time,,,,,,,,,,,"@inproceedings{10.1145/3514094.3534127,
author = {Puranik, Bhagyashree and Madhow, Upamanyu and Pedarsani, Ramtin},
title = {A Dynamic Decision-Making Framework Promoting Long-Term Fairness},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534127},
doi = {10.1145/3514094.3534127},
abstract = {With AI-based decisions playing an increasingly consequential role in our society, for example, in our financial and criminal justice systems, there is a great deal of interest in designing algorithms conforming to application-specific notions of fairness. In this work, we ask a complementary question: can AI-based decisions be designed to dynamically influence the evolution of fairness in our society over the long term? To explore this question, we propose a framework for sequential decision-making aimed at dynamically influencing long-term societal fairness, illustrated via the problem of selecting applicants from a pool consisting of two groups, one of which is under-represented. We consider a dynamic model for the composition of the applicant pool, in which admission of more applicants from a group in a given selection round positively reinforces more candidates from the group to participate in future selection rounds. Under such a model, we show the efficacy of the proposed Fair-Greedy selection policy which systematically trades the sum of the scores of the selected applicants (""greedy'') against the deviation of the proportion of selected applicants belonging to a given group from a target proportion (""fair''). In addition to experimenting on synthetic data, we adapt static real-world datasets on law school candidates and credit lending to simulate the dynamics of the composition of the applicant pool. We prove that the applicant pool composition converges to a target proportion set by the decision-maker when score distributions across the groups are identical.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {547–556},
numpages = {10},
keywords = {ai for social equity, fair selection, positive reinforcement, long-term fairness, sequential decision-making},
location = {Oxford, United Kingdom},
series = {AIES '22}
}
  
"
ICML,2017,Meritocratic Fairness for Cross Population Selection,https://proceedings.mlr.press/v70/kearns17a.html,"['Michael Kearns', 'Aaron Roth', 'Zhiwei Steven Wu']",60,"We consider the problem of selecting a strong pool of individuals from several populations with incomparable skills (e.g. soccer players, mathematicians, and singers) in a fair manner. The quality of an individual is defined to be their relative rank (by cumulative distribution value) within their own population, which permits cross-population comparisons. We study algorithms which attempt to select the highest quality subset despite the fact that true CDF values are not known, and can only be estimated from the finite pool of candidates. Specifically, we quantify the regret in quality imposed by ‚Äúmeritocratic‚Äù notions of fairness, which require that individuals are selected with probability that is monotonically increasing in their true quality. We give algorithms with provable fairness and regret guarantees, as well as lower bounds, and provide empirical results which suggest that our algorithms perform better than the theory suggests. We extend our results to a sequential batch setting, in which an algorithm must repeatedly select subsets of individuals from new pools of applicants, but has the benefit of being able to compare them to the accumulated data from previous rounds.",Maybe,,Metrics,Algorithms,Not a prediction problem,,,,,,,,,,,,"
@InProceedings{pmlr-v70-kearns17a,
  title = 	 {Meritocratic Fairness for Cross-Population Selection},
  author =       {Michael Kearns and Aaron Roth and Zhiwei Steven Wu},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {1828--1836},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/kearns17a/kearns17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/kearns17a.html},
  abstract = 	 {We consider the problem of selecting a strong pool of individuals from several populations with incomparable skills (e.g. soccer players, mathematicians, and singers) in a fair manner. The quality of an individual is defined to be their relative rank (by cumulative distribution value) within their own population, which permits cross-population comparisons. We study algorithms which attempt to select the highest quality subset despite the fact that true CDF values are not known, and can only be estimated from the finite pool of candidates. Specifically, we quantify the regret in quality imposed by “meritocratic” notions of fairness, which require that individuals are selected with probability that is monotonically increasing in their true quality. We give algorithms with provable fairness and regret guarantees, as well as lower bounds, and provide empirical results which suggest that our algorithms perform better than the theory suggests. We extend our results to a sequential batch setting, in which an algorithm must repeatedly select subsets of individuals from new pools of applicants, but has the benefit of being able to compare them to the accumulated data from previous rounds.}
}
"
Neurips,2017,Fair Clustering Through Fairlets,https://proceedings.neurips.cc//paper/2017/hash/978fce5bcc4eccc88ad48ce3914124a2-Abstract.html,"['Flavio Chierichetti', ' Ravi Kumar', ' Silvio Lattanzi', ' Sergei Vassilvitskii']",345,"We study the question of fair clustering under the {\em disparate impact} doctrine, where each protected class must have approximately equal representation in every cluster. We formulate the fair clustering problem under both the k-center and the k-median objectives, and show that even with two protected classes the problem is challenging, as the optimum solution can violate common conventions---for instance a point may no longer be assigned to its nearest cluster center! En route we introduce the concept of fairlets, which are minimal sets that satisfy fair representation while approximately preserving the clustering objective. We show that any fair clustering problem can be decomposed into first finding good fairlets, and then using existing machinery for traditional clustering algorithms. While finding good fairlets can be NP-hard, we proceed to obtain efficient approximation algorithms based on minimum cost flow. We empirically demonstrate the \emph{price of fairness} by quantifying the value of fair clustering on real-world datasets with sensitive attributes.",Maybe,,Data,,Methods,Fair clustering,,,,,,,,,,,"@inproceedings{NIPS2017_978fce5b,
 author = {Chierichetti, Flavio and Kumar, Ravi and Lattanzi, Silvio and Vassilvitskii, Sergei},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Fair Clustering Through Fairlets},
 url = {https://proceedings.neurips.cc/paper/2017/file/978fce5bcc4eccc88ad48ce3914124a2-Paper.pdf},
 volume = {30},
 year = {2017}
}

"
Neurips,2018,Enhancing the Accuracy and Fairness of Human Decision Making,https://proceedings.neurips.cc//paper/2018/hash/0a113ef6b61820daa5611c870ed8d5ee-Abstract.html,"['Isabel Valera', ' Adish Singla', ' Manuel Gomez Rodriguez']",34,"Societies often rely on human experts to take a wide variety of decisions affecting their members, from jail-or-release decisions taken by judges and stop-and-frisk decisions taken by police officers to accept-or-reject decisions taken by academics. In this context, each decision is taken by an expert who is typically chosen uniformly at random from a pool of experts. However, these decisions may be imperfect due to limited experience, implicit biases, or faulty probabilistic reasoning. Can we improve the accuracy and fairness of the overall decision making process by optimizing the assignment between experts and decisions? In this paper, we address the above problem from the perspective of sequential decision making and show that, for different fairness notions from the literature, it reduces to a sequence of (constrained) weighted bipartite matchings, which can be solved efficiently using algorithms with approximation guarantees. Moreover, these algorithms also benefit from posterior sampling to actively trade off exploitation---selecting expert assignments which lead to accurate and fair decisions---and exploration---selecting expert assignments to learn about the experts' preferences and biases. We demonstrate the effectiveness of our algorithms on both synthetic and real-world data and show that they can significantly improve both the accuracy and fairness of the decisions taken by pools of experts.",Maybe,,Organizational Realities,Algorithm,Methods,Shows how to allocate decisions among experts to reduce bias-- not a prediction,,,,,,,,,,,"@inproceedings{NEURIPS2018_0a113ef6,
 author = {Valera, Isabel and Singla, Adish and Gomez Rodriguez, Manuel},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Enhancing the Accuracy and Fairness of Human Decision Making},
 url = {https://proceedings.neurips.cc/paper/2018/file/0a113ef6b61820daa5611c870ed8d5ee-Paper.pdf},
 volume = {31},
 year = {2018}
}

"
Neurips,2018,Does mitigating ML's impact disparity require treatment disparity?,https://proceedings.neurips.cc//paper/2018/hash/8e0384779e58ce2af40eb365b318cc32-Abstract.html,"['Zachary Lipton', ' Julian McAuley', ' Alexandra Chouldechova']",165,"Following precedent in employment discrimination law, two notions of disparity are widely-discussed in papers on fairness and ML. Algorithms exhibit treatment disparity if they formally treat members of protected subgroups differently; algorithms exhibit impact disparity when outcomes differ across subgroups (even unintentionally). Naturally, we can achieve impact parity through purposeful treatment disparity. One line of papers aims to reconcile the two parities proposing disparate learning processes (DLPs). Here, the sensitive feature is used during training but a group-blind classifier is produced. In this paper, we show that: (i) when sensitive and (nominally) nonsensitive features are correlated, DLPs will indirectly implement treatment disparity, undermining the policy desiderata they are designed to address; (ii) when group membership is partly revealed by other features, DLPs induce within-class discrimination; and (iii) in general, DLPs provide suboptimal trade-offs between accuracy and impact parity. Experimental results on several real-world datasets highlight the practical consequences of applying DLPs.",Maybe,,Algorithm,,Detection,Shows that disparate learning processes can lead to fairness problems under certain situations. This can show when it is/isn't good to implement?,,,,,,,,,,,"@inproceedings{NEURIPS2018_8e038477,
 author = {Lipton, Zachary and McAuley, Julian and Chouldechova, Alexandra},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Does mitigating ML\textquotesingle s impact disparity require treatment disparity?},
 url = {https://proceedings.neurips.cc/paper/2018/file/8e0384779e58ce2af40eb365b318cc32-Paper.pdf},
 volume = {31},
 year = {2018}
}

"
Neurips,2019,On the Fairness of Disentangled Representations,https://proceedings.neurips.cc//paper/2019/hash/1b486d7a5189ebe8d8c46afc64b0d1b4-Abstract.html,"['Francesco Locatello', ' Gabriele Abbati', ' Thomas Rainforth', ' Stefan Bauer', ' Bernhard Sch√∂lkopf', ' Olivier Bachem']",162,"Recently there has been a significant interest in learning disentangled representations, as they promise increased interpretability, generalization to unseen scenarios and faster learning on downstream tasks. In this paper, we investigate the usefulness of different notions of disentanglement for improving the fairness of downstream prediction tasks based on representations. We consider the setting where the goal is to predict a target variable based on the learned representation of high-dimensional observations (such as images) that depend on both the target variable and an unobserved sensitive variable. We show that in this setting both the optimal and empirical predictions can be unfair, even if the target variable and the sensitive variable are independent. Analyzing the representations of more than 12600 trained state-of-the-art disentangled models, we observe that several disentanglement scores are consistently correlated with increased fairness, suggesting that disentanglement may be a useful property to encourage fairness when sensitive variables are not observed.",Maybe,,Data,Problem Identification,Methods,Shows that disentanglement is useful when training deep models with access to the protected attribute,,,,,,,,,,,"@inproceedings{NEURIPS2019_1b486d7a,
 author = {Locatello, Francesco and Abbati, Gabriele and Rainforth, Thomas and Bauer, Stefan and Sch\""{o}lkopf, Bernhard and Bachem, Olivier},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {On the Fairness of Disentangled Representations},
 url = {https://proceedings.neurips.cc/paper/2019/file/1b486d7a5189ebe8d8c46afc64b0d1b4-Paper.pdf},
 volume = {32},
 year = {2019}
}

"
Neurips,2019,PC-Fairness: A Unified Framework for Measuring Causality-based Fairness,https://proceedings.neurips.cc//paper/2019/hash/44a2e0804995faf8d2e3b084a1e2db1d-Abstract.html,"['Yongkai Wu', ' Lu Zhang', ' Xintao Wu', ' Hanghang Tong']",70,"A recent trend of fair machine learning is to define fairness as causality-based notions which concern the causal connection between protected attributes and decisions. However, one common challenge of all causality-based fairness notions is identifiability, i.e., whether they can be uniquely measured from observational data, which is a critical barrier to applying these notions to real-world situations. In this paper, we develop a framework for measuring different causality-based fairness. We propose a unified definition that covers most of previous causality-based fairness notions, namely the path-specific counterfactual fairness (PC fairness). Based on that, we propose a general method in the form of a constrained optimization problem for bounding the path-specific counterfactual fairness under all unidentifiable situations. Experiments on synthetic and real-world datasets show the correctness and effectiveness of our method.",Maybe,,General,Algorithm,Measurement,How to measure causality based fairness measures on the data/outcomes,,,,,,,,,,,"@inproceedings{NEURIPS2019_44a2e080,
 author = {Wu, Yongkai and Zhang, Lu and Wu, Xintao and Tong, Hanghang},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {PC-Fairness: A Unified Framework for Measuring Causality-based Fairness},
 url = {https://proceedings.neurips.cc/paper/2019/file/44a2e0804995faf8d2e3b084a1e2db1d-Paper.pdf},
 volume = {32},
 year = {2019}
}

"
Neurips,2019,The Case for Evaluating Causal Models Using Interventional Measures and Empirical Data,https://proceedings.neurips.cc//paper/2019/hash/a87c11b9100c608b7f8e98cfa316ff7b-Abstract.html,"['Amanda Gentzel', ' Dan Garant', ' David Jensen']",31,"Causal inference is central to many areas of artificial intelligence, including complex reasoning, planning, knowledge-base construction, robotics, explanation, and fairness. An active community of researchers develops and enhances algorithms that learn causal models from data, and this work has produced a series of impressive technical advances. However, evaluation techniques for causal modeling algorithms have remained somewhat primitive, limiting what we can learn from experimental studies of algorithm performance, constraining the types of algorithms and model representations that researchers consider, and creating a gap between theory and practice. We argue for more frequent use of evaluation techniques that examine interventional measures rather than structural or observational measures, and that evaluate those measures on empirical data rather than synthetic data. We survey the current practice in evaluation and show that the techniques we recommend are rarely used in practice. We show that such techniques are feasible and that data sets are available to conduct such evaluations. We also show that these techniques produce substantially different results than using structural measures and synthetic data.",Maybe,,,,Measurement,"Shows that many methods of evaluating causal models are not useful in practice, and argues for (and explains) more operationalized approach",,,,,,,,,,,"@inproceedings{NEURIPS2019_a87c11b9,
 author = {Gentzel, Amanda and Garant, Dan and Jensen, David},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {The Case for Evaluating Causal Models Using Interventional Measures and Empirical Data},
 url = {https://proceedings.neurips.cc/paper/2019/file/a87c11b9100c608b7f8e98cfa316ff7b-Paper.pdf},
 volume = {32},
 year = {2019}
}

"
Neurips,2019,Assessing Disparate Impact of Personalized Interventions: Identifiability and Bounds,https://proceedings.neurips.cc//paper/2019/hash/d54e99a6c03704e95e6965532dec148b-Abstract.html,"['Nathan Kallus', ' Angela Zhou']",17,"Personalized interventions in social services, education, and healthcare leverage individual-level causal effect predictions in order to give the best treatment to each individual or to prioritize program interventions for the individuals most likely to benefit. While the sensitivity of these domains compels us to evaluate the fairness of such policies, we show that actually auditing their disparate impacts per standard observational metrics, such as true positive rates, is impossible since ground truths are unknown. Whether our data is experimental or observational, an individual's actual outcome under an intervention different than that received can never be known, only predicted based on features. We prove how we can nonetheless point-identify these quantities under the additional assumption of monotone treatment response, which may be reasonable in many applications. We further provide a sensitivity analysis for this assumption via sharp partial-identification bounds under violations of monotonicity of varying strengths. We show how to use our results to audit personalized interventions using partially-identified ROC and xROC curves and demonstrate this in a case study of a French job training dataset.",Maybe,,Algorithm,,Measurement,Way of seeing,,,,,,,,,,,"@inproceedings{NEURIPS2019_d54e99a6,
 author = {Kallus, Nathan and Zhou, Angela},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Assessing Disparate Impact of Personalized Interventions: Identifiability and Bounds},
 url = {https://proceedings.neurips.cc/paper/2019/file/d54e99a6c03704e95e6965532dec148b-Paper.pdf},
 volume = {32},
 year = {2019}
}

"
Neurips,2020,Asymmetric Shapley values: incorporating causal knowledge into model-agnostic explainability,https://proceedings.neurips.cc//paper/2020/hash/0d770c496aa3da6d2c3f2bd19e7b9d6b-Abstract.html,"['Christopher Frye', ' Colin Rowat', ' Ilya Feige']",106,"Explaining AI systems is fundamental both to the development of high performing models and to the trust placed in them by their users. The Shapley framework for explainability has strength in its general applicability combined with its precise, rigorous foundation: it provides a common, model-agnostic language for AI explainability and uniquely satisfies a set of intuitive mathematical axioms. However, Shapley values are too restrictive in one significant regard: they ignore all causal structure in the data. We introduce a less restrictive framework, Asymmetric Shapley values (ASVs), which are rigorously founded on a set of axioms, applicable to any AI system, and can flexibly incorporate any causal structure known to be respected by the data. We demonstrate that ASVs can (i) improve model explanations by incorporating causal information, (ii) provide an unambiguous test for unfair discrimination in model predictions, (iii) enable sequentially incremental explanations in time-series models, and (iv) support feature-selection studies without the need for model retraining.",Maybe,,Explanation,Features,,Creates shapley values that take into account causal realities of the data----CAN BE USED AS A FEATURE SELECTION METRIC,,,,,,,,,,,"@inproceedings{NEURIPS2020_0d770c49,
 author = {Frye, Christopher and Rowat, Colin and Feige, Ilya},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {1229--1239},
 publisher = {Curran Associates, Inc.},
 title = {Asymmetric Shapley values: incorporating causal knowledge into model-agnostic explainability},
 url = {https://proceedings.neurips.cc/paper/2020/file/0d770c496aa3da6d2c3f2bd19e7b9d6b-Paper.pdf},
 volume = {33},
 year = {2020}
}

"
Neurips,2020,Fair Multiple Decision Making Through Soft Interventions,https://proceedings.neurips.cc//paper/2020/hash/d0921d442ee91b896ad95059d13df618-Abstract.html,"['Yaowei Hu', ' Yongkai Wu', ' Lu Zhang', ' Xintao Wu']",8,"Previous research in fair classification mostly focuses on a single decision model. In reality, there usually exist multiple decision models within a system and all of which may contain a certain amount of discrimination. Such realistic scenarios introduce new challenges to fair classification: since discrimination may be transmitted from upstream models to downstream models, building decision models separately without taking upstream models into consideration cannot guarantee to achieve fairness. In this paper, we propose an approach that learns multiple classifiers and achieves fairness for all of them simultaneously, by treating each decision model as a soft intervention and inferring the post-intervention distributions to formulate the loss function as well as the fairness constraints. We adopt surrogate functions to smooth the loss function and constraints, and theoretically show that the excess risk of the proposed loss function can be bounded in a form that is the same as that for traditional surrogated loss functions. Experiments using both synthetic and real-world datasets show the effectiveness of our approach.",Maybe,,Algorithm,,,Shows how to enforce fairness over multiple models at once when they are used in succession,,,,,,,,,,,"@inproceedings{NEURIPS2020_d0921d44,
 author = {Hu, Yaowei and Wu, Yongkai and Zhang, Lu and Wu, Xintao},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {17965--17975},
 publisher = {Curran Associates, Inc.},
 title = {Fair Multiple Decision Making Through Soft Interventions},
 url = {https://proceedings.neurips.cc/paper/2020/file/d0921d442ee91b896ad95059d13df618-Paper.pdf},
 volume = {33},
 year = {2020}
}

"
Neurips,2020,How do fair decisions fare in long-term qualification?,https://proceedings.neurips.cc//paper/2020/hash/d6d231705f96d5a35aeb3a76402e49a3-Abstract.html,"['Xueru Zhang', ' Ruibo Tu', ' Yang Liu', ' Mingyan Liu', ' Hedvig Kjellstrom', ' Kun Zhang', ' Cheng Zhang']",39,"Although many fairness criteria have been proposed for decision making, their long-term impact on the well-being of a population remains unclear. In this work, we study the dynamics of population qualification and algorithmic decisions under a partially observed Markov decision problem setting. By characterizing the equilibrium of such dynamics, we analyze the long-term impact of static fairness constraints on the equality and improvement of group well-being. Our results show that static fairness constraints can either promote equality or exacerbate disparity depending on the driving factor of qualification transitions and the effect of sensitive attributes on feature distributions. We also consider possible interventions that can effectively improve group qualification or promote equality of group qualification. Our theoretical results and experiments on static real-world datasets with simulated dynamics show that our framework can be used to facilitate social science studies.",Maybe,,,,,"Time component, very theoretical",,,,,,,,,,,"@inproceedings{NEURIPS2020_d6d23170,
 author = {Zhang, Xueru and Tu, Ruibo and Liu, Yang and Liu, Mingyan and Kjellstrom, Hedvig and Zhang, Kun and Zhang, Cheng},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {18457--18469},
 publisher = {Curran Associates, Inc.},
 title = {How do fair decisions fare in long-term qualification?},
 url = {https://proceedings.neurips.cc/paper/2020/file/d6d231705f96d5a35aeb3a76402e49a3-Paper.pdf},
 volume = {33},
 year = {2020}
}

"
Neurips,2021,Fair Classification with Adversarial Perturbations,https://proceedings.neurips.cc//paper/2021/hash/44e207aecc63505eb828d442de03f2e9-Abstract.html,"['L. Elisa Celis', ' Anay Mehrotra', ' Nisheeth Vishnoi']",19,"We study fair classification in the presence of an omniscient adversary that, given an $\eta$, is allowed to choose an arbitrary $\eta$-fraction of the training samples and arbitrarily perturb their protected attributes. The motivation comes from settings in which protected attributes can be incorrect due to strategic misreporting, malicious actors, or errors in imputation; and prior approaches that make stochastic or independence assumptions on errors may not satisfy their guarantees in this adversarial setting. Our main contribution is an optimization framework to learn fair classifiers in this adversarial setting that comes with provable guarantees on accuracy and fairness. Our framework works with multiple and non-binary protected attributes, is designed for the large class of linear-fractional fairness metrics, and can also handle perturbations besides protected attributes. We prove near-tightness of our framework's guarantees for natural hypothesis classes: no algorithm can have significantly better accuracy and any algorithm with better fairness must have lower accuracy. Empirically, we evaluate the classifiers produced by our framework for statistical rate on real-world and synthetic datasets for a family of adversaries.",Maybe,,Data,Algorithm,Methods,Shows how to create fair classifiers in the presence of data whose protected attribute labels have been corrupted by an adversar,,,,,,,,,,,"@inproceedings{NEURIPS2021_44e207ae,
 author = {Celis, L. Elisa and Mehrotra, Anay and Vishnoi, Nisheeth},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {8158--8171},
 publisher = {Curran Associates, Inc.},
 title = {Fair Classification with Adversarial Perturbations},
 url = {https://proceedings.neurips.cc/paper/2021/file/44e207aecc63505eb828d442de03f2e9-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
Neurips,2021,An Information-theoretic Approach to Distribution Shifts,https://proceedings.neurips.cc//paper/2021/hash/93661c10ed346f9692f4d512319799b3-Abstract.html,"['Marco Federici', ' Ryota Tomioka', ' Patrick Forr√©']",5,"Safely deploying machine learning models to the real world is often a challenging process. For example, models trained with data obtained from a specific geographic location tend to fail when queried with data obtained elsewhere, agents trained in a simulation can struggle to adapt when deployed in the real world or novel environments, and neural networks that are fit to a subset of the population might carry some selection bias into their decision process.In this work, we describe the problem of data shift from an information-theoretic perspective by (i) identifying and describing the different sources of error, (ii) comparing some of the most promising objectives explored in the recent domain generalization and fair classification literature. From our theoretical analysis and empirical evaluation, we conclude that the model selection procedure needs to be guided by careful considerations regarding the observed data, the factors used for correction, and the structure of the data-generating process.",Maybe,,In-situ,,Problem Identification,"Not explicitly fairness related, but actually shows how to deal with distribution shift as a result of different types of particularities with the data. Should be read in depth.",,,,,,,,,,,"@inproceedings{NEURIPS2021_93661c10,
 author = {Federici, Marco and Tomioka, Ryota and Forr\'{e}, Patrick},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {17628--17641},
 publisher = {Curran Associates, Inc.},
 title = {An Information-theoretic Approach to Distribution Shifts},
 url = {https://proceedings.neurips.cc/paper/2021/file/93661c10ed346f9692f4d512319799b3-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
ICLR,2020,Projection Based Constrained Policy Optimization,https://openreview.net/forum?id=rke3TJrtPS,"['Tsung-Yen Yang', 'Justinian Rosca', 'Karthik Narasimhan', 'Peter J. Ramadge']",100,"In this paper, we consider the problem of learning control policies that optimize areward function while satisfying constraints due to considerations of safety, fairness, or other costs. We propose a new algorithm - Projection Based ConstrainedPolicy Optimization (PCPO), an iterative method for optimizing policies in a two-step process - the first step performs an unconstrained update while the secondstep reconciles the constraint violation by projection the policy back onto the constraint set. We theoretically analyze PCPO and provide a lower bound on rewardimprovement, as well as an upper bound on constraint violation for each policy update. We further characterize the convergence of PCPO with projection basedon two different metrics - L2 norm and Kullback-Leibler divergence. Our empirical results over several control tasks demonstrate that our algorithm achievessuperior performance, averaging more than 3.5 times less constraint violation andaround 15% higher reward compared to state-of-the-art methods.",Maybe,,Algorithm,,,,,,,,,,,,,,"@inproceedings{
Yang2020Projection-Based,
title={Projection-Based Constrained Policy Optimization},
author={Tsung-Yen Yang and Justinian Rosca and Karthik Narasimhan and Peter J. Ramadge},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=rke3TJrtPS}
}"
ICLR,2021,On the Transfer of Disentangled Representations in Realistic Settings,https://openreview.net/forum?id=8VXvj1QNRl1,"['Andrea Dittadi', ' Frederik Tr√§uble', ' Francesco Locatello', ' Manuel Wuthrich', ' Vaibhav Agrawal', ' Ole Winther', ' Stefan Bauer', ' Bernhard Sch√∂lkopf']",50,"Learning meaningful representations that disentangle the underlying structure of the data generating process is considered to be of key importance in machine learning. While disentangled representations were found to be useful for diverse tasks such as abstract reasoning and fair classification, their scalability and real-world impact remain questionable.
 We introduce a new high-resolution dataset with 1M simulated images and over 1,800 annotated real-world images of the same setup. In contrast to previous work, this new dataset exhibits correlations, a complex underlying structure, and allows to evaluate transfer to unseen simulated and real-world settings where the encoder i) remains in distribution or ii) is out of distribution.
 We propose new architectures in order to scale disentangled representation learning to realistic high-resolution settings and conduct a large-scale empirical study of disentangled representations on this dataset. We observe that disentanglement is a good predictor for out-of-distribution (OOD) task performance.",Maybe,,Data,Not included,,,,,,,,,,,,,"@inproceedings{
dittadi2021on,
title={On the Transfer of Disentangled Representations in Realistic Settings},
author={Andrea Dittadi and Frederik Tr{\""a}uble and Francesco Locatello and Manuel Wuthrich and Vaibhav Agrawal and Ole Winther and Stefan Bauer and Bernhard Sch{\""o}lkopf},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=8VXvj1QNRl1}
}"
AIES,2018,"Value Alignment, Fair Play, and the Rights of Service Robots",https://dl.acm.org/doi/10.1145/3278721.3278730,['Daniel Estrada'],7,"Ethics and safety research in artificial intelligence is increasingly framed in terms of ""alignment'' with human values and interests. I argue that Turing's call for ""fair play for machines'' is an early and often overlooked contribution to the alignment literature. Turing's appeal to fair play suggests a need to correct human behavior to accommodate our machines, a surprising inversion of how value alignment is treated today. Reflections on ""fair play'' motivate a novel interpretation of Turing's notorious ""imitation game'' as a condition not of intelligence but instead of value alignment : a machine demonstrates a minimal degree of alignment (with the norms of conversation, for instance) when it can go undetected when interrogated by a human. I carefully distinguish this interpretation from the Moral Turing Test, which is not motivated by a principle of fair play, but instead depends on imitation of human moral behavior. Finally, I consider how the framework of fair play can be used to situate the debate over robot rights within the alignment literature. I argue that extending rights to service robots operating in public spaces is ""fair'' in precisely the sense that it encourages an alignment of interests between humans and machines.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3278721.3278730,
author = {Estrada, Daniel},
title = {Value Alignment, Fair Play, and the Rights of Service Robots},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278730},
doi = {10.1145/3278721.3278730},
abstract = {Ethics and safety research in artificial intelligence is increasingly framed in terms of ""alignment'' with human values and interests. I argue that Turing's call for ""fair play for machines'' is an early and often overlooked contribution to the alignment literature. Turing's appeal to fair play suggests a need to correct human behavior to accommodate our machines, a surprising inversion of how value alignment is treated today. Reflections on ""fair play'' motivate a novel interpretation of Turing's notorious ""imitation game'' as a condition not of intelligence but instead of value alignment : a machine demonstrates a minimal degree of alignment (with the norms of conversation, for instance) when it can go undetected when interrogated by a human. I carefully distinguish this interpretation from the Moral Turing Test, which is not motivated by a principle of fair play, but instead depends on imitation of human moral behavior. Finally, I consider how the framework of fair play can be used to situate the debate over robot rights within the alignment literature. I argue that extending rights to service robots operating in public spaces is ""fair'' in precisely the sense that it encourages an alignment of interests between humans and machines.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {102–107},
numpages = {6},
keywords = {robot rights, fair play, alan turing, value alignment},
location = {New Orleans, LA, USA},
series = {AIES '18}
}
  
"
AIES,2018,Modeling Epistemological Principles for Bias Mitigation in AI Systems: An Illustration in Hiring Decisions,https://dl.acm.org/doi/10.1145/3278721.3278751,"['Marisa Vasconcelos', 'Carlos Cardonha', 'Bernardo Gon√ßalves']",25,"Artificial Intelligence (AI) has been used extensively in automatic decision making in a broad variety of scenarios, ranging from credit ratings for loans to recommendations of movies. Traditional design guidelines for AI models focus essentially on accuracy maximization, but recent work has shown that economically irrational and socially unacceptable scenarios of discrimination and unfairness are likely to arise unless these issues are explicitly addressed. This undesirable behavior has several possible sources, such as biased datasets used for training that may not be detected in black-box models. After pointing out connections between such bias of AI and the problem of induction, we focus on Popper's contributions after Hume's, which offer a logical theory of preferences. An AI model can be preferred over others on purely rational grounds after one or more attempts at refutation based on accuracy and fairness. Inspired by such epistemological principles, this paper proposes a structured approach to mitigate discrimination and unfairness caused by bias in AI systems. In the proposed computational framework, models are selected and enhanced after attempts at refutation. To illustrate our discussion, we focus on hiring decision scenarios where an AI system filters in which job applicants should go to the interview phase.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3278721.3278751,
author = {Vasconcelos, Marisa and Cardonha, Carlos and Gon\c{c}alves, Bernardo},
title = {Modeling Epistemological Principles for Bias Mitigation in AI Systems: An Illustration in Hiring Decisions},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278751},
doi = {10.1145/3278721.3278751},
abstract = {Artificial Intelligence (AI) has been used extensively in automatic decision making in a broad variety of scenarios, ranging from credit ratings for loans to recommendations of movies. Traditional design guidelines for AI models focus essentially on accuracy maximization, but recent work has shown that economically irrational and socially unacceptable scenarios of discrimination and unfairness are likely to arise unless these issues are explicitly addressed. This undesirable behavior has several possible sources, such as biased datasets used for training that may not be detected in black-box models. After pointing out connections between such bias of AI and the problem of induction, we focus on Popper's contributions after Hume's, which offer a logical theory of preferences. An AI model can be preferred over others on purely rational grounds after one or more attempts at refutation based on accuracy and fairness. Inspired by such epistemological principles, this paper proposes a structured approach to mitigate discrimination and unfairness caused by bias in AI systems. In the proposed computational framework, models are selected and enhanced after attempts at refutation. To illustrate our discussion, we focus on hiring decision scenarios where an AI system filters in which job applicants should go to the interview phase.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {323–329},
numpages = {7},
keywords = {attempts at refutation, constrained models, hiring algorithms, problem of induction, bias of ai, semi-automatic decision making},
location = {New Orleans, LA, USA},
series = {AIES '18}
}
  
"
AIES,2018,Speed And Accuracy Are Not Enough! Trustworthy Machine Learning,https://dl.acm.org/doi/10.1145/3278721.3278796,['Shiva Kaul'],8,"Classical linear/shallow learning is relatively easy to analyze and understand, but the power of deep learning is often desirable. I am developing a hybrid approach in order to obtain learning algorithms that are both trustworthy and accurate. My research has mostly focused on learning from corrupted or inconsistent training data (`agnostic learning'). Recently, I, as well as independent researchers, have found these same techniques could help make algorithms more fair.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3278721.3278796,
author = {Kaul, Shiva},
title = {Speed And Accuracy Are Not Enough! Trustworthy Machine Learning},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278796},
doi = {10.1145/3278721.3278796},
abstract = {Classical linear/shallow learning is relatively easy to analyze and understand, but the power of deep learning is often desirable. I am developing a hybrid approach in order to obtain learning algorithms that are both trustworthy and accurate. My research has mostly focused on learning from corrupted or inconsistent training data (`agnostic learning'). Recently, I, as well as independent researchers, have found these same techniques could help make algorithms more fair.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {372–373},
numpages = {2},
keywords = {machine learning, agnostic learning, fairness, classification},
location = {New Orleans, LA, USA},
series = {AIES '18}
}
  
"
AIES,2018,A Win for Society! Conquering Barriers to Fair Elections,https://dl.acm.org/doi/10.1145/3278721.3278787,['Barton E. Lee'],0,"Social choice is a general framework used in the aggregation of agent preferences to make a collective decision, political elections whereby agents vote is a common example. It is often the case that society demands electoral systems which ensure, or election outcomes which satisfy, socially desirable outcomes such as representing large minorities and avoiding the 'tyranny of the majority'. Unfortunately, there are many natural barriers which may prevent desirable outcomes from being achieved. These barriers include the non-existence or computational intractability of achieving desirable outcomes, especially when combined with additional feasibility constraints, and the effect of strategic or manipulative agents. This thesis aims to improve our understanding of the scale of these barriers and if, or how, they can be overcome to provide socially desirable outcomes.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3278721.3278787,
author = {Lee, Barton E.},
title = {A Win for Society! Conquering Barriers to Fair Elections},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278787},
doi = {10.1145/3278721.3278787},
abstract = {Social choice is a general framework used in the aggregation of agent preferences to make a collective decision, political elections whereby agents vote is a common example. It is often the case that society demands electoral systems which ensure, or election outcomes which satisfy, socially desirable outcomes such as representing large minorities and avoiding the 'tyranny of the majority'. Unfortunately, there are many natural barriers which may prevent desirable outcomes from being achieved. These barriers include the non-existence or computational intractability of achieving desirable outcomes, especially when combined with additional feasibility constraints, and the effect of strategic or manipulative agents. This thesis aims to improve our understanding of the scale of these barriers and if, or how, they can be overcome to provide socially desirable outcomes.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {374–375},
numpages = {2},
keywords = {voting, game theory, computational social choice, preference aggregation, strategic agents},
location = {New Orleans, LA, USA},
series = {AIES '18}
}
  
"
AIES,2018,An Axiomatic Approach to Explain Computer Generated Decisions: Extended Abstract,https://dl.acm.org/doi/10.1145/3278721.3278788,['Martin Strobel'],2,"Recent years have seen the widespread implementation of data-driven algorithms making decisions in increasingly highstakes domains, such as finance, healthcare, transportation and public safety. Using novel ML techniques, these algorithms are able to process massive amounts of data and make highly accurate predictions; however, their inherent complexity makes it increasingly difficult for humans to understand why certain decisions were made. Indeed, these algorithms are black-box decision makers: their underlying decision processes are either hidden from human scrutiny by proprietary law, or (as is often the case) their inner workings are so complicated that even their own designers will be hard-pressed to explain the underlying reasoning behind their decision making processes. By obfuscating their function, data-driven classifiers run the risk of exposing human stakeholders to risks. These may include incorrect decisions (e.g. a loan application that was wrongly rejected due to system error), information leaks (e.g. an algorithm inadvertently uses information it should not have used), or discrimination (e.g. biased decisions against certain ethnic or gender groups). Government bodies and regulatory authorities have recently begun calling for algorithmic transparency: providing human-interpretable explanations of the underlying reasoning behind large-scale decision making algorithms. My thesis research will be concerned with an axiomatic analysis of automatically generated explanations of such classifiers. Especially, I'm interested in how to decide which explanation of a decision to trust given that there are many, potentially conflicting, possible explanations for any given decision.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3278721.3278788,
author = {Strobel, Martin},
title = {An Axiomatic Approach to Explain Computer Generated Decisions: Extended Abstract},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278788},
doi = {10.1145/3278721.3278788},
abstract = {Recent years have seen the widespread implementation of data-driven algorithms making decisions in increasingly highstakes domains, such as finance, healthcare, transportation and public safety. Using novel ML techniques, these algorithms are able to process massive amounts of data and make highly accurate predictions; however, their inherent complexity makes it increasingly difficult for humans to understand why certain decisions were made. Indeed, these algorithms are black-box decision makers: their underlying decision processes are either hidden from human scrutiny by proprietary law, or (as is often the case) their inner workings are so complicated that even their own designers will be hard-pressed to explain the underlying reasoning behind their decision making processes. By obfuscating their function, data-driven classifiers run the risk of exposing human stakeholders to risks. These may include incorrect decisions (e.g. a loan application that was wrongly rejected due to system error), information leaks (e.g. an algorithm inadvertently uses information it should not have used), or discrimination (e.g. biased decisions against certain ethnic or gender groups). Government bodies and regulatory authorities have recently begun calling for algorithmic transparency: providing human-interpretable explanations of the underlying reasoning behind large-scale decision making algorithms. My thesis research will be concerned with an axiomatic analysis of automatically generated explanations of such classifiers. Especially, I'm interested in how to decide which explanation of a decision to trust given that there are many, potentially conflicting, possible explanations for any given decision.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {380–381},
numpages = {2},
keywords = {explainable machine learning, axiomatic approach},
location = {New Orleans, LA, USA},
series = {AIES '18}
}
  
"
AIES,2019,How Do Fairness Definitions Fare?: Examining Public Attitudes Towards Algorithmic Definitions of Fairness,https://dl.acm.org/doi/10.1145/3306618.3314248,"['Nripsuta Ani Saxena', 'Karen Huang', 'Evan DeFilippis', 'Goran Radanovic', 'David C. Parkes', 'Yang Liu']",137,"What is the best way to define algorithmic fairness? While many definitions of fairness have been proposed in the computer science literature, there is no clear agreement over a particular definition. In this work, we investigate ordinary people's perceptions of three of these fairness definitions. Across two online experiments, we test which definitions people perceive to be the fairest in the context of loan decisions, and whether fairness perceptions change with the addition of sensitive information (i.e., race of the loan applicants). Overall, one definition (calibrated fairness) tends to be more pre- ferred than the others, and the results also provide support for the principle of affirmative action.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3306618.3314248,
author = {Saxena, Nripsuta Ani and Huang, Karen and DeFilippis, Evan and Radanovic, Goran and Parkes, David C. and Liu, Yang},
title = {How Do Fairness Definitions Fare? Examining Public Attitudes Towards Algorithmic Definitions of Fairness},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314248},
doi = {10.1145/3306618.3314248},
abstract = {What is the best way to define algorithmic fairness? While many definitions of fairness have been proposed in the computer science literature, there is no clear agreement over a particular definition. In this work, we investigate ordinary people's perceptions of three of these fairness definitions. Across two online experiments, we test which definitions people perceive to be the fairest in the context of loan decisions, and whether fairness perceptions change with the addition of sensitive information (i.e., race of the loan applicants). Overall, one definition (calibrated fairness) tends to be more pre- ferred than the others, and the results also provide support for the principle of affirmative action.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {99–106},
numpages = {8},
keywords = {human experiments, public attitudes, fairness, algorithmic definition},
location = {Honolulu, HI, USA},
series = {AIES '19}
}
  
"
AIES,2019,TED: Teaching AI to Explain its Decisions,https://dl.acm.org/doi/10.1145/3306618.3314273,"['Michael Hind', 'Dennis Wei', 'Murray Campbell', 'Noel C. F. Codella', 'Amit Dhurandhar', 'Aleksandra Mojsiloviƒá', 'Karthikeyan Natesan Ramamurthy', 'Kush R. Varshney']",95,"Artificial intelligence systems are being increasingly deployed due to their potential to increase the efficiency, scale, consistency, fairness, and accuracy of decisions. However, as many of these systems are opaque in their operation, there is a growing demand for such systems to provide explanations for their decisions. Conventional approaches to this problem attempt to expose or discover the inner workings of a machine learning model with the hope that the resulting explanations will be meaningful to the consumer. In contrast, this paper suggests a new approach to this problem. It introduces a simple, practical framework, called Teaching Explanations for Decisions (TED), that provides meaningful explanations that match the mental model of the consumer. We illustrate the generality and effectiveness of this approach with two different examples, resulting in highly accurate explanations with no loss of prediction accuracy for these two examples.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3306618.3314273,
author = {Hind, Michael and Wei, Dennis and Campbell, Murray and Codella, Noel C. F. and Dhurandhar, Amit and Mojsilovi\'{c}, Aleksandra and Natesan Ramamurthy, Karthikeyan and Varshney, Kush R.},
title = {TED: Teaching AI to Explain Its Decisions},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314273},
doi = {10.1145/3306618.3314273},
abstract = {Artificial intelligence systems are being increasingly deployed due to their potential to increase the efficiency, scale, consistency, fairness, and accuracy of decisions. However, as many of these systems are opaque in their operation, there is a growing demand for such systems to provide explanations for their decisions. Conventional approaches to this problem attempt to expose or discover the inner workings of a machine learning model with the hope that the resulting explanations will be meaningful to the consumer. In contrast, this paper suggests a new approach to this problem. It introduces a simple, practical framework, called Teaching Explanations for Decisions (TED), that provides meaningful explanations that match the mental model of the consumer. We illustrate the generality and effectiveness of this approach with two different examples, resulting in highly accurate explanations with no loss of prediction accuracy for these two examples.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {123–129},
numpages = {7},
keywords = {elicitation, AI ethics, supervised classification, meaningful explanation, machine learning, explainable AI},
location = {Honolulu, HI, USA},
series = {AIES '19}
}
  
"
AIES,2019,Compensation at the Crossroads: Autonomous Vehicles and Alternative Victim Compensation Schemes,https://dl.acm.org/doi/10.1145/3306618.3314249,['Tracy Hresko Pearl'],30,"Over the last five years, a small but growing number of vehicle accidents involving fully or partially autonomous vehicles have raised a new and profoundly novel legal issue: who should be liable (if anyone) and how victims should be compensated (if at all) when a vehicle controlled by an algorithm rather than a human driver causes injury. The answer to this question has implications far beyond the resolution of individual autonomous vehicle crash cases. Whether the American legal system is capable of handling these cases fairly and efficiently implicates the likelihood that (a) consumers will adopt autonomous vehicles, and (b) the rate at which they will (or will not) do so. These implications should concern law and policy makers immensely. If autonomous cars stand to drastically reduce the number of fatalities and injuries on U.S. roadways-and virtually every scholar believes that they will-getting the adjudication and compensation aspect of autonomous vehicle injuries ""wrong,"" so to speak, risks stymieing adoption of this technology and leaving more Americans at risk of dying at the hands of human drivers.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3306618.3314249,
author = {Pearl, Tracy Hresko},
title = {Compensation at the Crossroads: Autonomous Vehicles and Alternative Victim Compensation Schemes},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314249},
doi = {10.1145/3306618.3314249},
abstract = {Over the last five years, a small but growing number of vehicle accidents involving fully or partially autonomous vehicles have raised a new and profoundly novel legal issue: who should be liable (if anyone) and how victims should be compensated (if at all) when a vehicle controlled by an algorithm rather than a human driver causes injury. The answer to this question has implications far beyond the resolution of individual autonomous vehicle crash cases. Whether the American legal system is capable of handling these cases fairly and efficiently implicates the likelihood that (a) consumers will adopt autonomous vehicles, and (b) the rate at which they will (or will not) do so. These implications should concern law and policy makers immensely. If autonomous cars stand to drastically reduce the number of fatalities and injuries on U.S. roadways-and virtually every scholar believes that they will-getting the adjudication and compensation aspect of autonomous vehicle injuries ""wrong,"" so to speak, risks stymieing adoption of this technology and leaving more Americans at risk of dying at the hands of human drivers.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {187–193},
numpages = {7},
keywords = {victim compensation fund, emerging technology, law, law and society, law and technology, liability issues, self-driving cars, autonomous vehicles},
location = {Honolulu, HI, USA},
series = {AIES '19}
}
  
"
AIES,2019,Costs and Benefits of Fair Representation Learning,https://dl.acm.org/doi/10.1145/3306618.3317964,"['Daniel McNamara', 'Cheng Soon Ong', 'Robert C. Williamson']",46,"Machine learning algorithms are increasingly used to make or support important decisions about people's lives. This has led to interest in the problem of fair classification, which involves learning to make decisions that are non-discriminatory with respect to a sensitive variable such as race or gender. Several methods have been proposed to solve this problem, including fair representation learning, which cleans the input data used by the algorithm to remove information about the sensitive variable. We show that using fair representation learning as an intermediate step in fair classification incurs a cost compared to directly solving the problem, which we refer to as thecost of mistrust. We show that fair representation learning in fact addresses a different problem, which is of interest when the data user is not trusted to access the sensitive variable. We quantify the benefits of fair representation learning, by showing that any subsequent use of the cleaned data will not be too unfair. The benefits we identify result from restricting the decisions of adversarial data users, while the costs are due to applying those same restrictions to other data users.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3306618.3317964,
author = {McNamara, Daniel and Ong, Cheng Soon and Williamson, Robert C.},
title = {Costs and Benefits of Fair Representation Learning},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3317964},
doi = {10.1145/3306618.3317964},
abstract = {Machine learning algorithms are increasingly used to make or support important decisions about people's lives. This has led to interest in the problem of fair classification, which involves learning to make decisions that are non-discriminatory with respect to a sensitive variable such as race or gender. Several methods have been proposed to solve this problem, including fair representation learning, which cleans the input data used by the algorithm to remove information about the sensitive variable. We show that using fair representation learning as an intermediate step in fair classification incurs a cost compared to directly solving the problem, which we refer to as thecost of mistrust. We show that fair representation learning in fact addresses a different problem, which is of interest when the data user is not trusted to access the sensitive variable. We quantify the benefits of fair representation learning, by showing that any subsequent use of the cleaned data will not be too unfair. The benefits we identify result from restricting the decisions of adversarial data users, while the costs are due to applying those same restrictions to other data users.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {263–270},
numpages = {8},
keywords = {fairness, machine learning, representation learning},
location = {Honolulu, HI, USA},
series = {AIES '19}
}
  
"
AIES,2019,Creating Fair Models of Atherosclerotic Cardiovascular Disease Risk,https://dl.acm.org/doi/10.1145/3306618.3314278,"['Stephen Pfohl', 'Ben Marafino', 'Adrien Coulet', 'Fatima Rodriguez', 'Latha Palaniappan', 'Nigam H. Shah']",48,"Guidelines for the management of atherosclerotic cardiovascular disease (ASCVD) recommend the use of risk stratification models to identify patients most likely to benefit from cholesterol-lowering and other therapies. These models have differential performance across race and gender groups with inconsistent behavior across studies, potentially resulting in an inequitable distribution of beneficial therapy. In this work, we leverage adversarial learning and a large observational cohort extracted from electronic health records (EHRs) to develop a ""fair"" ASCVD risk prediction model with reduced variability in error rates across groups. We empirically demonstrate that our approach is capable of aligning the distribution of risk predictions conditioned on the outcome across several groups simultaneously for models built from high-dimensional EHR data. We also discuss the relevance of these results in the context of the empirical trade-off between fairness and model performance.",NOT,,,,,Might need a second look--case study of a particular areas,,,,,,,,,,,"@inproceedings{10.1145/3306618.3314278,
author = {Pfohl, Stephen and Marafino, Ben and Coulet, Adrien and Rodriguez, Fatima and Palaniappan, Latha and Shah, Nigam H.},
title = {Creating Fair Models of Atherosclerotic Cardiovascular Disease Risk},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314278},
doi = {10.1145/3306618.3314278},
abstract = {Guidelines for the management of atherosclerotic cardiovascular disease (ASCVD) recommend the use of risk stratification models to identify patients most likely to benefit from cholesterol-lowering and other therapies. These models have differential performance across race and gender groups with inconsistent behavior across studies, potentially resulting in an inequitable distribution of beneficial therapy. In this work, we leverage adversarial learning and a large observational cohort extracted from electronic health records (EHRs) to develop a ""fair"" ASCVD risk prediction model with reduced variability in error rates across groups. We empirically demonstrate that our approach is capable of aligning the distribution of risk predictions conditioned on the outcome across several groups simultaneously for models built from high-dimensional EHR data. We also discuss the relevance of these results in the context of the empirical trade-off between fairness and model performance.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {271–278},
numpages = {8},
keywords = {adversarial learning, machine learning, electronic health records, cardiovascular disease, risk prediction, fairness},
location = {Honolulu, HI, USA},
series = {AIES '19}
}
  
"
AIES,2019,Equalized Odds Implies Partially Equalized Outcomes Under Realistic Assumptions,https://dl.acm.org/doi/10.1145/3306618.3314290,['Daniel McNamara'],5,"Equalized odds -- where the true positive rates and false positive rates are equal across groups (e.g. racial groups) -- is a common quantitative measure of fairness. Equalized outcomes -- where the difference in predicted outcomes between groups is less than the difference observed in the training data -- is more contentious, because it is incompatible with perfectly accurate predictions. We formalize and quantify the relationship between these two important but seemingly distinct notions of fairness. We show that under realistic assumptions, equalized odds implies partially equalized outcomes. We prove a comparable result for approximately equalized odds. In addition, we generalize a well-known previous result about the incompatibility of equalized odds and another definition of fairness known as calibration, by showing that partially equalized outcomes implies non-calibration. Our results highlight the risks of using trends observed across groups to make predictions about individuals.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3306618.3314290,
author = {McNamara, Daniel},
title = {Equalized Odds Implies Partially Equalized Outcomes Under Realistic Assumptions},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314290},
doi = {10.1145/3306618.3314290},
abstract = {Equalized odds -- where the true positive rates and false positive rates are equal across groups (e.g. racial groups) -- is a common quantitative measure of fairness. Equalized outcomes -- where the difference in predicted outcomes between groups is less than the difference observed in the training data -- is more contentious, because it is incompatible with perfectly accurate predictions. We formalize and quantify the relationship between these two important but seemingly distinct notions of fairness. We show that under realistic assumptions, equalized odds implies partially equalized outcomes. We prove a comparable result for approximately equalized odds. In addition, we generalize a well-known previous result about the incompatibility of equalized odds and another definition of fairness known as calibration, by showing that partially equalized outcomes implies non-calibration. Our results highlight the risks of using trends observed across groups to make predictions about individuals.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {313–320},
numpages = {8},
keywords = {fairness, equalized odds, machine learning, equalized outcomes},
location = {Honolulu, HI, USA},
series = {AIES '19}
}
  
"
AIES,2019,Actionable Auditing: Investigating the Impact of Publicly Naming Biased Performance Results of Commercial AI Products,https://dl.acm.org/doi/10.1145/3306618.3314244,"['Inioluwa Deborah Raji', 'Joy Buolamwini']",341,"Although algorithmic auditing has emerged as a key strategy to expose systematic biases embedded in software platforms, we struggle to understand the real-world impact of these audits, as scholarship on the impact of algorithmic audits on increasing algorithmic fairness and transparency in commercial systems is nascent. To analyze the impact of publicly naming and disclosing performance results of biased AI systems, we investigate the commercial impact of Gender Shades, the first algorithmic audit of gender and skin type performance disparities in commercial facial analysis models. This paper 1) outlines the audit design and structured disclosure procedure used in the Gender Shades study, 2) presents new performance metrics from targeted companies IBM, Microsoft and Megvii (Face++) on the Pilot Parliaments Benchmark (PPB) as of August 2018, 3) provides performance results on PPB by non-target companies Amazon and Kairos and, 4) explores differences in company responses as shared through corporate communications that contextualize differences in performance on PPB. Within 7 months of the original audit, we find that all three targets released new API versions. All targets reduced accuracy disparities between males and females and darker and lighter-skinned subgroups, with the most significant update occurring for the darker-skinned female subgroup, that underwent a 17.7% - 30.4% reduction in error between audit periods. Minimizing these disparities led to a 5.72% to 8.3% reduction in overall error on the Pilot Parliaments Benchmark (PPB) for target corporation APIs. The overall performance of non-targets Amazon and Kairos lags significantly behind that of the targets, with error rates of 8.66% and 6.60% overall, and error rates of 31.37% and 22.50% for the darker female subgroup, respectively.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3306618.3314244,
author = {Raji, Inioluwa Deborah and Buolamwini, Joy},
title = {Actionable Auditing: Investigating the Impact of Publicly Naming Biased Performance Results of Commercial AI Products},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314244},
doi = {10.1145/3306618.3314244},
abstract = {Although algorithmic auditing has emerged as a key strategy to expose systematic biases embedded in software platforms, we struggle to understand the real-world impact of these audits, as scholarship on the impact of algorithmic audits on increasing algorithmic fairness and transparency in commercial systems is nascent. To analyze the impact of publicly naming and disclosing performance results of biased AI systems, we investigate the commercial impact of Gender Shades, the first algorithmic audit of gender and skin type performance disparities in commercial facial analysis models. This paper 1) outlines the audit design and structured disclosure procedure used in the Gender Shades study, 2) presents new performance metrics from targeted companies IBM, Microsoft and Megvii (Face++) on the Pilot Parliaments Benchmark (PPB) as of August 2018, 3) provides performance results on PPB by non-target companies Amazon and Kairos and, 4) explores differences in company responses as shared through corporate communications that contextualize differences in performance on PPB. Within 7 months of the original audit, we find that all three targets released new API versions. All targets reduced accuracy disparities between males and females and darker and lighter-skinned subgroups, with the most significant update occurring for the darker-skinned female subgroup, that underwent a 17.7% - 30.4% reduction in error between audit periods. Minimizing these disparities led to a 5.72% to 8.3% reduction in overall error on the Pilot Parliaments Benchmark (PPB) for target corporation APIs. The overall performance of non-targets Amazon and Kairos lags significantly behind that of the targets, with error rates of 8.66% and 6.60% overall, and error rates of 31.37% and 22.50% for the darker female subgroup, respectively.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {429–435},
numpages = {7},
keywords = {ethics, computer vision, facial recognition, machine learning, commercial applications, fairness, artificial intelligence},
location = {Honolulu, HI, USA},
series = {AIES '19}
}
  
"
AIES,2019,A Framework for Benchmarking Discrimination-Aware Models in Machine Learning,https://dl.acm.org/doi/10.1145/3306618.3314262,"['Rodrigo L. Cardoso', 'Wagner Meira Jr.', 'Virgilio Almeida', 'Mohammed J. Zaki']",12,"Discrimination-aware models in machine learning are a recent topic of study that aim to minimize the adverse impact of machine learning decisions for certain groups of people due to ethical and legal implications. We propose a benchmark framework for assessing discrimination-aware models. Our framework consists of systematically generated biased datasets that are similar to real world data, created by a Bayesian network approach. Experimental results show that we can assess the quality of techniques through known metrics of discrimination, and our flexible framework can be extended to most real datasets and fairness measures to support a diversity of assessments.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3306618.3314262,
author = {L. Cardoso, Rodrigo and Meira Jr., Wagner and Almeida, Virgilio and J. Zaki, Mohammed},
title = {A Framework for Benchmarking Discrimination-Aware Models in Machine Learning},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314262},
doi = {10.1145/3306618.3314262},
abstract = {Discrimination-aware models in machine learning are a recent topic of study that aim to minimize the adverse impact of machine learning decisions for certain groups of people due to ethical and legal implications. We propose a benchmark framework for assessing discrimination-aware models. Our framework consists of systematically generated biased datasets that are similar to real world data, created by a Bayesian network approach. Experimental results show that we can assess the quality of techniques through known metrics of discrimination, and our flexible framework can be extended to most real datasets and fairness measures to support a diversity of assessments.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {437–444},
numpages = {8},
keywords = {discrimination-aware benchmarks, fairness-aware data mining, disparate impact, Bayesian networks, disparate mistreatment},
location = {Honolulu, HI, USA},
series = {AIES '19}
}
  
"
AIES,2019,Towards a Just Theory of Measurement: A Principled Social Measurement Assurance Program for Machine Learning,https://dl.acm.org/doi/10.1145/3306618.3314275,"['McKane Andrus', 'Thomas K. Gilbert']",4,"While formal definitions of fairness in machine learning (ML) have been proposed, its place within a broader institutional model of fair decision-making remains ambiguous. In this paper we interpret ML as a tool for revealing when and how measures fail to capture purported constructs of interest, augmenting a given institution's understanding of its own interventions and priorities. Rather than codifying ""fair"" principles into ML models directly, the use of ML can thus be understood as a form of quality assurance for existing institutions, exposing the epistemic fault lines of their own measurement practices. Drawing from Friedler et al's [2016] recent discussion of representational mappings and previous discussions on the ontology of measurement, we propose a social measurement assurance program (sMAP) in which ML encourages expert deliberation on a given decision-making procedure by examining unanticipated or previously unexamined covariates. As an example, we apply Rawlsian principles of fairness to sMAP and produce a provisional just theory of measurement that would guide the use of ML for achieving fairness in the case of child abuse in Allegheny County.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3306618.3314275,
author = {Andrus, McKane and Gilbert, Thomas K.},
title = {Towards a Just Theory of Measurement: A Principled Social Measurement Assurance Program for Machine Learning},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314275},
doi = {10.1145/3306618.3314275},
abstract = {While formal definitions of fairness in machine learning (ML) have been proposed, its place within a broader institutional model of fair decision-making remains ambiguous. In this paper we interpret ML as a tool for revealing when and how measures fail to capture purported constructs of interest, augmenting a given institution's understanding of its own interventions and priorities. Rather than codifying ""fair"" principles into ML models directly, the use of ML can thus be understood as a form of quality assurance for existing institutions, exposing the epistemic fault lines of their own measurement practices. Drawing from Friedler et al's [2016] recent discussion of representational mappings and previous discussions on the ontology of measurement, we propose a social measurement assurance program (sMAP) in which ML encourages expert deliberation on a given decision-making procedure by examining unanticipated or previously unexamined covariates. As an example, we apply Rawlsian principles of fairness to sMAP and produce a provisional just theory of measurement that would guide the use of ML for achieving fairness in the case of child abuse in Allegheny County.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {445–451},
numpages = {7},
keywords = {institutional decision-making, measurement assurance, machine learning, fairness, measurement, justice, representational measurement, rawls},
location = {Honolulu, HI, USA},
series = {AIES '19}
}
  
"
AIES,2020,Machines Judging Humans: The Promise and Perils of Formalizing Evaluative Criteria,https://dl.acm.org/doi/10.1145/3375627.3375839,['Frank Pasquale'],52,"Over the past decade, algorithmic accountability has become an important concern for social scientists, computer scientists, journalists, and lawyers [1]. Expos√©s have sparked vibrant debates about algorithmic sentencing. Researchers have exposed tech giants showing women ads for lower-paying jobs, discriminating against the aged, deploying deceptive dark patterns to trick consumers into buying things, and manipulating users toward rabbit holes of extremist content. Public-spirited regulators have begun to address algorithmic transparency and online fairness, building on the work of legal scholars who have called for technological due process, platform neutrality, and nondiscrimination principles. This policy work is just beginning, as experts translate academic research and activist demands into statutes and regulations. Lawmakers are proposing bills requiring basic standards of algorithmic transparency and auditing. We are starting down on a long road toward ensuring that AI-based hiring practices and financial underwriting are not used if they have a disparate impact on historically marginalized communities. And just as this ""first wave"" of algorithmic accountability research and activism has targeted existing systems, an emerging ""second wave"" of algorithmic accountability has begun to address more structural concerns. Both waves will be essential to ensure a fairer, and more genuinely emancipatory, political economy of technology. Second wave work is particularly important when it comes to illuminating the promise & perils of formalizing evaluative criteria.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3375627.3375839,
author = {Pasquale, Frank},
title = {Machines Judging Humans: The Promise and Perils of Formalizing Evaluative Criteria},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375839},
doi = {10.1145/3375627.3375839},
abstract = {Over the past decade, algorithmic accountability has become an important concern for social scientists, computer scientists, journalists, and lawyers [1]. Expos\'{e}s have sparked vibrant debates about algorithmic sentencing. Researchers have exposed tech giants showing women ads for lower-paying jobs, discriminating against the aged, deploying deceptive dark patterns to trick consumers into buying things, and manipulating users toward rabbit holes of extremist content. Public-spirited regulators have begun to address algorithmic transparency and online fairness, building on the work of legal scholars who have called for technological due process, platform neutrality, and nondiscrimination principles.This policy work is just beginning, as experts translate academic research and activist demands into statutes and regulations. Lawmakers are proposing bills requiring basic standards of algorithmic transparency and auditing. We are starting down on a long road toward ensuring that AI-based hiring practices and financial underwriting are not used if they have a disparate impact on historically marginalized communities. And just as this ""first wave"" of algorithmic accountability research and activism has targeted existing systems, an emerging ""second wave"" of algorithmic accountability has begun to address more structural concerns. Both waves will be essential to ensure a fairer, and more genuinely emancipatory, political economy of technology. Second wave work is particularly important when it comes to illuminating the promise & perils of formalizing evaluative criteria.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {7},
numpages = {1},
keywords = {opportunity, ai for good, bias, accountability, emancipatory computing, anticipatory social research, inequality, egalitarianism, fatml, ai},
location = {New York, NY, USA},
series = {AIES '20}
}
  
"
AIES,2020,Algorithmic Fairness from a Non-ideal Perspective,https://dl.acm.org/doi/10.1145/3375627.3375828,"['Sina Fazelpour', 'Zachary C. Lipton']",63,"Inspired by recent breakthroughs in predictive modeling, practitioners in both industry and government have turned to machine learning with hopes of operationalizing predictions to drive automated decisions. Unfortunately, many social desiderata concerning consequential decisions, such as justice or fairness, have no natural formulation within a purely predictive framework. In the hopes of mitigating these problems, researchers have proposed a variety of metrics for quantifying deviations from various statistical parities that we might hope to observe in a fair world, offering a variety of algorithms that attempt to satisfy subsets of these parities or to trade off the degree to which they are satisfied against utility. In this paper, we connect this approach to fair machine learning to the literature on ideal and non-ideal methodological approaches in political philosophy. The ideal approach requires positing the principles according to which a just world would operate. In the most straightforward application of ideal theory, one supports a proposed policy by arguing that it closes a discrepancy between the real and ideal worlds. However, by failing to account for the mechanisms by which our non-ideal world arose, the responsibilities of various decision-makers, and the impacts of their actions, naive applications of ideal thinking can lead to misguided policies. In this paper, we demonstrate a connection between the recent literature on fair machine learning and the ideal approach in political philosophy, and show that some recently uncovered shortcomings in proposed algorithms reflect broader troubles faced by the ideal approach. We work this analysis through for different formulations of fairness and conclude with a critical discussion of real-world impacts and directions for new research.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3375627.3375828,
author = {Fazelpour, Sina and Lipton, Zachary C.},
title = {Algorithmic Fairness from a Non-Ideal Perspective},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375828},
doi = {10.1145/3375627.3375828},
abstract = {Inspired by recent breakthroughs in predictive modeling, practitioners in both industry and government have turned to machine learning with hopes of operationalizing predictions to drive automated decisions. Unfortunately, many social desiderata concerning consequential decisions, such as justice or fairness, have no natural formulation within a purely predictive framework. In the hopes of mitigating these problems, researchers have proposed a variety of metrics for quantifying deviations from various statistical parities that we might hope to observe in a fair world, offering a variety of algorithms that attempt to satisfy subsets of these parities or to trade off the degree to which they are satisfied against utility. In this paper, we connect this approach to fair machine learning to the literature on ideal and non-ideal methodological approaches in political philosophy. The ideal approach requires positing the principles according to which a just world would operate. In the most straightforward application of ideal theory, one supports a proposed policy by arguing that it closes a discrepancy between the real and ideal worlds. However, by failing to account for the mechanisms by which our non-ideal world arose, the responsibilities of various decision-makers, and the impacts of their actions, naive applications of ideal thinking can lead to misguided policies. In this paper, we demonstrate a connection between the recent literature on fair machine learning and the ideal approach in political philosophy, and show that some recently uncovered shortcomings in proposed algorithms reflect broader troubles faced by the ideal approach. We work this analysis through for different formulations of fairness and conclude with a critical discussion of real-world impacts and directions for new research.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {57–63},
numpages = {7},
keywords = {political philosophy, fairness in machine learning, justice, algorithmic decision-making},
location = {New York, NY, USA},
series = {AIES '20}
}
  
"
AIES,2020,When Your Only Tool Is A Hammer: Ethical Limitations of Algorithmic Fairness Solutions in Healthcare Machine Learning,https://dl.acm.org/doi/10.1145/3375627.3375824,"['Melissa McCradden', 'Mjaye Mazwi', 'Shalmali Joshi', 'James A. Anderson']",14,"It is no longer a hypothetical worry that artificial intelligence - more specifically, machine learning (ML) - can propagate the effects of pernicious bias in healthcare. To address these problems, some have proposed the development of 'algorithmic fairness' solutions. The primary goal of these solutions is to constrain the effect of pernicious bias with respect to a given outcome of interest as a function of one's protected identity (i.e., characteristics generally protected by civil or human rights legislation. The technical limitations of these solutions have been well-characterized. Ethically, the problematic implication - of developers, potentially, and end users - is that by virtue of algorithmic fairness solutions a model can be rendered 'objective' (i.e., free from the influence of pernicious bias). The ostensible neutrality of these solutions may unintentionally prompt new consequences for vulnerable groups by obscuring downstream problems due to the persistence of real-world bias. The main epistemic limitation of algorithmic fairness is that it assumes the relationship between the extent of bias's impact on a given health outcome and one's protected identity is mathematically quantifiable. The reality is that social and structural factors confluence in complex and unknown ways to produce health inequalities. Some of these are biologic in nature, and differences like these are directly relevant to predicting a health event and should be incorporated into the model's design. Others are reflective of prejudice, lack of access to healthcare, or implicit bias. Sometimes, there may be a combination. With respect to any specific task, it is difficult to untangle the complex relationships between potentially influential factors and which ones are 'fair' and which are not to inform their inclusion or mitigation in the model's design.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3375627.3375824,
author = {McCradden, Melissa and Mazwi, Mjaye and Joshi, Shalmali and Anderson, James A.},
title = {When Your Only Tool Is A Hammer: Ethical Limitations of Algorithmic Fairness Solutions in Healthcare Machine Learning},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375824},
doi = {10.1145/3375627.3375824},
abstract = {It is no longer a hypothetical worry that artificial intelligence - more specifically, machine learning (ML) - can propagate the effects of pernicious bias in healthcare. To address these problems, some have proposed the development of 'algorithmic fairness' solutions. The primary goal of these solutions is to constrain the effect of pernicious bias with respect to a given outcome of interest as a function of one's protected identity (i.e., characteristics generally protected by civil or human rights legislation. The technical limitations of these solutions have been well-characterized. Ethically, the problematic implication - of developers, potentially, and end users - is that by virtue of algorithmic fairness solutions a model can be rendered 'objective' (i.e., free from the influence of pernicious bias). The ostensible neutrality of these solutions may unintentionally prompt new consequences for vulnerable groups by obscuring downstream problems due to the persistence of real-world bias.The main epistemic limitation of algorithmic fairness is that it assumes the relationship between the extent of bias's impact on a given health outcome and one's protected identity is mathematically quantifiable. The reality is that social and structural factors confluence in complex and unknown ways to produce health inequalities. Some of these are biologic in nature, and differences like these are directly relevant to predicting a health event and should be incorporated into the model's design. Others are reflective of prejudice, lack of access to healthcare, or implicit bias. Sometimes, there may be a combination. With respect to any specific task, it is difficult to untangle the complex relationships between potentially influential factors and which ones are 'fair' and which are not to inform their inclusion or mitigation in the model's design.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {109},
numpages = {1},
keywords = {racism, machine learning, healthcare, sexism, bias, discrimination, bioethics, algorithmic fairness, ethics, medicine},
location = {New York, NY, USA},
series = {AIES '20}
}
  
"
AIES,2020,Human Comprehension of Fairness in Machine Learning,https://dl.acm.org/doi/10.1145/3375627.3375819,"['Debjani Saha', 'Candice Schumann', 'Duncan C. McElfresh', 'John P. Dickerson', 'Michelle L. Mazurek', 'Michael Carl Tschantz']",14,"Bias in machine learning has manifested injustice in several areas, with notable examples including gender bias in job-related ads [4], racial bias in evaluating names on resumes [3], and racial bias in predicting criminal recidivism [1]. In response, research into algorithmic fairness has grown in both importance and volume over the past few years. Different metrics and approaches to algorithmic fairness have been proposed, many of which are based on prior legal and philosophical concepts [2]. The rapid expansion of this field makes it difficult for professionals to keep up, let alone the general public. Furthermore, misinformation about notions of fairness can have significant legal implications. Computer scientists have largely focused on developing mathematical notions of fairness and incorporating them in fielded ML systems. A much smaller collection of studies has measured public perception of bias and (un)fairness in algorithmic decision-making. However, one major question underlying the study of ML fairness remains unanswered in the literature: Does the general public understand mathematical definitions of ML fairness and their behavior in ML applications? We take a first step towards answering this question by studying non-expert comprehension and perceptions of one popular definition of ML fairness, demographic parity [5]. Specifically, we developed an online survey to address the following: (1) Does a non-technical audience comprehend the definition and implications of demographic parity? (2) Do demographics play a role in comprehension? (3) How are comprehension and sentiment related? (4) Does the application scenario affect comprehension?",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3375627.3375819,
author = {Saha, Debjani and Schumann, Candice and McElfresh, Duncan C. and Dickerson, John P. and Mazurek, Michelle L. and Tschantz, Michael Carl},
title = {Human Comprehension of Fairness in Machine Learning},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375819},
doi = {10.1145/3375627.3375819},
abstract = {Bias in machine learning has manifested injustice in several areas, with notable examples including gender bias in job-related ads [4], racial bias in evaluating names on resumes [3], and racial bias in predicting criminal recidivism [1]. In response, research into algorithmic fairness has grown in both importance and volume over the past few years. Different metrics and approaches to algorithmic fairness have been proposed, many of which are based on prior legal and philosophical concepts [2]. The rapid expansion of this field makes it difficult for professionals to keep up, let alone the general public. Furthermore, misinformation about notions of fairness can have significant legal implications.Computer scientists have largely focused on developing mathematical notions of fairness and incorporating them in fielded ML systems. A much smaller collection of studies has measured public perception of bias and (un)fairness in algorithmic decision-making. However, one major question underlying the study of ML fairness remains unanswered in the literature: Does the general public understand mathematical definitions of ML fairness and their behavior in ML applications? We take a first step towards answering this question by studying non-expert comprehension and perceptions of one popular definition of ML fairness, demographic parity [5]. Specifically, we developed an online survey to address the following: (1) Does a non-technical audience comprehend the definition and implications of demographic parity? (2) Do demographics play a role in comprehension? (3) How are comprehension and sentiment related? (4) Does the application scenario affect comprehension?},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {152},
numpages = {1},
keywords = {human-computer interaction, empirical study, fair machine learning, algorithmic bias},
location = {New York, NY, USA},
series = {AIES '20}
}
  
"
AIES,2020,AI and Holistic Review: Informing Human Reading in College Admissions,https://dl.acm.org/doi/10.1145/3375627.3375871,"['A.J. Alvero', 'Noah Arthurs', 'anthony lising antonio', 'Benjamin W. Domingue', 'Ben Gebre-Medhin', 'Sonia Giebel', 'Mitchell L. Stevens']",17,"College admissions in the United States is carried out by a human-centered method of evaluation known as holistic review, which typically involves reading original narrative essays submitted by each applicant. The legitimacy and fairness of holistic review, which gives human readers significant discretion over determining each applicant's fitness for admission, has been repeatedly challenged in courtrooms and the public sphere. Using a unique corpus of 283,676 application essays submitted to a large, selective, state university system between 2015 and 2016, we assess the extent to which applicant demographic characteristics can be inferred from application essays. We find a relatively interpretable classifier (logistic regression) was able to predict gender and household income with high levels of accuracy. Findings suggest that data auditing might be useful in informing holistic review, and perhaps other evaluative systems, by checking potential bias in human or computational readings.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3375627.3375871,
author = {Alvero, A.J. and Arthurs, Noah and antonio, anthony lising and Domingue, Benjamin W. and Gebre-Medhin, Ben and Giebel, Sonia and Stevens, Mitchell L.},
title = {AI and Holistic Review: Informing Human Reading in College Admissions},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375871},
doi = {10.1145/3375627.3375871},
abstract = {College admissions in the United States is carried out by a human-centered method of evaluation known as holistic review, which typically involves reading original narrative essays submitted by each applicant. The legitimacy and fairness of holistic review, which gives human readers significant discretion over determining each applicant's fitness for admission, has been repeatedly challenged in courtrooms and the public sphere. Using a unique corpus of 283,676 application essays submitted to a large, selective, state university system between 2015 and 2016, we assess the extent to which applicant demographic characteristics can be inferred from application essays. We find a relatively interpretable classifier (logistic regression) was able to predict gender and household income with high levels of accuracy. Findings suggest that data auditing might be useful in informing holistic review, and perhaps other evaluative systems, by checking potential bias in human or computational readings.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {200–206},
numpages = {7},
keywords = {supervised learning, natural language processing, holistic review, college admissions, fairness, bias, data auditing, text analysis},
location = {New York, NY, USA},
series = {AIES '20}
}
  
"
AIES,2020,Auditing Algorithms: On Lessons Learned and the Risks of Data Minimization,https://dl.acm.org/doi/10.1145/3375627.3375852,"['Gemma Galdon Clavell', 'Mariano Mart√≠n Zamorano', 'Carlos Castillo', 'Oliver Smith', 'Aleksandar Matic']",15,"In this paper, we present the Algorithmic Audit (AA) of REM!X, a personalized well-being recommendation app developed by Telef√≥nica Innovaci√≥n Alpha. The main goal of the AA was to identify and mitigate algorithmic biases in the recommendation system that could lead to the discrimination of protected groups. The audit was conducted through a qualitative methodology that included five focus groups with developers and a digital ethnography relying on users comments reported in the Google Play Store. To minimize the collection of personal information, as required by best practice and the GDPR [1], the REM!X app did not collect gender, age, race, religion, or other protected attributes from its users. This limited the algorithmic assessment and the ability to control for different algorithmic biases. Indirect evidence was thus used as a partial mitigation for the lack of data on protected attributes, and allowed the AA to identify four domains where bias and discrimination were still possible, even without direct personal identifiers. Our analysis provides important insights into how general data ethics principles such as data minimization, fairness, non-discrimination and transparency can be operationalized via algorithmic auditing, their potential and limitations, and how the collaboration between developers and algorithmic auditors can lead to better technologies",NOT,,Auditing,Case Study,,Might need a second look,,,,,,,,,,,"@inproceedings{10.1145/3375627.3375852,
author = {Galdon Clavell, Gemma and Mart\'{\i}n Zamorano, Mariano and Castillo, Carlos and Smith, Oliver and Matic, Aleksandar},
title = {Auditing Algorithms: On Lessons Learned and the Risks of Data Minimization},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375852},
doi = {10.1145/3375627.3375852},
abstract = {In this paper, we present the Algorithmic Audit (AA) of REM!X, a personalized well-being recommendation app developed by Telef\'{o}nica Innovaci\'{o}n Alpha. The main goal of the AA was to identify and mitigate algorithmic biases in the recommendation system that could lead to the discrimination of protected groups. The audit was conducted through a qualitative methodology that included five focus groups with developers and a digital ethnography relying on users comments reported in the Google Play Store. To minimize the collection of personal information, as required by best practice and the GDPR [1], the REM!X app did not collect gender, age, race, religion, or other protected attributes from its users. This limited the algorithmic assessment and the ability to control for different algorithmic biases. Indirect evidence was thus used as a partial mitigation for the lack of data on protected attributes, and allowed the AA to identify four domains where bias and discrimination were still possible, even without direct personal identifiers. Our analysis provides important insights into how general data ethics principles such as data minimization, fairness, non-discrimination and transparency can be operationalized via algorithmic auditing, their potential and limitations, and how the collaboration between developers and algorithmic auditors can lead to better technologies},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {265–271},
numpages = {7},
keywords = {bias, algorithms, ai, recommender systems, gdpr, data ethics},
location = {New York, NY, USA},
series = {AIES '20}
}
  
"
AIES,2020,The Perils of Objectivity: Towards a Normative Framework for Fair Judicial Decision-Making,https://dl.acm.org/doi/10.1145/3375627.3375869,"['Andi Peng', 'Malina Simard-Halm']",0,"Fair decision-making in criminal justice relies on the recognition and incorporation of infinite shades of grey. In this paper, we detail how algorithmic risk assessment tools are counteractive to fair legal proceedings in social institutions where desired states of the world are contested ethically and practically. We provide a normative framework for assessing fair judicial decision-making, one that does not seek the elimination of human bias from decision-making as algorithmic fairness efforts currently focus on, but instead centers on sophisticating the incorporation of individualized or discretionary bias--a process that is requisitely human. Through analysis of a case study on social disadvantage, we use this framework to provide an assessment of potential features of consideration, such as political disempowerment and demographic exclusion, that are irreconcilable by current algorithmic efforts and recommend their incorporation in future reform.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3375627.3375869,
author = {Peng, Andi and Simard-Halm, Malina},
title = {The Perils of Objectivity: Towards a Normative Framework for Fair Judicial Decision-Making},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375869},
doi = {10.1145/3375627.3375869},
abstract = {Fair decision-making in criminal justice relies on the recognition and incorporation of infinite shades of grey. In this paper, we detail how algorithmic risk assessment tools are counteractive to fair legal proceedings in social institutions where desired states of the world are contested ethically and practically. We provide a normative framework for assessing fair judicial decision-making, one that does not seek the elimination of human bias from decision-making as algorithmic fairness efforts currently focus on, but instead centers on sophisticating the incorporation of individualized or discretionary bias--a process that is requisitely human. Through analysis of a case study on social disadvantage, we use this framework to provide an assessment of potential features of consideration, such as political disempowerment and demographic exclusion, that are irreconcilable by current algorithmic efforts and recommend their incorporation in future reform.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {343},
numpages = {1},
keywords = {fairness, criminal justice, bias, decision-making, risk assessment},
location = {New York, NY, USA},
series = {AIES '20}
}
  
"
AIES,2021,Artificial Intelligence and the Purpose of Social Systems,https://dl.acm.org/doi/10.1145/3461702.3462526,"['Sebastian Benthall', 'Jake Goldenfein']",7,"The law and ethics of Western democratic states have their basis in liberalism. This extends to regulation and ethical discussion of technology and businesses doing data processing. Liberalism relies on the privacy and autonomy of individuals, their ordering through a public market, and, more recently, a measure of equality guaranteed by the state. We argue that these forms of regulation and ethical analysis are largely incompatible with the techno-political and techno-economic dimensions of artificial intelligence. By analyzing liberal regulatory solutions in the form of privacy and data protection, regulation of public markets, and fairness in AI, we expose how the data economy and artificial intelligence have transcended liberal legal imagination. Organizations use artificial intelligence to exceed the bounded rationality of individuals and each other. This has led to the private consolidation of markets and an unequal hierarchy of control operating mainly for the purpose of shareholder value. An artificial intelligence will be only as ethical as the purpose of the social system that operates it. Inspired by the science of artificial life as an alternative to artificial intelligence, we consider data intermediaries: sociotechnical systems composed of individuals associated around collectively pursued purposes. An attention cooperative, that prioritizes its incoming and outgoing data flows, is one model of a social system that could form and maintain its own autonomous purpose.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3461702.3462526,
author = {Benthall, Sebastian and Goldenfein, Jake},
title = {Artificial Intelligence and the Purpose of Social Systems},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462526},
doi = {10.1145/3461702.3462526},
abstract = {The law and ethics of Western democratic states have their basis in liberalism. This extends to regulation and ethical discussion of technology and businesses doing data processing. Liberalism relies on the privacy and autonomy of individuals, their ordering through a public market, and, more recently, a measure of equality guaranteed by the state. We argue that these forms of regulation and ethical analysis are largely incompatible with the techno-political and techno-economic dimensions of artificial intelligence. By analyzing liberal regulatory solutions in the form of privacy and data protection, regulation of public markets, and fairness in AI, we expose how the data economy and artificial intelligence have transcended liberal legal imagination. Organizations use artificial intelligence to exceed the bounded rationality of individuals and each other. This has led to the private consolidation of markets and an unequal hierarchy of control operating mainly for the purpose of shareholder value. An artificial intelligence will be only as ethical as the purpose of the social system that operates it. Inspired by the science of artificial life as an alternative to artificial intelligence, we consider data intermediaries: sociotechnical systems composed of individuals associated around collectively pursued purposes. An attention cooperative, that prioritizes its incoming and outgoing data flows, is one model of a social system that could form and maintain its own autonomous purpose.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {3–12},
numpages = {10},
keywords = {liberalism, platforms, economics, intermediaries, privacy, artificial intelligence, cybernetics},
location = {Virtual Event, USA},
series = {AIES '21}
}
  
"
AIES,2021,Emergent Unfairness in Algorithmic Fairness-Accuracy Trade-Off Research,https://dl.acm.org/doi/10.1145/3461702.3462519,"['A. Feder Cooper', 'Ellen Abrams', 'NA NA']",20,"Across machine learning (ML) sub-disciplines, researchers make explicit mathematical assumptions in order to facilitate proof-writing. We note that, specifically in the area of fairness-accuracy trade-off optimization scholarship, similar attention is not paid to the normative assumptions that ground this approach. Such assumptions presume that 1) accuracy and fairness are in inherent opposition to one another, 2) strict notions of mathematical equality can adequately model fairness, 3) it is possible to measure the accuracy and fairness of decisions independent from historical context, and 4) collecting more data on marginalized individuals is a reasonable solution to mitigate the effects of the trade-off. We argue that such assumptions, which are often left implicit and unexamined, lead to inconsistent conclusions: While the intended goal of this work may be to improve the fairness of machine learning models, these unexamined, implicit assumptions can in fact result in emergent unfairness. We conclude by suggesting a concrete path forward toward a potential resolution.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3461702.3462519,
author = {Cooper, A. Feder and Abrams, Ellen and NA, NA},
title = {Emergent Unfairness in Algorithmic Fairness-Accuracy Trade-Off Research},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462519},
doi = {10.1145/3461702.3462519},
abstract = {Across machine learning (ML) sub-disciplines, researchers make explicit mathematical assumptions in order to facilitate proof-writing. We note that, specifically in the area of fairness-accuracy trade-off optimization scholarship, similar attention is not paid to the normative assumptions that ground this approach. Such assumptions presume that 1) accuracy and fairness are in inherent opposition to one another, 2) strict notions of mathematical equality can adequately model fairness, 3) it is possible to measure the accuracy and fairness of decisions independent from historical context, and 4) collecting more data on marginalized individuals is a reasonable solution to mitigate the effects of the trade-off. We argue that such assumptions, which are often left implicit and unexamined, lead to inconsistent conclusions: While the intended goal of this work may be to improve the fairness of machine learning models, these unexamined, implicit assumptions can in fact result in emergent unfairness. We conclude by suggesting a concrete path forward toward a potential resolution.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {46–54},
numpages = {9},
keywords = {algorithmic fairness, machine learning, societal implications of AI},
location = {Virtual Event, USA},
series = {AIES '21}
}
  
"
AIES,2021,Reflexive Design for Fairness and Other Human Values in Formal Models,https://dl.acm.org/doi/10.1145/3461702.3462518,"['Benjamin Fish', 'Luke Stark']",14,"Algorithms and other formal models purportedly incorporating human values like fairness have grown increasingly popular in computer science. In response to sociotechnical challenges in the use of these models, designers and researchers have taken widely divergent positions on how formal models incorporating aspects of human values should be used: encouraging their use, moving away from them, or ignoring the normative consequences altogether. In this paper, we seek to resolve these divergent positions by identifying the main conceptual limits of formal modeling, and develop four reflexive values--value fidelity, appropriate accuracy, value legibility, and value contestation--vital for incorporating human values adequately into formal models. We then provide a brief methodology for reflexively designing formal models incorporating human values.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3461702.3462518,
author = {Fish, Benjamin and Stark, Luke},
title = {Reflexive Design for Fairness and Other Human Values in Formal Models},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462518},
doi = {10.1145/3461702.3462518},
abstract = {Algorithms and other formal models purportedly incorporating human values like fairness have grown increasingly popular in computer science. In response to sociotechnical challenges in the use of these models, designers and researchers have taken widely divergent positions on how formal models incorporating aspects of human values should be used: encouraging their use, moving away from them, or ignoring the normative consequences altogether. In this paper, we seek to resolve these divergent positions by identifying the main conceptual limits of formal modeling, and develop four reflexive values--value fidelity, appropriate accuracy, value legibility, and value contestation--vital for incorporating human values adequately into formal models. We then provide a brief methodology for reflexively designing formal models incorporating human values.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {89–99},
numpages = {11},
keywords = {algorithmic fairness, artificial intelligence, reflexive design, values in design, formal models, machine learning},
location = {Virtual Event, USA},
series = {AIES '21}
}
  
"
AIES,2021,Fairness and Data Protection Impact Assessments,https://dl.acm.org/doi/10.1145/3461702.3462528,"['Atoosa Kasirzadeh', 'Damian Clifford']",3,"In this paper, we critically examine the effectiveness of the requirement to conduct a Data Protection Impact Assessment (DPIA) in Article 35 of the General Data Protection Regulation (GDPR) in light of fairness metrics. Through this analysis, we explore the role of the fairness principle as introduced in Article 5(1)(a) and its multifaceted interpretation in the obligation to conduct a DPIA. Our paper argues that although there is a significant theoretical role for the considerations of fairness in the DPIA process, an analysis of the various guidance documents issued by data protection authorities on the obligation to conduct a DPIA reveals that they rarely mention the fairness principle in practice. Our analysis questions this omission, and assesses the capacity of fairness metrics to be truly operationalized within DPIAs. We conclude by exploring the practical effectiveness of DPIA with particular reference to (1) technical challenges that have an impact on the usefulness of DPIAs irrespective of a controller's willingness to actively engage in the process, (2) the context dependent nature of the fairness principle, and (3) the key role played by data controllers in the determination of what is fair.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3461702.3462528,
author = {Kasirzadeh, Atoosa and Clifford, Damian},
title = {Fairness and Data Protection Impact Assessments},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462528},
doi = {10.1145/3461702.3462528},
abstract = {In this paper, we critically examine the effectiveness of the requirement to conduct a Data Protection Impact Assessment (DPIA) in Article 35 of the General Data Protection Regulation (GDPR) in light of fairness metrics. Through this analysis, we explore the role of the fairness principle as introduced in Article 5(1)(a) and its multifaceted interpretation in the obligation to conduct a DPIA. Our paper argues that although there is a significant theoretical role for the considerations of fairness in the DPIA process, an analysis of the various guidance documents issued by data protection authorities on the obligation to conduct a DPIA reveals that they rarely mention the fairness principle in practice. Our analysis questions this omission, and assesses the capacity of fairness metrics to be truly operationalized within DPIAs. We conclude by exploring the practical effectiveness of DPIA with particular reference to (1) technical challenges that have an impact on the usefulness of DPIAs irrespective of a controller's willingness to actively engage in the process, (2) the context dependent nature of the fairness principle, and (3) the key role played by data controllers in the determination of what is fair.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {146–153},
numpages = {8},
keywords = {algorithmic fairness, regulation of artificial intelligence, fairness principle, general data protection regulation, data protection impact assessments, ethics of artificial intelligence},
location = {Virtual Event, USA},
series = {AIES '21}
}
  
"
AIES,2021,Precarity: Modeling the Long Term Effects of Compounded Decisions on Individual Instability,https://dl.acm.org/doi/10.1145/3461702.3462529,"['Pegah Nokhiz', 'Aravinda Kanchana Ruwanpathirana', 'Neal Patwari', 'Suresh Venkatasubramanian']",4,"When it comes to studying the impacts of decision making, the research has been largely focused on examining the fairness of the decisions, the long-term effects of the decision pipelines, and utility-based perspectives considering both the decision-maker and the individuals. However, there has hardly been any focus on precarity which is the term that encapsulates the instability in people's lives. That is, a negative outcome can overspread to other decisions and measures of well-being. Studying precarity necessitates a shift in focus -- from the point of view of the decision-maker to the perspective of the decision subject. This centering of the subject is an important direction that unlocks the importance of parting with aggregate measures to examine the long-term effects of decision making. To address this issue, in this paper, we propose a modeling framework that simulates the effects of compounded decision-making on precarity over time. Through our simulations, we are able to show the heterogeneity of precarity by the non-uniform ruinous aftereffects of negative decisions on different income classes of the underlying population and how policy interventions can help mitigate such effects.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3461702.3462529,
author = {Nokhiz, Pegah and Ruwanpathirana, Aravinda Kanchana and Patwari, Neal and Venkatasubramanian, Suresh},
title = {Precarity: Modeling the Long Term Effects of Compounded Decisions on Individual Instability},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462529},
doi = {10.1145/3461702.3462529},
abstract = {When it comes to studying the impacts of decision making, the research has been largely focused on examining the fairness of the decisions, the long-term effects of the decision pipelines, and utility-based perspectives considering both the decision-maker and the individuals. However, there has hardly been any focus on precarity which is the term that encapsulates the instability in people's lives. That is, a negative outcome can overspread to other decisions and measures of well-being. Studying precarity necessitates a shift in focus -- from the point of view of the decision-maker to the perspective of the decision subject. This centering of the subject is an important direction that unlocks the importance of parting with aggregate measures to examine the long-term effects of decision making. To address this issue, in this paper, we propose a modeling framework that simulates the effects of compounded decision-making on precarity over time. Through our simulations, we are able to show the heterogeneity of precarity by the non-uniform ruinous aftereffects of negative decisions on different income classes of the underlying population and how policy interventions can help mitigate such effects.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {199–208},
numpages = {10},
keywords = {economics, precarity, algorithmic decision-making, long-term effects},
location = {Virtual Event, USA},
series = {AIES '21}
}
  
"
AIES,2021,Moral Disagreement and Artificial Intelligence,https://dl.acm.org/doi/10.1145/3461702.3462534,['Pamela Robinson'],0,"Artificially intelligent systems will be used to make increasingly important decisions about us. Many of these decisions will have to be made without consensus about the relevant moral facts. I argue that what makes moral disagreement especially challenging is that there are two different ways of handling it: political solutions, which aim to find a fair compromise, and epistemic solutions, which aim at moral truth.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3461702.3462534,
author = {Robinson, Pamela},
title = {Moral Disagreement and Artificial Intelligence},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462534},
doi = {10.1145/3461702.3462534},
abstract = {Artificially intelligent systems will be used to make increasingly important decisions about us. Many of these decisions will have to be made without consensus about the relevant moral facts. I argue that what makes moral disagreement especially challenging is that there are two different ways of handling it: political solutions, which aim to find a fair compromise, and epistemic solutions, which aim at moral truth.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {209},
numpages = {1},
keywords = {artificial intelligence, moral uncertainty, moral disagreement},
location = {Virtual Event, USA},
series = {AIES '21}
}
  
"
AIES,2021,Fairness for Unobserved Characteristics: Insights from Technological Impacts on Queer Communities,https://dl.acm.org/doi/10.1145/3461702.3462540,"['Nenad Tomasev', 'Kevin R. McKee', 'Jackie Kay', 'Shakir Mohamed']",28,"Advances in algorithmic fairness have largely omitted sexual orientation and gender identity. We explore queer concerns in privacy, censorship, language, online safety, health, and employment to study the positive and negative effects of artificial intelligence on queer communities. These issues underscore the need for new directions in fairness research that take into account a multiplicity of considerations, from privacy preservation, context sensitivity and process fairness, to an awareness of sociotechnical impact and the increasingly important role of inclusive and participatory research processes. Most current approaches for algorithmic fairness assume that the target characteristics for fairness---frequently, race and legal gender---can be observed or recorded. Sexual orientation and gender identity are prototypical instances of unobserved characteristics, which are frequently missing, unknown or fundamentally unmeasurable. This paper highlights the importance of developing new approaches for algorithmic fairness that break away from the prevailing assumption of observed characteristics.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3461702.3462540,
author = {Tomasev, Nenad and McKee, Kevin R. and Kay, Jackie and Mohamed, Shakir},
title = {Fairness for Unobserved Characteristics: Insights from Technological Impacts on Queer Communities},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462540},
doi = {10.1145/3461702.3462540},
abstract = {Advances in algorithmic fairness have largely omitted sexual orientation and gender identity. We explore queer concerns in privacy, censorship, language, online safety, health, and employment to study the positive and negative effects of artificial intelligence on queer communities. These issues underscore the need for new directions in fairness research that take into account a multiplicity of considerations, from privacy preservation, context sensitivity and process fairness, to an awareness of sociotechnical impact and the increasingly important role of inclusive and participatory research processes. Most current approaches for algorithmic fairness assume that the target characteristics for fairness---frequently, race and legal gender---can be observed or recorded. Sexual orientation and gender identity are prototypical instances of unobserved characteristics, which are frequently missing, unknown or fundamentally unmeasurable. This paper highlights the importance of developing new approaches for algorithmic fairness that break away from the prevailing assumption of observed characteristics.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {254–265},
numpages = {12},
keywords = {sexual orientation, gender identity, queer communities, machine learning, marginalised groups, algorithmic fairness},
location = {Virtual Event, USA},
series = {AIES '21}
}
  
"
AIES,2021,Causality in Neural Networks - An Extended Abstract,https://dl.acm.org/doi/10.1145/3461702.3462467,['Abbavaram Gowtham Reddy'],1,"Causal reasoning is the main learning and explanation tool used by humans. AI systems should possess causal reasoning capabilities to be deployed in the real world with trust and reliability. Introducing the ideas of causality to machine learning helps in providing better learning and explainable models. Explainability, causal disentanglement are some important aspects of any machine learning model. Causal explanations are required to believe in a model's decision and causal disentanglement learning is important for transfer learning applications. We exploit the ideas of causality to be used in deep learning models to achieve better and causally explainable models that are useful in fairness, disentangled representation, etc.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3461702.3462467,
author = {Gowtham Reddy, Abbavaram},
title = {Causality in Neural Networks - An Extended Abstract},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462467},
doi = {10.1145/3461702.3462467},
abstract = {Causal reasoning is the main learning and explanation tool used by humans. AI systems should possess causal reasoning capabilities to be deployed in the real world with trust and reliability. Introducing the ideas of causality to machine learning helps in providing better learning and explainable models. Explainability, causal disentanglement are some important aspects of any machine learning model. Causal explanations are required to believe in a model's decision and causal disentanglement learning is important for transfer learning applications. We exploit the ideas of causality to be used in deep learning models to achieve better and causally explainable models that are useful in fairness, disentangled representation, etc.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {271–272},
numpages = {2},
keywords = {disentanglement, explainability, causality, counterfactuals, machine learning, neural networks},
location = {Virtual Event, USA},
series = {AIES '21}
}
  
"
AIES,2021,Examining Religion Bias in AI Text Generators,https://dl.acm.org/doi/10.1145/3461702.3462469,['Deepa Muralidhar'],2,"One of the biggest reasons artificial intelligence (AI) gets a backlash is because of inherent biases in AI software. Deep learning algorithms use data fed into the systems to find patterns to draw conclusions used to make application decisions. Patterns in data fed into machine learning algorithms have revealed that the AI software decisions have biases embedded within them. Algorithmic audits can certify that the software is making responsible decisions. These audits verify the standards centered around the various AI principles such as explainability, accountability, human-centered values, such as, fairness and transparency, to increase the trust in the algorithm and the software systems that implement AI algorithms.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3461702.3462469,
author = {Muralidhar, Deepa},
title = {Examining Religion Bias in AI Text Generators},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462469},
doi = {10.1145/3461702.3462469},
abstract = {One of the biggest reasons artificial intelligence (AI) gets a backlash is because of inherent biases in AI software. Deep learning algorithms use data fed into the systems to find patterns to draw conclusions used to make application decisions. Patterns in data fed into machine learning algorithms have revealed that the AI software decisions have biases embedded within them. Algorithmic audits can certify that the software is making responsible decisions. These audits verify the standards centered around the various AI principles such as explainability, accountability, human-centered values, such as, fairness and transparency, to increase the trust in the algorithm and the software systems that implement AI algorithms.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {273–274},
numpages = {2},
keywords = {tool-kit, algorithm, religious-bias, NLP, audit},
location = {Virtual Event, USA},
series = {AIES '21}
}
  
"
AIES,2021,Designing Effective and Accessible Consumer Protections against Unfair Treatment in Markets where Automated Decision Making is used to Determine Access to Essential Services: A Case Study in Australia's Housing Market,https://dl.acm.org/doi/10.1145/3461702.3462468,['Linda Przhedetsky'],1,"The use of data-driven Automated Decision Making (ADM) to determine access to products or services in competitive markets can enhance or limit access to equality and fair treatment. In cases where essential services such housing, energy and telecommunications, are accessed through a competitive market, consumers who are denied access to one or more of these services may not be able to access a suitable alternative if there are none available to match their needs, budget, and unique circumstances. Being denied access to an essential service such as electricity or housing can be an issue of life or death. Competitive essential services markets therefore illuminate the ways that using ADM to determine access to products or services, if not balanced by appropriate consumer protections, can cause significant harm. My research explores existing and emerging consumer protections that are effective in preventing consumers being harmed by ADM-facilitated decisions in essential services markets.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3461702.3462468,
author = {Przhedetsky, Linda},
title = {Designing Effective and Accessible Consumer Protections against Unfair Treatment in Markets Where Automated Decision Making is Used to Determine Access to Essential Services: A Case Study in Australia's Housing Market},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462468},
doi = {10.1145/3461702.3462468},
abstract = {The use of data-driven Automated Decision Making (ADM) to determine access to products or services in competitive markets can enhance or limit access to equality and fair treatment. In cases where essential services such housing, energy and telecommunications, are accessed through a competitive market, consumers who are denied access to one or more of these services may not be able to access a suitable alternative if there are none available to match their needs, budget, and unique circumstances. Being denied access to an essential service such as electricity or housing can be an issue of life or death. Competitive essential services markets therefore illuminate the ways that using ADM to determine access to products or services, if not balanced by appropriate consumer protections, can cause significant harm. My research explores existing and emerging consumer protections that are effective in preventing consumers being harmed by ADM-facilitated decisions in essential services markets.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {279–280},
numpages = {2},
keywords = {algorithms, essential services, regulation, automated decision making, automation},
location = {Virtual Event, USA},
series = {AIES '21}
}
  
"
AIES,2021,"Uncertainty as a Form of Transparency: Measuring, Communicating, and Using Uncertainty",https://dl.acm.org/doi/10.1145/3461702.3462571,"['Umang Bhatt', 'Javier Antor√°n', 'Yunfeng Zhang', 'Q. Vera Liao', 'Prasanna Sattigeri', 'Riccardo Fogliato', 'Gabrielle Melan√ßon', 'Ranganath Krishnan', 'Jason Stanley', 'Omesh Tickoo', 'Lama Nachman', 'Rumi Chunara', 'Madhulika Srikumar', 'Adrian Weller', 'Alice Xiang']",87,"Algorithmic transparency entails exposing system properties to various stakeholders for purposes that include understanding, improving, and contesting predictions. Until now, most research into algorithmic transparency has predominantly focused on explainability. Explainability attempts to provide reasons for a machine learning model's behavior to stakeholders. However, understanding a model's specific behavior alone might not be enough for stakeholders to gauge whether the model is wrong or lacks sufficient knowledge to solve the task at hand. In this paper, we argue for considering a complementary form of transparency by estimating and communicating the uncertainty associated with model predictions. First, we discuss methods for assessing uncertainty. Then, we characterize how uncertainty can be used to mitigate model unfairness, augment decision-making, and build trustworthy systems. Finally, we outline methods for displaying uncertainty to stakeholders and recommend how to collect information required for incorporating uncertainty into existing ML pipelines. This work constitutes an interdisciplinary review drawn from literature spanning machine learning, visualization/HCI, design, decision-making, and fairness. We aim to encourage researchers and practitioners to measure, communicate, and use uncertainty as a form of transparency.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3461702.3462571,
author = {Bhatt, Umang and Antor\'{a}n, Javier and Zhang, Yunfeng and Liao, Q. Vera and Sattigeri, Prasanna and Fogliato, Riccardo and Melan\c{c}on, Gabrielle and Krishnan, Ranganath and Stanley, Jason and Tickoo, Omesh and Nachman, Lama and Chunara, Rumi and Srikumar, Madhulika and Weller, Adrian and Xiang, Alice},
title = {Uncertainty as a Form of Transparency: Measuring, Communicating, and Using Uncertainty},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462571},
doi = {10.1145/3461702.3462571},
abstract = {Algorithmic transparency entails exposing system properties to various stakeholders for purposes that include understanding, improving, and contesting predictions. Until now, most research into algorithmic transparency has predominantly focused on explainability. Explainability attempts to provide reasons for a machine learning model's behavior to stakeholders. However, understanding a model's specific behavior alone might not be enough for stakeholders to gauge whether the model is wrong or lacks sufficient knowledge to solve the task at hand. In this paper, we argue for considering a complementary form of transparency by estimating and communicating the uncertainty associated with model predictions. First, we discuss methods for assessing uncertainty. Then, we characterize how uncertainty can be used to mitigate model unfairness, augment decision-making, and build trustworthy systems. Finally, we outline methods for displaying uncertainty to stakeholders and recommend how to collect information required for incorporating uncertainty into existing ML pipelines. This work constitutes an interdisciplinary review drawn from literature spanning machine learning, visualization/HCI, design, decision-making, and fairness. We aim to encourage researchers and practitioners to measure, communicate, and use uncertainty as a form of transparency.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {401–413},
numpages = {13},
keywords = {machine learning, uncertainty, transparency, visualization},
location = {Virtual Event, USA},
series = {AIES '21}
}
  
"
AIES,2021,Envisioning Communities: A Participatory Approach Towards AI for Social Good,https://dl.acm.org/doi/10.1145/3461702.3462612,"['Elizabeth Bondi', 'Lily Xu', 'Diana Acosta-Navas', 'Jackson A. Killian']",20,"Research in artificial intelligence (AI) for social good presupposes some definition of social good, but potential definitions have been seldom suggested and never agreed upon. The normative question of what AI for social good research should be ""for"" is not thoughtfully elaborated, or is frequently addressed with a utilitarian outlook that prioritizes the needs of the majority over those who have been historically marginalized, brushing aside realities of injustice and inequity. We argue that AI for social good ought to be assessed by the communities that the AI system will impact, using as a guide the capabilities approach, a framework to measure the ability of different policies to improve human welfare equity. Furthermore, we lay out how AI research has the potential to catalyze social progress by expanding and equalizing capabilities. We show how the capabilities approach aligns with a participatory approach for the design and implementation of AI for social good research in a framework we introduce called PACT, in which community members affected should be brought in as partners and their input prioritized throughout the project. We conclude by providing an incomplete set of guiding questions for carrying out such participatory AI research in a way that elicits and respects a community's own definition of social good.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3461702.3462612,
author = {Bondi, Elizabeth and Xu, Lily and Acosta-Navas, Diana and Killian, Jackson A.},
title = {Envisioning Communities: A Participatory Approach Towards AI for Social Good},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462612},
doi = {10.1145/3461702.3462612},
abstract = {Research in artificial intelligence (AI) for social good presupposes some definition of social good, but potential definitions have been seldom suggested and never agreed upon. The normative question of what AI for social good research should be ""for"" is not thoughtfully elaborated, or is frequently addressed with a utilitarian outlook that prioritizes the needs of the majority over those who have been historically marginalized, brushing aside realities of injustice and inequity. We argue that AI for social good ought to be assessed by the communities that the AI system will impact, using as a guide the capabilities approach, a framework to measure the ability of different policies to improve human welfare equity. Furthermore, we lay out how AI research has the potential to catalyze social progress by expanding and equalizing capabilities. We show how the capabilities approach aligns with a participatory approach for the design and implementation of AI for social good research in a framework we introduce called PACT, in which community members affected should be brought in as partners and their input prioritized throughout the project. We conclude by providing an incomplete set of guiding questions for carrying out such participatory AI research in a way that elicits and respects a community's own definition of social good.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {425–436},
numpages = {12},
keywords = {artificial intelligence for social good, participatory design, capabilities approach},
location = {Virtual Event, USA},
series = {AIES '21}
}
  
"
AIES,2021,Fairness and Machine Fairness,https://dl.acm.org/doi/10.1145/3461702.3462577,"['Clinton Castro', ""David O'Brien"", 'Ben Schwan']",1,"Prediction-based decisions, which are often made by utilizing the tools of machine learning, influence nearly all facets of modern life. Ethical concerns about this widespread practice have given rise to the field of fair machine learning and a number of fairness measures, mathematically precise definitions of fairness that purport to determine whether a given prediction-based decision system is fair. Following Reuben Binns (2017), we take ""fairness"" in this context to be a placeholder for a variety of normative egalitarian considerations. We explore a few fairness measures to suss out their egalitarian roots and evaluate them, both as formalizations of egalitarian ideas and as assertions of what fairness demands of predictive systems. We pay special attention to a recent and popular fairness measure, counterfactual fairness, which holds that a prediction about an individual is fair if it is the same in the actual world and any counterfactual world where the individual belongs to a different demographic group (cf. Kusner et al. 2018).",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3461702.3462577,
author = {Castro, Clinton and O'Brien, David and Schwan, Ben},
title = {Fairness and Machine Fairness},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462577},
doi = {10.1145/3461702.3462577},
abstract = {Prediction-based decisions, which are often made by utilizing the tools of machine learning, influence nearly all facets of modern life. Ethical concerns about this widespread practice have given rise to the field of fair machine learning and a number of fairness measures, mathematically precise definitions of fairness that purport to determine whether a given prediction-based decision system is fair. Following Reuben Binns (2017), we take ""fairness"" in this context to be a placeholder for a variety of normative egalitarian considerations. We explore a few fairness measures to suss out their egalitarian roots and evaluate them, both as formalizations of egalitarian ideas and as assertions of what fairness demands of predictive systems. We pay special attention to a recent and popular fairness measure, counterfactual fairness, which holds that a prediction about an individual is fair if it is the same in the actual world and any counterfactual world where the individual belongs to a different demographic group (cf. Kusner et al. 2018).},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {446},
numpages = {1},
keywords = {fairness, fair machine learning, technology ethics},
location = {Virtual Event, USA},
series = {AIES '21}
}
  
"
AIES,2021,Reconfiguring Diversity and Inclusion for AI Ethics,https://dl.acm.org/doi/10.1145/3461702.3462622,"['Nicole Chi', 'Emma Lurie', 'Deirdre K. Mulligan']",6,"Activists, journalists, and scholars have long raised critical questions about the relationship between diversity, representation, and structural exclusions in data-intensive tools and services. We build on work mapping the emergent landscape of corporate AI ethics to center one outcome of these conversations: the incorporation of diversity and inclusion in corporate AI ethics activities. Using interpretive document analysis and analytic tools from the values in design field, we examine how diversity and inclusion work is articulated in public-facing AI ethics documentation produced by three companies that create application and services layer AI infrastructure: Google, Microsoft, and Salesforce. We find that as these documents make diversity and inclusion more tractable to engineers and technical clients, they reveal a drift away from civil rights justifications that resonates with the ""managerialization of diversity"" by corporations in the mid-1980s. The focus on technical artifacts - such as diverse and inclusive datasets - and the replacement of equity with fairness make ethical work more actionable for everyday practitioners. Yet, they appear divorced from broader DEI initiatives and relevant subject matter experts that could provide needed context to nuanced decisions around how to operationalize these values and new solutions. Finally, diversity and inclusion, as configured by engineering logic, positions firms not as ""ethics owners"" but as ethics allocators; while these companies claim expertise on AI ethics, the responsibility of defining who diversity and inclusion are meant to protect and where it is relevant is pushed downstream to their customers.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3461702.3462622,
author = {Chi, Nicole and Lurie, Emma and Mulligan, Deirdre K.},
title = {Reconfiguring Diversity and Inclusion for AI Ethics},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462622},
doi = {10.1145/3461702.3462622},
abstract = {Activists, journalists, and scholars have long raised critical questions about the relationship between diversity, representation, and structural exclusions in data-intensive tools and services. We build on work mapping the emergent landscape of corporate AI ethics to center one outcome of these conversations: the incorporation of diversity and inclusion in corporate AI ethics activities. Using interpretive document analysis and analytic tools from the values in design field, we examine how diversity and inclusion work is articulated in public-facing AI ethics documentation produced by three companies that create application and services layer AI infrastructure: Google, Microsoft, and Salesforce.We find that as these documents make diversity and inclusion more tractable to engineers and technical clients, they reveal a drift away from civil rights justifications that resonates with the ""managerialization of diversity"" by corporations in the mid-1980s. The focus on technical artifacts - such as diverse and inclusive datasets - and the replacement of equity with fairness make ethical work more actionable for everyday practitioners. Yet, they appear divorced from broader DEI initiatives and relevant subject matter experts that could provide needed context to nuanced decisions around how to operationalize these values and new solutions. Finally, diversity and inclusion, as configured by engineering logic, positions firms not as ""ethics owners"" but as ethics allocators; while these companies claim expertise on AI ethics, the responsibility of defining who diversity and inclusion are meant to protect and where it is relevant is pushed downstream to their customers.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {447–457},
numpages = {11},
keywords = {law, human rights, DEI, corporate ethics, AI ethics, diversity, inclusion, equity, fairness},
location = {Virtual Event, USA},
series = {AIES '21}
}
  
"
AIES,2021,Algorithmic Audit of Italian Car Insurance: Evidence of Unfairness in Access and Pricing,https://dl.acm.org/doi/10.1145/3461702.3462569,"['Alessandro Fabris', 'Alan Mishler', 'Stefano Gottardi', 'Mattia Carletti', 'Matteo Daicampi', 'Gian Antonio Susto', 'Gianmaria Silvello']",4,"We conduct an audit of pricing algorithms employed by companies in the Italian car insurance industry, primarily by gathering quotes through a popular comparison website. While acknowledging the complexity of the industry, we find evidence of several problematic practices. We show that birthplace and gender have a direct and sizeable impact on the prices quoted to drivers, despite national and international regulations against their use. Birthplace, in particular, is used quite frequently to the disadvantage of foreign-born drivers and drivers born in certain Italian cities. In extreme cases, a driver born in Laos may be charged 1,000 more than a driver born in Milan, all else being equal. For a subset of our sample, we collect quotes directly on a company website, where the direct influence of gender and birthplace is confirmed. Finally, we find that drivers with riskier profiles tend to see fewer quotes in the aggregator result pages, substantiating concerns of differential treatment raised in the past by Italian insurance regulators.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3461702.3462569,
author = {Fabris, Alessandro and Mishler, Alan and Gottardi, Stefano and Carletti, Mattia and Daicampi, Matteo and Susto, Gian Antonio and Silvello, Gianmaria},
title = {Algorithmic Audit of Italian Car Insurance: Evidence of Unfairness in Access and Pricing},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462569},
doi = {10.1145/3461702.3462569},
abstract = {We conduct an audit of pricing algorithms employed by companies in the Italian car insurance industry, primarily by gathering quotes through a popular comparison website. While acknowledging the complexity of the industry, we find evidence of several problematic practices. We show that birthplace and gender have a direct and sizeable impact on the prices quoted to drivers, despite national and international regulations against their use. Birthplace, in particular, is used quite frequently to the disadvantage of foreign-born drivers and drivers born in certain Italian cities. In extreme cases, a driver born in Laos may be charged 1,000 more than a driver born in Milan, all else being equal. For a subset of our sample, we collect quotes directly on a company website, where the direct influence of gender and birthplace is confirmed. Finally, we find that drivers with riskier profiles tend to see fewer quotes in the aggregator result pages, substantiating concerns of differential treatment raised in the past by Italian insurance regulators.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {458–468},
numpages = {11},
keywords = {car insurance, algorithmic audit, algorithmic fairness, fairness through unawareness},
location = {Virtual Event, USA},
series = {AIES '21}
}
  
"
AIES,2021,What's Fair about Individual Fairness?,https://dl.acm.org/doi/10.1145/3461702.3462621,['Will Fleisher'],17,"One of the main lines of research in algorithmic fairness involves individual fairness (IF) methods. Individual fairness is motivated by an intuitive principle, similar treatment, which requires that similar individuals be treated similarly. IF offers a precise account of this principle using distance metrics to evaluate the similarity of individuals. Proponents of individual fairness have argued that it gives the correct definition of algorithmic fairness, and that it should therefore be preferred to other methods for determining fairness. I argue that individual fairness cannot serve as a definition of fairness. Moreover, IF methods should not be given priority over other fairness methods, nor used in isolation from them. To support these conclusions, I describe four in-principle problems for individual fairness as a definition and as a method for ensuring fairness: (1) counterexamples show that similar treatment (and therefore IF) are insufficient to guarantee fairness; (2) IF methods for learning similarity metrics are at risk of encoding human implicit bias; (3) IF requires prior moral judgments, limiting its usefulness as a guide for fairness and undermining its claim to define fairness; and (4) the incommensurability of relevant moral values makes similarity metrics impossible for many tasks. In light of these limitations, I suggest that individual fairness cannot be a definition of fairness, and instead should be seen as one tool among several for ameliorating algorithmic bias.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3461702.3462621,
author = {Fleisher, Will},
title = {What's Fair about Individual Fairness?},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462621},
doi = {10.1145/3461702.3462621},
abstract = {One of the main lines of research in algorithmic fairness involves individual fairness (IF) methods. Individual fairness is motivated by an intuitive principle, similar treatment, which requires that similar individuals be treated similarly. IF offers a precise account of this principle using distance metrics to evaluate the similarity of individuals. Proponents of individual fairness have argued that it gives the correct definition of algorithmic fairness, and that it should therefore be preferred to other methods for determining fairness. I argue that individual fairness cannot serve as a definition of fairness. Moreover, IF methods should not be given priority over other fairness methods, nor used in isolation from them. To support these conclusions, I describe four in-principle problems for individual fairness as a definition and as a method for ensuring fairness: (1) counterexamples show that similar treatment (and therefore IF) are insufficient to guarantee fairness; (2) IF methods for learning similarity metrics are at risk of encoding human implicit bias; (3) IF requires prior moral judgments, limiting its usefulness as a guide for fairness and undermining its claim to define fairness; and (4) the incommensurability of relevant moral values makes similarity metrics impossible for many tasks. In light of these limitations, I suggest that individual fairness cannot be a definition of fairness, and instead should be seen as one tool among several for ameliorating algorithmic bias.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {480–490},
numpages = {11},
keywords = {individual fairness, algorithmic fairness, ethics of AI, incommensurable values},
location = {Virtual Event, USA},
series = {AIES '21}
}
  
"
AIES,2021,"Who Gets What, According to Whom? An Analysis of Fairness Perceptions in Service Allocation",https://dl.acm.org/doi/10.1145/3461702.3462568,"['Jacqueline Hannan', 'Huei-Yen Winnie Chen', 'Kenneth Joseph']",5,"Algorithmic fairness research has traditionally been linked to the disciplines of philosophy, ethics, and economics, where notions of fairness are prescriptive and seek objectivity. Increasingly, however, scholars are turning to the study of what different people perceive to be fair, and how these perceptions can or should help to shape the design of machine learning, particularly in the policy realm. The present work experimentally explores five novel research questions at the intersection of the ""Who,"" ""What,"" and ""How"" of fairness perceptions. Specifically, we present the results of a multi-factor conjoint analysis study that quantifies the effects of the specific context in which a question is asked, the framing of the given question, and who is answering it. Our results broadly suggest that the ""Who"" and ""What,"" at least, matter in ways that are 1) not easily explained by any one theoretical perspective, 2) have critical implications for how perceptions of fairness should be measured and/or integrated into algorithmic decision-making systems.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3461702.3462568,
author = {Hannan, Jacqueline and Chen, Huei-Yen Winnie and Joseph, Kenneth},
title = {Who Gets What, According to Whom? An Analysis of Fairness Perceptions in Service Allocation},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462568},
doi = {10.1145/3461702.3462568},
abstract = {Algorithmic fairness research has traditionally been linked to the disciplines of philosophy, ethics, and economics, where notions of fairness are prescriptive and seek objectivity. Increasingly, however, scholars are turning to the study of what different people perceive to be fair, and how these perceptions can or should help to shape the design of machine learning, particularly in the policy realm. The present work experimentally explores five novel research questions at the intersection of the ""Who,"" ""What,"" and ""How"" of fairness perceptions. Specifically, we present the results of a multi-factor conjoint analysis study that quantifies the effects of the specific context in which a question is asked, the framing of the given question, and who is answering it. Our results broadly suggest that the ""Who"" and ""What,"" at least, matter in ways that are 1) not easily explained by any one theoretical perspective, 2) have critical implications for how perceptions of fairness should be measured and/or integrated into algorithmic decision-making systems.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {555–565},
numpages = {11},
keywords = {service allocation, survey experiment, conjoint analysis, fairness, fairness perceptions},
location = {Virtual Event, USA},
series = {AIES '21}
}
  
"
AIES,2021,Measuring Group Advantage: A Comparative Study of Fair Ranking Metrics,https://dl.acm.org/doi/10.1145/3461702.3462588,"['Caitlin Kuhlman', 'Walter Gerych', 'Elke Rundensteiner']",14,"Ranking evaluation metrics play an important role in information retrieval, providing optimization objectives during development and means of assessment of deployed performance. Recently, fairness of rankings has been recognized as crucial, especially as automated systems are increasingly used for high impact decisions. While numerous fairness metrics have been proposed, a comparative analysis to understand their interrelationships is lacking. Even for fundamental statistical parity metrics which measure group advantage, it remains unclear whether metrics measure the same phenomena, or when one metric may produce different results than another. To address these open questions, we formulate a conceptual framework for analytical comparison of metrics. We prove that under reasonable assumptions, popular metrics in the literature exhibit the same behavior and that optimizing for one optimizes for all. However, our analysis also shows that the metrics vary in the degree of unfairness measured, in particular when one group has a strong majority. Based on this analysis, we design a practical statistical test to identify whether observed data is likely to exhibit predictable group bias. We provide a set of recommendations for practitioners to guide the choice of an appropriate fairness metric.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3461702.3462588,
author = {Kuhlman, Caitlin and Gerych, Walter and Rundensteiner, Elke},
title = {Measuring Group Advantage: A Comparative Study of Fair Ranking Metrics},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462588},
doi = {10.1145/3461702.3462588},
abstract = {Ranking evaluation metrics play an important role in information retrieval, providing optimization objectives during development and means of assessment of deployed performance. Recently, fairness of rankings has been recognized as crucial, especially as automated systems are increasingly used for high impact decisions. While numerous fairness metrics have been proposed, a comparative analysis to understand their interrelationships is lacking. Even for fundamental statistical parity metrics which measure group advantage, it remains unclear whether metrics measure the same phenomena, or when one metric may produce different results than another. To address these open questions, we formulate a conceptual framework for analytical comparison of metrics. We prove that under reasonable assumptions, popular metrics in the literature exhibit the same behavior and that optimizing for one optimizes for all. However, our analysis also shows that the metrics vary in the degree of unfairness measured, in particular when one group has a strong majority. Based on this analysis, we design a practical statistical test to identify whether observed data is likely to exhibit predictable group bias. We provide a set of recommendations for practitioners to guide the choice of an appropriate fairness metric.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {674–682},
numpages = {9},
keywords = {group advantage, statistical parity, fair ranking, fairness metrics},
location = {Virtual Event, USA},
series = {AIES '21}
}
  
"
AIES,2021,Fair Equality of Chances for Prediction-based Decisions,https://dl.acm.org/doi/10.1145/3461702.3462613,"['Michele Loi', 'Anders Herlitz', 'Hoda Heidari']",6,"This is a one-page summary of the paper ""A Philosophical Theory of Fairness for Prediction-based Decisions."" The full paper is available on SSRN at the following link: https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3450300",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3461702.3462613,
author = {Loi, Michele and Herlitz, Anders and Heidari, Hoda},
title = {Fair Equality of Chances for Prediction-Based Decisions},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462613},
doi = {10.1145/3461702.3462613},
abstract = {This is a one-page summary of the paper ""A Philosophical Theory of Fairness for Prediction-based Decisions."" The full paper is available on SSRN at the following link: https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3450300},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {756},
numpages = {1},
keywords = {statistical criteria, procedural justice, justice, group fairness, fairness, philosophy},
location = {Virtual Event, USA},
series = {AIES '21}
}
  
"
AIES,2021,How Do the Score Distributions of Subpopulations Influence Fairness Notions?,https://dl.acm.org/doi/10.1145/3461702.3462601,"['Carmen Mazijn', 'Jan Danckaert', 'Vincent Ginis']",1,"Automated decisions based on trained algorithms influence human life in an increasingly far-reaching way. In recent years, it has become clear that these decisions are often accompanied by bias and unfair treatment of different subpopulations.Meanwhile, several notions of fairness circulate in the scientific literature, with trade-offs between profit and fairness and between fairness metrics among themselves. Based on both analytical calculations and numerical simulations, we show in this study that some profit-fairness trade-offs and fairness-fairness trade-offs depend substantially on the underlying score distributions given to subpopulations and we present two complementary perspectives to visualize this influence. We further show that higher symmetry in scores of subpopulations can significantly reduce the trade-offs between fairness notions within a given acceptable strictness, even when sacrificing expressiveness. Our exploratory study may help to understand how to overcome the strict mathematical statements about the statistical incompatibility of certain fairness notions.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3461702.3462601,
author = {Mazijn, Carmen and Danckaert, Jan and Ginis, Vincent},
title = {How Do the Score Distributions of Subpopulations Influence Fairness Notions?},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462601},
doi = {10.1145/3461702.3462601},
abstract = {Automated decisions based on trained algorithms influence human life in an increasingly far-reaching way. In recent years, it has become clear that these decisions are often accompanied by bias and unfair treatment of different subpopulations.Meanwhile, several notions of fairness circulate in the scientific literature, with trade-offs between profit and fairness and between fairness metrics among themselves. Based on both analytical calculations and numerical simulations, we show in this study that some profit-fairness trade-offs and fairness-fairness trade-offs depend substantially on the underlying score distributions given to subpopulations and we present two complementary perspectives to visualize this influence. We further show that higher symmetry in scores of subpopulations can significantly reduce the trade-offs between fairness notions within a given acceptable strictness, even when sacrificing expressiveness. Our exploratory study may help to understand how to overcome the strict mathematical statements about the statistical incompatibility of certain fairness notions.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {767–776},
numpages = {10},
keywords = {decision-making algorithms, score distributions, fairness trade-offs, algorithmic fairness},
location = {Virtual Event, USA},
series = {AIES '21}
}
  
"
AIES,2021,Quantum Fair Machine Learning,https://dl.acm.org/doi/10.1145/3461702.3462611,['Elija Perrier'],4,"In this paper, we inaugurate the field of quantum fair machine learning. We undertake a comparative analysis of differences and similarities between classical and quantum fair machine learning algorithms, specifying how the unique features of quantum computation alter measures, metrics and remediation strategies when quantum algorithms are subject to fairness constraints. We present the first results in quantum fair machine learning by demonstrating the use of Grover's search algorithm to satisfy statistical parity constraints imposed on quantum algorithms. We provide lower-bounds on iterations needed to achieve such statistical parity within Œµ-tolerance. We extend canonical Lipschitz-conditioned individual fairness criteria to the quantum setting using quantum metrics. We examine the consequences for typical measures of fairness in machine learning context when quantum information processing and quantum data are involved. Finally, we propose open questions and research programmes for this new field of interest to researchers in computer science, ethics and quantum computation.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3461702.3462611,
author = {Perrier, Elija},
title = {Quantum Fair Machine Learning},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462611},
doi = {10.1145/3461702.3462611},
abstract = {In this paper, we inaugurate the field of quantum fair machine learning. We undertake a comparative analysis of differences and similarities between classical and quantum fair machine learning algorithms, specifying how the unique features of quantum computation alter measures, metrics and remediation strategies when quantum algorithms are subject to fairness constraints. We present the first results in quantum fair machine learning by demonstrating the use of Grover's search algorithm to satisfy statistical parity constraints imposed on quantum algorithms. We provide lower-bounds on iterations needed to achieve such statistical parity within ε-tolerance. We extend canonical Lipschitz-conditioned individual fairness criteria to the quantum setting using quantum metrics. We examine the consequences for typical measures of fairness in machine learning context when quantum information processing and quantum data are involved. Finally, we propose open questions and research programmes for this new field of interest to researchers in computer science, ethics and quantum computation.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {843–853},
numpages = {11},
keywords = {quantum, learning, machine, fair},
location = {Virtual Event, USA},
series = {AIES '21}
}
  
"
AIES,2021,Face Mis-ID: An Interactive Pedagogical Tool Demonstrating Disparate Accuracy Rates in Facial Recognition,https://dl.acm.org/doi/10.1145/3461702.3462627,"['Daniella Raz', 'Corinne Bintz', 'Vivian Guetler', 'Aaron Tam', 'Michael Katell', 'Dharma Dailey', 'Bernease Herman', 'P. M. Krafft', 'Meg Young']",7,"This paper reports on the making of an interactive demo to illustrate algorithmic bias in facial recognition. Facial recognition technology has been demonstrated to be more likely to misidentify women and minoritized people. This risk, among others, has elevated facial recognition into policy discussions across the country, where many jurisdictions have already passed bans on its use. Whereas scholarship on the disparate impacts of algorithmic systems is growing, general public awareness of this set of problems is limited in part by the illegibility of machine learning systems to non-specialists. Inspired by discussions with community organizers advocating for tech fairness issues, we created the Face Mis-ID Demo to reveal the algorithmic functions behind facial recognition technology and to demonstrate its risks to policymakers and members of the community. In this paper, we share the design process behind this interactive demo, its form and function, and the design decisions that honed its accessibility, toward its use for improving legibility of algorithmic systems and awareness of the sources of their disparate impacts.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3461702.3462627,
author = {Raz, Daniella and Bintz, Corinne and Guetler, Vivian and Tam, Aaron and Katell, Michael and Dailey, Dharma and Herman, Bernease and Krafft, P. M. and Young, Meg},
title = {Face Mis-ID: An Interactive Pedagogical Tool Demonstrating Disparate Accuracy Rates in Facial Recognition},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462627},
doi = {10.1145/3461702.3462627},
abstract = {This paper reports on the making of an interactive demo to illustrate algorithmic bias in facial recognition. Facial recognition technology has been demonstrated to be more likely to misidentify women and minoritized people. This risk, among others, has elevated facial recognition into policy discussions across the country, where many jurisdictions have already passed bans on its use. Whereas scholarship on the disparate impacts of algorithmic systems is growing, general public awareness of this set of problems is limited in part by the illegibility of machine learning systems to non-specialists. Inspired by discussions with community organizers advocating for tech fairness issues, we created the Face Mis-ID Demo to reveal the algorithmic functions behind facial recognition technology and to demonstrate its risks to policymakers and members of the community. In this paper, we share the design process behind this interactive demo, its form and function, and the design decisions that honed its accessibility, toward its use for improving legibility of algorithmic systems and awareness of the sources of their disparate impacts.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {895–904},
numpages = {10},
keywords = {legibility, educational tools, non-specialist understanding, interactive demo, algorithmic bias, facial recognition, surveillance, literacy, participatory design},
location = {Virtual Event, USA},
series = {AIES '21}
}
  
"
AIES,2021,Fairness in the Eyes of the Data: Certifying Machine-Learning Models,https://dl.acm.org/doi/10.1145/3461702.3462554,"['Shahar Segal', 'Yossi Adi', 'Benny Pinkas', 'Carsten Baum', 'Chaya Ganesh', 'Joseph Keshet']",16,"We present a framework that allows to certify the fairness degree of a model based on an interactive and privacy-preserving test. The framework verifies any trained model, regardless of its training process and architecture. Thus, it allows us to evaluate any deep learning model on multiple fairness definitions empirically. We tackle two scenarios, where either the test data is privately available only to the tester or is publicly known in advance, even to the model creator. We investigate the soundness of the proposed approach using theoretical analysis and present statistical guarantees for the interactive test. Finally, we provide a cryptographic technique to automate fairness testing and certified inference with only black-box access to the model at hand while hiding the participants' sensitive data.",NOT,,,,,Testing,,,,,,,,,,,"@inproceedings{10.1145/3461702.3462554,
author = {Segal, Shahar and Adi, Yossi and Pinkas, Benny and Baum, Carsten and Ganesh, Chaya and Keshet, Joseph},
title = {Fairness in the Eyes of the Data: Certifying Machine-Learning Models},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462554},
doi = {10.1145/3461702.3462554},
abstract = {We present a framework that allows to certify the fairness degree of a model based on an interactive and privacy-preserving test. The framework verifies any trained model, regardless of its training process and architecture. Thus, it allows us to evaluate any deep learning model on multiple fairness definitions empirically. We tackle two scenarios, where either the test data is privately available only to the tester or is publicly known in advance, even to the model creator. We investigate the soundness of the proposed approach using theoretical analysis and present statistical guarantees for the interactive test. Finally, we provide a cryptographic technique to automate fairness testing and certified inference with only black-box access to the model at hand while hiding the participants' sensitive data.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {926–935},
numpages = {10},
keywords = {privacy, cryptography, machine-learning, fairness},
location = {Virtual Event, USA},
series = {AIES '21}
}
  
"
AIES,2021,Machine Learning and the Meaning of Equal Treatment,https://dl.acm.org/doi/10.1145/3461702.3462556,"['Joshua Simons', 'Sophia Adams Bhatti', 'Adrian Weller']",2,"Approaches to non-discrimination are generally informed by two principles: striving for equality of treatment, and advancing various notions of equality of outcome. We consider when and why there are trade-offs in machine learning between respecting formalistic interpretations of equal treatment and advancing equality of outcome. Exploring a hypothetical discrimination suit against Facebook, we argue that interpretations of equal treatment which require blindness to difference may constrain how machine learning can be deployed to advance equality of outcome. When machine learning models predict outcomes that are unevenly distributed across racial groups, using those models to advance racial justice will often require deliberately taking race into account. We then explore the normative stakes of this tension. We describe three pragmatic policy options underpinned by distinct interpretations and applications of equal treatment. A status quo approach insists on blindness to difference, permitting the design of machine learning models that compound existing patterns of disadvantage. An industry-led approach would specify a narrow set of domains in which institutions were permitted to use protected characteristics to actively reduce inequalities of outcome. A government-led approach would impose positive duties that require institutions to consider how best to advance equality of outcomes and permit the use of protected characteristics to achieve that goal. We argue that while machine learning offers significant possibilities for advancing racial justice and outcome-based equality, harnessing those possibilities will require a shift in the normative commitments that underpin the interpretation and application of equal treatment in non-discrimination law and the governance of machine learning.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3461702.3462556,
author = {Simons, Joshua and Adams Bhatti, Sophia and Weller, Adrian},
title = {Machine Learning and the Meaning of Equal Treatment},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462556},
doi = {10.1145/3461702.3462556},
abstract = {Approaches to non-discrimination are generally informed by two principles: striving for equality of treatment, and advancing various notions of equality of outcome. We consider when and why there are trade-offs in machine learning between respecting formalistic interpretations of equal treatment and advancing equality of outcome. Exploring a hypothetical discrimination suit against Facebook, we argue that interpretations of equal treatment which require blindness to difference may constrain how machine learning can be deployed to advance equality of outcome. When machine learning models predict outcomes that are unevenly distributed across racial groups, using those models to advance racial justice will often require deliberately taking race into account. We then explore the normative stakes of this tension. We describe three pragmatic policy options underpinned by distinct interpretations and applications of equal treatment. A status quo approach insists on blindness to difference, permitting the design of machine learning models that compound existing patterns of disadvantage. An industry-led approach would specify a narrow set of domains in which institutions were permitted to use protected characteristics to actively reduce inequalities of outcome. A government-led approach would impose positive duties that require institutions to consider how best to advance equality of outcomes and permit the use of protected characteristics to achieve that goal. We argue that while machine learning offers significant possibilities for advancing racial justice and outcome-based equality, harnessing those possibilities will require a shift in the normative commitments that underpin the interpretation and application of equal treatment in non-discrimination law and the governance of machine learning.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {956–966},
numpages = {11},
keywords = {philosophy, machine learning, fairness, equal treatment, politics},
location = {Virtual Event, USA},
series = {AIES '21}
}
  
"
AIES,2021,Does Fair Ranking Improve Minority Outcomes? Understanding the Interplay of Human and Algorithmic Biases in Online Hiring,https://dl.acm.org/doi/10.1145/3461702.3462602,"['Tom S√ºhr', 'Sophie Hilgard', 'Himabindu Lakkaraju']",19,"Ranking algorithms are being widely employed in various online hiring platforms including LinkedIn, TaskRabbit, and Fiverr. Prior research has demonstrated that ranking algorithms employed by these platforms are prone to a variety of undesirable biases, leading to the proposal of fair ranking algorithms (e.g., Det-Greedy) which increase exposure of underrepresented candidates. However, there is little to no work that explores whether fair ranking algorithms actually improve real world outcomes (e.g., hiring decisions) for underrepresented groups. Furthermore, there is no clear understanding as to how other factors (e.g., job context, inherent biases of the employers) may impact the efficacy of fair ranking in practice. In this work, we analyze various sources of gender biases in online hiring platforms, including the job context and inherent biases of employers and establish how these factors interact with ranking algorithms to affect hiring decisions. To the best of our knowledge, this work makes the first attempt at studying the interplay between the aforementioned factors in the context of online hiring. We carry out a large-scale user study simulating online hiring scenarios with data from TaskRabbit, a popular online freelancing site. Our results demonstrate that while fair ranking algorithms generally improve the selection rates of underrepresented minorities, their effectiveness relies heavily on the job contexts and candidate profiles.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3461702.3462602,
author = {S\""{u}hr, Tom and Hilgard, Sophie and Lakkaraju, Himabindu},
title = {Does Fair Ranking Improve Minority Outcomes? Understanding the Interplay of Human and Algorithmic Biases in Online Hiring},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462602},
doi = {10.1145/3461702.3462602},
abstract = {Ranking algorithms are being widely employed in various online hiring platforms including LinkedIn, TaskRabbit, and Fiverr. Prior research has demonstrated that ranking algorithms employed by these platforms are prone to a variety of undesirable biases, leading to the proposal of fair ranking algorithms (e.g., Det-Greedy) which increase exposure of underrepresented candidates. However, there is little to no work that explores whether fair ranking algorithms actually improve real world outcomes (e.g., hiring decisions) for underrepresented groups. Furthermore, there is no clear understanding as to how other factors (e.g., job context, inherent biases of the employers) may impact the efficacy of fair ranking in practice.In this work, we analyze various sources of gender biases in online hiring platforms, including the job context and inherent biases of employers and establish how these factors interact with ranking algorithms to affect hiring decisions. To the best of our knowledge, this work makes the first attempt at studying the interplay between the aforementioned factors in the context of online hiring. We carry out a large-scale user study simulating online hiring scenarios with data from TaskRabbit, a popular online freelancing site. Our results demonstrate that while fair ranking algorithms generally improve the selection rates of underrepresented minorities, their effectiveness relies heavily on the job contexts and candidate profiles.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {989–999},
numpages = {11},
keywords = {discrimination, gender, online hiring, fair ranking, user studies},
location = {Virtual Event, USA},
series = {AIES '21}
}
  
"
AIES,2021,A Human-in-the-loop Framework to Construct Context-aware Mathematical Notions of Outcome Fairness,https://dl.acm.org/doi/10.1145/3461702.3462583,"['Mohammad Yaghini', 'Andreas Krause', 'Hoda Heidari']",6,"Existing mathematical notions of fairness fail to account for the context of decision-making. We argue that moral consideration of contextual factors is an inherently human task. So we present a framework to learn context-aware mathematical formulations of fairness by eliciting people's situated fairness assessments. Our family of fairness notions corresponds to a new interpretation of economic models of Equality of Opportunity (EOP), and it includes most existing notions of fairness as special cases. Our human-in-the-loop approach is designed to learn the appropriate parameters of the EOP family by utilizing human responses to pair-wise questions about decision subjects' circumstance and deservingness, and the harm/benefit imposed on them. We illustrate our framework in a hypothetical criminal risk assessment scenario by conducting a series of human-subject experiments on Amazon Mechanical Turk. Our work takes an important initial step toward empowering stakeholders to have a voice in the formulation of fairness for Machine Learning.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3461702.3462583,
author = {Yaghini, Mohammad and Krause, Andreas and Heidari, Hoda},
title = {A Human-in-the-Loop Framework to Construct Context-Aware Mathematical Notions of Outcome Fairness},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462583},
doi = {10.1145/3461702.3462583},
abstract = {Existing mathematical notions of fairness fail to account for the context of decision-making. We argue that moral consideration of contextual factors is an inherently human task. So we present a framework to learn context-aware mathematical formulations of fairness by eliciting people's situated fairness assessments. Our family of fairness notions corresponds to a new interpretation of economic models of Equality of Opportunity (EOP), and it includes most existing notions of fairness as special cases. Our human-in-the-loop approach is designed to learn the appropriate parameters of the EOP family by utilizing human responses to pair-wise questions about decision subjects' circumstance and deservingness, and the harm/benefit imposed on them. We illustrate our framework in a hypothetical criminal risk assessment scenario by conducting a series of human-subject experiments on Amazon Mechanical Turk. Our work takes an important initial step toward empowering stakeholders to have a voice in the formulation of fairness for Machine Learning.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {1023–1033},
numpages = {11},
keywords = {machine learning, human judgment, fairness, equality of opportunity},
location = {Virtual Event, USA},
series = {AIES '21}
}
  
"
AIES,2022,The Limits of Fairness,https://dl.acm.org/doi/10.1145/3514094.3539568,['Abeba Birhane'],0,"Complex adaptive systems such as human behaviour and social systems are inherently dynamic, messy, ambiguous, incompressible, non-determinable, and non-predictable. Due to their incompressibility, neither datasets nor models can capture complex systems in their entirety. Instead, large scale datasets and predictive models pick-up societal and historical stereotypes and injustices. Yet, machine learning tools that sort, categorize, and predict the social sphere have become common place, developed and deployed in various domains from education, law enforcement, to medicine and border control. In this talk, I emphasize that machine learning models of social systems are at best a snapshot of a moving target, and at worst systems that encode the status quo and exacerbate racism, sexism, and historical injustice. I first review some of the robust body of work that has emerged in this space. I, then, argue that equitable algorithmic systems require we look beyond technical solutions and call for acknowledgement and in depth understanding of complexity, historical injustice, and current societal power asymmetries. As well as challenging simple fairness metrics when algorithmic systems fail, broader structural rethinking, deeper understanding and envisioning an alternative future -- one that is based on current realities of machine learning systems and their impacts -- are critical.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3514094.3539568,
author = {Birhane, Abeba},
title = {The Limits of Fairness},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3539568},
doi = {10.1145/3514094.3539568},
abstract = {Complex adaptive systems such as human behaviour and social systems are inherently dynamic, messy, ambiguous, incompressible, non-determinable, and non-predictable. Due to their incompressibility, neither datasets nor models can capture complex systems in their entirety. Instead, large scale datasets and predictive models pick-up societal and historical stereotypes and injustices. Yet, machine learning tools that sort, categorize, and predict the social sphere have become common place, developed and deployed in various domains from education, law enforcement, to medicine and border control. In this talk, I emphasize that machine learning models of social systems are at best a snapshot of a moving target, and at worst systems that encode the status quo and exacerbate racism, sexism, and historical injustice. I first review some of the robust body of work that has emerged in this space. I, then, argue that equitable algorithmic systems require we look beyond technical solutions and call for acknowledgement and in depth understanding of complexity, historical injustice, and current societal power asymmetries. As well as challenging simple fairness metrics when algorithmic systems fail, broader structural rethinking, deeper understanding and envisioning an alternative future -- one that is based on current realities of machine learning systems and their impacts -- are critical.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {2},
numpages = {1},
keywords = {large scale dataset, complex systems, justice, models, fairness, ethics},
location = {Oxford, United Kingdom},
series = {AIES '22}
}
  
"
AIES,2022,Beyond Fairness and Explanation: Foundations of Trustworthiness of Artificial Agents,https://dl.acm.org/doi/10.1145/3514094.3539570,['Bertram F. Malle'],0,"The topics of fairness and explainability have dominated recent discussions of ethical AI. However, these are only two criteria that would make artificial agents anywhere close to ethical. I frame the question of ethical AI, and especially ethical social robots, as the question of what would make them worthy of human trust and actually eliciting human trust. Relying on a recent investigation of the multi-dimensionality of human trust, I lay out five criteria of trustworthiness-being competent, reliable, transparent, benevolent, and having ethical integrity. I will argue that an essential ingredient of such trustworthiness is norm competence-the ability to represent, comply with, and learn relevant social-moral norms (including fairness as one among many). I discuss the challenges to implementing norm competence and the critical role that justification, not just explanation, will play in providing evidence for such competence.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3514094.3539570,
author = {Malle, Bertram F.},
title = {Beyond Fairness and Explanation: Foundations of Trustworthiness of Artificial Agents},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3539570},
doi = {10.1145/3514094.3539570},
abstract = {The topics of fairness and explainability have dominated recent discussions of ethical AI. However, these are only two criteria that would make artificial agents anywhere close to ethical. I frame the question of ethical AI, and especially ethical social robots, as the question of what would make them worthy of human trust and actually eliciting human trust. Relying on a recent investigation of the multi-dimensionality of human trust, I lay out five criteria of trustworthiness-being competent, reliable, transparent, benevolent, and having ethical integrity. I will argue that an essential ingredient of such trustworthiness is norm competence-the ability to represent, comply with, and learn relevant social-moral norms (including fairness as one among many). I discuss the challenges to implementing norm competence and the critical role that justification, not just explanation, will play in providing evidence for such competence.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {4},
numpages = {1},
keywords = {norms, robotethics, trust, trustworthiness, xai, ethical ai, explainability, fairness},
location = {Oxford, United Kingdom},
series = {AIES '22}
}
  
"
AIES,2022,The AI Mirror: Reclaiming our Humanity in an Age of Machine Thinking,https://dl.acm.org/doi/10.1145/3514094.3539567,['Shannon Vallor'],0,"The new interdisciplinary field of AI ethics has revealed the extent to which AI systems tend to reflect back and amplify human vices: our unfair biases and discriminatory behaviours, our penchant for consuming and spreading misinformation, and our tendency to pursue narrow gains while losing sight of the bigger picture. While this is true, the mirror metaphor conveys the misleading and dangerous impression that AI merely captures and replicates our humanity in software. Yet we all know that a mirror does not capture the embodied human presence. Glass mirrors erase and occlude much of our material and conscious reality. Mirror images convey no smell, no depth, no softness, no fear, no hope, no imagination. What does the AI mirror occlude? In this talk I explore the dimensions of our humanity that AI's transformation of the socioeconomic and moral order makes it harder for us to see in ourselves and in one another, and why our futures depend upon bringing these vital aspects of our humanity back into view.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3514094.3539567,
author = {Vallor, Shannon},
title = {The AI Mirror: Reclaiming Our Humanity in an Age of Machine Thinking},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3539567},
doi = {10.1145/3514094.3539567},
abstract = {The new interdisciplinary field of AI ethics has revealed the extent to which AI systems tend to reflect back and amplify human vices: our unfair biases and discriminatory behaviours, our penchant for consuming and spreading misinformation, and our tendency to pursue narrow gains while losing sight of the bigger picture. While this is true, the mirror metaphor conveys the misleading and dangerous impression that AI merely captures and replicates our humanity in software. Yet we all know that a mirror does not capture the embodied human presence. Glass mirrors erase and occlude much of our material and conscious reality. Mirror images convey no smell, no depth, no softness, no fear, no hope, no imagination. What does the AI mirror occlude? In this talk I explore the dimensions of our humanity that AI's transformation of the socioeconomic and moral order makes it harder for us to see in ourselves and in one another, and why our futures depend upon bringing these vital aspects of our humanity back into view.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {6},
numpages = {1},
keywords = {bias, artificial intelligence, ethics, consciousness, humanity},
location = {Oxford, United Kingdom},
series = {AIES '22}
}
  
"
AIES,2022,SCALES: From Fairness Principles to Constrained Decision-Making,https://dl.acm.org/doi/10.1145/3514094.3534190,"['Sreejith Balakrishnan', 'Jianxin Bi', 'Harold Soh']",0,"This paper proposes SCALES, a general framework that translates well-established fairness principles into a common representation based on the Constraint Markov Decision Process (CMDP). With the help of causal language, our framework can place constraints on both the procedure of decision making (procedural fairness) as well as the outcomes resulting from decisions (outcome fairness). Specifically, we show that well-known fairness principles can be encoded either as a utility component, a non-causal component, or a causal component in a SCALES-CMDP. We illustrate SCALES using a set of case studies involving a simulated healthcare scenario and the real-world COMPAS dataset. Experiments demonstrate that our framework produces fair policies that embody alternative fairness principles in single-step and sequential decision-making scenarios.",NOT,,,,Metrics,,,,,,,,,,,,"@inproceedings{10.1145/3514094.3534190,
author = {Balakrishnan, Sreejith and Bi, Jianxin and Soh, Harold},
title = {SCALES: From Fairness Principles to Constrained Decision-Making},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534190},
doi = {10.1145/3514094.3534190},
abstract = {This paper proposes SCALES, a general framework that translates well-established fairness principles into a common representation based on the Constraint Markov Decision Process (CMDP). With the help of causal language, our framework can place constraints on both the procedure of decision making (procedural fairness) as well as the outcomes resulting from decisions (outcome fairness). Specifically, we show that well-known fairness principles can be encoded either as a utility component, a non-causal component, or a causal component in a SCALES-CMDP. We illustrate SCALES using a set of case studies involving a simulated healthcare scenario and the real-world COMPAS dataset. Experiments demonstrate that our framework produces fair policies that embody alternative fairness principles in single-step and sequential decision-making scenarios.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {46–55},
numpages = {10},
keywords = {constrained reinforcement learning, fairness},
location = {Oxford, United Kingdom},
series = {AIES '22}
}
  
"
AIES,2022,Fairness in Agreement With European Values: An Interdisciplinary Perspective on AI Regulation,https://dl.acm.org/doi/10.1145/3514094.3534158,"['Alejandra Bringas Colmenarejo', 'Luca Nannini', 'Alisa Rieger', 'Kristen M. Scott', 'Xuan Zhao', 'Gourab K Patro', 'Gjergji Kasneci', 'Katharina Kinder-Kurlanda']",2,"With increasing digitalization, Artificial Intelligence (AI) is becoming ubiquitous. AI-based systems to identify, optimize, automate, and scale solutions to complex economic and societal problems are being proposed and implemented. This has motivated regulation efforts, including the Proposal of an EU AI Act. This interdisciplinary position paper considers various concerns surrounding fairness and discrimination in AI, and discusses how AI regulations address them, focusing on (but not limited to) the Proposal. We first look at AI and fairness through the lenses of law, (AI) industry, sociotechnology, and (moral) philosophy, and present various perspectives. Then, we map these perspectives along three axes of interests: (i) Standardization vs. Localization, (ii) Utilitarianism vs. Egalitarianism, and (iii) Consequential vs. Deontological ethics which leads us to identify a pattern of common arguments and tensions between these axes. Positioning the discussion within the axes of interest and with a focus on reconciling the key tensions, we identify and propose the roles AI Regulation should take to make the endeavor of the AI Act a success in terms of AI fairness concerns.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3514094.3534158,
author = {Bringas Colmenarejo, Alejandra and Nannini, Luca and Rieger, Alisa and Scott, Kristen M. and Zhao, Xuan and Patro, Gourab K and Kasneci, Gjergji and Kinder-Kurlanda, Katharina},
title = {Fairness in Agreement With European Values: An Interdisciplinary Perspective on AI Regulation},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534158},
doi = {10.1145/3514094.3534158},
abstract = {With increasing digitalization, Artificial Intelligence (AI) is becoming ubiquitous. AI-based systems to identify, optimize, automate, and scale solutions to complex economic and societal problems are being proposed and implemented. This has motivated regulation efforts, including the Proposal of an EU AI Act. This interdisciplinary position paper considers various concerns surrounding fairness and discrimination in AI, and discusses how AI regulations address them, focusing on (but not limited to) the Proposal. We first look at AI and fairness through the lenses of law, (AI) industry, sociotechnology, and (moral) philosophy, and present various perspectives. Then, we map these perspectives along three axes of interests: (i) Standardization vs. Localization, (ii) Utilitarianism vs. Egalitarianism, and (iii) Consequential vs. Deontological ethics which leads us to identify a pattern of common arguments and tensions between these axes. Positioning the discussion within the axes of interest and with a focus on reconciling the key tensions, we identify and propose the roles AI Regulation should take to make the endeavor of the AI Act a success in terms of AI fairness concerns.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {107–118},
numpages = {12},
keywords = {deontological ethics, consequential ethics, utilitarian welfare, standardization, ai regulation, localization, eu ai proposal, egalitarian welfare},
location = {Oxford, United Kingdom},
series = {AIES '22}
}
  
"
AIES,2022,FINS Auditing Framework: Group Fairness for Subset Selections,https://dl.acm.org/doi/10.1145/3514094.3534160,"['Kathleen Cachel', 'Elke Rundensteiner']",1,"Subset selection is an integral component of AI systems that is increasingly affecting people's livelihoods in applications ranging from hiring, healthcare, education, to financial decisions. Subset selections powered by AI-based methods include top-k analytics, data summarization, clustering, and multi-winner voting. While group fairness auditing tools have been proposed for classification systems, these state-of-the-art tools are not directly applicable to measuring and conceptualizing fairness in selected subsets. In this work, we introduce the first comprehensive auditing framework, FINS, to support stakeholders in interpretably quantifying group fairness across a diverse range of subset-specific fairness concerns. FINS offers a family of novel measures that provide a flexible means to audit group fairness for fairness goals ranging from item-based, score-based, and a combination thereof. FINS provides one unified easy-to-understand interpretation across these different fairness problems. Further, we develop guidelines through the FINS Fair Subset Chart, that supports auditors in determining which measures are relevant to their problem context and fairness objectives. We provide a comprehensive mapping between each fairness measure and the belief system (i.e., worldview) that is encoded within its measurement of fairness. Lastly, we demonstrate the interpretability and efficacy of FINS in supporting the identification of real bias with case studies using AirBnB listings and voter records.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3514094.3534160,
author = {Cachel, Kathleen and Rundensteiner, Elke},
title = {FINS Auditing Framework: Group Fairness for Subset Selections},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534160},
doi = {10.1145/3514094.3534160},
abstract = {Subset selection is an integral component of AI systems that is increasingly affecting people's livelihoods in applications ranging from hiring, healthcare, education, to financial decisions. Subset selections powered by AI-based methods include top-k analytics, data summarization, clustering, and multi-winner voting. While group fairness auditing tools have been proposed for classification systems, these state-of-the-art tools are not directly applicable to measuring and conceptualizing fairness in selected subsets. In this work, we introduce the first comprehensive auditing framework, FINS, to support stakeholders in interpretably quantifying group fairness across a diverse range of subset-specific fairness concerns. FINS offers a family of novel measures that provide a flexible means to audit group fairness for fairness goals ranging from item-based, score-based, and a combination thereof. FINS provides one unified easy-to-understand interpretation across these different fairness problems. Further, we develop guidelines through the FINS Fair Subset Chart, that supports auditors in determining which measures are relevant to their problem context and fairness objectives. We provide a comprehensive mapping between each fairness measure and the belief system (i.e., worldview) that is encoded within its measurement of fairness. Lastly, we demonstrate the interpretability and efficacy of FINS in supporting the identification of real bias with case studies using AirBnB listings and voter records.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {144–155},
numpages = {12},
keywords = {algorithmic fairness, subset selection, machine learning fairness},
location = {Oxford, United Kingdom},
series = {AIES '22}
}
  
"
AIES,2022,Fairness via Explanation Quality: Evaluating Disparities in the Quality of Post hoc Explanations,https://dl.acm.org/doi/10.1145/3514094.3534159,"['Jessica Dai', 'Sohini Upadhyay', 'Ulrich Aivodji', 'Stephen H. Bach', 'Himabindu Lakkaraju']",6,"As post hoc explanation methods are increasingly being leveraged to explain complex models in high-stakes settings, it becomes critical to ensure that the quality of the resulting explanations is consistently high across all subgroups of a population. For instance, it should not be the case that explanations associated with instances belonging to, e.g., women, are less accurate than those associated with other genders. In this work, we initiate the study of identifying group-based disparities in explanation quality. To this end, we first outline several key properties that contribute to explanation quality-namely, fidelity (accuracy), stability, consistency, and sparsity-and discuss why and how disparities in these properties can be particularly problematic. We then propose an evaluation framework which can quantitatively measure disparities in the quality of explanations. Using this framework, we carry out an empirical analysis with three datasets, six post hoc explanation methods, and different model classes to understand if and when group-based disparities in explanation quality arise. Our results indicate that such disparities are more likely to occur when the models being explained are complex and non-linear. We also observe that certain post hoc explanation methods (e.g., Integrated Gradients, SHAP) are more likely to exhibit disparities. Our work sheds light on previously unexplored ways in which explanation methods may introduce unfairness in real world decision making.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3514094.3534159,
author = {Dai, Jessica and Upadhyay, Sohini and Aivodji, Ulrich and Bach, Stephen H. and Lakkaraju, Himabindu},
title = {Fairness via Explanation Quality: Evaluating Disparities in the Quality of Post Hoc Explanations},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534159},
doi = {10.1145/3514094.3534159},
abstract = {As post hoc explanation methods are increasingly being leveraged to explain complex models in high-stakes settings, it becomes critical to ensure that the quality of the resulting explanations is consistently high across all subgroups of a population. For instance, it should not be the case that explanations associated with instances belonging to, e.g., women, are less accurate than those associated with other genders. In this work, we initiate the study of identifying group-based disparities in explanation quality. To this end, we first outline several key properties that contribute to explanation quality-namely, fidelity (accuracy), stability, consistency, and sparsity-and discuss why and how disparities in these properties can be particularly problematic. We then propose an evaluation framework which can quantitatively measure disparities in the quality of explanations. Using this framework, we carry out an empirical analysis with three datasets, six post hoc explanation methods, and different model classes to understand if and when group-based disparities in explanation quality arise. Our results indicate that such disparities are more likely to occur when the models being explained are complex and non-linear. We also observe that certain post hoc explanation methods (e.g., Integrated Gradients, SHAP) are more likely to exhibit disparities. Our work sheds light on previously unexplored ways in which explanation methods may introduce unfairness in real world decision making.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {203–214},
numpages = {12},
keywords = {fairness, robustness, explainable machine learning, interpretability},
location = {Oxford, United Kingdom},
series = {AIES '22}
}
  
"
AIES,2022,"Does AI De-Bias Recruitment?: Race, Gender, and AI's 'Eradication of Differences Between Groups'",https://dl.acm.org/doi/10.1145/3514094.3534151,"['Eleanor Drage', 'Kerry Mackereth']",0,"In this paper, we analyze two key claims offered by recruitment AI companies in relation to the development and deployment of AI-powered HR tools: 1) recruitment AI can objectively assess candidates by removing gender and race from their systems, and 2) this removal of gender and race will make recruitment fairer, help customers attain their DEI goals, and lay the foundations for a truly meritocratic culture to thrive within an organization. We argue that these claims are misleading for four reasons: First, attempts to 'strip' gender and race from AI systems often misunderstand what gender and race are, casting them as isolatable attributes rather than broader systems of power. Second, the attempted outsourcing of 'diversity work' to AI-powered hiring tools may unintentionally entrench cultures of inequality and discrimination by failing to address the systemic problems within organizations. Third, AI hiring tools' supposedly neutral assessment of candidates' traits belies the power relationship between the observer and the observed. Specifically, the racialized history of character analysis and its associated processes of classification and categorisation play into longer histories of taxonomical sorting and reflect the current demands and desires of the job market, even when not explicitly conducted along the lines of gender and race. Fourth, recruitment AI tools help produce the 'ideal candidate' that they supposedly identify through by constructing associations between words and people's bodies. From these four conclusions outlined above, we offer three key recommendations to AI HR firms, their customers, and policy makers going forward.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3514094.3534151,
author = {Drage, Eleanor and Mackereth, Kerry},
title = {Does AI De-Bias Recruitment? Race, Gender, and AI's 'Eradication of Differences Between Groups'},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534151},
doi = {10.1145/3514094.3534151},
abstract = {In this paper, we analyze two key claims offered by recruitment AI companies in relation to the development and deployment of AI-powered HR tools: 1) recruitment AI can objectively assess candidates by removing gender and race from their systems, and 2) this removal of gender and race will make recruitment fairer, help customers attain their DEI goals, and lay the foundations for a truly meritocratic culture to thrive within an organization. We argue that these claims are misleading for four reasons: First, attempts to 'strip' gender and race from AI systems often misunderstand what gender and race are, casting them as isolatable attributes rather than broader systems of power. Second, the attempted outsourcing of 'diversity work' to AI-powered hiring tools may unintentionally entrench cultures of inequality and discrimination by failing to address the systemic problems within organizations. Third, AI hiring tools' supposedly neutral assessment of candidates' traits belies the power relationship between the observer and the observed. Specifically, the racialized history of character analysis and its associated processes of classification and categorisation play into longer histories of taxonomical sorting and reflect the current demands and desires of the job market, even when not explicitly conducted along the lines of gender and race. Fourth, recruitment AI tools help produce the 'ideal candidate' that they supposedly identify through by constructing associations between words and people's bodies. From these four conclusions outlined above, we offer three key recommendations to AI HR firms, their customers, and policy makers going forward.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {237},
numpages = {1},
keywords = {race, recruitment, gender, bias, ai ethics},
location = {Oxford, United Kingdom},
series = {AIES '22}
}
  
"
AIES,2022,An Ontology for Fairness Metrics,https://dl.acm.org/doi/10.1145/3514094.3534137,"['Jade S. Franklin', 'Karan Bhanot', 'Mohamed Ghalwash', 'Kristin P. Bennett', 'Jamie McCusker', 'Deborah L. McGuinness']",2,"Recent research has revealed that many machine-learning models and the datasets they are trained on suffer from various forms of bias, and a large number of different fairness metrics have been created to measure this bias. However, determining which metrics to use, as well as interpreting their results, is difficult for a non-expert due to a lack of clear guidance and issues of ambiguity or alternate naming schemes between different research papers. To address this knowledge gap, we present the Fairness Metrics Ontology (FMO), a comprehensive and extensible knowledge resource that defines each fairness metric, describes their use cases, and details the relationships between them. We include additional concepts related to fairness and machine learning models, enabling the representation of specific fairness information within a resource description framework (RDF) knowledge graph. We evaluate the ontology by examining the process of how reasoning-based queries to the ontology were used to guide the fairness metric-based evaluation of a synthetic data model.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3514094.3534137,
author = {Franklin, Jade S. and Bhanot, Karan and Ghalwash, Mohamed and Bennett, Kristin P. and McCusker, Jamie and McGuinness, Deborah L.},
title = {An Ontology for Fairness Metrics},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534137},
doi = {10.1145/3514094.3534137},
abstract = {Recent research has revealed that many machine-learning models and the datasets they are trained on suffer from various forms of bias, and a large number of different fairness metrics have been created to measure this bias. However, determining which metrics to use, as well as interpreting their results, is difficult for a non-expert due to a lack of clear guidance and issues of ambiguity or alternate naming schemes between different research papers. To address this knowledge gap, we present the Fairness Metrics Ontology (FMO), a comprehensive and extensible knowledge resource that defines each fairness metric, describes their use cases, and details the relationships between them. We include additional concepts related to fairness and machine learning models, enabling the representation of specific fairness information within a resource description framework (RDF) knowledge graph. We evaluate the ontology by examining the process of how reasoning-based queries to the ontology were used to guide the fairness metric-based evaluation of a synthetic data model.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {265–275},
numpages = {11},
keywords = {machine learning evaluation, bias, rdf knowledge graph, fairness metric},
location = {Oxford, United Kingdom},
series = {AIES '22}
}
  
"
AIES,2022,Understanding Decision Subjects' Fairness Perceptions and Retention in Repeated Interactions with AI-Based Decision Systems,https://dl.acm.org/doi/10.1145/3514094.3534201,"['Meric Altug Gemalmaz', 'Ming Yin']",0,"The wide application of AI-based decision systems in many high-stake domains has raised concerns regarding fairness of these systems. As these systems will lead to real-life consequences to people who are subject to their decisions, understanding what these decision subjects perceive as a fair or unfair system is of vital importance. In this paper, we extend prior work in this direction by taking a perspective of repeated interactions---We ask that when decision subjects interact with an AI-based decision system repeatedly and can strategically respond to the system by determining whether to stay in the system, what factors will affect the decision subjects' fairness perceptions and retention in the system and how. To answer these questions, we conducted two randomized human-subject experiments in the context of an AI-based loan lending system. Our results suggest that in repeated interactions with the AI-based decision system, overall, decision subjects' fairness perceptions and retention in the system are significantly affected by whether the system is in favor of the group that subjects themselves belong to, rather than whether the system treats different groups in an unbiased way. However, decision subjects with different qualification levels have different reactions to the AI system's biased treatment across groups or the AI system's tendency to favor/disfavor their own group. Finally, we also find that while subjects' retention in the AI-based decision system is largely driven by their own prospects of receiving the favorable decision from the system, their fairness perceptions of the system is influenced by the system's treatment to people in other groups in a complex way.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3514094.3534201,
author = {Gemalmaz, Meric Altug and Yin, Ming},
title = {Understanding Decision Subjects' Fairness Perceptions and Retention in Repeated Interactions with AI-Based Decision Systems},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534201},
doi = {10.1145/3514094.3534201},
abstract = {The wide application of AI-based decision systems in many high-stake domains has raised concerns regarding fairness of these systems. As these systems will lead to real-life consequences to people who are subject to their decisions, understanding what these decision subjects perceive as a fair or unfair system is of vital importance. In this paper, we extend prior work in this direction by taking a perspective of repeated interactions---We ask that when decision subjects interact with an AI-based decision system repeatedly and can strategically respond to the system by determining whether to stay in the system, what factors will affect the decision subjects' fairness perceptions and retention in the system and how. To answer these questions, we conducted two randomized human-subject experiments in the context of an AI-based loan lending system. Our results suggest that in repeated interactions with the AI-based decision system, overall, decision subjects' fairness perceptions and retention in the system are significantly affected by whether the system is in favor of the group that subjects themselves belong to, rather than whether the system treats different groups in an unbiased way. However, decision subjects with different qualification levels have different reactions to the AI system's biased treatment across groups or the AI system's tendency to favor/disfavor their own group. Finally, we also find that while subjects' retention in the AI-based decision system is largely driven by their own prospects of receiving the favorable decision from the system, their fairness perceptions of the system is influenced by the system's treatment to people in other groups in a complex way.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {295–306},
numpages = {12},
keywords = {fairness, human-subject experiments, AI-based decision systems, human-AI interaction, fairness perceptions, retention},
location = {Oxford, United Kingdom},
series = {AIES '22}
}
  
"
AIES,2022,Learning Fairer Interventions,https://dl.acm.org/doi/10.1145/3514094.3534172,"['Yuzi He', 'Keith Burghardt', 'Siyi Guo', 'Kristina Lerman']",1,"Explicit and implicit bias clouds human judgment, leading to discriminatory treatment of disadvantaged groups. A fundamental goal of automated decisions is to avoid the pitfalls in human judgment by developing decision strategies that can be applied to all protected groups. Improving fairness of interventions via automated decision-inspired methods, however, has been under-utilized. In this paper, we propose a causal framework that learns optimal intervention policies from data subject to novel fairness constraints. We define two measures of treatment bias and infer treatment assignments that minimize the bias against protected groups while optimizing overall outcomes. We demonstrate the existence of trade-offs when balancing fairness and overall benefit; however, allowing preferential treatment of protected groups in certain circumstances (affirmative action) can dramatically improve the overall benefit while also preserving fairness. We apply our framework to data containing outcomes on standardized tests and show how it can be used to design real-world policies that fairly improve academic performance for different geographic areas. Our framework provides a principled way to learn fair treatment policies in real-world settings.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3514094.3534172,
author = {He, Yuzi and Burghardt, Keith and Guo, Siyi and Lerman, Kristina},
title = {Learning Fairer Interventions},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534172},
doi = {10.1145/3514094.3534172},
abstract = {Explicit and implicit bias clouds human judgment, leading to discriminatory treatment of disadvantaged groups. A fundamental goal of automated decisions is to avoid the pitfalls in human judgment by developing decision strategies that can be applied to all protected groups. Improving fairness of interventions via automated decision-inspired methods, however, has been under-utilized. In this paper, we propose a causal framework that learns optimal intervention policies from data subject to novel fairness constraints. We define two measures of treatment bias and infer treatment assignments that minimize the bias against protected groups while optimizing overall outcomes. We demonstrate the existence of trade-offs when balancing fairness and overall benefit; however, allowing preferential treatment of protected groups in certain circumstances (affirmative action) can dramatically improve the overall benefit while also preserving fairness. We apply our framework to data containing outcomes on standardized tests and show how it can be used to design real-world policies that fairly improve academic performance for different geographic areas. Our framework provides a principled way to learn fair treatment policies in real-world settings.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {317–323},
numpages = {7},
keywords = {treatment biases, policy recommendations, fair treatment, fairness metrics, affirmative action},
location = {Oxford, United Kingdom},
series = {AIES '22}
}
  
"
AIES,2022,Algorithmic Fairness and Structural Injustice: Insights from Feminist Political Philosophy,https://dl.acm.org/doi/10.1145/3514094.3534188,['Atoosa Kasirzadeh'],2,"Data-driven predictive algorithms are widely used to automate and guide high-stake decision making such as bail and parole recommendation, medical resource distribution, and mortgage allocation. Nevertheless, harmful outcomes biased against vulnerable groups have been reported. The growing research field known as 'algorithmic fairness' aims to mitigate these harmful biases. Its primary methodology consists in proposing mathematical metrics to address the social harms resulting from an algorithm's biased outputs. The metrics are typically motivated by -- or substantively rooted in -- ideals of distributive justice, as formulated by political and legal philosophers. The perspectives of feminist political philosophers on social justice, by contrast, have been largely neglected. Some feminist philosophers have criticized the local scope of the paradigm of distributive justice and have proposed corrective amendments to surmount its limitations. The present paper brings some key insights of feminist political philosophy to algorithmic fairness. The paper has three goals. First, I show that algorithmic fairness does not accommodate structural injustices in its current scope. Second, I defend the relevance of structural injustices -- as pioneered in the contemporary philosophical literature by Iris Marion Young -- to algorithmic fairness. Third, I take some steps in developing the paradigm of 'responsible algorithmic fairness' to correct for errors in the current scope and implementation of algorithmic fairness. I close by some reflections of directions for future research.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3514094.3534188,
author = {Kasirzadeh, Atoosa},
title = {Algorithmic Fairness and Structural Injustice: Insights from Feminist Political Philosophy},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534188},
doi = {10.1145/3514094.3534188},
abstract = {Data-driven predictive algorithms are widely used to automate and guide high-stake decision making such as bail and parole recommendation, medical resource distribution, and mortgage allocation. Nevertheless, harmful outcomes biased against vulnerable groups have been reported. The growing research field known as 'algorithmic fairness' aims to mitigate these harmful biases. Its primary methodology consists in proposing mathematical metrics to address the social harms resulting from an algorithm's biased outputs. The metrics are typically motivated by -- or substantively rooted in -- ideals of distributive justice, as formulated by political and legal philosophers. The perspectives of feminist political philosophers on social justice, by contrast, have been largely neglected. Some feminist philosophers have criticized the local scope of the paradigm of distributive justice and have proposed corrective amendments to surmount its limitations. The present paper brings some key insights of feminist political philosophy to algorithmic fairness. The paper has three goals. First, I show that algorithmic fairness does not accommodate structural injustices in its current scope. Second, I defend the relevance of structural injustices -- as pioneered in the contemporary philosophical literature by Iris Marion Young -- to algorithmic fairness. Third, I take some steps in developing the paradigm of 'responsible algorithmic fairness' to correct for errors in the current scope and implementation of algorithmic fairness. I close by some reflections of directions for future research.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {349–356},
numpages = {8},
keywords = {distributive justice, feminist philosophy, political philosophy, structural injustice, algorithmic bias, responsibility, ethical machine learning, algorithmic fairness, algorithmic justice, ethics of artificial intelligence},
location = {Oxford, United Kingdom},
series = {AIES '22}
}
  
"
AIES,2022,Equalizing Credit Opportunity in Algorithms: Aligning Algorithmic Fairness Research with U.S. Fair Lending Regulation,https://dl.acm.org/doi/10.1145/3514094.3534154,"['I. Elizabeth Kumar', 'Keegan E. Hines', 'John P. Dickerson']",2,"Credit is an essential component of financial wellbeing in America, and unequal access to it is a large factor in the economic disparities between demographic groups that exist today. Today, machine learning algorithms, sometimes trained on alternative data, are increasingly being used to determine access to credit, yet research has shown that machine learning can encode many different versions of ""unfairness,"" thus raising the concern that banks and other financial institutions could---potentially unwittingly---engage in illegal discrimination through the use of this technology. In the US, there are laws in place to make sure discrimination does not happen in lending and agencies charged with enforcing them. However, conversations around fair credit models in computer science and in policy are often misaligned: fair machine learning research often lacks legal and practical considerations specific to existing fair lending policy, and regulators have yet to issue new guidance on how, if at all, credit risk models should be utilizing practices and techniques from the research community. This paper aims to better align these sides of the conversation. We describe the current state of credit discrimination regulation in the United States, contextualize results from fair ML research to identify the specific fairness concerns raised by the use of machine learning in lending, and discuss regulatory opportunities to address these concerns.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3514094.3534154,
author = {Kumar, I. Elizabeth and Hines, Keegan E. and Dickerson, John P.},
title = {Equalizing Credit Opportunity in Algorithms: Aligning Algorithmic Fairness Research with U.S. Fair Lending Regulation},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534154},
doi = {10.1145/3514094.3534154},
abstract = {Credit is an essential component of financial wellbeing in America, and unequal access to it is a large factor in the economic disparities between demographic groups that exist today. Today, machine learning algorithms, sometimes trained on alternative data, are increasingly being used to determine access to credit, yet research has shown that machine learning can encode many different versions of ""unfairness,"" thus raising the concern that banks and other financial institutions could---potentially unwittingly---engage in illegal discrimination through the use of this technology. In the US, there are laws in place to make sure discrimination does not happen in lending and agencies charged with enforcing them. However, conversations around fair credit models in computer science and in policy are often misaligned: fair machine learning research often lacks legal and practical considerations specific to existing fair lending policy, and regulators have yet to issue new guidance on how, if at all, credit risk models should be utilizing practices and techniques from the research community. This paper aims to better align these sides of the conversation. We describe the current state of credit discrimination regulation in the United States, contextualize results from fair ML research to identify the specific fairness concerns raised by the use of machine learning in lending, and discuss regulatory opportunities to address these concerns.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {357–368},
numpages = {12},
keywords = {fair machine learning, machine learning, algorithmic fairness, ecoa, fair lending law},
location = {Oxford, United Kingdom},
series = {AIES '22}
}
  
"
AIES,2022,Investigating Debiasing Effects on Classification and Explainability,https://dl.acm.org/doi/10.1145/3514094.3534170,"['Marta Marchiori Manerba', 'Riccardo Guidotti']",0,"During each stage of a dataset creation and development process, harmful biases can be accidentally introduced, leading to models that perpetuates marginalization and discrimination of minorities, as the role of the data used during the training is critical. We propose an evaluation framework that investigates the impact on classification and explainability of bias mitigation preprocessing techniques used to assess data imbalances concerning minorities' representativeness and mitigate the skewed distributions discovered. Our evaluation focuses on assessing fairness, explainability and performance metrics. We analyze the behavior of local model-agnostic explainers on the original and mitigated datasets to examine whether the proxy models learned by the explainability techniques to mimic the black-boxes disproportionately rely on sensitive attributes, demonstrating biases rooted in the explainers. We conduct several experiments about known biased datasets to demonstrate our proposal's novelty and effectiveness for evaluation and bias detection purposes.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3514094.3534170,
author = {Marchiori Manerba, Marta and Guidotti, Riccardo},
title = {Investigating Debiasing Effects on Classification and Explainability},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534170},
doi = {10.1145/3514094.3534170},
abstract = {During each stage of a dataset creation and development process, harmful biases can be accidentally introduced, leading to models that perpetuates marginalization and discrimination of minorities, as the role of the data used during the training is critical. We propose an evaluation framework that investigates the impact on classification and explainability of bias mitigation preprocessing techniques used to assess data imbalances concerning minorities' representativeness and mitigate the skewed distributions discovered. Our evaluation focuses on assessing fairness, explainability and performance metrics. We analyze the behavior of local model-agnostic explainers on the original and mitigated datasets to examine whether the proxy models learned by the explainability techniques to mimic the black-boxes disproportionately rely on sensitive attributes, demonstrating biases rooted in the explainers. We conduct several experiments about known biased datasets to demonstrate our proposal's novelty and effectiveness for evaluation and bias detection purposes.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {468–478},
numpages = {11},
keywords = {data equity, algorithmic bias, algorithmic auditing, fairness in ml, ml evaluation, xai, bias mitigation},
location = {Oxford, United Kingdom},
series = {AIES '22}
}
  
"
AIES,2022,Respect as a Lens for the Design of AI Systems,https://dl.acm.org/doi/10.1145/3514094.3534186,"['William Seymour', 'Max Van Kleek', 'Reuben Binns', 'Dave Murray-Rust']",1,"Critical examinations of AI systems often apply principles such as fairness, justice, accountability, and safety, which is reflected in AI regulations such as the EU AI Act. Are such principles sufficient to promote the design of systems that support human flourishing? Even if a system is in some sense fair, just, or 'safe', it can nonetheless be exploitative, coercive, inconvenient, or otherwise conflict with cultural, individual, or social values. This paper proposes a dimension of interactional ethics thus far overlooked: the ways AI systems should treat human beings. For this purpose, we explore the philosophical concept of respect: if respect is something everyone needs and deserves, shouldn't technology aim to be respectful? Despite its intuitive simplicity, respect in philosophy is a complex concept with many disparate senses. Like fairness or justice, respect can characterise how people deserve to be treated; but rather than relating primarily to the distribution of benefits or punishments, respect relates to how people regard one another, and how this translates to perception, treatment, and behaviour. We explore respect broadly across several literatures, synthesising perspectives on respect from Kantian, post-Kantian, dramaturgical, and agential realist design perspectives with a goal of drawing together a view of what respect could mean for AI. In so doing, we identify ways that respect may guide us towards more sociable artefacts that ethically and inclusively honour and recognise humans using the rich social language that we have evolved to interact with one another every day.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3514094.3534186,
author = {Seymour, William and Van Kleek, Max and Binns, Reuben and Murray-Rust, Dave},
title = {Respect as a Lens for the Design of AI Systems},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534186},
doi = {10.1145/3514094.3534186},
abstract = {Critical examinations of AI systems often apply principles such as fairness, justice, accountability, and safety, which is reflected in AI regulations such as the EU AI Act. Are such principles sufficient to promote the design of systems that support human flourishing? Even if a system is in some sense fair, just, or 'safe', it can nonetheless be exploitative, coercive, inconvenient, or otherwise conflict with cultural, individual, or social values. This paper proposes a dimension of interactional ethics thus far overlooked: the ways AI systems should treat human beings. For this purpose, we explore the philosophical concept of respect: if respect is something everyone needs and deserves, shouldn't technology aim to be respectful? Despite its intuitive simplicity, respect in philosophy is a complex concept with many disparate senses. Like fairness or justice, respect can characterise how people deserve to be treated; but rather than relating primarily to the distribution of benefits or punishments, respect relates to how people regard one another, and how this translates to perception, treatment, and behaviour. We explore respect broadly across several literatures, synthesising perspectives on respect from Kantian, post-Kantian, dramaturgical, and agential realist design perspectives with a goal of drawing together a view of what respect could mean for AI. In so doing, we identify ways that respect may guide us towards more sociable artefacts that ethically and inclusively honour and recognise humans using the rich social language that we have evolved to interact with one another every day.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {641–652},
numpages = {12},
keywords = {respect, ethical design, AI systems},
location = {Oxford, United Kingdom},
series = {AIES '22}
}
  
"
AIES,2022,Generating Deontic Obligations From Utility-Maximizing Systems,https://dl.acm.org/doi/10.1145/3514094.3534163,"['Colin Shea-Blymyer', 'Houssam Abbas']",0,"This work gives a logical characterization of the (ethical and social) obligations of an agent trained with Reinforcement Learning (RL). An RL agent takes actions by following a utility-maximizing policy. We maintain that the choice of utility function embeds ethical and social values implicitly, and that it is necessary to make these values explicit. This work provides a basis for doing so. First, we propose a probabilistic deontic logic that is suited for formally specifying the obligations of a stochastic system, including its ethical obligations. We prove some useful validities about this logic, and how its semantics are compatible with those of Markov Decision Processes (MDPs). Second, we show that model checking allows us to prove that an agent has a given obligation to bring about some state of affairs - meaning that by acting optimally, it is seeking to reach that state of affairs. We develop a model checker for our logic against MDPs. Third, we observe that it is useful for a system designer to obtain a logical characterization of her system's obligations, which is potentially more interpretable and helpful in debugging than the expression of a utility function. Enumerating all the obligations of an agent is impractical, so we propose a Bayesian optimization routine that learns to generate a system's obligations that the system designer deems interesting. We implement the model checking and Bayesian optimization routines, and demonstrate their effectiveness with an initial pilot study. This work provides a rigorous method to characterize utility-maximizing agents in terms of the (ethical and social) obligations that they implicitly seek to satisfy.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3514094.3534163,
author = {Shea-Blymyer, Colin and Abbas, Houssam},
title = {Generating Deontic Obligations From Utility-Maximizing Systems},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534163},
doi = {10.1145/3514094.3534163},
abstract = {This work gives a logical characterization of the (ethical and social) obligations of an agent trained with Reinforcement Learning (RL). An RL agent takes actions by following a utility-maximizing policy. We maintain that the choice of utility function embeds ethical and social values implicitly, and that it is necessary to make these values explicit. This work provides a basis for doing so. First, we propose a probabilistic deontic logic that is suited for formally specifying the obligations of a stochastic system, including its ethical obligations. We prove some useful validities about this logic, and how its semantics are compatible with those of Markov Decision Processes (MDPs). Second, we show that model checking allows us to prove that an agent has a given obligation to bring about some state of affairs - meaning that by acting optimally, it is seeking to reach that state of affairs. We develop a model checker for our logic against MDPs. Third, we observe that it is useful for a system designer to obtain a logical characterization of her system's obligations, which is potentially more interpretable and helpful in debugging than the expression of a utility function. Enumerating all the obligations of an agent is impractical, so we propose a Bayesian optimization routine that learns to generate a system's obligations that the system designer deems interesting. We implement the model checking and Bayesian optimization routines, and demonstrate their effectiveness with an initial pilot study. This work provides a rigorous method to characterize utility-maximizing agents in terms of the (ethical and social) obligations that they implicitly seek to satisfy.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {653–663},
numpages = {11},
keywords = {model checking, normative systems, deontic logic, explainability, machine ethics},
location = {Oxford, United Kingdom},
series = {AIES '22}
}
  
"
AIES,2022,Achievement and Fragility of Long-term Equitability,https://dl.acm.org/doi/10.1145/3514094.3534132,"['Andrea Simonetto', 'Ivano Notarnicola']",0,"Equipping current decision-making tools with notions of fairness, equitability, or other ethically motivated outcomes, is one of the top priorities in recent research efforts in machine learning, AI, and optimization. In this paper, we investigate how to allocate limited resources to locally interacting communities in a way to maximize a pertinent notion of equitability. In particular, we look at the dynamic setting where the allocation is repeated across multiple periods (e.g., yearly), the local communities evolve in the meantime (driven by the provided allocation), and the allocations are modulated by feedback coming from the communities themselves. We employ recent mathematical tools stemming from data-driven feedback online optimization, by which communities can learn their (possibly unknown) evolution, satisfaction, as well as they can share information with the deciding bodies. We design dynamic policies that converge to an allocation that maximize equitability in the long term. We further demonstrate our model and methodology with realistic examples of healthcare and education subsidies design in Sub-Saharian countries. One of the key empirical takeaways from our setting is that long-term equitability is fragile, in the sense that it can be easily lost when deciding bodies weigh in other factors (e.g., equality in allocation) in the allocation strategy. Moreover, a naive compromise, while not providing significant advantage to the communities, can promote inequality in social outcomes.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3514094.3534132,
author = {Simonetto, Andrea and Notarnicola, Ivano},
title = {Achievement and Fragility of Long-Term Equitability},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534132},
doi = {10.1145/3514094.3534132},
abstract = {Equipping current decision-making tools with notions of fairness, equitability, or other ethically motivated outcomes, is one of the top priorities in recent research efforts in machine learning, AI, and optimization. In this paper, we investigate how to allocate limited resources to locally interacting communities in a way to maximize a pertinent notion of equitability. In particular, we look at the dynamic setting where the allocation is repeated across multiple periods (e.g., yearly), the local communities evolve in the meantime (driven by the provided allocation), and the allocations are modulated by feedback coming from the communities themselves. We employ recent mathematical tools stemming from data-driven feedback online optimization, by which communities can learn their (possibly unknown) evolution, satisfaction, as well as they can share information with the deciding bodies. We design dynamic policies that converge to an allocation that maximize equitability in the long term. We further demonstrate our model and methodology with realistic examples of healthcare and education subsidies design in Sub-Saharian countries. One of the key empirical takeaways from our setting is that long-term equitability is fragile, in the sense that it can be easily lost when deciding bodies weigh in other factors (e.g., equality in allocation) in the allocation strategy. Moreover, a naive compromise, while not providing significant advantage to the communities, can promote inequality in social outcomes.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {675–685},
numpages = {11},
keywords = {dynamical systems, fairness, equitability, optimization, subsidies design},
location = {Oxford, United Kingdom},
series = {AIES '22}
}
  
"
AIES,2022,"AI and Us: Ethical Concerns, Public Knowledge and Public Attitudes on Artificial Intelligence",https://dl.acm.org/doi/10.1145/3514094.3539518,['Marina Budic'],0,"In the first part I present a theoretical study of ethical challenges arising from the development and application of AI, while in the second part I present an empirical study of public attitudes towards the various aspects of AI use, which I have conducted. Artificial intelligence (AI) describes systems that mimic human cognitive functions and that are designed to achieve specific goals, execute tasks, make decisions or achieve goals in complex situations autonomously. AI can greatly facilitate people's daily and business lives, but it also brings challenges, such as privacy, transparency, discrimination, job loss, and responsibility. One of the main challenges related to the adoption and implementation of AI is the current connotations and perceptions of the subject. To some people, AI is a mysterious concept, and it is difficult to understand how it manifests in their daily lives. The development and application of AI raise several ethical issues, including the interaction of AI with human rights such as privacy and discrimination [2]. Previous studies [1, 3-5] have examined some aspects of AI. In this study, I have employed a new scale to measure the general attitudes of the public towards AI. I have conducted an empirical study of public attitudes towards various aspects of the use of AI. The participants were the public in Serbia (N=737) who filled out an online questionnaire. The questionnaire consisted of general socio-demographic questions, questions about familiarity with AI, a brief introduction to the concept of AI, and questions about participants' attitudes towards the use of AI. Participants were asked to what degree they (dis)agree with the statements on the 5-point Likert scale. I have investigated whether age, education, previous knowledge of AI, profession, and level of religiosity influences people's attitudes towards AI. Objectives were to (1) examine the public's attitudes in Serbia toward the use of AI; (2) determine which factors influence these attitudes; (3) examine whether the public shares the concerns identified by ethical philosophers in the debates on AI; (4) examine prior knowledge of the public on AI. Data were analyzed in the R programming language. The general attitude towards the use of AI was assessed by creating a composite score based on the questions in which the respondents expressed their attitudes towards different aspects of the use of AI. The results showed that the public in Serbia has a divided opinion on the use of AI. Half of them have positive, and the other half negative attitudes. These attitudes are influenced by variables such as the respondents' age, education, profession, level of religiosity, and prior knowledge of AI, in a way that young, highly educated, non-religious, those with highly qualified jobs, especially IT professionals, and those who are more familiar with the concept of AI, have more positive attitudes towards the use of AI. Also, the results showed that the public is concerned about the disappearance of professions due to the development of AI and discrimination by AI systems. The results suggest a need to educate the public about challenges and ways to prevent them. It is necessary to increase transparency in decision-making processes related to the implementation of AI and dialogue between the public on the one hand and the state and the private sector on the other. I argue that considering different aspects of public attitudes toward AI enhances this debate. The results are valuable for future work on this topic because similar public opinion polls have not been conducted, especially not in Serbia. Public opinion has not shaped conversations about the use of AI. Another focus should be on further research of the attitudes of the public and experts on specific applications of AI and related ethical challenges. This research has implications for future research, particularly when forming an AI attitude scale.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3514094.3539518,
author = {Budic, Marina},
title = {AI and Us: Ethical Concerns, Public Knowledge and Public Attitudes on Artificial Intelligence},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3539518},
doi = {10.1145/3514094.3539518},
abstract = {In the first part I present a theoretical study of ethical challenges arising from the development and application of AI, while in the second part I present an empirical study of public attitudes towards the various aspects of AI use, which I have conducted. Artificial intelligence (AI) describes systems that mimic human cognitive functions and that are designed to achieve specific goals, execute tasks, make decisions or achieve goals in complex situations autonomously.AI can greatly facilitate people's daily and business lives, but it also brings challenges, such as privacy, transparency, discrimination, job loss, and responsibility. One of the main challenges related to the adoption and implementation of AI is the current connotations and perceptions of the subject. To some people, AI is a mysterious concept, and it is difficult to understand how it manifests in their daily lives. The development and application of AI raise several ethical issues, including the interaction of AI with human rights such as privacy and discrimination [2]. Previous studies [1, 3-5] have examined some aspects of AI. In this study, I have employed a new scale to measure the general attitudes of the public towards AI.I have conducted an empirical study of public attitudes towards various aspects of the use of AI. The participants were the public in Serbia (N=737) who filled out an online questionnaire. The questionnaire consisted of general socio-demographic questions, questions about familiarity with AI, a brief introduction to the concept of AI, and questions about participants' attitudes towards the use of AI. Participants were asked to what degree they (dis)agree with the statements on the 5-point Likert scale. I have investigated whether age, education, previous knowledge of AI, profession, and level of religiosity influences people's attitudes towards AI. Objectives were to (1) examine the public's attitudes in Serbia toward the use of AI; (2) determine which factors influence these attitudes; (3) examine whether the public shares the concerns identified by ethical philosophers in the debates on AI; (4) examine prior knowledge of the public on AI. Data were analyzed in the R programming language. The general attitude towards the use of AI was assessed by creating a composite score based on the questions in which the respondents expressed their attitudes towards different aspects of the use of AI.The results showed that the public in Serbia has a divided opinion on the use of AI. Half of them have positive, and the other half negative attitudes. These attitudes are influenced by variables such as the respondents' age, education, profession, level of religiosity, and prior knowledge of AI, in a way that young, highly educated, non-religious, those with highly qualified jobs, especially IT professionals, and those who are more familiar with the concept of AI, have more positive attitudes towards the use of AI. Also, the results showed that the public is concerned about the disappearance of professions due to the development of AI and discrimination by AI systems. The results suggest a need to educate the public about challenges and ways to prevent them. It is necessary to increase transparency in decision-making processes related to the implementation of AI and dialogue between the public on the one hand and the state and the private sector on the other. I argue that considering different aspects of public attitudes toward AI enhances this debate. The results are valuable for future work on this topic because similar public opinion polls have not been conducted, especially not in Serbia. Public opinion has not shaped conversations about the use of AI. Another focus should be on further research of the attitudes of the public and experts on specific applications of AI and related ethical challenges. This research has implications for future research, particularly when forming an AI attitude scale.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {892},
numpages = {1},
keywords = {public attitudes, ethical concerns, AI, knowledge},
location = {Oxford, United Kingdom},
series = {AIES '22}
}
  
"
AIES,2022,Exploitation and Algorithmic Pricing: Regressive Distributive Outcomes in Consumer-to-Business Transactions,https://dl.acm.org/doi/10.1145/3514094.3539542,['Arianna Dini'],0,"I present a theory of exploitation that can be used to appraise algorithmic pricing and identify when it might contribute to injustice. I develop a structural, consequentialist theory, which shows that exploitation is parasitic on existing inequalities. Exploitation is a sub-theme within the broader concerns of distributive egalitarians. I use ""inequality"" to signify unjust distributive inequity resulting from morally arbitrary, rather than justified, differences. This distinction is murky: skills are often as much a product of privilege as they are of work ethic. Nevertheless, some causes of distributive inequality (class, ethnicity, or gender) are more obviously morally arbitrary than others (skill or work ethic). There are many reasons for supporting distributive equality: I favour the Humean idea that equality is necessary for reciprocity, and that reciprocity is the foundation of a just social equilibrium [1][2]. However, a defence of this theory is beyond the scope of this paper; I therefore take it as a given that a reasonable degree of distributive equality is desirable in contemporary nation states, without specifying how much is ""reasonable"". The information asymmetry between sellers and consumers is the most obvious inequality in transactions mediated by algorithmic pricing. However, even without information asymmetry, dynamic pricing algorithms are designed to detect any imbalance between supply and demand, and to exploit this as much as possible to maximise profits. Since demand is a reflection of scarcity and urgency, dynamic pricing pushes prices up as far as the consumer's bargaining position will allow-without losing the consumer. I therefore address information asymmetry separately (forthcoming); here I show that the sufficient conditions for exploitation to emerge are minimal, and do not require the intent to manipulate consumers.I respond to potential criticisms of my theory. I explain why the Paretian defence-that mutually beneficial transactions are permissible, irrespective of the difference in benefit [3]-only holds up in case there is a dichotomy (deal/no deal), but that the value of a theory of exploitation is precisely that it highlights the contingency of such dichotomies. This allows me answer the concerns of critics who worry that a consequentialist conception of exploitation guts the concept of its inherent wrongness [4]. In some cases, an exploitative contract may be pragmatically better than no contract, but an injustice-albeit a ""lesser evil""-is occurring. Calling it ""exploitation"" remains a useful way of drawing attention to the structural circumstances which gave rise to unfair option sets. I explain how my theory applies to algorithmic pricing in consumer-to-platform transactions, by presenting a 2020 study, which found that ride-hailing apps in Chicago were charging passengers from low-income neighbourhoods and ethnic minority backgrounds more for lifts than white passengers from more affluent neighbourhoods [5]. Reserve price is not only a marker of disposable income, but rather, of a whole set of factors that influence willingness-to-pay. Scarcity drives demand up. In low-income neighbourhoods, fewer residents are likely to demand ride-hailing services. Due to low demand, drivers are less likely to enter these areas, as doing so will decrease the chance of finding another passenger quickly. Low-income neighbourhoods thereby become under-serviced, and residents who do hail rides are effectively penalised by having to pay a surcharge. In cases that attracted widespread condemnation, price surges occurred in the wake of mass shootings, terrorist attacks, or natural disasters [6]. Such cases may be wrong for other reasons, but do not fall into the category of exploitation as I define it, since the consumers' bargaining power is diminished due to highly rare and improbable circumstances rather than structural inequality. Thus, dynamic pricing can produce regressive outcomes. This should concern anyone interested in distributive equality, and might justify interventions to mitigate the effects on vulnerable groups.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3514094.3539542,
author = {Dini, Arianna},
title = {Exploitation and Algorithmic Pricing: Regressive Distributive Outcomes in Consumer-to-Business Transactions},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3539542},
doi = {10.1145/3514094.3539542},
abstract = {I present a theory of exploitation that can be used to appraise algorithmic pricing and identify when it might contribute to injustice. I develop a structural, consequentialist theory, which shows that exploitation is parasitic on existing inequalities. Exploitation is a sub-theme within the broader concerns of distributive egalitarians. I use ""inequality"" to signify unjust distributive inequity resulting from morally arbitrary, rather than justified, differences. This distinction is murky: skills are often as much a product of privilege as they are of work ethic. Nevertheless, some causes of distributive inequality (class, ethnicity, or gender) are more obviously morally arbitrary than others (skill or work ethic). There are many reasons for supporting distributive equality: I favour the Humean idea that equality is necessary for reciprocity, and that reciprocity is the foundation of a just social equilibrium [1][2]. However, a defence of this theory is beyond the scope of this paper; I therefore take it as a given that a reasonable degree of distributive equality is desirable in contemporary nation states, without specifying how much is ""reasonable"".The information asymmetry between sellers and consumers is the most obvious inequality in transactions mediated by algorithmic pricing. However, even without information asymmetry, dynamic pricing algorithms are designed to detect any imbalance between supply and demand, and to exploit this as much as possible to maximise profits. Since demand is a reflection of scarcity and urgency, dynamic pricing pushes prices up as far as the consumer's bargaining position will allow-without losing the consumer. I therefore address information asymmetry separately (forthcoming); here I show that the sufficient conditions for exploitation to emerge are minimal, and do not require the intent to manipulate consumers.I respond to potential criticisms of my theory. I explain why the Paretian defence-that mutually beneficial transactions are permissible, irrespective of the difference in benefit [3]-only holds up in case there is a dichotomy (deal/no deal), but that the value of a theory of exploitation is precisely that it highlights the contingency of such dichotomies.This allows me answer the concerns of critics who worry that a consequentialist conception of exploitation guts the concept of its inherent wrongness [4]. In some cases, an exploitative contract may be pragmatically better than no contract, but an injustice-albeit a ""lesser evil""-is occurring. Calling it ""exploitation"" remains a useful way of drawing attention to the structural circumstances which gave rise to unfair option sets.I explain how my theory applies to algorithmic pricing in consumer-to-platform transactions, by presenting a 2020 study, which found that ride-hailing apps in Chicago were charging passengers from low-income neighbourhoods and ethnic minority backgrounds more for lifts than white passengers from more affluent neighbourhoods [5]. Reserve price is not only a marker of disposable income, but rather, of a whole set of factors that influence willingness-to-pay. Scarcity drives demand up. In low-income neighbourhoods, fewer residents are likely to demand ride-hailing services. Due to low demand, drivers are less likely to enter these areas, as doing so will decrease the chance of finding another passenger quickly. Low-income neighbourhoods thereby become under-serviced, and residents who do hail rides are effectively penalised by having to pay a surcharge. In cases that attracted widespread condemnation, price surges occurred in the wake of mass shootings, terrorist attacks, or natural disasters [6]. Such cases may be wrong for other reasons, but do not fall into the category of exploitation as I define it, since the consumers' bargaining power is diminished due to highly rare and improbable circumstances rather than structural inequality. Thus, dynamic pricing can produce regressive outcomes. This should concern anyone interested in distributive equality, and might justify interventions to mitigate the effects on vulnerable groups.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {896},
numpages = {1},
keywords = {exploitation, distributive justice, algorithmic pricing, structural exploitation, big data, dynamic pricing, pricing algorithms},
location = {Oxford, United Kingdom},
series = {AIES '22}
}
  
"
AIES,2022,The Opacity of Automated Decision-Making Systems (ADMS) and its Challenges for Political Legitimacy in a Democracy,https://dl.acm.org/doi/10.1145/3514094.3539563,['Mar√≠a Carolina Jim√©nez'],0,"This paper focuses specifically on Automated Decision-Making Systems (ADMS) based on Artificial Intelligence (AI). Since the last decades, AI systems are increasingly deployed by governments across the planet to manage public infrastructures and resources, as well as to engage with citizens for the provision of public services. Their introduction is advertised as a cost-cutting tool, as well as an instrument to combat traditional institutional disfunctions such as inefficiency, understaffing, corruption and human bias. While AI offers an incredible potential for progress, an emerging body of literature highlights the challenges that AI-driven decision-making may raise for a public sector ethics. A common trait of these challenges is their being related to some form of ""epistemological opacity"" that undermines the capacity of humans to explain and justify decisions based on AI systems, detect errors or unfairness and adopt corrective actions. The situation may entail public officers and citizens taking the outcomes of AI systems at face value, thus basing their actions (wholly or in part) on pieces of information that cannot be scrutinized and/or corrected if necessary. This paper intends to contribute to an emerging but still underdeveloped trend in normative political theory that study how AI-driven decision-making is reshaping the conceptualization and assessment of interactions between citizens and public officials. The overall goal of the paper is to analyze how various sources of ""epistemological opacity"" (algorithmic/legal/illiteracy/discursive) affecting AI systems, may undermine the democratic legitimacy of public decisions based on them. Broadly speaking, legitimacy is the property that grounds the exercise of political authority, where authority standardly means the right to rule [1]. In this paper, democratic legitimacy is understood as a distinctive form of political authority grounded in the recognition of citizens as joint legislators. The paper offers a conception of democratic legitimacy conditional on the capacity of decision-making procedures and outcomes to realize the principle of public equality, which requires citizens' control over public decision-making, as well as respect for their equal status as political decision-makers. Specifically, the paper argues that the ""epistemological opacity"" affecting AI-driven decision-making systems, brings about a mistreatment of citizens as coauthors of public decisions, which is a premise of the idea of democratic citizenship. The main conjecture is that different sources of ""epistemological opacity"" (algorithmic/legal/illiteracy/discursive) are causing the disengagement of citizens and public officers from public decision-making, either because they directly undermine necessary conditions for the realization of public equality (co-authorship/accountability/publicity), or because they hide from the public eye instances of illegitimate automation and privatization of decisional power. The paper offers a normative conception of democratic legitimacy that may contribute to efforts in various fields, including ""AI fairness"" and ""Explainable AI"", to better adapt technological tools to equality requirements distinctive of public decision-making within democratic societies.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3514094.3539563,
author = {Jim\'{e}nez, Mar\'{\i}a Carolina},
title = {The Opacity of Automated Decision-Making Systems (ADMS) and Its Challenges for Political Legitimacy in a Democracy},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3539563},
doi = {10.1145/3514094.3539563},
abstract = {This paper focuses specifically on Automated Decision-Making Systems (ADMS) based on Artificial Intelligence (AI). Since the last decades, AI systems are increasingly deployed by governments across the planet to manage public infrastructures and resources, as well as to engage with citizens for the provision of public services. Their introduction is advertised as a cost-cutting tool, as well as an instrument to combat traditional institutional disfunctions such as inefficiency, understaffing, corruption and human bias.While AI offers an incredible potential for progress, an emerging body of literature highlights the challenges that AI-driven decision-making may raise for a public sector ethics. A common trait of these challenges is their being related to some form of ""epistemological opacity"" that undermines the capacity of humans to explain and justify decisions based on AI systems, detect errors or unfairness and adopt corrective actions. The situation may entail public officers and citizens taking the outcomes of AI systems at face value, thus basing their actions (wholly or in part) on pieces of information that cannot be scrutinized and/or corrected if necessary.This paper intends to contribute to an emerging but still underdeveloped trend in normative political theory that study how AI-driven decision-making is reshaping the conceptualization and assessment of interactions between citizens and public officials. The overall goal of the paper is to analyze how various sources of ""epistemological opacity"" (algorithmic/legal/illiteracy/discursive) affecting AI systems, may undermine the democratic legitimacy of public decisions based on them.Broadly speaking, legitimacy is the property that grounds the exercise of political authority, where authority standardly means the right to rule [1]. In this paper, democratic legitimacy is understood as a distinctive form of political authority grounded in the recognition of citizens as joint legislators. The paper offers a conception of democratic legitimacy conditional on the capacity of decision-making procedures and outcomes to realize the principle of public equality, which requires citizens' control over public decision-making, as well as respect for their equal status as political decision-makers.Specifically, the paper argues that the ""epistemological opacity"" affecting AI-driven decision-making systems, brings about a mistreatment of citizens as coauthors of public decisions, which is a premise of the idea of democratic citizenship.The main conjecture is that different sources of ""epistemological opacity"" (algorithmic/legal/illiteracy/discursive) are causing the disengagement of citizens and public officers from public decision-making, either because they directly undermine necessary conditions for the realization of public equality (co-authorship/accountability/publicity), or because they hide from the public eye instances of illegitimate automation and privatization of decisional power.The paper offers a normative conception of democratic legitimacy that may contribute to efforts in various fields, including ""AI fairness"" and ""Explainable AI"", to better adapt technological tools to equality requirements distinctive of public decision-making within democratic societies.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {901},
numpages = {1},
keywords = {algorithmic opacity, AI ethics, automation, democratic legitimacy, machine learning, public sector},
location = {Oxford, United Kingdom},
series = {AIES '22}
}
  
"
AIES,2022,Artificial Intelligence Diffusion in Public Administration,https://dl.acm.org/doi/10.1145/3514094.3539529,['Rohit Madan'],0,"The use of Artificial Intelligence (AI) in public administration has immense benefits but also embodies ethical dilemmas of fairness, transparency, privacy, and human rights. Several frameworks have been developed by governments and technology companies to guide the ethical development of AI solutions. However, within public administration implementations, there is a lack of clarity on how decisions on these dilemmas are made and what is the effect of such decisions on public values. This research aims to undertake a mixed-method study exploring the mechanisms and causal links between AI tensions and public value creation.",NOT,,,,,Might need a second look,,,,,,,,,,,"@inproceedings{10.1145/3514094.3539529,
author = {Madan, Rohit},
title = {Artificial Intelligence Diffusion in Public Administration},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3539529},
doi = {10.1145/3514094.3539529},
abstract = {The use of Artificial Intelligence (AI) in public administration has immense benefits but also embodies ethical dilemmas of fairness, transparency, privacy, and human rights. Several frameworks have been developed by governments and technology companies to guide the ethical development of AI solutions. However, within public administration implementations, there is a lack of clarity on how decisions on these dilemmas are made and what is the effect of such decisions on public values. This research aims to undertake a mixed-method study exploring the mechanisms and causal links between AI tensions and public value creation.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {903},
numpages = {1},
keywords = {algorithmic governance, AI ethics, public value, machine learning},
location = {Oxford, United Kingdom},
series = {AIES '22}
}
  
"
AIES,2022,Socially-Aware Artificial Intelligence for Fair Mobility,https://dl.acm.org/doi/10.1145/3514094.3539545,['Dimitris Michailidis'],0,"Mobility systems have a fundamental impact on well-being. It is thus crucial to address the disproportional benefits that their design can lead to. In my research, I explore the trade-off between utility and fairness in transport network design and argue that AI can be used to create networks that achieve different notions of fairness.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3514094.3539545,
author = {Michailidis, Dimitris},
title = {Socially-Aware Artificial Intelligence for Fair Mobility},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3539545},
doi = {10.1145/3514094.3539545},
abstract = {Mobility systems have a fundamental impact on well-being. It is thus crucial to address the disproportional benefits that their design can lead to. In my research, I explore the trade-off between utility and fairness in transport network design and argue that AI can be used to create networks that achieve different notions of fairness.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {904},
numpages = {1},
keywords = {fairness, mobility, reinforcement learning, socially-aware ai},
location = {Oxford, United Kingdom},
series = {AIES '22}
}
  
"
AIES,2022,The Danger of Citizen Domination through Algorithmic Decision-Making in the Public Sector,https://dl.acm.org/doi/10.1145/3514094.3539515,['Jana Misic'],0,"Applying for and gaining welfare benefits from the government is a different practice than a decade ago. Digital technologies, including emerging ones such as artificial intelligence and big data are increasingly popular in the public services sector. One such instance is a pilot project employing citizen profiling through artificial intelligence to implement a more efficient welfare policy in The Hague. The 'Bijstand' (prim. trans. 'Assistance') project relies on the use of machine learning to detect unemployed citizens that could be most efficiently helped by the municipality in finding employment. Intuitively, deployment of such projects has a potentially negative impact on how just and fair procedures of access to employment are and is therefore a cause for concern in respect to citizens' wellbeing. In this paper I aim to validate or challenge the intuition that the citizens who are subjected to public sector decision-making algorithms are in fact dominated upon. Is it feasible or fair to call the algorithmic toll a dominator or should we focus attention on the human agents? I draw on the neo-republican concept of freedom as non-domination and argue for the importance of investigating potential for domination as a structurally constituted form of power when algorithmization of public services takes place. Going beyond, I combine the insights from the neo-republican theory of domination with applied ethics of technology, drawing on the relationships between virtues of freedom and wellbeing vis-a-vis algorithmic accountability. The case study reveals that a new procedural element of unaccountable decision-making is being introduced into the welfare services. A hopeful takeaway from using the neo-republican lens shows that the state can be a guarantor of freedom by establishing the right procedures and venues for the citizens to challenge algorithmic decision-making.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3514094.3539515,
author = {Misic, Jana},
title = {The Danger of Citizen Domination through Algorithmic Decision-Making in the Public Sector},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3539515},
doi = {10.1145/3514094.3539515},
abstract = {Applying for and gaining welfare benefits from the government is a different practice than a decade ago. Digital technologies, including emerging ones such as artificial intelligence and big data are increasingly popular in the public services sector. One such instance is a pilot project employing citizen profiling through artificial intelligence to implement a more efficient welfare policy in The Hague. The 'Bijstand' (prim. trans. 'Assistance') project relies on the use of machine learning to detect unemployed citizens that could be most efficiently helped by the municipality in finding employment. Intuitively, deployment of such projects has a potentially negative impact on how just and fair procedures of access to employment are and is therefore a cause for concern in respect to citizens' wellbeing.In this paper I aim to validate or challenge the intuition that the citizens who are subjected to public sector decision-making algorithms are in fact dominated upon. Is it feasible or fair to call the algorithmic toll a dominator or should we focus attention on the human agents? I draw on the neo-republican concept of freedom as non-domination and argue for the importance of investigating potential for domination as a structurally constituted form of power when algorithmization of public services takes place. Going beyond, I combine the insights from the neo-republican theory of domination with applied ethics of technology, drawing on the relationships between virtues of freedom and wellbeing vis-a-vis algorithmic accountability. The case study reveals that a new procedural element of unaccountable decision-making is being introduced into the welfare services. A hopeful takeaway from using the neo-republican lens shows that the state can be a guarantor of freedom by establishing the right procedures and venues for the citizens to challenge algorithmic decision-making.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {905},
numpages = {1},
keywords = {automated decision-making, public sector, algorithms, non-domination},
location = {Oxford, United Kingdom},
series = {AIES '22}
}
  
"
AIES,2022,What's (Not) Ideal about Fair Machine Learning?,https://dl.acm.org/doi/10.1145/3514094.3539543,['Otto Sahlgren'],0,"Fair machine learning frameworks are normative models that specify and guide the implementation of non-discrimination principles in machine learning (ML) systems. The dominant methodological approach involves (i) defining a fairness metric, the maximum value of which constitutes a target, an end-state of ""ideal fairness"", and (ii) applying a ""bias mitigation"" method that improves the system against this metric. Recent works have charged severe critiques against existing proposals in fair ML, attributing many alleged shortcomings to the dominant ""idealized"" methodology therein. These charges echo critiques of so-called ""ideal theory"" in political philosophy. I review methodological critiques of fair machine learning and contextualize them against the background of the ""ideal theory"" debate, drawing lessons for ""nonideal"" approaches to fair machine learning.",NOT,,,,,Maybe for related work,,,,,,,,,,,"@inproceedings{10.1145/3514094.3539543,
author = {Sahlgren, Otto},
title = {What's (Not) Ideal about Fair Machine Learning?},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3539543},
doi = {10.1145/3514094.3539543},
abstract = {Fair machine learning frameworks are normative models that specify and guide the implementation of non-discrimination principles in machine learning (ML) systems. The dominant methodological approach involves (i) defining a fairness metric, the maximum value of which constitutes a target, an end-state of ""ideal fairness"", and (ii) applying a ""bias mitigation"" method that improves the system against this metric. Recent works have charged severe critiques against existing proposals in fair ML, attributing many alleged shortcomings to the dominant ""idealized"" methodology therein. These charges echo critiques of so-called ""ideal theory"" in political philosophy. I review methodological critiques of fair machine learning and contextualize them against the background of the ""ideal theory"" debate, drawing lessons for ""nonideal"" approaches to fair machine learning.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {911},
numpages = {1},
keywords = {algorithmic fairness, fair machine learning, sociotechnical systems, ideal theory, methodology of political philosophy},
location = {Oxford, United Kingdom},
series = {AIES '22}
}
  
"
EAAMO,2021,Opportunities for a More Interdisciplinary Approach to Measuring Perceptions of Fairness in Machine Learning,https://dl.acm.org/doi/10.1145/3465416.3483302,"['C. Malik Boykin', 'Sophia T. Dasch', 'Vincent Rice Jr.', 'Venkat R. Lakshminarayanan', 'Taiwo A. Togun', 'Sarah M. Brown']",0,"As machine learning (ML) is deployed in high-stakes domains, such as disease diagnosis or prison sentencing, questions of fairness have become an area of concern in its development. This interest has produced a variety of statistical fairness definitions derived from classical performance metrics which further expand the decisions that ML practitioners must make in building a system. The need to choose between these definitions raises questions about what conditions influence people to perceive an algorithm as fair or not. Recent results highlight the heavily contextual nature of fairness perceptions, and the specific conditions under which psychological principles such as framing can reliably sway these perceptions. Additional interdisciplinary insights include lessons from the replication crisis within psychology, from which we can glean best-practices for reproducible empirical research. We survey key research at the intersection of ML and psychology, focusing on psychological mechanisms underlying fairness preferences. We conclude by stating the continued need for interdisciplinary research, and underscore best-practices that can inform the state-of-the-art practice. We consider this research to be of a descriptive nature, enabling a deeper understanding and a substantiated discussion.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3465416.3483302,
author = {Boykin, C. Malik and Dasch, Sophia T. and Rice Jr., Vincent and Lakshminarayanan, Venkat R. and Togun, Taiwo A. and Brown, Sarah M.},
title = {Opportunities for a More Interdisciplinary Approach to Measuring Perceptions of Fairness in Machine Learning},
year = {2021},
isbn = {9781450385534},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3465416.3483302},
doi = {10.1145/3465416.3483302},
abstract = {As machine learning (ML) is deployed in high-stakes domains, such as disease diagnosis or prison sentencing, questions of fairness have become an area of concern in its development. This interest has produced a variety of statistical fairness definitions derived from classical performance metrics which further expand the decisions that ML practitioners must make in building a system. The need to choose between these definitions raises questions about what conditions influence people to perceive an algorithm as fair or not. Recent results highlight the heavily contextual nature of fairness perceptions, and the specific conditions under which psychological principles such as framing can reliably sway these perceptions. Additional interdisciplinary insights include lessons from the replication crisis within psychology, from which we can glean best-practices for reproducible empirical research. We survey key research at the intersection of ML and psychology, focusing on psychological mechanisms underlying fairness preferences. We conclude by stating the continued need for interdisciplinary research, and underscore best-practices that can inform the state-of-the-art practice. We consider this research to be of a descriptive nature, enabling a deeper understanding and a substantiated discussion.},
booktitle = {Equity and Access in Algorithms, Mechanisms, and Optimization},
articleno = {1},
numpages = {9},
keywords = {machine learning, fairness, experiment design},
location = {--, NY, USA},
series = {EAAMO '21}
}
  
"
EAAMO,2021,Disaggregated Interventions to Reduce Inequality,https://dl.acm.org/doi/10.1145/3465416.3483286,"['Lucius Bynum', 'Joshua Loftus', 'Julia Stoyanovich']",2,"A significant body of research in the data sciences considers unfair discrimination against social categories such as race or gender that could occur or be amplified as a result of algorithmic decisions. Simultaneously, real-world disparities continue to exist, even before algorithmic decisions are made. In this work, we draw on insights from the social sciences brought into the realm of causal modeling and constrained optimization, and develop a novel algorithmic framework for tackling pre-existing real-world disparities. The purpose of our framework, which we call the ‚Äúimpact remediation framework,‚Äù is to measure real-world disparities and discover the optimal intervention policies that could help improve equity or access to opportunity for those who are underserved with respect to an outcome of interest. We develop a disaggregated approach to tackling pre-existing disparities that relaxes the typical set of assumptions required for the use of social categories in structural causal models. Our approach flexibly incorporates counterfactuals and is compatible with various ontological assumptions about the nature of social categories. We demonstrate impact remediation with a hypothetical case study and compare our disaggregated approach to an existing state-of-the-art approach, comparing its structure and resulting policy recommendations. In contrast to most work on optimal policy learning, we explore disparity reduction itself as an objective, explicitly focusing the power of algorithms on reducing inequality.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3465416.3483286,
author = {Bynum, Lucius and Loftus, Joshua and Stoyanovich, Julia},
title = {Disaggregated Interventions to Reduce Inequality},
year = {2021},
isbn = {9781450385534},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3465416.3483286},
doi = {10.1145/3465416.3483286},
abstract = {A significant body of research in the data sciences considers unfair discrimination against social categories such as race or gender that could occur or be amplified as a result of algorithmic decisions. Simultaneously, real-world disparities continue to exist, even before algorithmic decisions are made. In this work, we draw on insights from the social sciences brought into the realm of causal modeling and constrained optimization, and develop a novel algorithmic framework for tackling pre-existing real-world disparities. The purpose of our framework, which we call the “impact remediation framework,” is to measure real-world disparities and discover the optimal intervention policies that could help improve equity or access to opportunity for those who are underserved with respect to an outcome of interest. We develop a disaggregated approach to tackling pre-existing disparities that relaxes the typical set of assumptions required for the use of social categories in structural causal models. Our approach flexibly incorporates counterfactuals and is compatible with various ontological assumptions about the nature of social categories. We demonstrate impact remediation with a hypothetical case study and compare our disaggregated approach to an existing state-of-the-art approach, comparing its structure and resulting policy recommendations. In contrast to most work on optimal policy learning, we explore disparity reduction itself as an objective, explicitly focusing the power of algorithms on reducing inequality.},
booktitle = {Equity and Access in Algorithms, Mechanisms, and Optimization},
articleno = {2},
numpages = {13},
keywords = {causal modeling, social categories, fairness, inequality},
location = {--, NY, USA},
series = {EAAMO '21}
}
  
"
EAAMO,2021,Open Data Standard and Analysis Framework: Towards Response Equity in Local Governments,https://dl.acm.org/doi/10.1145/3465416.3483290,"['Joy Hsu', 'Ramya Ravichandran', 'Edwin Zhang', 'Christine Keung']",1,"There is an increasing need for open data in governments and systems to analyze equity at large scale. Local governments often lack the necessary technical tools to identify and tackle inequities in their communities. Moreover, these tools may not generalize across departments and cities nor be accessible to the public. To this end, we propose a system that facilitates centralized analyses of publicly available government datasets through 1) a US Census-linked API, 2) an equity analysis playbook, and 3) an open data standard to regulate data intake and support equitable policymaking.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3465416.3483290,
author = {Hsu, Joy and Ravichandran, Ramya and Zhang, Edwin and Keung, Christine},
title = {Open Data Standard and Analysis Framework: Towards Response Equity in Local Governments},
year = {2021},
isbn = {9781450385534},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3465416.3483290},
doi = {10.1145/3465416.3483290},
abstract = {There is an increasing need for open data in governments and systems to analyze equity at large scale. Local governments often lack the necessary technical tools to identify and tackle inequities in their communities. Moreover, these tools may not generalize across departments and cities nor be accessible to the public. To this end, we propose a system that facilitates centralized analyses of publicly available government datasets through 1) a US Census-linked API, 2) an equity analysis playbook, and 3) an open data standard to regulate data intake and support equitable policymaking.},
booktitle = {Equity and Access in Algorithms, Mechanisms, and Optimization},
articleno = {8},
numpages = {8},
keywords = {equity analysis, local government data, open data standard},
location = {--, NY, USA},
series = {EAAMO '21}
}
  
"
EAAMO,2021,When Efficiency meets Equity in Congestion Pricing and Revenue Refunding Schemes,https://dl.acm.org/doi/10.1145/3465416.3483296,"['Devansh Jalota', 'Kiril Solovey', 'Karthik Gopalakrishnan', 'Stephen Zoepf', 'Hamsa Balakrishnan', 'Marco Pavone']",7,"Congestion pricing has long been hailed as a means to mitigate traffic congestion; however, its practical adoption has been limited due to social inequity issues, e.g., low-income users are priced out off certain roads. This issue has spurred interest in the design of equitable mechanisms that refund the collected toll revenues to users. Although revenue refunding has been extensively studied, there has been no characterization of how such schemes can be designed to simultaneously achieve system efficiency and equity objectives. In this work, we bridge this gap through the study of congestion pricing and revenue refunding (CPRR) schemes in non-atomic congestion games. We first develop CPRR schemes, which in comparison to the untolled case, simultaneously (i) increase system efficiency and (ii) decrease wealth inequality, while being (iii) user-favorable: irrespective of their initial wealth or values-of-time (which may differ across users) users would experience a lower travel cost after the implementation of the proposed scheme. We then characterize the set of optimal user-favorable CPRR schemes that simultaneously maximize system efficiency and minimize wealth inequality. These results assume a well-studied behavior model of users minimizing a linear function of their travel times and tolls, without considering refunds. Overall, our work demonstrates that through appropriate refunding policies we can achieve system efficiency while reducing wealth inequality.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3465416.3483296,
author = {Jalota, Devansh and Solovey, Kiril and Gopalakrishnan, Karthik and Zoepf, Stephen and Balakrishnan, Hamsa and Pavone, Marco},
title = {When Efficiency Meets Equity in Congestion Pricing and Revenue Refunding Schemes},
year = {2021},
isbn = {9781450385534},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3465416.3483296},
doi = {10.1145/3465416.3483296},
abstract = {Congestion pricing has long been hailed as a means to mitigate traffic congestion; however, its practical adoption has been limited due to social inequity issues, e.g., low-income users are priced out off certain roads. This issue has spurred interest in the design of equitable mechanisms that refund the collected toll revenues to users. Although revenue refunding has been extensively studied, there has been no characterization of how such schemes can be designed to simultaneously achieve system efficiency and equity objectives. In this work, we bridge this gap through the study of congestion pricing and revenue refunding (CPRR) schemes in non-atomic congestion games. We first develop CPRR schemes, which in comparison to the untolled case, simultaneously (i) increase system efficiency and (ii) decrease wealth inequality, while being (iii) user-favorable: irrespective of their initial wealth or values-of-time (which may differ across users) users would experience a lower travel cost after the implementation of the proposed scheme. We then characterize the set of optimal user-favorable CPRR schemes that simultaneously maximize system efficiency and minimize wealth inequality. These results assume a well-studied behavior model of users minimizing a linear function of their travel times and tolls, without considering refunds. Overall, our work demonstrates that through appropriate refunding policies we can achieve system efficiency while reducing wealth inequality.},
booktitle = {Equity and Access in Algorithms, Mechanisms, and Optimization},
articleno = {9},
numpages = {11},
keywords = {Traffic Routing, Wealth Inequality, Congestion Games},
location = {--, NY, USA},
series = {EAAMO '21}
}
  
"
EAAMO,2021,Breaking Taboos in Fair Machine Learning: An Experimental Study,https://dl.acm.org/doi/10.1145/3465416.3483291,"['Julian Nyarko', 'Sharad Goel', 'Roseanna Sommers']",10,"Many scholars, engineers, and policymakers believe that algorithmic fairness requires disregarding information about certain characteristics of individuals, such as their race or gender. Often, the mandate to ‚Äúblind‚Äù algorithms in this way is conveyed as an unconditional ethical imperative‚Äîa minimal requirement of fair treatment‚Äîand any contrary practice is assumed to be morally and politically untenable. However, in some circumstances, prohibiting algorithms from considering information about race or gender can in fact lead to worse outcomes for racial minorities and women, complicating the rationale for blinding. In this paper, we conduct a series of randomized studies to investigate attitudes toward blinding algorithms, both among the general public as well as among computer scientists and professional lawyers. We find, first, that people are generally averse to the use of race and gender in algorithmic determinations of ‚Äúpretrial risk‚Äù‚Äîthe risk that criminal defendants pose to the public if released while awaiting trial. We find, however, that this preference for blinding shifts in response to a relatively mild intervention. In particular, we show that support for the use of race and gender in algorithmic decision-making increases substantially after respondents read a short passage about the possibility that blinding could lead to higher detention rates for Black and female defendants, respectively. Similar effect sizes are observed among the general public, computer scientists, and professional lawyers. These findings suggest that, while many respondents attest that they prefer blind algorithms, their preference is not based on an absolute principle. Rather, blinding is perceived as a way to ensure better outcomes for members of marginalized groups. Accordingly, in circumstances where blinding serves to disadvantage marginalized groups, respondents no longer view the exclusion of protected characteristics as a moral imperative, and the use of such information may become politically viable.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3465416.3483291,
author = {Nyarko, Julian and Goel, Sharad and Sommers, Roseanna},
title = {Breaking Taboos in Fair Machine Learning: An Experimental Study},
year = {2021},
isbn = {9781450385534},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3465416.3483291},
doi = {10.1145/3465416.3483291},
abstract = {Many scholars, engineers, and policymakers believe that algorithmic fairness requires disregarding information about certain characteristics of individuals, such as their race or gender. Often, the mandate to “blind” algorithms in this way is conveyed as an unconditional ethical imperative—a minimal requirement of fair treatment—and any contrary practice is assumed to be morally and politically untenable. However, in some circumstances, prohibiting algorithms from considering information about race or gender can in fact lead to worse outcomes for racial minorities and women, complicating the rationale for blinding. In this paper, we conduct a series of randomized studies to investigate attitudes toward blinding algorithms, both among the general public as well as among computer scientists and professional lawyers. We find, first, that people are generally averse to the use of race and gender in algorithmic determinations of “pretrial risk”—the risk that criminal defendants pose to the public if released while awaiting trial. We find, however, that this preference for blinding shifts in response to a relatively mild intervention. In particular, we show that support for the use of race and gender in algorithmic decision-making increases substantially after respondents read a short passage about the possibility that blinding could lead to higher detention rates for Black and female defendants, respectively. Similar effect sizes are observed among the general public, computer scientists, and professional lawyers. These findings suggest that, while many respondents attest that they prefer blind algorithms, their preference is not based on an absolute principle. Rather, blinding is perceived as a way to ensure better outcomes for members of marginalized groups. Accordingly, in circumstances where blinding serves to disadvantage marginalized groups, respondents no longer view the exclusion of protected characteristics as a moral imperative, and the use of such information may become politically viable.},
booktitle = {Equity and Access in Algorithms, Mechanisms, and Optimization},
articleno = {14},
numpages = {11},
location = {--, NY, USA},
series = {EAAMO '21}
}
  
"
EAAMO,2021,Preserving Diversity when Partitioning: A Geometric Approach,https://dl.acm.org/doi/10.1145/3465416.3483297,"['Sebastian Perez-Salazar', 'Alfredo Torrico', 'Victor Verdugo']",1,"Diversity plays a crucial role in multiple contexts such as team formation, representation of minority groups and generally when allocating resources fairly. Given a community composed by individuals of different types, we study the problem of partitioning this community such that the global diversity is preserved as much as possible in each subgroup. We consider the diversity metric introduced by Simpson in his influential work that, roughly speaking, corresponds to the inverse probability that two individuals are from the same type when taken uniformly at random, with replacement, from the community of interest. We provide a novel perspective by reinterpreting this quantity in geometric terms. We characterize the instances in which the optimal partition exactly preserves the global diversity in each subgroup. When this is not possible, we provide an efficient polynomial-time algorithm that outputs an optimal partition for the problem with two types. Finally, we discuss further challenges and open questions for the problem that considers more than two types.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3465416.3483297,
author = {Perez-Salazar, Sebastian and Torrico, Alfredo and Verdugo, Victor},
title = {Preserving Diversity When Partitioning: A Geometric Approach},
year = {2021},
isbn = {9781450385534},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3465416.3483297},
doi = {10.1145/3465416.3483297},
abstract = {Diversity plays a crucial role in multiple contexts such as team formation, representation of minority groups and generally when allocating resources fairly. Given a community composed by individuals of different types, we study the problem of partitioning this community such that the global diversity is preserved as much as possible in each subgroup. We consider the diversity metric introduced by Simpson in his influential work that, roughly speaking, corresponds to the inverse probability that two individuals are from the same type when taken uniformly at random, with replacement, from the community of interest. We provide a novel perspective by reinterpreting this quantity in geometric terms. We characterize the instances in which the optimal partition exactly preserves the global diversity in each subgroup. When this is not possible, we provide an efficient polynomial-time algorithm that outputs an optimal partition for the problem with two types. Finally, we discuss further challenges and open questions for the problem that considers more than two types.},
booktitle = {Equity and Access in Algorithms, Mechanisms, and Optimization},
articleno = {15},
numpages = {11},
keywords = {Simpson’s index, Partitioning, Fair Division, Diversity},
location = {--, NY, USA},
series = {EAAMO '21}
}
  
"
EAAMO,2021,Algorithmic Auditing and Social Justice: Lessons from the History of Audit Studies,https://dl.acm.org/doi/10.1145/3465416.3483294,"['Briana Vecchione', 'Karen Levy', 'Solon Barocas']",16,"‚ÄúAlgorithmic audits‚Äù have been embraced as tools to investigate the functioning and consequences of sociotechnical systems. Though the term is used somewhat loosely in the algorithmic context and encompasses a variety of methods, it maintains a close connection to audit studies in the social sciences‚Äîwhich have, for decades, used experimental methods to measure the prevalence of discrimination across domains like housing and employment. In the social sciences, audit studies originated in a strong tradition of social justice and participatory action, often involving collaboration between researchers and communities; but scholars have argued that, over time, social science audits have become somewhat distanced from these original goals and priorities. We draw from this history in order to highlight difficult tensions that have shaped the development of social science audits, and to assess their implications in the context of algorithmic auditing. In doing so, we put forth considerations to assist in the development of robust and engaged assessments of sociotechnical systems that draw from auditing‚Äôs roots in racial equity and social justice.",NOT,,,,Might need a second look,,,,,,,,,,,,"@inproceedings{10.1145/3465416.3483294,
author = {Vecchione, Briana and Levy, Karen and Barocas, Solon},
title = {Algorithmic Auditing and Social Justice: Lessons from the History of Audit Studies},
year = {2021},
isbn = {9781450385534},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3465416.3483294},
doi = {10.1145/3465416.3483294},
abstract = {“Algorithmic audits” have been embraced as tools to investigate the functioning and consequences of sociotechnical systems. Though the term is used somewhat loosely in the algorithmic context and encompasses a variety of methods, it maintains a close connection to audit studies in the social sciences—which have, for decades, used experimental methods to measure the prevalence of discrimination across domains like housing and employment. In the social sciences, audit studies originated in a strong tradition of social justice and participatory action, often involving collaboration between researchers and communities; but scholars have argued that, over time, social science audits have become somewhat distanced from these original goals and priorities. We draw from this history in order to highlight difficult tensions that have shaped the development of social science audits, and to assess their implications in the context of algorithmic auditing. In doing so, we put forth considerations to assist in the development of robust and engaged assessments of sociotechnical systems that draw from auditing’s roots in racial equity and social justice.},
booktitle = {Equity and Access in Algorithms, Mechanisms, and Optimization},
articleno = {19},
numpages = {9},
location = {--, NY, USA},
series = {EAAMO '21}
}
  
"
EAAMO,2022,Tackling Documentation Debt: A Survey on Algorithmic Fairness Datasets,https://dl.acm.org/doi/10.1145/3551624.3555286,"['Alessandro Fabris', 'Stefano Messina', 'Gianmaria Silvello', 'Gian Antonio Susto']",0,"A growing community of researchers has been investigating the equity of algorithms, advancing the understanding of risks and opportunities of automated decision-making for historically disadvantaged populations. Progress in fair Machine Learning (ML) hinges on data, which can be appropriately used only if adequately documented. Unfortunately, the research community, as a whole, suffers from a collective data documentation debt caused by a lack of information on specific resources (opacity) and scatteredness of available information (sparsity). In this work, we survey over two hundred datasets employed in algorithmic fairness research, producing standardized and searchable documentation for each of them. Moreover we rigorously identify the three most popular fairness datasets, namely Adult, COMPAS, and German Credit, for which we compile in-depth documentation. This unifying documentation effort targets documentation sparsity and supports multiple contributions. In the first part of this work, we summarize the merits and limitations of Adult, COMPAS, and German Credit, adding to and unifying recent scholarship, calling into question their suitability as general-purpose fairness benchmarks. To overcome this limitation, we document hundreds of available alternatives, annotating their domain and the algorithmic fairness tasks they support, along with additional properties of interest for fairness practitioners and researchers, including their format, cardinality, and the sensitive attributes they encode. In the second part, we summarize this information, zooming in on the domains and tasks supported by these resources. Overall, we assemble and summarize sparse information on hundreds of datasets into a single resource, which we make available to the community, with the aim of tackling the data documentation debt.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3551624.3555286,
author = {Fabris, Alessandro and Messina, Stefano and Silvello, Gianmaria and Susto, Gian Antonio},
title = {Tackling Documentation Debt: A Survey on Algorithmic Fairness Datasets},
year = {2022},
isbn = {9781450394772},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551624.3555286},
doi = {10.1145/3551624.3555286},
abstract = {A growing community of researchers has been investigating the equity of algorithms, advancing the understanding of risks and opportunities of automated decision-making for historically disadvantaged populations. Progress in fair Machine Learning (ML) hinges on data, which can be appropriately used only if adequately documented. Unfortunately, the research community, as a whole, suffers from a collective data documentation debt caused by a lack of information on specific resources (opacity) and scatteredness of available information (sparsity). In this work, we survey over two hundred datasets employed in algorithmic fairness research, producing standardized and searchable documentation for each of them. Moreover we rigorously identify the three most popular fairness datasets, namely Adult, COMPAS, and German Credit, for which we compile in-depth documentation. This unifying documentation effort targets documentation sparsity and supports multiple contributions. In the first part of this work, we summarize the merits and limitations of Adult, COMPAS, and German Credit, adding to and unifying recent scholarship, calling into question their suitability as general-purpose fairness benchmarks. To overcome this limitation, we document hundreds of available alternatives, annotating their domain and the algorithmic fairness tasks they support, along with additional properties of interest for fairness practitioners and researchers, including their format, cardinality, and the sensitive attributes they encode. In the second part, we summarize this information, zooming in on the domains and tasks supported by these resources. Overall, we assemble and summarize sparse information on hundreds of datasets into a single resource, which we make available to the community, with the aim of tackling the data documentation debt.},
booktitle = {Equity and Access in Algorithms, Mechanisms, and Optimization},
articleno = {2},
numpages = {13},
keywords = {Data studies, Documentation debt., Algorithmic fairness},
location = {Arlington, VA, USA},
series = {EAAMO '22}
}
  
"
EAAMO,2022,A Heterogeneous Schelling Model for Wealth Disparity and its Effect on Segregation,https://dl.acm.org/doi/10.1145/3551624.3555293,"['Zhanzhan Zhao', 'Dana Randall']",0,"The Schelling model of segregation was introduced in economics to show how micro-motives can influence macro-behavior. Agents on a lattice have two colors and try to move to a different location if the number of their neighbors with a different color exceeds some threshold. Simulations reveal that even such mild local color preferences, or homophily, are sufficient to cause segregation. In this work, we propose a stochastic generalization of the Schelling model, based on both race and wealth, to understand how carefully architected placement of incentives, such as urban infrastructure, might affect segregation. In our model, each agent is assigned one of two colors along with a label, rich or poor. Further, we designate certain vertices on the lattice as ‚Äúurban sites,‚Äù providing civic infrastructure that most benefits the poorer population, thus incentivizing the occupation of such vertices by poor agents of either color. We look at the stationary distribution of a Markov process reflecting these preferences to understand the long-term effects. We prove that when incentives are large enough, we will have ‚Äùurbanization of poverty,‚Äù an observed effect whereby poor people tend to congregate on urban sites. Moreover, even when homophily preferences are very small, if the incentives are large and there is income inequality in the two-color classes, we can get racial segregation on urban sites but integration on non-urban sites. In contrast, we find an overall mitigation of segregation when the urban sites are distributed throughout the lattice and the incentives for urban sites exceed the homophily biases. We prove that in this case, no matter how strong homophily preferences are, it will be exponentially unlikely that a configuration chosen from stationarity will have large, homogeneous clusters of agents of either color, suggesting we will have racial integration with high probability.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3551624.3555293,
author = {Zhao, Zhanzhan and Randall, Dana},
title = {A Heterogeneous Schelling Model for Wealth Disparity and Its Effect on Segregation},
year = {2022},
isbn = {9781450394772},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551624.3555293},
doi = {10.1145/3551624.3555293},
abstract = {The Schelling model of segregation was introduced in economics to show how micro-motives can influence macro-behavior. Agents on a lattice have two colors and try to move to a different location if the number of their neighbors with a different color exceeds some threshold. Simulations reveal that even such mild local color preferences, or homophily, are sufficient to cause segregation. In this work, we propose a stochastic generalization of the Schelling model, based on both race and wealth, to understand how carefully architected placement of incentives, such as urban infrastructure, might affect segregation. In our model, each agent is assigned one of two colors along with a label, rich or poor. Further, we designate certain vertices on the lattice as “urban sites,” providing civic infrastructure that most benefits the poorer population, thus incentivizing the occupation of such vertices by poor agents of either color. We look at the stationary distribution of a Markov process reflecting these preferences to understand the long-term effects. We prove that when incentives are large enough, we will have ”urbanization of poverty,” an observed effect whereby poor people tend to congregate on urban sites. Moreover, even when homophily preferences are very small, if the incentives are large and there is income inequality in the two-color classes, we can get racial segregation on urban sites but integration on non-urban sites. In contrast, we find an overall mitigation of segregation when the urban sites are distributed throughout the lattice and the incentives for urban sites exceed the homophily biases. We prove that in this case, no matter how strong homophily preferences are, it will be exponentially unlikely that a configuration chosen from stationarity will have large, homogeneous clusters of agents of either color, suggesting we will have racial integration with high probability.},
booktitle = {Equity and Access in Algorithms, Mechanisms, and Optimization},
articleno = {8},
numpages = {10},
keywords = {stochastic processes, wealth disparity, Schelling segregation model, stationary distribution, Peierls arguments},
location = {Arlington, VA, USA},
series = {EAAMO '22}
}
  
"
EAAMO,2022,Diverse Representation via Computational Participatory Elections - Lessons from a Case Study,https://dl.acm.org/doi/10.1145/3551624.3555297,"['Florian Evequoz', 'Johan Rochel', 'Vijay Keswani', 'L. Elisa Celis']",0,"Elections are the central institution of democratic processes, and often the elected body ‚Äì in either public or private governance ‚Äì is a committee of individuals. To ensure the legitimacy of elected bodies, the electoral processes should guarantee that diverse groups are represented, in particular members of groups that are marginalized due to gender, ethnicity, or other socially salient attributes. To address this challenge of representation, we have designed a novel participatory electoral process coined the Representation Pact, implemented with the support of a computational system. That process explicitly enables voters to flexibly decide on representation criteria in a first round, and then lets them vote for candidates in a second round. After the two rounds, a counting method is applied, which selects the committee of candidates that maximizes the number of votes received in the second round, conditioned on satisfying the criteria provided in the first round. With the help of a detailed use case that applied this process in a primary election of 96 representatives in Switzerland, we explain how this method contributes to fairness in political elections by achieving a better ‚Äúdescriptive representation‚Äù. Further, based on this use case, we identify lessons learnt that are applicable to participatory computational systems used in societal or political contexts. Good practices are identified and presented.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3551624.3555297,
author = {Evequoz, Florian and Rochel, Johan and Keswani, Vijay and Celis, L. Elisa},
title = {Diverse Representation via Computational Participatory Elections - Lessons from a Case Study},
year = {2022},
isbn = {9781450394772},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551624.3555297},
doi = {10.1145/3551624.3555297},
abstract = {Elections are the central institution of democratic processes, and often the elected body – in either public or private governance – is a committee of individuals. To ensure the legitimacy of elected bodies, the electoral processes should guarantee that diverse groups are represented, in particular members of groups that are marginalized due to gender, ethnicity, or other socially salient attributes. To address this challenge of representation, we have designed a novel participatory electoral process coined the Representation Pact, implemented with the support of a computational system. That process explicitly enables voters to flexibly decide on representation criteria in a first round, and then lets them vote for candidates in a second round. After the two rounds, a counting method is applied, which selects the committee of candidates that maximizes the number of votes received in the second round, conditioned on satisfying the criteria provided in the first round. With the help of a detailed use case that applied this process in a primary election of 96 representatives in Switzerland, we explain how this method contributes to fairness in political elections by achieving a better “descriptive representation”. Further, based on this use case, we identify lessons learnt that are applicable to participatory computational systems used in societal or political contexts. Good practices are identified and presented.},
booktitle = {Equity and Access in Algorithms, Mechanisms, and Optimization},
articleno = {12},
numpages = {11},
keywords = {representation, democracy, fairness, election, voting},
location = {Arlington, VA, USA},
series = {EAAMO '22}
}
  
"
EAAMO,2022,Towards Substantive Conceptions of Algorithmic Fairness: Normative Guidance from Equal Opportunity Doctrines,https://dl.acm.org/doi/10.1145/3551624.3555303,"['Falaah Arif Khan', 'Eleni Manis', 'Julia Stoyanovich']",0,"In this work we use Equal Opportunity (EO) doctrines from political philosophy to make explicit the normative judgements embedded in different conceptions of algorithmic fairness. We contrast formal EO approaches that narrowly focus on fair contests at discrete decision points, with substantive EO doctrines that look at people‚Äôs fair life chances more holistically over the course of a lifetime. We use this taxonomy to provide a moral interpretation of the impossibility results as the incompatibility between different conceptions of a fair contest ‚Äî foward-facing versus backward-facing ‚Äî when people do not have fair life chances. We use this result to motivate substantive conceptions of algorithmic fairness and outline two plausible fair decision procedures based on the luck egalitarian doctrine of EO, and Rawls‚Äôs principle of fair equality of opportunity.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3551624.3555303,
author = {Arif Khan, Falaah and Manis, Eleni and Stoyanovich, Julia},
title = {Towards Substantive Conceptions of Algorithmic Fairness: Normative Guidance from Equal Opportunity Doctrines},
year = {2022},
isbn = {9781450394772},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551624.3555303},
doi = {10.1145/3551624.3555303},
abstract = {In this work we use Equal Opportunity (EO) doctrines from political philosophy to make explicit the normative judgements embedded in different conceptions of algorithmic fairness. We contrast formal EO approaches that narrowly focus on fair contests at discrete decision points, with substantive EO doctrines that look at people’s fair life chances more holistically over the course of a lifetime. We use this taxonomy to provide a moral interpretation of the impossibility results as the incompatibility between different conceptions of a fair contest — foward-facing versus backward-facing — when people do not have fair life chances. We use this result to motivate substantive conceptions of algorithmic fairness and outline two plausible fair decision procedures based on the luck egalitarian doctrine of EO, and Rawls’s principle of fair equality of opportunity.},
booktitle = {Equity and Access in Algorithms, Mechanisms, and Optimization},
articleno = {18},
numpages = {10},
location = {Arlington, VA, USA},
series = {EAAMO '22}
}
  
"
EAAMO,2022,Attrition of Workers with Minoritized Identities on AI Teams,https://dl.acm.org/doi/10.1145/3551624.3555304,"['Jeffrey Brown', 'Tina Park', 'Jiyoo Chang', 'Mckane Andrus', 'Alice Xiang', 'Christine Custis']",0,"The effects of AI systems are far-reaching and affect diverse communities all over the world. The demographics of AI teams, however, do not reflect this diversity. Instead, these teams, particularly at big tech companies, are dominated by Western, White, and male workers. Strategies for preventing harms done by AI must also include making these teams more representative of the diverse communities that these technologies affect. The pipeline of students from K-12 and university level contributes to this - those with minoritized identities are underrepresented or excluded from pursuing computer science careers. However there has been relatively little attention given to how the culture at tech companies, let alone AI teams, contribute to attrition of minoritized people in the workplace. The current study uses semi-structured interviews with minoritized workers on AI teams, managers of AI teams, and leaders working on diversity, equity, and inclusion (DEI) in the tech field (N = 43), to investigate the reasons why these workers leave these AI teams. The themes from these interviews describe how the culture and climate of these teams may contribute to attrition of minoritized workers, and strategies for making these teams more inclusive and representative of the diverse communities affected by technologies developed by these AI teams. Specifically, the current study found that AI teams in which minoritized workers thrive tend to foster a strong sense of interdisciplinary collaboration, support professional career development, and are run by diverse leaders who understand the importance of undoing the traditional White, Eurocentric, and male workplace norms. These go beyond the ‚Äúquick fixes‚Äù that are prevalent in DEI practices.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3551624.3555304,
author = {Brown, Jeffrey and Park, Tina and Chang, Jiyoo and Andrus, Mckane and Xiang, Alice and Custis, Christine},
title = {Attrition of Workers with Minoritized Identities on AI Teams},
year = {2022},
isbn = {9781450394772},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551624.3555304},
doi = {10.1145/3551624.3555304},
abstract = {The effects of AI systems are far-reaching and affect diverse communities all over the world. The demographics of AI teams, however, do not reflect this diversity. Instead, these teams, particularly at big tech companies, are dominated by Western, White, and male workers. Strategies for preventing harms done by AI must also include making these teams more representative of the diverse communities that these technologies affect. The pipeline of students from K-12 and university level contributes to this - those with minoritized identities are underrepresented or excluded from pursuing computer science careers. However there has been relatively little attention given to how the culture at tech companies, let alone AI teams, contribute to attrition of minoritized people in the workplace. The current study uses semi-structured interviews with minoritized workers on AI teams, managers of AI teams, and leaders working on diversity, equity, and inclusion (DEI) in the tech field (N = 43), to investigate the reasons why these workers leave these AI teams. The themes from these interviews describe how the culture and climate of these teams may contribute to attrition of minoritized workers, and strategies for making these teams more inclusive and representative of the diverse communities affected by technologies developed by these AI teams. Specifically, the current study found that AI teams in which minoritized workers thrive tend to foster a strong sense of interdisciplinary collaboration, support professional career development, and are run by diverse leaders who understand the importance of undoing the traditional White, Eurocentric, and male workplace norms. These go beyond the “quick fixes” that are prevalent in DEI practices.},
booktitle = {Equity and Access in Algorithms, Mechanisms, and Optimization},
articleno = {19},
numpages = {9},
location = {Arlington, VA, USA},
series = {EAAMO '22}
}
  
"
EAAMO,2022,On Meritocracy in Optimal Set Selection,https://dl.acm.org/doi/10.1145/3551624.3555305,"['Thomas Kleine Buening', 'Meirav Segal', 'Debabrota Basu', 'Anne-Marie George', 'Christos Dimitrakakis']",0,"Typically, merit is defined with respect to some intrinsic measure of worth. We instead consider a setting where an individual‚Äôs worth is relative: when a decision maker (DM) selects a set of individuals from a population to maximise expected utility, it is natural to consider the expected marginal contribution (EMC) of each person to the utility. We show that this notion satisfies an axiomatic definition of fairness for this setting. We also show that for certain policy structures, this notion of fairness is aligned with maximising expected utility, while for linear utility functions it is identical to the Shapley value. However, for certain natural policies, such as those that select individuals with a specific set of attributes (e.g. high enough test scores for college admissions), there is a trade-off between meritocracy and utility maximisation. We analyse the effect of constraints on the policy on both utility and fairness in an extensive experiments based on college admissions and outcomes in Norwegian universities.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3551624.3555305,
author = {Kleine Buening, Thomas and Segal, Meirav and Basu, Debabrota and George, Anne-Marie and Dimitrakakis, Christos},
title = {On Meritocracy in Optimal Set Selection},
year = {2022},
isbn = {9781450394772},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551624.3555305},
doi = {10.1145/3551624.3555305},
abstract = {Typically, merit is defined with respect to some intrinsic measure of worth. We instead consider a setting where an individual’s worth is relative: when a decision maker (DM) selects a set of individuals from a population to maximise expected utility, it is natural to consider the expected marginal contribution (EMC) of each person to the utility. We show that this notion satisfies an axiomatic definition of fairness for this setting. We also show that for certain policy structures, this notion of fairness is aligned with maximising expected utility, while for linear utility functions it is identical to the Shapley value. However, for certain natural policies, such as those that select individuals with a specific set of attributes (e.g. high enough test scores for college admissions), there is a trade-off between meritocracy and utility maximisation. We analyse the effect of constraints on the policy on both utility and fairness in an extensive experiments based on college admissions and outcomes in Norwegian universities.},
booktitle = {Equity and Access in Algorithms, Mechanisms, and Optimization},
articleno = {20},
numpages = {14},
location = {Arlington, VA, USA},
series = {EAAMO '22}
}
  
"
EAAMO,2022,Dimensions of Diversity in Human Perceptions of Algorithmic Fairness,https://dl.acm.org/doi/10.1145/3551624.3555306,"['Nina Grgiƒá-Hlaƒça', 'Gabriel Lima', 'Adrian Weller', 'Elissa M. Redmiles']",15,"A growing number of oversight boards and regulatory bodies seek to monitor and govern algorithms that make decisions about people‚Äôs lives. Prior work has explored how people believe algorithmic decisions should be made, but there is little understanding of how individual factors like sociodemographics or direct experience with a decision-making scenario may affect their ethical views. We take a step toward filling this gap by exploring how people‚Äôs perceptions of one aspect of procedural algorithmic fairness (the fairness of using particular features in an algorithmic decision) relate to their (i) demographics (age, education, gender, race, political views) and (ii) personal experiences with the algorithmic decision-making scenario. We find that political views and personal experience with the algorithmic decision context significantly influence perceptions about the fairness of using different features for bail decision-making. Drawing on our results, we discuss the implications for stakeholder engagement and algorithmic oversight including the need to consider multiple dimensions of diversity in composing oversight and regulatory bodies.",NOT,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3551624.3555306,
author = {Grgi\'{c}-Hla\v{c}a, Nina and Lima, Gabriel and Weller, Adrian and Redmiles, Elissa M.},
title = {Dimensions of Diversity in Human Perceptions of Algorithmic Fairness},
year = {2022},
isbn = {9781450394772},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551624.3555306},
doi = {10.1145/3551624.3555306},
abstract = {A growing number of oversight boards and regulatory bodies seek to monitor and govern algorithms that make decisions about people’s lives. Prior work has explored how people believe algorithmic decisions should be made, but there is little understanding of how individual factors like sociodemographics or direct experience with a decision-making scenario may affect their ethical views. We take a step toward filling this gap by exploring how people’s perceptions of one aspect of procedural algorithmic fairness (the fairness of using particular features in an algorithmic decision) relate to their (i) demographics (age, education, gender, race, political views) and (ii) personal experiences with the algorithmic decision-making scenario. We find that political views and personal experience with the algorithmic decision context significantly influence perceptions about the fairness of using different features for bail decision-making. Drawing on our results, we discuss the implications for stakeholder engagement and algorithmic oversight including the need to consider multiple dimensions of diversity in composing oversight and regulatory bodies.},
booktitle = {Equity and Access in Algorithms, Mechanisms, and Optimization},
articleno = {21},
numpages = {12},
keywords = {Human Perceptions of Fairness, Human Factors in Machine Learning, Algorithmic Fairness, Human-Centered AI, Fairness in Machine Learning},
location = {Arlington, VA, USA},
series = {EAAMO '22}
}
  
"
ICML,2017,Globally Induced Forest: A Prepruning Compression Scheme,https://proceedings.mlr.press/v70/begon17a.html,"['Jean-Michel Begon', 'Arnaud Joly', 'Pierre Geurts']",12,"Tree-based ensemble models are heavy memory-wise. An undesired state of affairs considering nowadays datasets, memory-constrained environment and fitting/prediction times. In this paper, we propose the Globally Induced Forest (GIF) to remedy this problem. GIF is a fast prepruning approach to build lightweight ensembles by iteratively deepening the current forest. It mixes local and global optimizations to produce accurate predictions under memory constraints in reasonable time. We show that the proposed method is more than competitive with standard tree-based ensembles under corresponding constraints, and can sometimes even surpass much larger models.",NOT,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v70-begon17a,
  title = 	 {Globally Induced Forest: A Prepruning Compression Scheme},
  author =       {Jean-Michel Begon and Arnaud Joly and Pierre Geurts},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {420--428},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/begon17a/begon17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/begon17a.html},
  abstract = 	 {Tree-based ensemble models are heavy memory-wise. An undesired state of affairs considering nowadays datasets, memory-constrained environment and fitting/prediction times. In this paper, we propose the Globally Induced Forest (GIF) to remedy this problem. GIF is a fast prepruning approach to build lightweight ensembles by iteratively deepening the current forest. It mixes local and global optimizations to produce accurate predictions under memory constraints in reasonable time. We show that the proposed method is more than competitive with standard tree-based ensembles under corresponding constraints, and can sometimes even surpass much larger models.}
}
"
ICML,2017,Multi objective Bandits: Optimizing the Generalized {G}ini Index,https://proceedings.mlr.press/v70/busa-fekete17a.html,"[""R{\\'o}bert Busa-Fekete"", 'Bal{\\\'a}zs Sz{\\""o}r{\\\'e}nyi', 'Paul Weng', 'Shie Mannor']",27,"We study the multi-armed bandit (MAB) problem where the agent receives a vectorial feedback that encodes many possibly competing objectives to be optimized. The goal of the agent is to find a policy, which can optimize these objectives simultaneously in a fair way. This multi-objective online optimization problem is formalized by using the Generalized Gini Index (GGI) aggregation function. We propose an online gradient descent algorithm which exploits the convexity of the GGI aggregation function, and controls the exploration in a careful way achieving a distribution-free regret $\tilde{O}(T^{-1/2} )$ with high probability. We test our algorithm on synthetic data as well as on an electric battery control problem where the goal is to trade off the use of the different cells of a battery in order to balance their respective degradation rates.",NOT,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v70-busa-fekete17a,
  title = 	 {Multi-objective Bandits: Optimizing the Generalized {G}ini Index},
  author =       {R{\'o}bert Busa-Fekete and Bal{\'a}zs Sz{\""o}r{\'e}nyi and Paul Weng and Shie Mannor},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {625--634},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/busa-fekete17a/busa-fekete17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/busa-fekete17a.html},
  abstract = 	 {We study the multi-armed bandit (MAB) problem where the agent receives a vectorial feedback that encodes many possibly competing objectives to be optimized. The goal of the agent is to find a policy, which can optimize these objectives simultaneously in a fair way. This multi-objective online optimization problem is formalized by using the Generalized Gini Index (GGI) aggregation function. We propose an online gradient descent algorithm which exploits the convexity of the GGI aggregation function, and controls the exploration in a careful way achieving a distribution-free regret $\tilde{O}(T^{-1/2} )$ with high probability. We test our algorithm on synthetic data as well as on an electric battery control problem where the goal is to trade off the use of the different cells of a battery in order to balance their respective degradation rates.}
}
"
ICML,2017,Fairness in Reinforcement Learning,https://proceedings.mlr.press/v70/jabbari17a.html,"['Shahin Jabbari', 'Matthew Joseph', 'Michael Kearns', 'Jamie Morgenstern', 'Aaron Roth']",4,"We initiate the study of fairness in reinforcement learning, where the actions of a learning algorithm may affect its environment and future rewards. Our fairness constraint requires that an algorithm never prefers one action over another if the long-term (discounted) reward of choosing the latter action is higher. Our first result is negative: despite the fact that fairness is consistent with the optimal policy, any learning algorithm satisfying fairness must take time exponential in the number of states to achieve non-trivial approximation to the optimal policy. We then provide a provably fair polynomial time algorithm under an approximate notion of fairness, thus establishing an exponential gap between exact and approximate fairness.",NOT,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v70-jabbari17a,
  title = 	 {Fairness in Reinforcement Learning},
  author =       {Shahin Jabbari and Matthew Joseph and Michael Kearns and Jamie Morgenstern and Aaron Roth},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {1617--1626},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/jabbari17a/jabbari17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/jabbari17a.html},
  abstract = 	 {We initiate the study of fairness in reinforcement learning, where the actions of a learning algorithm may affect its environment and future rewards. Our fairness constraint requires that an algorithm never prefers one action over another if the long-term (discounted) reward of choosing the latter action is higher. Our first result is negative: despite the fact that fairness is consistent with the optimal policy, any learning algorithm satisfying fairness must take time exponential in the number of states to achieve non-trivial approximation to the optimal policy. We then provide a provably fair polynomial time algorithm under an approximate notion of fairness, thus establishing an exponential gap between exact and approximate fairness.}
}
"
Neurips,2017,Bayesian GAN,https://proceedings.neurips.cc//paper/2017/hash/312351bff07989769097660a56395065-Abstract.html,"['Yunus Saatci', ' Andrew G. Wilson']",138,"Generative adversarial networks (GANs) can implicitly learn rich distributions over images, audio, and data which are hard to model with an explicit likelihood. We present a practical Bayesian formulation for unsupervised and semi-supervised learning with GANs. Within this framework, we use stochastic gradient Hamiltonian Monte Carlo to marginalize the weights of the generator and discriminator networks. The resulting approach is straightforward and obtains good performance without any standard interventions such as feature matching or mini-batch discrimination. By exploring an expressive posterior over the parameters of the generator, the Bayesian GAN avoids mode-collapse, produces interpretable and diverse candidate samples, and provides state-of-the-art quantitative results for semi-supervised learning on benchmarks including SVHN, CelebA, and CIFAR-10, outperforming DCGAN, Wasserstein GANs, and DCGAN ensembles.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NIPS2017_312351bf,
 author = {Saatci, Yunus and Wilson, Andrew G},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Bayesian GAN},
 url = {https://proceedings.neurips.cc/paper/2017/file/312351bff07989769097660a56395065-Paper.pdf},
 volume = {30},
 year = {2017}
}

"
Neurips,2017,Quantifying how much sensory information in a neural code is relevant for behavior,https://proceedings.neurips.cc//paper/2017/hash/a9813e9550fee3110373c21fa012eee7-Abstract.html,"['Giuseppe Pica', ' Eugenio Piasini', ' Houman Safaai', ' Caroline Runyan', ' Christopher Harvey', ' Mathew Diamond', ' Christoph Kayser', ' Tommaso Fellin', ' Stefano Panzeri']",30,"Determining how much of the sensory information carried by a neural code contributes to behavioral performance is key to understand sensory function and neural information flow. However, there are as yet no analytical tools to compute this information that lies at the intersection between sensory coding and behavioral readout. Here we develop a novel measure, termed the information-theoretic intersection information $\III(S;R;C)$, that quantifies how much of the sensory information carried by a neural response $R$ is used for behavior during perceptual discrimination tasks. Building on the Partial Information Decomposition framework, we define $\III(S;R;C)$ as the part of the mutual information between the stimulus $S$ and the response $R$ that also informs the consequent behavioral choice $C$. We compute $\III(S;R;C)$ in the analysis of two experimental cortical datasets, to show how this measure can be used to compare quantitatively the contributions of spike timing and spike rates to task performance, and to identify brain areas or neural populations that specifically transform sensory information into choice.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NIPS2017_a9813e95,
 author = {Pica, Giuseppe and Piasini, Eugenio and Safaai, Houman and Runyan, Caroline and Harvey, Christopher and Diamond, Mathew and Kayser, Christoph and Fellin, Tommaso and Panzeri, Stefano},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Quantifying how much sensory information in a neural code is relevant for behavior},
 url = {https://proceedings.neurips.cc/paper/2017/file/a9813e9550fee3110373c21fa012eee7-Paper.pdf},
 volume = {30},
 year = {2017}
}

"
Neurips,2017,On Fairness and Calibration,https://proceedings.neurips.cc//paper/2017/hash/b8b9c74ac526fffbeb2d39ab038d1cd7-Abstract.html,"['Geoff Pleiss', ' Manish Raghavan', ' Felix Wu', ' Jon Kleinberg', ' Kilian Q. Weinberger']",637,"The machine learning community has become increasingly concerned with the potential for bias and discrimination in predictive models. This has motivated a growing line of work on what it means for a classification procedure to be ""fair."" In this paper, we investigate the tension between minimizing error disparity across different population groups while maintaining calibrated probability estimates. We show that calibration is compatible only with a single error constraint (i.e. equal false-negatives rates across groups), and show that any algorithm that satisfies this relaxation is no better than randomizing a percentage of predictions for an existing classifier. These unsettling findings, which extend and generalize existing results, are empirically confirmed on several datasets.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NIPS2017_b8b9c74a,
 author = {Pleiss, Geoff and Raghavan, Manish and Wu, Felix and Kleinberg, Jon and Weinberger, Kilian Q},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {On Fairness and Calibration},
 url = {https://proceedings.neurips.cc/paper/2017/file/b8b9c74ac526fffbeb2d39ab038d1cd7-Paper.pdf},
 volume = {30},
 year = {2017}
}

"
Neurips,2017,Hierarchical Clustering Beyond the Worst-Case,https://proceedings.neurips.cc//paper/2017/hash/e8bf0f27d70d480d3ab793bb7619aaa5-Abstract.html,"['Vincent Cohen-Addad', ' Varun Kanade', ' Frederik Mallmann-Trenn']",46,"Hiererachical clustering, that is computing a recursive partitioning of a dataset to obtain clusters at increasingly finer granularity is a fundamental problem in data analysis. Although hierarchical clustering has mostly been studied through procedures such as linkage algorithms, or top-down heuristics, rather than as optimization problems, recently Dasgupta [1] proposed an objective function for hierarchical clustering and initiated a line of work developing algorithms that explicitly optimize an objective (see also [2, 3, 4]). In this paper, we consider a fairly general random graph model for hierarchical clustering, called the hierarchical stochastic blockmodel (HSBM), and show that in certain regimes the SVD approach of McSherry [5] combined with specific linkage methods results in a clustering that give an O(1)-approximation to Dasgupta‚Äôs cost function. We also show that an approach based on SDP relaxations for balanced cuts based on the work of Makarychev et al. [6], combined with the recursive sparsest cut algorithm of Dasgupta, yields an O(1) approximation in slightly larger regimes and also in the semi-random setting, where an adversary may remove edges from the random graph generated according to an HSBM. Finally, we report empirical evaluation on synthetic and real-world data showing that our proposed SVD-based method does indeed achieve a better cost than other widely-used heurstics and also results in a better classification accuracy when the underlying problem was that of multi-class classification.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NIPS2017_e8bf0f27,
 author = {Cohen-Addad, Vincent and Kanade, Varun and Mallmann-Trenn, Frederik},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Hierarchical Clustering Beyond the Worst-Case},
 url = {https://proceedings.neurips.cc/paper/2017/file/e8bf0f27d70d480d3ab793bb7619aaa5-Paper.pdf},
 volume = {30},
 year = {2017}
}

"
Neurips,2017,Gradient Descent Can Take Exponential Time to Escape Saddle Points,https://proceedings.neurips.cc//paper/2017/hash/f79921bbae40a577928b76d2fc3edc2a-Abstract.html,"['Simon S. Du', ' Chi Jin', ' Jason D. Lee', ' Michael I. Jordan', ' Aarti Singh', ' Barnabas Poczos']",226,"Although gradient descent (GD) almost always escapes saddle points asymptotically [Lee et al., 2016], this paper shows that even with fairly natural random initialization schemes and non-pathological functions, GD can be significantly slowed down by saddle points, taking exponential time to escape. On the other hand, gradient descent with perturbations [Ge et al., 2015, Jin et al., 2017] is not slowed down by saddle points‚Äîit can find an approximate local minimizer in polynomial time. This result implies that GD is inherently slower than perturbed GD, and justifies the importance of adding perturbations for efficient non-convex optimization. While our focus is theoretical, we also present experiments that illustrate our theoretical findings.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NIPS2017_f79921bb,
 author = {Du, Simon S and Jin, Chi and Lee, Jason D and Jordan, Michael I and Singh, Aarti and Poczos, Barnabas},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Gradient Descent Can Take Exponential Time to Escape Saddle Points},
 url = {https://proceedings.neurips.cc/paper/2017/file/f79921bbae40a577928b76d2fc3edc2a-Paper.pdf},
 volume = {30},
 year = {2017}
}

"
Neurips,2018,Provably Correct Automatic Sub-Differentiation for Qualified Programs,https://proceedings.neurips.cc//paper/2018/hash/142c65e00f4f7cf2e6c4c996e34005df-Abstract.html,"['Sham M. Kakade', ' Jason D. Lee']",37,"The \emph{Cheap Gradient Principle}~\citep{Griewank:2008:EDP:1455489} --- the computational cost of computing a $d$-dimensional vector of partial derivatives of a scalar function is nearly the same (often within a factor of $5$) as that of simply computing the scalar function itself --- is of central importance in optimization; it allows us to quickly obtain (high-dimensional) gradients of scalar loss functions which are subsequently used in black box gradient-based optimization procedures. The current state of affairs is markedly different with regards to computing sub-derivatives: widely used ML libraries, including TensorFlow and PyTorch, do \emph{not} correctly compute (generalized) sub-derivatives even on simple differentiable examples. This work considers the question: is there a \emph{Cheap Sub-gradient Principle}? Our main result shows that, under certain restrictions on our library of non-smooth functions (standard in non-linear programming), provably correct generalized sub-derivatives can be computed at a computational cost that is within a (dimension-free) factor of $6$ of the cost of computing the scalar function itself.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2018_142c65e0,
 author = {Kakade, Sham M and Lee, Jason D},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Provably Correct Automatic Sub-Differentiation for Qualified Programs},
 url = {https://proceedings.neurips.cc/paper/2018/file/142c65e00f4f7cf2e6c4c996e34005df-Paper.pdf},
 volume = {31},
 year = {2018}
}

"
Neurips,2018,On preserving non-discrimination when combining expert advice,https://proceedings.neurips.cc//paper/2018/hash/2e855f9489df0712b4bd8ea9e2848c5a-Abstract.html,"['Avrim Blum', ' Suriya Gunasekar', ' Thodoris Lykouris', ' Nati Srebro']",31,"We study the interplay between sequential decision making and avoiding discrimination against protected groups, when examples arrive online and do not follow distributional assumptions. We consider the most basic extension of classical online learning: Given a class of predictors that are individually non-discriminatory with respect to a particular metric, how can we combine them to perform as well as the best predictor, while preserving non-discrimination? Surprisingly we show that this task is unachievable for the prevalent notion of ""equalized odds"" that requires equal false negative rates and equal false positive rates across groups. On the positive side, for another notion of non-discrimination, ""equalized error rates"", we show that running separate instances of the classical multiplicative weights algorithm for each group achieves this guarantee. Interestingly, even for this notion, we show that algorithms with stronger performance guarantees than multiplicative weights cannot preserve non-discrimination.",NOT,,,,,Sequantial decision making and how to stay fair,,,,,,,,,,,"@inproceedings{NEURIPS2018_2e855f94,
 author = {Blum, Avrim and Gunasekar, Suriya and Lykouris, Thodoris and Srebro, Nati},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {On preserving non-discrimination when combining expert advice},
 url = {https://proceedings.neurips.cc/paper/2018/file/2e855f9489df0712b4bd8ea9e2848c5a-Paper.pdf},
 volume = {31},
 year = {2018}
}

"
Neurips,2018,Approximation algorithms for stochastic clustering,https://proceedings.neurips.cc//paper/2018/hash/3e60e09c222f206c725385f53d7e567c-Abstract.html,"['David Harris', ' Shi Li', ' Aravind Srinivasan', ' Khoa Trinh', ' Thomas Pensyl']",14,"We consider stochastic settings for clustering, and develop provably-good (approximation) algorithms for a number of these notions. These algorithms allow one to obtain better approximation ratios compared to the usual deterministic clustering setting. Additionally, they offer a number of advantages including providing fairer clustering and clustering which has better long-term behavior for each user. In particular, they ensure that every user is guaranteed to get good service (on average). We also complement some of these with impossibility results.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2018_3e60e09c,
 author = {Harris, David and Li, Shi and Srinivasan, Aravind and Trinh, Khoa and Pensyl, Thomas},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Approximation algorithms for stochastic clustering},
 url = {https://proceedings.neurips.cc/paper/2018/file/3e60e09c222f206c725385f53d7e567c-Paper.pdf},
 volume = {31},
 year = {2018}
}

"
Neurips,2018,The Everlasting Database: Statistical Validity at a Fair Price,https://proceedings.neurips.cc//paper/2018/hash/4ad13f04ef4373992c9d3046200aa350-Abstract.html,"['Blake E. Woodworth', ' Vitaly Feldman', ' Saharon Rosset', ' Nati Srebro']",2,"The problem of handling adaptivity in data analysis, intentional or not, permeates a variety of fields, including test-set overfitting in ML challenges and the accumulation of invalid scientific discoveries. We propose a mechanism for answering an arbitrarily long sequence of potentially adaptive statistical queries, by charging a price for each query and using the proceeds to collect additional samples. Crucially, we guarantee statistical validity without any assumptions on how the queries are generated. We also ensure with high probability that the cost for $M$ non-adaptive queries is $O(\log M)$, while the cost to a potentially adaptive user who makes $M$ queries that do not depend on any others is $O(\sqrt{M})$.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2018_4ad13f04,
 author = {Woodworth, Blake E and Feldman, Vitaly and Rosset, Saharon and Srebro, Nati},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {The Everlasting Database: Statistical Validity at a Fair Price},
 url = {https://proceedings.neurips.cc/paper/2018/file/4ad13f04ef4373992c9d3046200aa350-Paper.pdf},
 volume = {31},
 year = {2018}
}

"
Neurips,2018,Online Learning with an Unknown Fairness Metric,https://proceedings.neurips.cc//paper/2018/hash/50905d7b2216bfeccb5b41016357176b-Abstract.html,"['Stephen Gillen', ' Christopher Jung', ' Michael Kearns', ' Aaron Roth']",124,"We consider the problem of online learning in the linear contextual bandits setting, but in which there are also strong individual fairness constraints governed by an unknown similarity metric. These constraints demand that we select similar actions or individuals with approximately equal probability DHPRZ12, which may be at odds with optimizing reward, thus modeling settings where profit and social policy are in tension. We assume we learn about an unknown Mahalanobis similarity metric from only weak feedback that identifies fairness violations, but does not quantify their extent. This is intended to represent the interventions of a regulator who ""knows unfairness when he sees it"" but nevertheless cannot enunciate a quantitative fairness metric over individuals. Our main result is an algorithm in the adversarial context setting that has a number of fairness violations that depends only logarithmically on T, while obtaining an optimal O(sqrt(T)) regret bound to the best fair policy.",NOT,,Organizational Realities,Algorithm,Methods,Online Learning/Bandits for interacting with outside adversarial agents,,,,,,,,,,,"@inproceedings{NEURIPS2018_50905d7b,
 author = {Gillen, Stephen and Jung, Christopher and Kearns, Michael and Roth, Aaron},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Online Learning with an Unknown Fairness Metric},
 url = {https://proceedings.neurips.cc/paper/2018/file/50905d7b2216bfeccb5b41016357176b-Paper.pdf},
 volume = {31},
 year = {2018}
}

"
Neurips,2018,Discrimination-aware Channel Pruning for Deep Neural Networks,https://proceedings.neurips.cc//paper/2018/hash/55a7cf9c71f1c9c495413f934dd1a158-Abstract.html,"['Zhuangwei Zhuang', ' Mingkui Tan', ' Bohan Zhuang', ' Jing Liu', ' Yong Guo', ' Qingyao Wu', ' Junzhou Huang', ' Jinhui Zhu']",563,"Channel pruning is one of the predominant approaches for deep model compression. Existing pruning methods either train from scratch with sparsity constraints on channels, or minimize the reconstruction error between the pre-trained feature maps and the compressed ones. Both strategies suffer from some limitations: the former kind is computationally expensive and difficult to converge, whilst the latter kind optimizes the reconstruction error but ignores the discriminative power of channels. To overcome these drawbacks, we investigate a simple-yet-effective method, called discrimination-aware channel pruning, to choose those channels that really contribute to discriminative power. To this end, we introduce additional losses into the network to increase the discriminative power of intermediate layers and then select the most discriminative channels for each layer by considering the additional loss and the reconstruction error. Last, we propose a greedy algorithm to conduct channel selection and parameter optimization in an iterative way. Extensive experiments demonstrate the effectiveness of our method. For example, on ILSVRC-12, our pruned ResNet-50 with 30% reduction of channels even outperforms the original model by 0.39% in top-1 accuracy.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2018_55a7cf9c,
 author = {Zhuang, Zhuangwei and Tan, Mingkui and Zhuang, Bohan and Liu, Jing and Guo, Yong and Wu, Qingyao and Huang, Junzhou and Zhu, Jinhui},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Discrimination-aware Channel Pruning for Deep Neural Networks},
 url = {https://proceedings.neurips.cc/paper/2018/file/55a7cf9c71f1c9c495413f934dd1a158-Paper.pdf},
 volume = {31},
 year = {2018}
}

"
Neurips,2018,Inequity aversion improves cooperation in intertemporal social dilemmas,https://proceedings.neurips.cc//paper/2018/hash/7fea637fd6d02b8f0adf6f7dc36aed93-Abstract.html,"['Edward Hughes', ' Joel Z. Leibo', ' Matthew Phillips', ' Karl Tuyls', ' Edgar Due√±ez-Guzman', ' Antonio Garc√≠a Casta√±eda', ' Iain Dunning', ' Tina Zhu', ' Kevin McKee', ' Raphael Koster', ' Heather Roff', ' Thore Graepel']",150,"Groups of humans are often able to find ways to cooperate with one another in complex, temporally extended social dilemmas. Models based on behavioral economics are only able to explain this phenomenon for unrealistic stateless matrix games. Recently, multi-agent reinforcement learning has been applied to generalize social dilemma problems to temporally and spatially extended Markov games. However, this has not yet generated an agent that learns to cooperate in social dilemmas as humans do. A key insight is that many, but not all, human individuals have inequity averse social preferences. This promotes a particular resolution of the matrix game social dilemma wherein inequity-averse individuals are personally pro-social and punish defectors. Here we extend this idea to Markov games and show that it promotes cooperation in several types of sequential social dilemma, via a profitable interaction with policy learnability. In particular, we find that inequity aversion improves temporal credit assignment for the important class of intertemporal social dilemmas. These results help explain how large-scale cooperation may emerge and persist.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2018_7fea637f,
 author = {Hughes, Edward and Leibo, Joel Z and Phillips, Matthew and Tuyls, Karl and Due\~{n}ez-Guzman, Edgar and Garc\'{\i}a Casta\~{n}eda, Antonio and Dunning, Iain and Zhu, Tina and McKee, Kevin and Koster, Raphael and Roff, Heather and Graepel, Thore},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Inequity aversion improves cooperation in intertemporal social dilemmas},
 url = {https://proceedings.neurips.cc/paper/2018/file/7fea637fd6d02b8f0adf6f7dc36aed93-Paper.pdf},
 volume = {31},
 year = {2018}
}

"
Neurips,2018,A Game-Theoretic Approach to Recommendation Systems with Strategic Content Providers,https://proceedings.neurips.cc//paper/2018/hash/a9a1d5317a33ae8cef33961c34144f84-Abstract.html,"['Omer Ben-Porat', ' Moshe Tennenholtz']",44,"We introduce a game-theoretic approach to the study of recommendation systems with strategic content providers. Such systems should be fair and stable. Showing that traditional approaches fail to satisfy these requirements, we propose the Shapley mediator. We show that the Shapley mediator satisfies the fairness and stability requirements, runs in linear time, and is the only economically efficient mechanism satisfying these properties.",NOT,,Algorithm,,,Fairness in a particular context with strategic agents,,,,,,,,,,,"@inproceedings{NEURIPS2018_a9a1d531,
 author = {Ben-Porat, Omer and Tennenholtz, Moshe},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {A Game-Theoretic Approach to Recommendation Systems with Strategic Content Providers},
 url = {https://proceedings.neurips.cc/paper/2018/file/a9a1d5317a33ae8cef33961c34144f84-Paper.pdf},
 volume = {31},
 year = {2018}
}

"
Neurips,2018,Practical Deep Stereo (PDS): Toward applications-friendly deep stereo matching,https://proceedings.neurips.cc//paper/2018/hash/ade55409d1224074754035a5a937d2e0-Abstract.html,"['Stepan Tulyakov', ' Anton Ivanov', ' Fran√ßois Fleuret']",109,"End-to-end deep-learning networks recently demonstrated extremely good performance for stereo matching. However, existing networks are difficult to use for practical applications since (1) they are memory-hungry and unable to process even modest-size images, (2) they have to be fully re-trained to handle a different disparity range. The Practical Deep Stereo (PDS) network that we propose addresses both issues: First, its architecture relies on novel bottleneck modules that drastically reduce the memory footprint in inference, and additional design choices allow to handle greater image size during training. This results in a model that leverages large image context to resolve matching ambiguities. Second, a novel sub-pixel cross-entropy loss combined with a MAP estimator make this network less sensitive to ambiguous matches, and applicable to any disparity range without re-training. We compare PDS to state-of-the-art methods published over the recent months, and demonstrate its superior performance on FlyingThings3D and KITTI sets.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2018_ade55409,
 author = {Tulyakov, Stepan and Ivanov, Anton and Fleuret, Fran\c{c}ois},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Practical Deep Stereo (PDS): Toward applications-friendly deep stereo matching},
 url = {https://proceedings.neurips.cc/paper/2018/file/ade55409d1224074754035a5a937d2e0-Paper.pdf},
 volume = {31},
 year = {2018}
}

"
Neurips,2018,Efficient inference for time-varying behavior during learning,https://proceedings.neurips.cc//paper/2018/hash/cdcb2f5c7b071143529ef7f2705dfbc4-Abstract.html,"['Nicholas A. Roy', ' Ji Hyun Bak', ' Athena Akrami', ' Carlos Brody', ' Jonathan W. Pillow']",19,"The process of learning new behaviors over time is a problem of great interest in both neuroscience and artificial intelligence. However, most standard analyses of animal training data either treat behavior as fixed or track only coarse performance statistics (e.g., accuracy, bias), providing limited insight into the evolution of the policies governing behavior. To overcome these limitations, we propose a dynamic psychophysical model that efficiently tracks trial-to-trial changes in behavior over the course of training. Our model consists of a dynamic logistic regression model, parametrized by a set of time-varying weights that express dependence on sensory stimuli as well as task-irrelevant covariates, such as stimulus, choice, and answer history. Our implementation scales to large behavioral datasets, allowing us to infer 500K parameters (e.g. 10 weights over 50K trials) in minutes on a desktop computer. We optimize hyperparameters governing how rapidly each weight evolves over time using the decoupled Laplace approximation, an efficient method for maximizing marginal likelihood in non-conjugate models. To illustrate performance, we apply our method to psychophysical data from both rats and human subjects learning a delayed sensory discrimination task. The model successfully tracks the psychophysical weights of rats over the course of training, capturing day-to-day and trial-to-trial fluctuations that underlie changes in performance, choice bias, and dependencies on task history. Finally, we investigate why rats frequently make mistakes on easy trials, and suggest that apparent lapses can be explained by sub-optimal weighting of known task covariates.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2018_cdcb2f5c,
 author = {Roy, Nicholas A. and Bak, Ji Hyun and Akrami, Athena and Brody, Carlos and Pillow, Jonathan W},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Efficient inference for time-varying behavior during learning},
 url = {https://proceedings.neurips.cc/paper/2018/file/cdcb2f5c7b071143529ef7f2705dfbc4-Paper.pdf},
 volume = {31},
 year = {2018}
}

"
Neurips,2019,Equal Opportunity in Online Classification with Partial Feedback,https://proceedings.neurips.cc//paper/2019/hash/084afd913ab1e6ea58b8ca73f6cb41a6-Abstract.html,"['Yahav Bechavod', ' Katrina Ligett', ' Aaron Roth', ' Bo Waggoner', ' Steven Z. Wu']",48,"We study an online classification problem with partial feedback in which individuals arrive one at a time from a fixed but unknown distribution, and must be classified as positive or negative. Our algorithm only observes the true label of an individual if they are given a positive classification. This setting captures many classification problems for which fairness is a concern: for example, in criminal recidivism prediction, recidivism is only observed if the inmate is released; in lending applications, loan repayment is only observed if the loan is granted. We require that our algorithms satisfy common statistical fairness constraints (such as equalizing false positive or negative rates --- introduced as ""equal opportunity"" in Hardt et al. (2016)) at every round, with respect to the underlying distribution. We give upper and lower bounds characterizing the cost of this constraint in terms of the regret rate (and show that it is mild), and give an oracle efficient algorithm that achieves the upper bound.",NOT,,Algorithm,,,Online learning for dealing with the selective labels problem,,,,,,,,,,,"@inproceedings{NEURIPS2019_084afd91,
 author = {Bechavod, Yahav and Ligett, Katrina and Roth, Aaron and Waggoner, Bo and Wu, Steven Z.},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Equal Opportunity in Online Classification with Partial Feedback},
 url = {https://proceedings.neurips.cc/paper/2019/file/084afd913ab1e6ea58b8ca73f6cb41a6-Paper.pdf},
 volume = {32},
 year = {2019}
}

"
Neurips,2019,Concentration of risk measures: A Wasserstein distance approach,https://proceedings.neurips.cc//paper/2019/hash/091bc5440296cc0e41dd60ce22fbaf88-Abstract.html,"['Sanjay P. Bhat', ' Prashanth L.A.']",35,"Known finite-sample concentration bounds for the Wasserstein distance between the empirical and true distribution of a random variable are used to derive a two-sided concentration bound for the error between the true conditional value-at-risk (CVaR) of a (possibly unbounded) random variable and a standard estimate of its CVaR computed from an i.i.d. sample. The bound applies under fairly general assumptions on the random variable, and improves upon previous bounds which were either one sided, or applied only to bounded random variables. Specializations of the bound to sub-Gaussian and sub-exponential random variables are also derived. A similar procedure is followed to derive concentration bounds for the error between the true and estimated Cumulative Prospect Theory (CPT) value of a random variable, in cases where the random variable is bounded or sub-Gaussian. These bounds are shown to match a known bound in the bounded case, and improve upon the known bound in the sub-Gaussian case. The usefulness of the bounds is illustrated through an algorithm, and corresponding regret bound for a stochastic bandit problem, where the underlying risk measure to be optimized is CVaR.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2019_091bc544,
 author = {Bhat, Sanjay P. and L.A., Prashanth},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Concentration of risk measures: A Wasserstein distance approach},
 url = {https://proceedings.neurips.cc/paper/2019/file/091bc5440296cc0e41dd60ce22fbaf88-Paper.pdf},
 volume = {32},
 year = {2019}
}

"
Neurips,2019,Fast Low-rank Metric Learning for Large-scale and High-dimensional Data,https://proceedings.neurips.cc//paper/2019/hash/0d0fd7c6e093f7b804fa0150b875b868-Abstract.html,"['Han Liu', ' Zhizhong Han', ' Yu-Shen Liu', ' Ming Gu']",8,"Low-rank metric learning aims to learn better discrimination of data subject to low-rank constraints. It keeps the intrinsic low-rank structure of datasets and reduces the time cost and memory usage in metric learning. However, it is still a challenge for current methods to handle datasets with both high dimensions and large numbers of samples. To address this issue, we present a novel fast low-rank metric learning (FLRML) method. FLRML casts the low-rank metric learning problem into an unconstrained optimization on the Stiefel manifold, which can be efficiently solved by searching along the descent curves of the manifold. FLRML significantly reduces the complexity and memory usage in optimization, which makes the method scalable to both high dimensions and large numbers of samples. Furthermore, we introduce a mini-batch version of FLRML to make the method scalable to larger datasets which are hard to be loaded and decomposed in limited memory. The outperforming experimental results show that our method is with high accuracy and much faster than the state-of-the-art methods under several benchmarks with large numbers of high-dimensional data. Code has been made available at https://github.com/highan911/FLRML.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2019_0d0fd7c6,
 author = {Liu, Han and Han, Zhizhong and Liu, Yu-Shen and Gu, Ming},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Fast Low-rank Metric Learning for Large-scale and High-dimensional Data},
 url = {https://proceedings.neurips.cc/paper/2019/file/0d0fd7c6e093f7b804fa0150b875b868-Paper.pdf},
 volume = {32},
 year = {2019}
}

"
Neurips,2019,Discovery of Useful Questions as Auxiliary Tasks,https://proceedings.neurips.cc//paper/2019/hash/10ff0b5e85e5b85cc3095d431d8c08b4-Abstract.html,"['Vivek Veeriah', ' Matteo Hessel', ' Zhongwen Xu', ' Janarthanan Rajendran', ' Richard L. Lewis', ' Junhyuk Oh', ' Hado P. van Hasselt', ' David Silver', ' Satinder Singh']",71,"Arguably, intelligent agents ought to be able to discover their own questions so that in learning answers for them they learn unanticipated useful knowledge and skills; this departs from the focus in much of machine learning on agents learning answers to externally defined questions. We present a novel method for a reinforcement learning (RL) agent to discover questions formulated as general value functions or GVFs, a fairly rich form of knowledge representation. Specifically, our method uses non-myopic meta-gradients to learn GVF-questions such that learning answers to them, as an auxiliary task, induces useful representations for the main task faced by the RL agent. We demonstrate that auxiliary tasks based on the discovered GVFs are sufficient, on their own, to build representations that support main task learning, and that they do so better than popular hand-designed auxiliary tasks from the literature. Furthermore, we show, in the context of Atari2600 videogames, how such auxiliary tasks, meta-learned alongside the main task, can improve the data efficiency of an actor-critic agent.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2019_10ff0b5e,
 author = {Veeriah, Vivek and Hessel, Matteo and Xu, Zhongwen and Rajendran, Janarthanan and Lewis, Richard L and Oh, Junhyuk and van Hasselt, Hado P and Silver, David and Singh, Satinder},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Discovery of Useful Questions as Auxiliary Tasks},
 url = {https://proceedings.neurips.cc/paper/2019/file/10ff0b5e85e5b85cc3095d431d8c08b4-Paper.pdf},
 volume = {32},
 year = {2019}
}

"
Neurips,2019,Locality-Sensitive Hashing for f-Divergences: Mutual Information Loss and Beyond,https://proceedings.neurips.cc//paper/2019/hash/21b29648a47a45ad16bb0da0c004dfba-Abstract.html,"['Lin Chen', ' Hossein Esfandiari', ' Gang Fu', ' Vahab Mirrokni']",10,"Computing approximate nearest neighbors in high dimensional spaces is a central problem in large-scale data mining with a wide range of applications in machine learning and data science. A popular and effective technique in computing nearest neighbors approximately is the locality-sensitive hashing (LSH) scheme. In this paper, we aim to develop LSH schemes for distance functions that measure the distance between two probability distributions, particularly for f-divergences as well as a generalization to capture mutual information loss. First, we provide a general framework to design LHS schemes for f-divergence distance functions and develop LSH schemes for the generalized Jensen-Shannon divergence and triangular discrimination in this framework. We show a two-sided approximation result for approximation of the generalized Jensen-Shannon divergence by the Hellinger distance, which may be of independent interest. Next, we show a general method of reducing the problem of designing an LSH scheme for a Krein kernel (which can be expressed as the difference of two positive definite kernels) to the problem of maximum inner product search. We exemplify this method by applying it to the mutual information loss, due to its several important applications such as model compression.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2019_21b29648,
 author = {Chen, Lin and Esfandiari, Hossein and Fu, Gang and Mirrokni, Vahab},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Locality-Sensitive Hashing for f-Divergences: Mutual Information Loss and Beyond},
 url = {https://proceedings.neurips.cc/paper/2019/file/21b29648a47a45ad16bb0da0c004dfba-Paper.pdf},
 volume = {32},
 year = {2019}
}

"
Neurips,2019,Solving a Class of Non-Convex Min-Max Games Using Iterative First Order Methods,https://proceedings.neurips.cc//paper/2019/hash/25048eb6a33209cb5a815bff0cf6887c-Abstract.html,"['Maher Nouiehed', ' Maziar Sanjabi', ' Tianjian Huang', ' Jason D. Lee', ' Meisam Razaviyayn']",236,"Recent applications that arise in machine learning have surged significant interest in solving min-max saddle point games. This problem has been extensively studied in the convex-concave regime for which a global equilibrium solution can be computed efficiently. In this paper, we study the problem in the non-convex regime and show that an $\varepsilon$--first order stationary point of the game can be computed when one of the player‚Äôs objective can be optimized to global optimality efficiently. In particular, we first consider the case where the objective of one of the players satisfies the Polyak-{\L}ojasiewicz (PL) condition. For such a game, we show that a simple multi-step gradient descent-ascent algorithm finds an $\varepsilon$--first order stationary point of the problem in $\widetilde{\mathcal{O}}(\varepsilon^{-2})$ iterations. Then we show that our framework can also be applied to the case where the objective of the ``max-player"" is concave. In this case, we propose a multi-step gradient descent-ascent algorithm that finds an $\varepsilon$--first order stationary point of the game in $\widetilde{\cal O}(\varepsilon^{-3.5})$ iterations, which is the best known rate in the literature. We applied our algorithm to a fair classification problem of Fashion-MNIST dataset and observed that the proposed algorithm results in smoother training and better generalization.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2019_25048eb6,
 author = {Nouiehed, Maher and Sanjabi, Maziar and Huang, Tianjian and Lee, Jason D and Razaviyayn, Meisam},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Solving a Class of Non-Convex Min-Max Games Using Iterative First Order Methods},
 url = {https://proceedings.neurips.cc/paper/2019/file/25048eb6a33209cb5a815bff0cf6887c-Paper.pdf},
 volume = {32},
 year = {2019}
}

"
Neurips,2019,Modeling Tabular data using Conditional GAN,https://proceedings.neurips.cc//paper/2019/hash/254ed7d2de3b23ab10936522dd547b78-Abstract.html,"['Lei Xu', ' Maria Skoularidou', ' Alfredo Cuesta-Infante', ' Kalyan Veeramachaneni']",441,"Modeling the probability distribution of rows in tabular data and generating realistic synthetic data is a non-trivial task. Tabular data usually contains a mix of discrete and continuous columns. Continuous columns may have multiple modes whereas discrete columns are sometimes imbalanced making the modeling difficult. Existing statistical and deep neural network models fail to properly model this type of data. We design CTGAN, which uses a conditional generative adversarial network to address these challenges. To aid in a fair and thorough comparison, we design a benchmark with 7 simulated and 8 real datasets and several Bayesian network baselines. CTGAN outperforms Bayesian methods on most of the real datasets whereas other deep learning methods could not.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2019_254ed7d2,
 author = {Xu, Lei and Skoularidou, Maria and Cuesta-Infante, Alfredo and Veeramachaneni, Kalyan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Modeling Tabular data using Conditional GAN},
 url = {https://proceedings.neurips.cc/paper/2019/file/254ed7d2de3b23ab10936522dd547b78-Paper.pdf},
 volume = {32},
 year = {2019}
}

"
Neurips,2019,Random Quadratic Forms with Dependence: Applications to Restricted Isometry and Beyond,https://proceedings.neurips.cc//paper/2019/hash/2751fae77b24c37382cf6464173d145e-Abstract.html,"['Arindam Banerjee', ' Qilong Gu', ' Vidyashankar Sivakumar', ' Steven Z. Wu']",4,"Several important families of computational and statistical results in machine learning and randomized algorithms rely on uniform bounds on quadratic forms of random vectors or matrices. Such results include the Johnson-Lindenstrauss (J-L) Lemma, the Restricted Isometry Property (RIP), randomized sketching algorithms, and approximate linear algebra. The existing results critically depend on statistical independence, e.g., independent entries for random vectors, independent rows for random matrices, etc., which prevent their usage in dependent or adaptive modeling settings. In this paper, we show that such independence is in fact not needed for such results which continue to hold under fairly general dependence structures. In particular, we present uniform bounds on random quadratic forms of stochastic processes which are conditionally independent and sub-Gaussian given another (latent) process. Our setup allows general dependencies of the stochastic process on the history of the latent process and the latent process to be influenced by realizations of the stochastic process. The results are thus applicable to adaptive modeling settings and also allows for sequential design of random vectors and matrices. We also discuss stochastic process based forms of J-L, RIP, and sketching, to illustrate the generality of the results.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2019_2751fae7,
 author = {Banerjee, Arindam and Gu, Qilong and Sivakumar, Vidyashankar and Wu, Steven Z.},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Random Quadratic Forms with Dependence: Applications to Restricted Isometry and Beyond},
 url = {https://proceedings.neurips.cc/paper/2019/file/2751fae77b24c37382cf6464173d145e-Paper.pdf},
 volume = {32},
 year = {2019}
}

"
Neurips,2019,Learning Non-Convergent Non-Persistent Short-Run MCMC Toward Energy-Based Model,https://proceedings.neurips.cc//paper/2019/hash/2bc8ae25856bc2a6a1333d1331a3b7a6-Abstract.html,"['Erik Nijkamp', ' Mitch Hill', ' Song-Chun Zhu', ' Ying Nian Wu']",146,"This paper studies a curious phenomenon in learning energy-based model (EBM) using MCMC. In each learning iteration, we generate synthesized examples by running a non-convergent, non-mixing, and non-persistent short-run MCMC toward the current model, always starting from the same initial distribution such as uniform noise distribution, and always running a fixed number of MCMC steps. After generating synthesized examples, we then update the model parameters according to the maximum likelihood learning gradient, as if the synthesized examples are fair samples from the current model. We treat this non-convergent short-run MCMC as a learned generator model or a flow model. We provide arguments for treating the learned non-convergent short-run MCMC as a valid model. We show that the learned short-run MCMC is capable of generating realistic images. More interestingly, unlike traditional EBM or MCMC, the learned short-run MCMC is capable of reconstructing observed images and interpolating between images, like generator or flow models. The code can be found in the Appendix.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2019_2bc8ae25,
 author = {Nijkamp, Erik and Hill, Mitch and Zhu, Song-Chun and Wu, Ying Nian},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Learning Non-Convergent Non-Persistent Short-Run MCMC Toward Energy-Based Model},
 url = {https://proceedings.neurips.cc/paper/2019/file/2bc8ae25856bc2a6a1333d1331a3b7a6-Paper.pdf},
 volume = {32},
 year = {2019}
}

"
Neurips,2019,Invariance and identifiability issues for word embeddings,https://proceedings.neurips.cc//paper/2019/hash/44885837c518b06e3f98b41ab8cedc0f-Abstract.html,"['Rachel Carrington', ' Karthik Bharath', ' Simon Preston']",1,"Word embeddings are commonly obtained as optimisers of a criterion function f of a text corpus, but assessed on word-task performance using a different evaluation function g of the test data. We contend that a possible source of disparity in performance on tasks is the incompatibility between classes of transformations that leave f and g invariant. In particular, word embeddings defined by f are not unique; they are defined only up to a class of transformations to which f is invariant, and this class is larger than the class to which g is invariant. One implication of this is that the apparent superiority of one word embedding over another, as measured by word task performance, may largely be a consequence of the arbitrary elements selected from the respective solution sets. We provide a formal treatment of the above identifiability issue, present some numerical examples, and discuss possible resolutions.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2019_44885837,
 author = {Carrington, Rachel and Bharath, Karthik and Preston, Simon},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Invariance and identifiability issues for word embeddings},
 url = {https://proceedings.neurips.cc/paper/2019/file/44885837c518b06e3f98b41ab8cedc0f-Paper.pdf},
 volume = {32},
 year = {2019}
}

"
Neurips,2019,On Making Stochastic Classifiers Deterministic,https://proceedings.neurips.cc//paper/2019/hash/5fc34ed307aac159a30d81181c99847e-Abstract.html,"['Andrew Cotter', ' Maya Gupta', ' Harikrishna Narasimhan']",22,"Stochastic classifiers arise in a number of machine learning problems, and have become especially prominent of late, as they often result from constrained optimization problems, e.g. for fairness, churn, or custom losses. Despite their utility, the inherent randomness of stochastic classifiers may cause them to be problematic to use in practice for a variety of practical reasons. In this paper, we attempt to answer the theoretical question of how well a stochastic classifier can be approximated by a deterministic one, and compare several different approaches, proving lower and upper bounds. We also experimentally investigate the pros and cons of these methods, not only in regard to how successfully each deterministic classifier approximates the original stochastic classifier, but also in terms of how well each addresses the other issues that can make stochastic classifiers undesirable.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2019_5fc34ed3,
 author = {Cotter, Andrew and Gupta, Maya and Narasimhan, Harikrishna},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {On Making Stochastic Classifiers Deterministic},
 url = {https://proceedings.neurips.cc/paper/2019/file/5fc34ed307aac159a30d81181c99847e-Paper.pdf},
 volume = {32},
 year = {2019}
}

"
Neurips,2019,"Accurate, reliable and fast robustness evaluation",https://proceedings.neurips.cc//paper/2019/hash/885fe656777008c335ac96072a45be15-Abstract.html,"['Wieland Brendel', ' Jonas Rauber', ' Matthias K√ºmmerer', ' Ivan Ustyuzhaninov', ' Matthias Bethge']",77,"Throughout the past five years, the susceptibility of neural networks to minimal adversarial perturbations has moved from a peculiar phenomenon to a core issue in Deep Learning. Despite much attention, however, progress towards more robust models is significantly impaired by the difficulty of evaluating the robustness of neural network models. Today's methods are either fast but brittle (gradient-based attacks), or they are fairly reliable but slow (score- and decision-based attacks). We here develop a new set of gradient-based adversarial attacks which (a) are more reliable in the face of gradient-masking than other gradient-based attacks, (b) perform better and are more query efficient than current state-of-the-art gradient-based attacks, (c) can be flexibly adapted to a wide range of adversarial criteria and (d) require virtually no hyperparameter tuning. These findings are carefully validated across a diverse set of six different models and hold for L0, L1, L2 and Linf in both targeted as well as untargeted scenarios. Implementations will soon be available in all major toolboxes (Foolbox, CleverHans and ART). We hope that this class of attacks will make robustness evaluations easier and more reliable, thus contributing to more signal in the search for more robust machine learning models.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2019_885fe656,
 author = {Brendel, Wieland and Rauber, Jonas and K\""{u}mmerer, Matthias and Ustyuzhaninov, Ivan and Bethge, Matthias},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Accurate, reliable and fast robustness evaluation},
 url = {https://proceedings.neurips.cc/paper/2019/file/885fe656777008c335ac96072a45be15-Paper.pdf},
 volume = {32},
 year = {2019}
}

"
Neurips,2019,Perceiving the arrow of time in autoregressive motion,https://proceedings.neurips.cc//paper/2019/hash/8d3369c4c086f236fabf61d614a32818-Abstract.html,"['Kristof Meding', ' Dominik Janzing', ' Bernhard Sch√∂lkopf', ' Felix A. Wichmann']",2,"Understanding the principles of causal inference in the visual system has a long history at least since the seminal studies by Albert Michotte. Many cognitive and machine learning scientists believe that intelligent behavior requires agents to possess causal models of the world. Recent ML algorithms exploit the dependence structure of additive noise terms for inferring causal structures from observational data, e.g. to detect the direction of time series; the arrow of time. This raises the question whether the subtle asymmetries between the time directions can also be perceived by humans. Here we show that human observers can indeed discriminate forward and backward autoregressive motion with non-Gaussian additive independent noise, i.e. they appear sensitive to subtle asymmetries between the time directions. We employ a so-called frozen noise paradigm enabling us to compare human performance with four different algorithms on a trial-by-trial basis: A causal inference algorithm exploiting the dependence structure of additive noise terms, a neurally inspired network, a Bayesian ideal observer model as well as a simple heuristic. Our results suggest that all human observers use similar cues or strategies to solve the arrow of time motion discrimination task, but the human algorithm is significantly different from the three machine algorithms we compared it to. In fact, our simple heuristic appears most similar to our human observers.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2019_8d3369c4,
 author = {Meding, Kristof and Janzing, Dominik and Sch\""{o}lkopf, Bernhard and Wichmann, Felix A.},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Perceiving the arrow of time in autoregressive motion},
 url = {https://proceedings.neurips.cc/paper/2019/file/8d3369c4c086f236fabf61d614a32818-Paper.pdf},
 volume = {32},
 year = {2019}
}

"
Neurips,2019,Inherent Tradeoffs in Learning Fair Representations,https://proceedings.neurips.cc//paper/2019/hash/b4189d9de0fb2b9cce090bd1a15e3420-Abstract.html,"['Han Zhao', ' Geoff Gordon']",124,"With the prevalence of machine learning in high-stakes applications, especially the ones regulated by anti-discrimination laws or societal norms, it is crucial to ensure that the predictive models do not propagate any existing bias or discrimination. Due to the ability of deep neural nets to learn rich representations, recent advances in algorithmic fairness have focused on learning fair representations with adversarial techniques to reduce bias in data while preserving utility simultaneously. In this paper, through the lens of information theory, we provide the first result that quantitatively characterizes the tradeoff between demographic parity and the joint utility across different population groups. Specifically, when the base rates differ between groups, we show that any method aiming to learn fair representations admits an information-theoretic lower bound on the joint error across these groups. To complement our negative results, we also prove that if the optimal decision functions across different groups are close, then learning fair representations leads to an alternative notion of fairness, known as the accuracy parity, which states that the error rates are close between groups. Finally, our theoretical findings are also confirmed empirically on real-world datasets.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2019_b4189d9d,
 author = {Zhao, Han and Gordon, Geoff},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Inherent Tradeoffs in Learning Fair Representations},
 url = {https://proceedings.neurips.cc/paper/2019/file/b4189d9de0fb2b9cce090bd1a15e3420-Paper.pdf},
 volume = {32},
 year = {2019}
}

"
Neurips,2019,Paradoxes in Fair Machine Learning,https://proceedings.neurips.cc//paper/2019/hash/bbc92a647199b832ec90d7cf57074e9e-Abstract.html,"['Paul Goelz', ' Anson Kahng', ' Ariel D. Procaccia']",26,"Equalized odds is a statistical notion of fairness in machine learning that ensures that classification algorithms do not discriminate against protected groups. We extend equalized odds to the setting of cardinality-constrained fair classification, where we have a bounded amount of a resource to distribute. This setting coincides with classic fair division problems, which allows us to apply concepts from that literature in parallel to equalized odds. In particular, we consider the axioms of resource monotonicity, consistency, and population monotonicity, all three of which relate different allocation instances to prevent paradoxes. Using a geometric characterization of equalized odds, we examine the compatibility of equalized odds with these axioms. We empirically evaluate the cost of allocation rules that satisfy both equalized odds and axioms of fair division on a dataset of FICO credit scores.",NOT,,Metrics,,,,,,,,,,,,,,"@inproceedings{NEURIPS2019_bbc92a64,
 author = {Goelz, Paul and Kahng, Anson and Procaccia, Ariel D},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Paradoxes in Fair Machine Learning},
 url = {https://proceedings.neurips.cc/paper/2019/file/bbc92a647199b832ec90d7cf57074e9e-Paper.pdf},
 volume = {32},
 year = {2019}
}

"
Neurips,2019,Re-examination of the Role of Latent Variables in Sequence Modeling,https://proceedings.neurips.cc//paper/2019/hash/d0ac1ed0c5cb9ecbca3d2496ec1ad984-Abstract.html,"['Guokun Lai', ' Zihang Dai', ' Yiming Yang', ' Shinjae Yoo']",6,"With latent variables, stochastic recurrent models have achieved state-of-the-art performance in modeling sound-wave sequence. However, opposite results are also observed in other domains, where standard recurrent networks often outperform stochastic models. To better understand this discrepancy, we re-examine the roles of latent variables in stochastic recurrent models for speech density estimation. Our analysis reveals that under the restriction of fully factorized output distribution in previous evaluations, the stochastic variants were implicitly leveraging intra-step correlation but the deterministic recurrent baselines were prohibited to do so, resulting in an unfair comparison. To correct the unfairness, we remove such restriction in our re-examination, where all the models can explicitly leverage intra-step correlation with an auto-regressive structure. Over a diverse set of univariate and multivariate sequential data, including human speech, MIDI music, handwriting trajectory, and frame-permuted speech, our results show that stochastic recurrent models fail to deliver the performance advantage claimed in previous work. %exhibit any practical advantage despite the claimed theoretical superiority. In contrast, standard recurrent models equipped with an auto-regressive output distribution consistently perform better, dramatically advancing the state-of-the-art results on three speech datasets.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2019_d0ac1ed0,
 author = {Lai, Guokun and Dai, Zihang and Yang, Yiming and Yoo, Shinjae},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Re-examination of the Role of Latent Variables in Sequence Modeling},
 url = {https://proceedings.neurips.cc/paper/2019/file/d0ac1ed0c5cb9ecbca3d2496ec1ad984-Paper.pdf},
 volume = {32},
 year = {2019}
}

"
Neurips,2019,Meta-Weight-Net: Learning an Explicit Mapping For Sample Weighting,https://proceedings.neurips.cc//paper/2019/hash/e58cc5ca94270acaceed13bc82dfedf7-Abstract.html,"['Jun Shu', ' Qi Xie', ' Lixuan Yi', ' Qian Zhao', ' Sanping Zhou', ' Zongben Xu', ' Deyu Meng']",470,"Current deep neural networks(DNNs) can easily overfit to biased training data with corrupted labels or class imbalance. Sample re-weighting strategy is commonly used to alleviate this issue by designing a weighting function mapping from training loss to sample weight, and then iterating between weight recalculating and classifier updating. Current approaches, however, need manually pre-specify the weighting function as well as its additional hyper-parameters. It makes them fairly hard to be generally applied in practice due to the significant variation of proper weighting schemes relying on the investigated problem and training data. To address this issue, we propose a method capable of adaptively learning an explicit weighting function directly from data. The weighting function is an MLP with one hidden layer, constituting a universal approximator to almost any continuous functions, making the method able to fit a wide range of weighting function forms including those assumed in conventional research. Guided by a small amount of unbiased meta-data, the parameters of the weighting function can be finely updated simultaneously with the learning process of the classifiers. Synthetic and real experiments substantiate the capability of our method for achieving proper weighting functions in class imbalance and noisy label cases, fully complying with the common settings in traditional methods, and more complicated scenarios beyond conventional cases. This naturally leads to its better accuracy than other state-of-the-art methods.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2019_e58cc5ca,
 author = {Shu, Jun and Xie, Qi and Yi, Lixuan and Zhao, Qian and Zhou, Sanping and Xu, Zongben and Meng, Deyu},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Meta-Weight-Net: Learning an Explicit Mapping For Sample Weighting},
 url = {https://proceedings.neurips.cc/paper/2019/file/e58cc5ca94270acaceed13bc82dfedf7-Paper.pdf},
 volume = {32},
 year = {2019}
}

"
Neurips,2019,Modeling Expectation Violation in Intuitive Physics with Coarse Probabilistic Object Representations,https://proceedings.neurips.cc//paper/2019/hash/e88f243bf341ded9b4ced444795c3f17-Abstract.html,"['Kevin Smith', ' Lingjie Mei', ' Shunyu Yao', ' Jiajun Wu', ' Elizabeth Spelke', ' Josh Tenenbaum', ' Tomer Ullman']",42,"From infancy, humans have expectations about how objects will move and interact. Even young children expect objects not to move through one another, teleport, or disappear. They are surprised by mismatches between physical expectations and perceptual observations, even in unfamiliar scenes with completely novel objects. A model that exhibits human-like understanding of physics should be similarly surprised, and adjust its beliefs accordingly. We propose ADEPT, a model that uses a coarse (approximate geometry) object-centric representation for dynamic 3D scene understanding. Inference integrates deep recognition networks, extended probabilistic physical simulation, and particle filtering for forming predictions and expectations across occlusion. We also present a new test set for measuring violations of physical expectations, using a range of scenarios derived from developmental psychology. We systematically compare ADEPT, baseline models, and human expectations on this test set. ADEPT outperforms standard network architectures in discriminating physically implausible scenes, and often performs this discrimination at the same level as people.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2019_e88f243b,
 author = {Smith, Kevin and Mei, Lingjie and Yao, Shunyu and Wu, Jiajun and Spelke, Elizabeth and Tenenbaum, Josh and Ullman, Tomer},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Modeling Expectation Violation in Intuitive Physics with Coarse Probabilistic Object Representations},
 url = {https://proceedings.neurips.cc/paper/2019/file/e88f243bf341ded9b4ced444795c3f17-Paper.pdf},
 volume = {32},
 year = {2019}
}

"
Neurips,2019,On Tractable Computation of Expected Predictions,https://proceedings.neurips.cc//paper/2019/hash/fccc64972a9468a11f125cadb090e89e-Abstract.html,"['Pasha Khosravi', ' YooJung Choi', ' Yitao Liang', ' Antonio Vergari', ' Guy Van den Broeck']",35,"Computing expected predictions of discriminative models is a fundamental task in machine learning that appears in many interesting applications such as fairness, handling missing values, and data analysis. Unfortunately, computing expectations of a discriminative model with respect to a probability distribution defined by an arbitrary generative model has been proven to be hard in general. In fact, the task is intractable even for simple models such as logistic regression and a naive Bayes distribution. In this paper, we identify a pair of generative and discriminative models that enables tractable computation of expectations, as well as moments of any order, of the latter with respect to the former in case of regression. Specifically, we consider expressive probabilistic circuits with certain structural constraints that support tractable probabilistic inference. Moreover, we exploit the tractable computation of high-order moments to derive an algorithm to approximate the expectations for classification scenarios in which exact computations are intractable. Our framework to compute expected predictions allows for handling of missing data during prediction time in a principled and accurate way and enables reasoning about the behavior of discriminative models. We empirically show our algorithm to consistently outperform standard imputation techniques on a variety of datasets. Finally, we illustrate how our framework can be used for exploratory data analysis.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2019_fccc6497,
 author = {Khosravi, Pasha and Choi, YooJung and Liang, Yitao and Vergari, Antonio and Van den Broeck, Guy},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {On Tractable Computation of Expected Predictions},
 url = {https://proceedings.neurips.cc/paper/2019/file/fccc64972a9468a11f125cadb090e89e-Paper.pdf},
 volume = {32},
 year = {2019}
}

"
Neurips,2020,Statistical Guarantees of Distributed Nearest Neighbor Classification,https://proceedings.neurips.cc//paper/2020/hash/022e0ee5162c13d9a7bb3bd00fb032ce-Abstract.html,"['Jiexin Duan', ' Xingye Qiao', ' Guang Cheng']",4,"Nearest neighbor is a popular nonparametric method for classification and regression with many appealing properties. In the big data era, the sheer volume and spatial/temporal disparity of big data may prohibit centrally processing and storing the data. This has imposed considerable hurdle for nearest neighbor predictions since the entire training data must be memorized. One effective way to overcome this issue is the distributed learning framework. Through majority voting, the distributed nearest neighbor classifier achieves the same rate of convergence as its oracle version in terms of the regret, up to a multiplicative constant that depends solely on the data dimension. The multiplicative difference can be eliminated by replacing majority voting with the weighted voting scheme. In addition, we provide sharp theoretical upper bounds of the number of subsamples in order for the distributed nearest neighbor classifier to reach the optimal convergence rate. It is interesting to note that the weighted voting scheme allows a larger number of subsamples than the majority voting one. Our findings are supported by numerical studies.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2020_022e0ee5,
 author = {Duan, Jiexin and Qiao, Xingye and Cheng, Guang},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {229--240},
 publisher = {Curran Associates, Inc.},
 title = {Statistical Guarantees of Distributed Nearest Neighbor Classification},
 url = {https://proceedings.neurips.cc/paper/2020/file/022e0ee5162c13d9a7bb3bd00fb032ce-Paper.pdf},
 volume = {33},
 year = {2020}
}

"
Neurips,2020,Automatic Perturbation Analysis for Scalable Certified Robustness and Beyond,https://proceedings.neurips.cc//paper/2020/hash/0cbc5671ae26f67871cb914d81ef8fc1-Abstract.html,"['Kaidi Xu', ' Zhouxing Shi', ' Huan Zhang', ' Yihan Wang', ' Kai-Wei Chang', ' Minlie Huang', ' Bhavya Kailkhura', ' Xue Lin', ' Cho-Jui Hsieh']",119,"Linear relaxation based perturbation analysis (LiRPA) for neural networks, which computes provable linear bounds of output neurons given a certain amount of input perturbation, has become a core component in robustness verification and certified defense. The majority of LiRPA-based methods focus on simple feed-forward networks and need particular manual derivations and implementations when extended to other architectures. In this paper, we develop an automatic framework to enable perturbation analysis on any neural network structures, by generalizing existing LiRPA algorithms such as CROWN to operate on general computational graphs. The flexibility, differentiability and ease of use of our framework allow us to obtain state-of-the-art results on LiRPA based certified defense on fairly complicated networks like DenseNet, ResNeXt and Transformer that are not supported by prior works. Our framework also enables loss fusion, a technique that significantly reduces the computational complexity of LiRPA for certified defense. For the first time, we demonstrate LiRPA based certified defense on Tiny ImageNet and Downscaled ImageNet where previous approaches cannot scale to due to the relatively large number of classes. Our work also yields an open-source library for the community to apply LiRPA to areas beyond certified defense without much LiRPA expertise, e.g., we create a neural network with a provably flat optimization landscape by applying LiRPA to network parameters. Our open source library is available at https://github.com/KaidiXu/auto_LiRPA.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2020_0cbc5671,
 author = {Xu, Kaidi and Shi, Zhouxing and Zhang, Huan and Wang, Yihan and Chang, Kai-Wei and Huang, Minlie and Kailkhura, Bhavya and Lin, Xue and Hsieh, Cho-Jui},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {1129--1141},
 publisher = {Curran Associates, Inc.},
 title = {Automatic Perturbation Analysis for Scalable Certified Robustness and Beyond},
 url = {https://proceedings.neurips.cc/paper/2020/file/0cbc5671ae26f67871cb914d81ef8fc1-Paper.pdf},
 volume = {33},
 year = {2020}
}

"
Neurips,2020,Revisiting Parameter Sharing for Automatic Neural Channel Number Search,https://proceedings.neurips.cc//paper/2020/hash/42cd63cb189c30ed03e42ce2c069566c-Abstract.html,"['Jiaxing Wang', ' Haoli Bai', ' Jiaxiang Wu', ' Xupeng Shi', ' Junzhou Huang', ' Irwin King', ' Michael Lyu', ' Jian Cheng']",22,"Recent advances in neural architecture search inspire many channel number search algorithms~(CNS) for convolutional neural networks. To improve searching efficiency, parameter sharing is widely applied, which reuses parameters among different channel configurations. Nevertheless, it is unclear how parameter sharing affects the searching process. In this paper, we aim at providing a better understanding and exploitation of parameter sharing for CNS. Specifically, we propose affine parameter sharing~(APS) as a general formulation to unify and quantitatively analyze existing channel search algorithms. It is found that with parameter sharing, weight updates of one architecture can simultaneously benefit other candidates. However, it also results in less confidence in choosing good architectures. We thus propose a new strategy of parameter sharing towards a better balance between training efficiency and architecture discrimination. Extensive analysis and experiments demonstrate the superiority of the proposed strategy in channel configuration against many state-of-the-art counterparts on benchmark datasets.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2020_42cd63cb,
 author = {Wang, Jiaxing and Bai, Haoli and Wu, Jiaxiang and Shi, Xupeng and Huang, Junzhou and King, Irwin and Lyu, Michael and Cheng, Jian},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {5991--6002},
 publisher = {Curran Associates, Inc.},
 title = {Revisiting Parameter Sharing for Automatic Neural Channel Number Search},
 url = {https://proceedings.neurips.cc/paper/2020/file/42cd63cb189c30ed03e42ce2c069566c-Paper.pdf},
 volume = {33},
 year = {2020}
}

"
Neurips,2020,Neutralizing Self-Selection Bias in Sampling for Sortition,https://proceedings.neurips.cc//paper/2020/hash/48237d9f2dea8c74c2a72126cf63d933-Abstract.html,"['Bailey Flanigan', ' Paul G√∂lz', ' Anupam Gupta', ' Ariel D. Procaccia']",17,"Sortition is a political system in which decisions are made by panels of randomly selected citizens. The process for selecting a sortition panel is traditionally thought of as uniform sampling without replacement, which has strong fairness properties. In practice, however, sampling without replacement is not possible since only a fraction of agents is willing to participate in a panel when invited, and different demographic groups participate at different rates. In order to still produce panels whose composition resembles that of the population, we develop a sampling algorithm that restores close-to-equal representation probabilities for all agents while satisfying meaningful demographic quotas. As part of its input, our algorithm requires probabilities indicating how likely each volunteer in the pool was to participate. Since these participation probabilities are not directly observable, we show how to learn them, and demonstrate our approach using data on a real sortition panel combined with information on the general population in the form of publicly available survey data.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2020_48237d9f,
 author = {Flanigan, Bailey and G\""{o}lz, Paul and Gupta, Anupam and Procaccia, Ariel D},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {6528--6539},
 publisher = {Curran Associates, Inc.},
 title = {Neutralizing Self-Selection Bias in Sampling for Sortition},
 url = {https://proceedings.neurips.cc/paper/2020/file/48237d9f2dea8c74c2a72126cf63d933-Paper.pdf},
 volume = {33},
 year = {2020}
}

"
Neurips,2020,Exactly Computing the Local Lipschitz Constant of ReLU Networks,https://proceedings.neurips.cc//paper/2020/hash/5227fa9a19dce7ba113f50a405dcaf09-Abstract.html,"['Matt Jordan', ' Alexandros G. Dimakis']",50,"The local Lipschitz constant of a neural network is a useful metric with applications in robustness, generalization, and fairness evaluation. We provide novel analytic results relating the local Lipschitz constant of nonsmooth vector-valued functions to a maximization over the norm of the generalized Jacobian. We present a sufficient condition for which backpropagation always returns an element of the generalized Jacobian, and reframe the problem over this broad class of functions. We show strong inapproximability results for estimating Lipschitz constants of ReLU networks, and then formulate an algorithm to compute these quantities exactly. We leverage this algorithm to evaluate the tightness of competing Lipschitz estimators and the effects of regularized training on the Lipschitz constant.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2020_5227fa9a,
 author = {Jordan, Matt and Dimakis, Alexandros G},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {7344--7353},
 publisher = {Curran Associates, Inc.},
 title = {Exactly Computing the Local Lipschitz Constant of ReLU Networks},
 url = {https://proceedings.neurips.cc/paper/2020/file/5227fa9a19dce7ba113f50a405dcaf09-Paper.pdf},
 volume = {33},
 year = {2020}
}

"
Neurips,2020,Theory-Inspired Path-Regularized Differential Network Architecture Search,https://proceedings.neurips.cc//paper/2020/hash/5e1b18c4c6a6d31695acbae3fd70ecc6-Abstract.html,"['Pan Zhou', ' Caiming Xiong', ' Richard Socher', ' Steven Chu Hong Hoi']",38,"Despite its high search efficiency, differential architecture search (DARTS) often selects network architectures with dominated skip connections which lead to performance degradation. However, theoretical understandings on this issue remain absent, hindering the development of more advanced methods in a principled way. In this work, we solve this problem by theoretically analyzing the effects of various types of operations, e.g. convolution, skip connection and zero operation, to the network optimization. We prove that the architectures with more skip connections can converge faster than the other candidates, and thus are selected by DARTS. This result, for the first time, theoretically and explicitly reveals the impact of skip connections to fast network optimization and its competitive advantage over other types of operations in DARTS. Then we propose a theory-inspired path-regularized DARTS that consists of two key modules: (i) a differential group-structured sparse binary gate introduced for each operation to avoid unfair competition among operations, and (ii) a path-depth-wise regularization used to incite search exploration for deep architectures that often converge slower than shallow ones as shown in our theory and are not well explored during search. Experimental results on image classification tasks validate its advantages. Codes and models will be released.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2020_5e1b18c4,
 author = {Zhou, Pan and Xiong, Caiming and Socher, Richard and Hoi, Steven Chu Hong},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {8296--8307},
 publisher = {Curran Associates, Inc.},
 title = {Theory-Inspired Path-Regularized Differential Network Architecture Search},
 url = {https://proceedings.neurips.cc/paper/2020/file/5e1b18c4c6a6d31695acbae3fd70ecc6-Paper.pdf},
 volume = {33},
 year = {2020}
}

"
Neurips,2020,Beyond Individualized Recourse: Interpretable and Interactive Summaries of Actionable Recourses,https://proceedings.neurips.cc//paper/2020/hash/8ee7730e97c67473a424ccfeff49ab20-Abstract.html,"['Kaivalya Rawal', ' Himabindu Lakkaraju']",31,"As predictive models are increasingly being deployed in high-stakes decision-making, there has been a lot of interest in developing algorithms which can provide recourses to affected individuals. While developing such tools is important, it is even more critical to analyze and interpret a predictive model, and vet it thoroughly to ensure that the recourses it offers are meaningful and non-discriminatory before it is deployed in the real world. To this end, we propose a novel model agnostic framework called Actionable Recourse Summaries (AReS) to construct global counterfactual explanations which provide an interpretable and accurate summary of recourses for the entire population. We formulate a novel objective which simultaneously optimizes for correctness of the recourses and interpretability of the explanations, while minimizing overall recourse costs across the entire population. More specifically, our objective enables us to learn, with optimality guarantees on recourse correctness, a small number of compact rule sets each of which capture recourses for well defined subpopulations within the data. We also demonstrate theoretically that several of the prior approaches proposed to generate recourses for individuals are special cases of our framework. Experimental evaluation with real world datasets and user studies demonstrate that our framework can provide decision makers with a comprehensive overview of recourses corresponding to any black box model, and consequently help detect undesirable model biases and discrimination.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2020_8ee7730e,
 author = {Rawal, Kaivalya and Lakkaraju, Himabindu},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {12187--12198},
 publisher = {Curran Associates, Inc.},
 title = {Beyond Individualized Recourse: Interpretable and Interactive Summaries of Actionable Recourses},
 url = {https://proceedings.neurips.cc/paper/2020/file/8ee7730e97c67473a424ccfeff49ab20-Paper.pdf},
 volume = {33},
 year = {2020}
}

"
Neurips,2020,Sample Complexity of Uniform Convergence for Multicalibration,https://proceedings.neurips.cc//paper/2020/hash/9a96876e2f8f3dc4f3cf45f02c61c0c1-Abstract.html,"['Eliran Shabat', ' Lee Cohen', ' Yishay Mansour']",17,"There is a growing interest in societal concerns in machine learning systems, especially in fairness. Multicalibration gives a comprehensive methodology to address group fairness. In this work, we address the multicalibration error and decouple it from the prediction error. The importance of decoupling the fairness metric (multicalibration) and the accuracy (prediction error) is due to the inherent trade-off between the two, and the societal decision regarding the ``right tradeoff'' (as imposed many times by regulators). Our work gives sample complexity bounds for uniform convergence guarantees of multicalibration error, which implies that regardless of the accuracy, we can guarantee that the empirical and (true) multicalibration errors are close. We emphasize that our results: (1) are more general than previous bounds, as they apply to both agnostic and realizable settings, and do not rely on a specific type of algorithm (such as differentially private), (2) improve over previous multicalibration sample complexity bounds and (3) implies uniform convergence guarantees for the classical calibration error.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2020_9a96876e,
 author = {Shabat, Eliran and Cohen, Lee and Mansour, Yishay},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {13331--13340},
 publisher = {Curran Associates, Inc.},
 title = {Sample Complexity of Uniform Convergence for Multicalibration},
 url = {https://proceedings.neurips.cc/paper/2020/file/9a96876e2f8f3dc4f3cf45f02c61c0c1-Paper.pdf},
 volume = {33},
 year = {2020}
}

"
Neurips,2020,Understanding and Exploring the Network with Stochastic Architectures,https://proceedings.neurips.cc//paper/2020/hash/aa85e45da94cb0d78853c50ba636a15a-Abstract.html,"['Zhijie Deng', ' Yinpeng Dong', ' Shifeng Zhang', ' Jun Zhu']",3,"There is an emerging trend to train a network with stochastic architectures to enable various architectures to be plugged and played during inference. However, the existing investigation is highly entangled with neural architecture search (NAS), limiting its widespread use across scenarios. In this work, we decouple the training of a network with stochastic architectures (NSA) from NAS and provide a first systematical investigation on it as a stand-alone problem. We first uncover the characteristics of NSA in various aspects ranging from training stability, convergence, predictive behaviour, to generalization capacity to unseen architectures. We identify various issues of the vanilla NSA, such as training/test disparity and function mode collapse, and further propose the solutions to these issues with theoretical and empirical insights. We believe that these results could also serve as good heuristics for NAS. Given these understandings, we further apply the NSA with our improvements into diverse scenarios to fully exploit its promise of inference-time architecture stochasticity, including model ensemble, uncertainty estimation and semi-supervised learning. Remarkable performance (e.g., 2.75% error rate and 0.0032 expected calibration error on CIFAR-10) validate the effectiveness of such a model, providing new perspectives of exploring the potential of the network with stochastic architectures, beyond NAS.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2020_aa85e45d,
 author = {Deng, Zhijie and Dong, Yinpeng and Zhang, Shifeng and Zhu, Jun},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {14903--14914},
 publisher = {Curran Associates, Inc.},
 title = {Understanding and Exploring the Network with Stochastic Architectures},
 url = {https://proceedings.neurips.cc/paper/2020/file/aa85e45da94cb0d78853c50ba636a15a-Paper.pdf},
 volume = {33},
 year = {2020}
}

"
Neurips,2020,Certified Monotonic Neural Networks,https://proceedings.neurips.cc//paper/2020/hash/b139aeda1c2914e3b579aafd3ceeb1bd-Abstract.html,"['Xingchao Liu', ' Xing Han', ' Na Zhang', ' Qiang Liu']",44,"Learning monotonic models with respect to a subset of the inputs is a desirable feature to effectively address the fairness, interpretability, and generalization issues in practice. Existing methods for learning monotonic neural networks either require specifically designed model structures to ensure monotonicity, which can be too restrictive/complicated, or enforce monotonicity by adjusting the learning process, which cannot provably guarantee the learned model is monotonic on selected features. In this work, we propose to certify the monotonicity of the general piece-wise linear neural networks by solving a mixed integer linear programming problem. This provides a new general approach for learning monotonic neural networks with arbitrary model structures. Our method allows us to train neural networks with heuristic monotonicity regularizations, and we can gradually increase the regularization magnitude until the learned network is certified monotonic. Compared to prior work, our method does not require human-designed constraints on the weight space and also yields more accurate approximation. Empirical studies on various datasets demonstrate the efficiency of our approach over the state-of-the-art methods, such as Deep Lattice Networks",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2020_b139aeda,
 author = {Liu, Xingchao and Han, Xing and Zhang, Na and Liu, Qiang},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {15427--15438},
 publisher = {Curran Associates, Inc.},
 title = {Certified Monotonic Neural Networks},
 url = {https://proceedings.neurips.cc/paper/2020/file/b139aeda1c2914e3b579aafd3ceeb1bd-Paper.pdf},
 volume = {33},
 year = {2020}
}

"
Neurips,2020,Parametric Instance Classification for Unsupervised Visual Feature learning,https://proceedings.neurips.cc//paper/2020/hash/b427426b8acd2c2e53827970f2c2f526-Abstract.html,"['Yue Cao', ' Zhenda Xie', ' Bin Liu', ' Yutong Lin', ' Zheng Zhang', ' Han Hu']",38,"This paper presents parametric instance classification (PIC) for unsupervised visual feature learning. Unlike the state-of-the-art approaches which do instance discrimination in a dual-branch non-parametric fashion, PIC directly performs a one-branch parametric instance classification, revealing a simple framework similar to supervised classification and without the need to address the information leakage issue. We show that the simple PIC framework can be as effective as the state-of-the-art approaches, i.e. SimCLR and MoCo v2, by adapting several common component settings used in the state-of-the-art approaches. We also propose two novel techniques to further improve effectiveness and practicality of PIC: 1) a sliding-window data scheduler, instead of the previous epoch-based data scheduler, which addresses the extremely infrequent instance visiting issue in PIC and improves the effectiveness; 2) a negative sampling and weight update correction approach to reduce the training time and GPU memory consumption, which also enables application of PIC to almost unlimited training images. We hope that the PIC framework can serve as a simple baseline to facilitate future study. The code and network configurations are available at \url{https://github.com/bl0/PIC}.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2020_b427426b,
 author = {Cao, Yue and Xie, Zhenda and Liu, Bin and Lin, Yutong and Zhang, Zheng and Hu, Han},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {15614--15624},
 publisher = {Curran Associates, Inc.},
 title = {Parametric Instance Classification for Unsupervised Visual Feature learning},
 url = {https://proceedings.neurips.cc/paper/2020/file/b427426b8acd2c2e53827970f2c2f526-Paper.pdf},
 volume = {33},
 year = {2020}
}

"
Neurips,2020,Learning efficient task-dependent representations with synaptic plasticity,https://proceedings.neurips.cc//paper/2020/hash/b599e8250e4481aaa405a715419c8179-Abstract.html,"['Colin Bredenberg', ' Eero Simoncelli', ' Cristina Savin']",7,"Neural populations encode the sensory world imperfectly: their capacity is limited by the number of neurons, availability of metabolic and other biophysical resources, and intrinsic noise. The brain is presumably shaped by these limitations, improving efficiency by discarding some aspects of incoming sensory streams, while preferentially preserving commonly occurring, behaviorally-relevant information. Here we construct a stochastic recurrent neural circuit model that can learn efficient, task-specific sensory codes using a novel form of reward-modulated Hebbian synaptic plasticity. We illustrate the flexibility of the model by training an initially unstructured neural network to solve two different tasks: stimulus estimation, and stimulus discrimination. The network achieves high performance in both tasks by appropriately allocating resources and using its recurrent circuitry to best compensate for different levels of noise. We also show how the interaction between stimulus priors and task structure dictates the emergent network representations.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2020_b599e825,
 author = {Bredenberg, Colin and Simoncelli, Eero and Savin, Cristina},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {15714--15724},
 publisher = {Curran Associates, Inc.},
 title = {Learning efficient task-dependent representations with synaptic plasticity},
 url = {https://proceedings.neurips.cc/paper/2020/file/b599e8250e4481aaa405a715419c8179-Paper.pdf},
 volume = {33},
 year = {2020}
}

"
Neurips,2020,Domain Generalization via Entropy Regularization,https://proceedings.neurips.cc//paper/2020/hash/b98249b38337c5088bbc660d8f872d6a-Abstract.html,"['Shanshan Zhao', ' Mingming Gong', ' Tongliang Liu', ' Huan Fu', ' Dacheng Tao']",102,"Domain generalization aims to learn from multiple source domains a predictive model that can generalize to unseen target domains. One essential problem in domain generalization is to learn discriminative domain-invariant features. To arrive at this, some methods introduce a domain discriminator through adversarial learning to match the feature distributions in multiple source domains. However, adversarial training can only guarantee that the learned features have invariant marginal distributions, while the invariance of conditional distributions is more important for prediction in new domains. To ensure the conditional invariance of learned features, we propose an entropy regularization term that measures the dependency between the learned features and the class labels. Combined with the typical task-related loss, e.g., cross-entropy loss for classification, and adversarial loss for domain discrimination, our overall objective is guaranteed to learn conditional-invariant features across all source domains and thus can learn classifiers with better generalization capabilities. We demonstrate the effectiveness of our method through comparison with state-of-the-art methods on both simulated and real-world datasets. Code is available at: https://github.com/sshan-zhao/DGviaER.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2020_b98249b3,
 author = {Zhao, Shanshan and Gong, Mingming and Liu, Tongliang and Fu, Huan and Tao, Dacheng},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {16096--16107},
 publisher = {Curran Associates, Inc.},
 title = {Domain Generalization via Entropy Regularization},
 url = {https://proceedings.neurips.cc/paper/2020/file/b98249b38337c5088bbc660d8f872d6a-Paper.pdf},
 volume = {33},
 year = {2020}
}

"
Neurips,2020,Probably Approximately Correct Constrained Learning,https://proceedings.neurips.cc//paper/2020/hash/c291b01517f3e6797c774c306591cc32-Abstract.html,"['Luiz Chamon', ' Alejandro Ribeiro']",17,"As learning solutions reach critical applications in social, industrial, and medical domains, the need to curtail their behavior has become paramount. There is now ample evidence that without explicit tailoring, learning can lead to biased, unsafe, and prejudiced solutions. To tackle these problems, we develop a generalization theory of constrained learning based on the probably approximately correct (PAC) learning framework. In particular, we show that imposing requirements does not make a learning problem harder in the sense that any PAC learnable class is also PAC constrained learnable using a constrained counterpart of the empirical risk minimization (ERM) rule. For typical parametrized models, however, this learner involves solving a constrained non-convex optimization program for which even obtaining a feasible solution is challenging. To overcome this issue, we prove that under mild conditions the empirical dual problem of constrained learning is also a PAC constrained learner that now leads to a practical constrained learning algorithm based solely on solving unconstrained problems. We analyze the generalization properties of this solution and use it to illustrate how constrained learning can address problems in fair and robust classification.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2020_c291b015,
 author = {Chamon, Luiz and Ribeiro, Alejandro},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {16722--16735},
 publisher = {Curran Associates, Inc.},
 title = {Probably Approximately Correct Constrained Learning},
 url = {https://proceedings.neurips.cc/paper/2020/file/c291b01517f3e6797c774c306591cc32-Paper.pdf},
 volume = {33},
 year = {2020}
}

"
Neurips,2020,PAC-Bayesian Bound for the Conditional Value at Risk,https://proceedings.neurips.cc//paper/2020/hash/d02e9bdc27a894e882fa0c9055c99722-Abstract.html,"['Zakaria Mhammedi', ' Benjamin Guedj', ' Robert C. Williamson']",18,"Conditional Value at Risk ($\textsc{CVaR}$) is a ``coherent risk measure'' which generalizes expectation (reduced to a boundary parameter setting). Widely used in mathematical finance, it is garnering increasing interest in machine learning as an alternate approach to regularization, and as a means for ensuring fairness. This paper presents a generalization bound for learning algorithms that minimize the $\textsc{CVaR}$ of the empirical loss. The bound is of PAC-Bayesian type and is guaranteed to be small when the empirical $\textsc{CVaR}$ is small. We achieve this by reducing the problem of estimating $\textsc{CVaR}$ to that of merely estimating an expectation. This then enables us, as a by-product, to obtain concentration inequalities for $\textsc{CVaR}$ even when the random variable in question is unbounded.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2020_d02e9bdc,
 author = {Mhammedi, Zakaria and Guedj, Benjamin and Williamson, Robert C},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {17919--17930},
 publisher = {Curran Associates, Inc.},
 title = {PAC-Bayesian Bound for the Conditional Value at Risk},
 url = {https://proceedings.neurips.cc/paper/2020/file/d02e9bdc27a894e882fa0c9055c99722-Paper.pdf},
 volume = {33},
 year = {2020}
}

"
Neurips,2020,Ensuring Fairness Beyond the Training Data,https://proceedings.neurips.cc//paper/2020/hash/d6539d3b57159babf6a72e106beb45bd-Abstract.html,"['Debmalya Mandal', ' Samuel Deng', ' Suman Jana', ' Jeannette Wing', ' Daniel J. Hsu']",31,"We initiate the study of fair classifiers that are robust to perturbations in the training distribution. Despite recent progress, the literature on fairness has largely ignored the design of fair and robust classifiers. In this work, we develop classifiers that are fair not only with respect to the training distribution but also for a class of distributions that are weighted perturbations of the training samples. We formulate a min-max objective function whose goal is to minimize a distributionally robust training loss, and at the same time, find a classifier that is fair with respect to a class of distributions. We first reduce this problem to finding a fair classifier that is robust with respect to the class of distributions. Based on an online learning algorithm, we develop an iterative algorithm that provably converges to such a fair and robust solution. Experiments on standard machine learning fairness datasets suggest that, compared to the state-of-the-art fair classifiers, our classifier retains fairness guarantees and test accuracy for a large class of perturbations on the test set. Furthermore, our experiments show that there is an inherent trade-off between fairness robustness and accuracy of such classifiers.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2020_d6539d3b,
 author = {Mandal, Debmalya and Deng, Samuel and Jana, Suman and Wing, Jeannette and Hsu, Daniel J},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {18445--18456},
 publisher = {Curran Associates, Inc.},
 title = {Ensuring Fairness Beyond the Training Data},
 url = {https://proceedings.neurips.cc/paper/2020/file/d6539d3b57159babf6a72e106beb45bd-Paper.pdf},
 volume = {33},
 year = {2020}
}

"
Neurips,2020,CircleGAN: Generative Adversarial Learning across Spherical Circles,https://proceedings.neurips.cc//paper/2020/hash/f14bc21be7eaeed046fed206a492e652-Abstract.html,"['Woohyeon Shim', ' Minsu Cho']",7,"We present a novel discriminator for GANs that improves realness and diversity of generated samples by learning a structured hypersphere embedding space using spherical circles. The proposed discriminator learns to populate realistic samples around the longest spherical circle, i.e., a great circle, while pushing unrealistic samples toward the poles perpendicular to the great circle. Since longer circles occupy larger area on the hypersphere, they encourage more diversity in representation learning, and vice versa. Discriminating samples based on their corresponding spherical circles can thus naturally induce diversity to generated samples. We also extend the proposed method for conditional settings with class labels by creating a hypersphere for each category and performing class-wise discrimination and update. In experiments, we validate the effectiveness for both unconditional and conditional generation on standard benchmarks, achieving the state of the art.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2020_f14bc21b,
 author = {Shim, Woohyeon and Cho, Minsu},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {21081--21091},
 publisher = {Curran Associates, Inc.},
 title = {CircleGAN: Generative Adversarial Learning across Spherical Circles},
 url = {https://proceedings.neurips.cc/paper/2020/file/f14bc21be7eaeed046fed206a492e652-Paper.pdf},
 volume = {33},
 year = {2020}
}

"
Neurips,2020,ContraGAN: Contrastive Learning for Conditional Image Generation,https://proceedings.neurips.cc//paper/2020/hash/f490c742cd8318b8ee6dca10af2a163f-Abstract.html,"['Minguk Kang', ' Jaesik Park']",80,"Conditional image generation is the task of generating diverse images using class label information. Although many conditional Generative Adversarial Networks (GAN) have shown realistic results, such methods consider pairwise relations between the embedding of an image and the embedding of the corresponding label (data-to-class relations) as the conditioning losses. In this paper, we propose ContraGAN that considers relations between multiple image embeddings in the same batch (data-to-data relations) as well as the data-to-class relations by using a conditional contrastive loss. The discriminator of ContraGAN discriminates the authenticity of given samples and minimizes a contrastive objective to learn the relations between training images. Simultaneously, the generator tries to generate realistic images that deceive the authenticity and have a low contrastive loss. The experimental results show that ContraGAN outperforms state-of-the-art-models by 7.3% and 7.7% on Tiny ImageNet and ImageNet datasets, respectively. Besides, we experimentally demonstrate that ContraGAN helps to relieve the overfitting of the discriminator. For a fair comparison, we re-implement twelve state-of-the-art GANs using the PyTorch library. The software package is available at https://github.com/POSTECH-CVLab/PyTorch-StudioGAN.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2020_f490c742,
 author = {Kang, Minguk and Park, Jaesik},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {21357--21369},
 publisher = {Curran Associates, Inc.},
 title = {ContraGAN: Contrastive Learning for Conditional Image Generation},
 url = {https://proceedings.neurips.cc/paper/2020/file/f490c742cd8318b8ee6dca10af2a163f-Paper.pdf},
 volume = {33},
 year = {2020}
}

"
Neurips,2020,Distributed Training with Heterogeneous Data: Bridging Median- and Mean-Based Algorithms,https://proceedings.neurips.cc//paper/2020/hash/f629ed9325990b10543ab5946c1362fb-Abstract.html,"['Xiangyi Chen', ' Tiancong Chen', ' Haoran Sun', ' Steven Z. Wu', ' Mingyi Hong']",42,"Recently, there is a growing interest in the study of median-based algorithms for distributed non-convex optimization. Two prominent examples include signSGD with majority vote, an effective approach for communication reduction via 1-bit compression on the local gradients, and medianSGD, an algorithm recently proposed to ensure robustness against Byzantine workers. The convergence analyses for these algorithms critically rely on the assumption that all the distributed data are drawn iid from the same distribution. However, in applications such as Federated Learning, the data across different nodes or machines can be inherently heterogeneous, which violates such an iid assumption. This work analyzes signSGD and medianSGD in distributed settings with heterogeneous data. We show that these algorithms are non-convergent whenever there is some disparity between the expected median and mean over the local gradients. To overcome this gap, we provide a novel gradient correction mechanism that perturbs the local gradients with noise, which we show can provably close the gap between mean and median of the gradients. The proposed methods largely preserve nice properties of these median-based algorithms, such as the low per-iteration communication complexity of signSGD, and further enjoy global convergence to stationary solutions. Our perturbation technique can be of independent interest when one wishes to estimate mean through a median estimator.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2020_f629ed93,
 author = {Chen, Xiangyi and Chen, Tiancong and Sun, Haoran and Wu, Steven Z. and Hong, Mingyi},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {21616--21626},
 publisher = {Curran Associates, Inc.},
 title = {Distributed Training with Heterogeneous Data: Bridging Median- and Mean-Based Algorithms},
 url = {https://proceedings.neurips.cc/paper/2020/file/f629ed9325990b10543ab5946c1362fb-Paper.pdf},
 volume = {33},
 year = {2020}
}

"
Neurips,2020,First-Order Methods for Large-Scale Market Equilibrium Computation,https://proceedings.neurips.cc//paper/2020/hash/f75526659f31040afeb61cb7133e4e6d-Abstract.html,"['Yuan Gao', ' Christian Kroer']",16,"Market equilibrium is a solution concept with many applications such as digital ad markets, fair division, and resource sharing. For many classes of utility functions, equilibria can be captured by convex programs. We develop simple first-order methods suitable for solving these programs for large-scale markets. We focus on three practically-relevant utility classes: linear, quasilinear, and Leontief utilities. Using structural properties of market equilibria under each utility class, we show that the corresponding convex programs can be reformulated as optimization of a structured smooth convex function over a polyhedral set, for which projected gradient achieves linear convergence. To do so, we utilize recent linear convergence results under weakened strong-convexity conditions, and further refine the relevant constants in existing convergence results. Then, we show that proximal gradient (a generalization of projected gradient) with a practical linesearch scheme achieves linear convergence under the Proximal-PL condition, a recently developed error bound condition for convex composite problems. For quasilinear utilities, we show that Mirror Descent applied to a new convex program achieves sublinear last-iterate convergence and yields a form of Proportional Response dynamics, an elegant, interpretable algorithm for computing market equilibria originally developed for linear utilities. Numerical experiments show that Proportional Response is highly efficient for computing approximate market equilibria, while projected gradient with linesearch can be much faster when higher-accuracy solutions are needed.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2020_f7552665,
 author = {Gao, Yuan and Kroer, Christian},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {21738--21750},
 publisher = {Curran Associates, Inc.},
 title = {First-Order Methods for Large-Scale Market Equilibrium Computation},
 url = {https://proceedings.neurips.cc/paper/2020/file/f75526659f31040afeb61cb7133e4e6d-Paper.pdf},
 volume = {33},
 year = {2020}
}

"
Neurips,2020,Wasserstein Distances for Stereo Disparity Estimation,https://proceedings.neurips.cc//paper/2020/hash/fe7ecc4de28b2c83c016b5c6c2acd826-Abstract.html,"['Divyansh Garg', ' Yan Wang', ' Bharath Hariharan', ' Mark Campbell', ' Kilian Q. Weinberger', ' Wei-Lun Chao']",32,"Existing approaches to depth or disparity estimation output a distribution over a set of pre-defined discrete values. This leads to inaccurate results when the true depth or disparity does not match any of these values. The fact that this distribution is usually learned indirectly through a regression loss causes further problems in ambiguous regions around object boundaries. We address these issues using a new neural network architecture that is capable of outputting arbitrary depth values, and a new loss function that is derived from the Wasserstein distance between the true and the predicted distributions. We validate our approach on a variety of tasks, including stereo disparity and depth estimation, and the downstream 3D object detection. Our approach drastically reduces the error in ambiguous regions, especially around object boundaries that greatly affect the localization of objects in 3D, achieving the state-of-the-art in 3D object detection for autonomous driving.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2020_fe7ecc4d,
 author = {Garg, Divyansh and Wang, Yan and Hariharan, Bharath and Campbell, Mark and Weinberger, Kilian Q and Chao, Wei-Lun},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {22517--22529},
 publisher = {Curran Associates, Inc.},
 title = {Wasserstein Distances for Stereo Disparity Estimation},
 url = {https://proceedings.neurips.cc/paper/2020/file/fe7ecc4de28b2c83c016b5c6c2acd826-Paper.pdf},
 volume = {33},
 year = {2020}
}

"
Neurips,2020,Sampling-Decomposable Generative Adversarial Recommender,https://proceedings.neurips.cc//paper/2020/hash/ff42b03a06a1bed4e936f0e04958e168-Abstract.html,"['Binbin Jin', ' Defu Lian', ' Zheng Liu', ' Qi Liu', ' Jianhui Ma', ' Xing Xie', ' Enhong Chen']",17,"Recommendation techniques are important approaches for alleviating information overload. Being often trained on implicit user feedback, many recommenders suffer from the sparsity challenge due to the lack of explicitly negative samples. The GAN-style recommenders (i.e., IRGAN) addresses the challenge by learning a generator and a discriminator adversarially, such that the generator produces increasingly difficult samples for the discriminator to accelerate optimizing the discrimination objective. However, producing samples from the generator is very time-consuming, and our empirical study shows that the discriminator performs poor in top-k item recommendation. To this end, a theoretical analysis is made for the GAN-style algorithms, showing that the generator of limit capacity is diverged from the optimal generator. This may interpret the limitation of discriminator's performance. Based on these findings, we propose a Sampling-Decomposable Generative Adversarial Recommender (SD-GAR). In the framework, the divergence between some generator and the optimum is compensated by self-normalized importance sampling; the efficiency of sample generation is improved with a sampling-decomposable generator, such that each sample can be generated in O(1) with the Vose-Alias method. Interestingly, due to decomposability of sampling, the generator can be optimized with the closed-form solutions in an alternating manner, being different from policy gradient in the GAN-style algorithms. We extensively evaluate the proposed algorithm with five real-world recommendation datasets. The results show that SD-GAR outperforms IRGAN by 12.4% and the SOTA recommender by 10% on average. Moreover, discriminator training can be 20x faster on the dataset with more than 120K items.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2020_ff42b03a,
 author = {Jin, Binbin and Lian, Defu and Liu, Zheng and Liu, Qi and Ma, Jianhui and Xie, Xing and Chen, Enhong},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {22629--22639},
 publisher = {Curran Associates, Inc.},
 title = {Sampling-Decomposable Generative Adversarial Recommender},
 url = {https://proceedings.neurips.cc/paper/2020/file/ff42b03a06a1bed4e936f0e04958e168-Paper.pdf},
 volume = {33},
 year = {2020}
}

"
Neurips,2021,Counterfactual Explanations Can Be Manipulated,https://proceedings.neurips.cc//paper/2021/hash/009c434cab57de48a31f6b669e7ba266-Abstract.html,"['Dylan Slack', ' Anna Hilgard', ' Himabindu Lakkaraju', ' Sameer Singh']",35,"Counterfactual explanations are emerging as an attractive option for providing recourse to individuals adversely impacted by algorithmic decisions. As they are deployed in critical applications (e.g. law enforcement, financial lending), it becomes important to ensure that we clearly understand the vulnerabilties of these methods and find ways to address them. However, there is little understanding of the vulnerabilities and shortcomings of counterfactual explanations. In this work, we introduce the first framework that describes the vulnerabilities of counterfactual explanations and shows how they can be manipulated. More specifically, we show counterfactual explanations may converge to drastically different counterfactuals under a small perturbation indicating they are not robust. Leveraging this insight, we introduce a novel objective to train seemingly fair models where counterfactual explanations find much lower cost recourse under a slight perturbation. We describe how these models can unfairly provide low-cost recourse for specific subgroups in the data while appearing fair to auditors. We perform experiments on loan and violent crime prediction data sets where certain subgroups achieve up to 20x lower cost recourse under the perturbation. These results raise concerns regarding the dependability of current counterfactual explanation techniques, which we hope will inspire investigations in robust counterfactual explanations.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2021_009c434c,
 author = {Slack, Dylan and Hilgard, Anna and Lakkaraju, Himabindu and Singh, Sameer},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {62--75},
 publisher = {Curran Associates, Inc.},
 title = {Counterfactual Explanations Can Be Manipulated},
 url = {https://proceedings.neurips.cc/paper/2021/file/009c434cab57de48a31f6b669e7ba266-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
Neurips,2021,From Canonical Correlation Analysis to Self-supervised Graph Neural Networks,https://proceedings.neurips.cc//paper/2021/hash/00ac8ed3b4327bdd4ebbebcb2ba10a00-Abstract.html,"['Hengrui Zhang', ' Qitian Wu', ' Junchi Yan', ' David Wipf', ' Philip S Yu']",42,"We introduce a conceptually simple yet effective model for self-supervised representation learning with graph data. It follows the previous methods that generate two views of an input graph through data augmentation. However, unlike contrastive methods that focus on instance-level discrimination, we optimize an innovative feature-level objective inspired by classical Canonical Correlation Analysis. Compared with other works, our approach requires none of the parameterized mutual information estimator, additional projector, asymmetric structures, and most importantly, negative samples which can be costly. We show that the new objective essentially 1) aims at discarding augmentation-variant information by learning invariant representations, and 2) can prevent degenerated solutions by decorrelating features in different dimensions. Our theoretical analysis further provides an understanding for the new objective which can be equivalently seen as an instantiation of the Information Bottleneck Principle under the self-supervised setting. Despite its simplicity, our method performs competitively on seven public graph datasets.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2021_00ac8ed3,
 author = {Zhang, Hengrui and Wu, Qitian and Yan, Junchi and Wipf, David and Yu, Philip S},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {76--89},
 publisher = {Curran Associates, Inc.},
 title = {From Canonical Correlation Analysis to Self-supervised Graph Neural Networks},
 url = {https://proceedings.neurips.cc/paper/2021/file/00ac8ed3b4327bdd4ebbebcb2ba10a00-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
Neurips,2021,Learning Optimal Predictive Checklists,https://proceedings.neurips.cc//paper/2021/hash/09676fac73eda6cac726c43e43e86c58-Abstract.html,"['Haoran Zhang', ' Quaid Morris', ' Berk Ustun', ' Marzyeh Ghassemi']",6,"Checklists are simple decision aids that are often used to promote safety and reliability in clinical applications. In this paper, we present a method to learn checklists for clinical decision support. We represent predictive checklists as discrete linear classifiers with binary features and unit weights. We then learn globally optimal predictive checklists from data by solving an integer programming problem. Our method allows users to customize checklists to obey complex constraints, including constraints to enforce group fairness and to binarize real-valued features at training time. In addition, it pairs models with an optimality gap that can inform model development and determine the feasibility of learning sufficiently accurate checklists on a given dataset. We pair our method with specialized techniques that speed up its ability to train a predictive checklist that performs well and has a small optimality gap. We benchmark the performance of our method on seven clinical classification problems, and demonstrate its practical benefits by training a short-form checklist for PTSD screening. Our results show that our method can fit simple predictive checklists that perform well and that can easily be customized to obey a rich class of custom constraints.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2021_09676fac,
 author = {Zhang, Haoran and Morris, Quaid and Ustun, Berk and Ghassemi, Marzyeh},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {1215--1229},
 publisher = {Curran Associates, Inc.},
 title = {Learning Optimal Predictive Checklists},
 url = {https://proceedings.neurips.cc/paper/2021/file/09676fac73eda6cac726c43e43e86c58-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
Neurips,2021,Model Adaptation: Historical Contrastive Learning for Unsupervised Domain Adaptation without Source Data,https://proceedings.neurips.cc//paper/2021/hash/1dba5eed8838571e1c80af145184e515-Abstract.html,"['Jiaxing Huang', ' Dayan Guan', ' Aoran Xiao', ' Shijian Lu']",52,"Unsupervised domain adaptation aims to align a labeled source domain and an unlabeled target domain, but it requires to access the source data which often raises concerns in data privacy, data portability and data transmission efficiency. We study unsupervised model adaptation (UMA), or called Unsupervised Domain Adaptation without Source Data, an alternative setting that aims to adapt source-trained models towards target distributions without accessing source data. To this end, we design an innovative historical contrastive learning (HCL) technique that exploits historical source hypothesis to make up for the absence of source data in UMA. HCL addresses the UMA challenge from two perspectives. First, it introduces historical contrastive instance discrimination (HCID) that learns from target samples by contrasting their embeddings which are generated by the currently adapted model and the historical models. With the historical models, HCID encourages UMA to learn instance-discriminative target representations while preserving the source hypothesis. Second, it introduces historical contrastive category discrimination (HCCD) that pseudo-labels target samples to learn category-discriminative target representations. Specifically, HCCD re-weights pseudo labels according to their prediction consistency across the current and historical models. Extensive experiments show that HCL outperforms and state-of-the-art methods consistently across a variety of visual tasks and setups.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2021_1dba5eed,
 author = {Huang, Jiaxing and Guan, Dayan and Xiao, Aoran and Lu, Shijian},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {3635--3649},
 publisher = {Curran Associates, Inc.},
 title = {Model Adaptation: Historical Contrastive Learning for Unsupervised Domain Adaptation without Source Data},
 url = {https://proceedings.neurips.cc/paper/2021/file/1dba5eed8838571e1c80af145184e515-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
Neurips,2021,Can contrastive learning avoid shortcut solutions?,https://proceedings.neurips.cc//paper/2021/hash/27934a1f19d678a1377c257b9a780e80-Abstract.html,"['Joshua Robinson', ' Li Sun', ' Ke Yu', ' Kayhan Batmanghelich', ' Stefanie Jegelka', ' Suvrit Sra']",39,"The generalization of representations learned via contrastive learning depends crucially on what features of the data are extracted. However, we observe that the contrastive loss does not always sufficiently guide which features are extracted, a behavior that can negatively impact the performance on downstream tasks via ‚Äúshortcuts"", i.e., by inadvertently suppressing important predictive features. We find that feature extraction is influenced by the difficulty of the so-called instance discrimination task (i.e., the task of discriminating pairs of similar points from pairs of dissimilar ones). Although harder pairs improve the representation of some features, the improvement comes at the cost of suppressing previously well represented features. In response, we propose implicit feature modification (IFM), a method for altering positive and negative samples in order to guide contrastive models towards capturing a wider variety of predictive features. Empirically, we observe that IFM reduces feature suppression, and as a result improves performance on vision and medical imaging tasks.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2021_27934a1f,
 author = {Robinson, Joshua and Sun, Li and Yu, Ke and Batmanghelich, Kayhan and Jegelka, Stefanie and Sra, Suvrit},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {4974--4986},
 publisher = {Curran Associates, Inc.},
 title = {Can contrastive learning avoid shortcut solutions?},
 url = {https://proceedings.neurips.cc/paper/2021/file/27934a1f19d678a1377c257b9a780e80-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
Neurips,2021,A Theory-Driven Self-Labeling Refinement Method for Contrastive Representation Learning,https://proceedings.neurips.cc//paper/2021/hash/310ce61c90f3a46e340ee8257bc70e93-Abstract.html,"['Pan Zhou', ' Caiming Xiong', ' Xiaotong Yuan', ' Steven Chu Hong Hoi']",7,"For an image query, unsupervised contrastive learning labels crops of the same image as positives, and other image crops as negatives. Although intuitive, such a native label assignment strategy cannot reveal the underlying semantic similarity between a query and its positives and negatives, and impairs performance, since some negatives are semantically similar to the query or even share the same semantic class as the query. In this work, we first prove that for contrastive learning, inaccurate label assignment heavily impairs its generalization for semantic instance discrimination, while accurate labels benefit its generalization. Inspired by this theory, we propose a novel self-labeling refinement approach for contrastive learning. It improves the label quality via two complementary modules: (i) self-labeling refinery (SLR) to generate accurate labels and (ii) momentum mixup (MM) to enhance similarity between query and its positive. SLR uses a positive of a query to estimate semantic similarity between a query and its positive and negatives, and combines estimated similarity with vanilla label assignment in contrastive learning to iteratively generate more accurate and informative soft labels. We theoretically show that our SLR can exactly recover the true semantic labels of label-corrupted data, and supervises networks to achieve zero prediction error on classification tasks. MM randomly combines queries and positives to increase semantic similarity between the generated virtual queries and their positives so as to improves label accuracy. Experimental results on CIFAR10, ImageNet, VOC and COCO show the effectiveness of our method.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2021_310ce61c,
 author = {Zhou, Pan and Xiong, Caiming and Yuan, Xiaotong and Hoi, Steven Chu Hong},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {6183--6197},
 publisher = {Curran Associates, Inc.},
 title = {A Theory-Driven Self-Labeling Refinement Method for Contrastive Representation Learning},
 url = {https://proceedings.neurips.cc/paper/2021/file/310ce61c90f3a46e340ee8257bc70e93-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
Neurips,2021,Ising Model Selection Using $\ell_{1}$-Regularized Linear Regression: A Statistical Mechanics Analysis,https://proceedings.neurips.cc//paper/2021/hash/31917677a66c6eddd3ab1f68b0679e2f-Abstract.html,"['Xiangming Meng', ' Tomoyuki Obuchi', ' Yoshiyuki Kabashima']",0,"We theoretically analyze the typical learning performance of $\ell_{1}$-regularized linear regression ($\ell_1$-LinR) for Ising model selection using the replica method from statistical mechanics. For typical random regular graphs in the paramagnetic phase, an accurate estimate of the typical sample complexity of $\ell_1$-LinR is obtained. Remarkably, despite the model misspecification, $\ell_1$-LinR is model selection consistent with the same order of sample complexity as $\ell_{1}$-regularized logistic regression ($\ell_1$-LogR), i.e., $M=\mathcal{O}\left(\log N\right)$, where $N$ is the number of variables of the Ising model. Moreover, we provide an efficient method to accurately predict the non-asymptotic behavior of $\ell_1$-LinR for moderate $M, N$, such as precision and recall. Simulations show a fairly good agreement between theoretical predictions and experimental results, even for graphs with many loops, which supports our findings. Although this paper mainly focuses on $\ell_1$-LinR, our method is readily applicable for precisely characterizing the typical learning performances of a wide class of $\ell_{1}$-regularized $M$-estimators including $\ell_1$-LogR and interaction screening.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2021_31917677,
 author = {Meng, Xiangming and Obuchi, Tomoyuki and Kabashima, Yoshiyuki},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {6290--6303},
 publisher = {Curran Associates, Inc.},
 title = {Ising Model Selection Using \textbackslash ell\_\lbrace 1\rbrace -Regularized Linear Regression: A Statistical Mechanics Analysis},
 url = {https://proceedings.neurips.cc/paper/2021/file/31917677a66c6eddd3ab1f68b0679e2f-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
Neurips,2021,Generalized and Discriminative Few-Shot Object Detection via SVD-Dictionary Enhancement,https://proceedings.neurips.cc//paper/2021/hash/325995af77a0e8b06d1204a171010b3a-Abstract.html,"['Aming WU', ' Suqi Zhao', ' Cheng Deng', ' Wei Liu']",18,"Few-shot object detection (FSOD) aims to detect new objects based on few annotated samples. To alleviate the impact of few samples, enhancing the generalization and discrimination abilities of detectors on new objects plays an important role. In this paper, we explore employing Singular Value Decomposition (SVD) to boost both the generalization and discrimination abilities. In specific, we propose a novel method, namely, SVD-Dictionary enhancement, to build two separated spaces based on the sorted singular values. Concretely, the eigenvectors corresponding to larger singular values are used to build the generalization space in which localization is performed, as these eigenvectors generally suppress certain variations (e.g., the variation of styles) and contain intrinsical characteristics of objects. Meanwhile, since the eigenvectors corresponding to relatively smaller singular values may contain richer category-related information, we can utilize them to build the discrimination space in which classification is performed. Dictionary learning is further leveraged to capture high-level discriminative information from the discrimination space, which is beneficial for improving detection accuracy. In the experiments, we separately verify the effectiveness of our method on PASCAL VOC and COCO benchmarks. Particularly, for the 2-shot case in VOC split1, our method significantly outperforms the baseline by 6.2\%. Moreover, visualization analysis shows that our method is instrumental in doing FSOD.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2021_325995af,
 author = {WU, Aming and Zhao, Suqi and Deng, Cheng and Liu, Wei},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {6353--6364},
 publisher = {Curran Associates, Inc.},
 title = {Generalized and Discriminative Few-Shot Object Detection via SVD-Dictionary Enhancement},
 url = {https://proceedings.neurips.cc/paper/2021/file/325995af77a0e8b06d1204a171010b3a-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
Neurips,2021,Visualizing the Emergence of Intermediate Visual Patterns in DNNs,https://proceedings.neurips.cc//paper/2021/hash/33ebd5b07dc7e407752fe773eed20635-Abstract.html,"['Mingjie Li', ' Shaobo Wang', ' Quanshi Zhang']",4,"This paper proposes a method to visualize the discrimination power of intermediate-layer visual patterns encoded by a DNN. Specifically, we visualize (1) how the DNN gradually learns regional visual patterns in each intermediate layer during the training process, and (2) the effects of the DNN using non-discriminative patterns in low layers to construct disciminative patterns in middle/high layers through the forward propagation. Based on our visualization method, we can quantify knowledge points (i.e. the number of discriminative visual patterns) learned by the DNN to evaluate the representation capacity of the DNN. Furthermore, this method also provides new insights into signal-processing behaviors of existing deep-learning techniques, such as adversarial attacks and knowledge distillation.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2021_33ebd5b0,
 author = {Li, Mingjie and Wang, Shaobo and Zhang, Quanshi},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {6594--6607},
 publisher = {Curran Associates, Inc.},
 title = {Visualizing the Emergence of Intermediate Visual Patterns in DNNs},
 url = {https://proceedings.neurips.cc/paper/2021/file/33ebd5b07dc7e407752fe773eed20635-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
Neurips,2021,Nested Counterfactual Identification from Arbitrary Surrogate Experiments,https://proceedings.neurips.cc//paper/2021/hash/36bedb6eb7152f39b16328448942822b-Abstract.html,"['Juan Correa', ' Sanghack Lee', ' Elias Bareinboim']",9,"The Ladder of Causation describes three qualitatively different types of activities an agent may be interested in engaging in, namely, seeing (observational), doing (interventional), and imagining (counterfactual) (Pearl and Mackenzie, 2018). The inferential challenge imposed by the causal hierarchy is that data is collected by an agent observing or intervening in a system (layers 1 and 2), while its goal may be to understand what would have happened had it taken a different course of action, contrary to what factually ended up happening (layer 3). While there exists a solid understanding of the conditions under which cross-layer inferences are allowed from observations to interventions, the results are somewhat scarcer when targeting counterfactual quantities. In this paper, we study the identification of nested counterfactuals from an arbitrary combination of observations and experiments. Specifically, building on a more explicit definition of nested counterfactuals, we prove the counterfactual unnesting theorem (CUT), which allows one to map arbitrary nested counterfactuals to unnested ones. For instance, applications in mediation and fairness analysis usually evoke notions of direct, indirect, and spurious effects, which naturally require nesting. Second, we introduce a sufficient and necessary graphical condition for counterfactual identification from an arbitrary combination of observational and experimental distributions. Lastly, we develop an efficient and complete algorithm for identifying nested counterfactuals; failure of the algorithm returning an expression for a query implies it is not identifiable.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2021_36bedb6e,
 author = {Correa, Juan and Lee, Sanghack and Bareinboim, Elias},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {6856--6867},
 publisher = {Curran Associates, Inc.},
 title = {Nested Counterfactual Identification from Arbitrary Surrogate Experiments},
 url = {https://proceedings.neurips.cc/paper/2021/file/36bedb6eb7152f39b16328448942822b-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
Neurips,2021,Edge Representation Learning with Hypergraphs,https://proceedings.neurips.cc//paper/2021/hash/3def184ad8f4755ff269862ea77393dd-Abstract.html,"['Jaehyeong Jo', ' Jinheon Baek', ' Seul Lee', ' Dongki Kim', ' Minki Kang', ' Sung Ju Hwang']",12,"Graph neural networks have recently achieved remarkable success in representing graph-structured data, with rapid progress in both the node embedding and graph pooling methods. Yet, they mostly focus on capturing information from the nodes considering their connectivity, and not much work has been done in representing the edges, which are essential components of a graph. However, for tasks such as graph reconstruction and generation, as well as graph classification tasks for which the edges are important for discrimination, accurately representing edges of a given graph is crucial to the success of the graph representation learning. To this end, we propose a novel edge representation learning framework based on Dual Hypergraph Transformation (DHT), which transforms the edges of a graph into the nodes of a hypergraph. This dual hypergraph construction allows us to apply message-passing techniques for node representations to edges. After obtaining edge representations from the hypergraphs, we then cluster or drop edges to obtain holistic graph-level edge representations. We validate our edge representation learning method with hypergraphs on diverse graph datasets for graph representation and generation performance, on which our method largely outperforms existing graph representation learning methods. Moreover, our edge representation learning and pooling method also largely outperforms state-of-the-art graph pooling methods on graph classification, not only because of its accurate edge representation learning, but also due to its lossless compression of the nodes and removal of irrelevant edges for effective message-passing.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2021_3def184a,
 author = {Jo, Jaehyeong and Baek, Jinheon and Lee, Seul and Kim, Dongki and Kang, Minki and Hwang, Sung Ju},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {7534--7546},
 publisher = {Curran Associates, Inc.},
 title = {Edge Representation Learning with Hypergraphs},
 url = {https://proceedings.neurips.cc/paper/2021/file/3def184ad8f4755ff269862ea77393dd-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
Neurips,2021,Distributed Deep Learning In Open Collaborations,https://proceedings.neurips.cc//paper/2021/hash/41a60377ba920919939d83326ebee5a1-Abstract.html,"['Michael Diskin', ' Alexey Bukhtiyarov', ' Max Ryabinin', ' Lucile Saulnier', ' quentin lhoest', ' Anton Sinitsin', ' Dmitry Popov', ' Dmitry V. Pyrkin', ' Maxim Kashirin', ' Alexander Borzunov', ' Albert Villanova del Moral', ' Denis Mazur', ' Ilia Kobelev', ' Yacine Jernite', ' Thomas Wolf', ' Gennady Pekhimenko']",9,"Modern deep learning applications require increasingly more compute to train state-of-the-art models. To address this demand, large corporations and institutions use dedicated High-Performance Computing clusters, whose construction and maintenance are both environmentally costly and well beyond the budget of most organizations. As a result, some research directions become the exclusive domain of a few large industrial and even fewer academic actors. To alleviate this disparity, smaller groups may pool their computational resources and run collaborative experiments that benefit all participants. This paradigm, known as grid- or volunteer computing, has seen successful applications in numerous scientific areas. However, using this approach for machine learning is difficult due to high latency, asymmetric bandwidth, and several challenges unique to volunteer computing. In this work, we carefully analyze these constraints and propose a novel algorithmic framework designed specifically for collaborative training. We demonstrate the effectiveness of our approach for SwAV and ALBERT pretraining in realistic conditions and achieve performance comparable to traditional setups at a fraction of the cost. Finally, we provide a detailed report of successful collaborative language model pretraining with nearly 50 participants.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2021_41a60377,
 author = {Diskin, Michael and Bukhtiyarov, Alexey and Ryabinin, Max and Saulnier, Lucile and lhoest, quentin and Sinitsin, Anton and Popov, Dmitry and Pyrkin, Dmitry V. and Kashirin, Maxim and Borzunov, Alexander and Villanova del Moral, Albert and Mazur, Denis and Kobelev, Ilia and Jernite, Yacine and Wolf, Thomas and Pekhimenko, Gennady},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {7879--7897},
 publisher = {Curran Associates, Inc.},
 title = {Distributed Deep Learning In Open Collaborations},
 url = {https://proceedings.neurips.cc/paper/2021/file/41a60377ba920919939d83326ebee5a1-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
Neurips,2021,Distributed Saddle-Point Problems Under Data Similarity,https://proceedings.neurips.cc//paper/2021/hash/44e65d3e9bc2f88b2b3d566de51a5381-Abstract.html,"['Aleksandr Beznosikov', ' Gesualdo Scutari', ' Alexander Rogozin', ' Alexander Gasnikov']",14,"We study solution methods for (strongly-)convex-(strongly)-concave Saddle-Point Problems (SPPs) over networks of two type--master/workers (thus centralized) architectures and mesh (thus decentralized) networks. The local functions at each node are assumed to be \textit{similar}, due to statistical data similarity or otherwise. We establish lower complexity bounds for a fairly general class of algorithms solving the SPP. We show that a given suboptimality $\epsilon>0$ is achieved over master/workers networks in $\Omega\big(\Delta\cdot \delta/\mu\cdot \log (1/\varepsilon)\big)$ rounds of communications, where $\delta>0$ measures the degree of similarity of the local functions, $\mu$ is their strong convexity constant, and $\Delta$ is the diameter of the network. The lower communication complexity bound over mesh networks reads $\Omega\big(1/{\sqrt{\rho}} \cdot {\delta}/{\mu}\cdot\log (1/\varepsilon)\big)$, where $\rho$ is the (normalized) eigengap of the gossip matrix used for the communication between neighbouring nodes. We then propose algorithms matching the lower bounds over either types of networks (up to log-factors). We assess the effectiveness of the proposed algorithms on a robust regression problem.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2021_44e65d3e,
 author = {Beznosikov, Aleksandr and Scutari, Gesualdo and Rogozin, Alexander and Gasnikov, Alexander},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {8172--8184},
 publisher = {Curran Associates, Inc.},
 title = {Distributed Saddle-Point Problems Under Data Similarity},
 url = {https://proceedings.neurips.cc/paper/2021/file/44e65d3e9bc2f88b2b3d566de51a5381-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
Neurips,2021,High Probability Complexity Bounds for Line Search Based on Stochastic Oracles,https://proceedings.neurips.cc//paper/2021/hash/4cb811134b9d39fc3104bd06ce75abad-Abstract.html,"['Billy Jin', ' Katya Scheinberg', ' Miaolan Xie']",9,"We consider a line-search method for continuous optimization under a stochastic setting where the function values and gradients are available only through inexact probabilistic zeroth and first-order oracles. These oracles capture multiple standard settings including expected loss minimization and zeroth-order optimization. Moreover, our framework is very general and allows the function and gradient estimates to be biased. The proposed algorithm is simple to describe, easy to implement, and uses these oracles in a similar way as the standard deterministic line search uses exact function and gradient values. Under fairly general conditions on the oracles, we derive a high probability tail bound on the iteration complexity of the algorithm when applied to non-convex smooth functions. These results are stronger than those for other existing stochastic line search methods and apply in more general settings.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2021_4cb81113,
 author = {Jin, Billy and Scheinberg, Katya and Xie, Miaolan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {9193--9203},
 publisher = {Curran Associates, Inc.},
 title = {High Probability Complexity Bounds for Line Search Based on Stochastic Oracles},
 url = {https://proceedings.neurips.cc/paper/2021/file/4cb811134b9d39fc3104bd06ce75abad-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
Neurips,2021,Qu-ANTI-zation: Exploiting Quantization Artifacts for Achieving Adversarial Outcomes,https://proceedings.neurips.cc//paper/2021/hash/4d8bd3f7351f4fee76ba17594f070ddd-Abstract.html,"['Sanghyun Hong', ' Michael-Andrei Panaitescu-Liess', ' Yigitcan Kaya', ' Tudor Dumitras']",4,"Quantization is a popular technique that transforms the parameter representation of a neural network from floating-point numbers into lower-precision ones (e.g., 8-bit integers). It reduces the memory footprint and the computational cost at inference, facilitating the deployment of resource-hungry models. However, the parameter perturbations caused by this transformation result in behavioral disparities between the model before and after quantization. For example, a quantized model can misclassify some test-time samples that are otherwise classified correctly. It is not known whether such differences lead to a new security vulnerability. We hypothesize that an adversary may control this disparity to introduce specific behaviors that activate upon quantization. To study this hypothesis, we weaponize quantization-aware training and propose a new training framework to implement adversarial quantization outcomes. Following this framework, we present three attacks we carry out with quantization: (i) an indiscriminate attack for significant accuracy loss; (ii) a targeted attack against specific samples; and (iii) a backdoor attack for controlling the model with an input trigger. We further show that a single compromised model defeats multiple quantization schemes, including robust quantization techniques. Moreover, in a federated learning scenario, we demonstrate that a set of malicious participants who conspire can inject our quantization-activated backdoor. Lastly, we discuss potential counter-measures and show that only re-training consistently removes the attack artifacts. Our code is available at https://github.com/Secure-AI-Systems-Group/Qu-ANTI-zation",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2021_4d8bd3f7,
 author = {Hong, Sanghyun and Panaitescu-Liess, Michael-Andrei and Kaya, Yigitcan and Dumitras, Tudor},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {9303--9316},
 publisher = {Curran Associates, Inc.},
 title = {Qu-ANTI-zation: Exploiting Quantization Artifacts for Achieving Adversarial Outcomes},
 url = {https://proceedings.neurips.cc/paper/2021/file/4d8bd3f7351f4fee76ba17594f070ddd-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
Neurips,2021,Data-Efficient Instance Generation from Instance Discrimination,https://proceedings.neurips.cc//paper/2021/hash/4e0d67e54ad6626e957d15b08ae128a6-Abstract.html,"['Ceyuan Yang', ' Yujun Shen', ' Yinghao Xu', ' Bolei Zhou']",32,"Generative Adversarial Networks (GANs) have significantly advanced image synthesis, however, the synthesis quality drops significantly given a limited amount of training data. To improve the data efficiency of GAN training, prior work typically employs data augmentation to mitigate the overfitting of the discriminator yet still learn the discriminator with a bi-classification ($\textit{i.e.}$, real $\textit{vs.}$ fake) task. In this work, we propose a data-efficient Instance Generation ($\textit{InsGen}$) method based on instance discrimination. Concretely, besides differentiating the real domain from the fake domain, the discriminator is required to distinguish every individual image, no matter it comes from the training set or from the generator. In this way, the discriminator can benefit from the infinite synthesized samples for training, alleviating the overfitting problem caused by insufficient training data. A noise perturbation strategy is further introduced to improve its discriminative power. Meanwhile, the learned instance discrimination capability from the discriminator is in turn exploited to encourage the generator for diverse generation. Extensive experiments demonstrate the effectiveness of our method on a variety of datasets and training settings. Noticeably, on the setting of $2K$ training images from the FFHQ dataset, we outperform the state-of-the-art approach with 23.5\% FID improvement.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2021_4e0d67e5,
 author = {Yang, Ceyuan and Shen, Yujun and Xu, Yinghao and Zhou, Bolei},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {9378--9390},
 publisher = {Curran Associates, Inc.},
 title = {Data-Efficient Instance Generation from Instance Discrimination},
 url = {https://proceedings.neurips.cc/paper/2021/file/4e0d67e54ad6626e957d15b08ae128a6-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
Neurips,2021,Characterizing the risk of fairwashing,https://proceedings.neurips.cc//paper/2021/hash/7caf5e22ea3eb8175ab518429c8589a4-Abstract.html,"['Ulrich A√Øvodji', ' Hiromi Arai', ' S√©bastien Gambs', ' Satoshi Hara']",5,"Fairwashing refers to the risk that an unfair black-box model can be explained by a fairer model through post-hoc explanation manipulation. In this paper, we investigate the capability of fairwashing attacks by analyzing their fidelity-unfairness trade-offs. In particular, we show that fairwashed explanation models can generalize beyond the suing group (i.e., data points that are being explained), meaning that a fairwashed explainer can be used to rationalize subsequent unfair decisions of a black-box model. We also demonstrate that fairwashing attacks can transfer across black-box models, meaning that other black-box models can perform fairwashing without explicitly using their predictions. This generalization and transferability of fairwashing attacks imply that their detection will be difficult in practice. Finally, we propose an approach to quantify the risk of fairwashing, which is based on the computation of the range of the unfairness of high-fidelity explainers.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2021_7caf5e22,
 author = {A\""{\i}vodji, Ulrich and Arai, Hiromi and Gambs, S\'{e}bastien and Hara, Satoshi},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {14822--14834},
 publisher = {Curran Associates, Inc.},
 title = {Characterizing the risk of fairwashing},
 url = {https://proceedings.neurips.cc/paper/2021/file/7caf5e22ea3eb8175ab518429c8589a4-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
Neurips,2021,Federated Multi-Task Learning under a Mixture of Distributions,https://proceedings.neurips.cc//paper/2021/hash/82599a4ec94aca066873c99b4c741ed8-Abstract.html,"['Othmane Marfoq', ' Giovanni Neglia', ' Aur√©lien Bellet', ' Laetitia Kameni', ' Richard Vidal']",74,"The increasing size of data generated by smartphones and IoT devices motivated the development of Federated Learning (FL), a framework for on-device collaborative training of machine learning models. First efforts in FL focused on learning a single global model with good average performance across clients, but the global model may be arbitrarily bad for a given client, due to the inherent heterogeneity of local data distributions. Federated multi-task learning (MTL) approaches can learn personalized models by formulating an opportune penalized optimization problem. The penalization term can capture complex relations among personalized models, but eschews clear statistical assumptions about local data distributions. In this work, we propose to study federated MTL under the flexible assumption that each local data distribution is a mixture of unknown underlying distributions. This assumption encompasses most of the existing personalized FL approaches and leads to federated EM-like algorithms for both client-server and fully decentralized settings. Moreover, it provides a principled way to serve personalized models to clients not seen at training time. The algorithms' convergence is analyzed through a novel federated surrogate optimization framework, which can be of general interest. Experimental results on FL benchmarks show that our approach provides models with higher accuracy and fairness than state-of-the-art methods.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2021_82599a4e,
 author = {Marfoq, Othmane and Neglia, Giovanni and Bellet, Aur\'{e}lien and Kameni, Laetitia and Vidal, Richard},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {15434--15447},
 publisher = {Curran Associates, Inc.},
 title = {Federated Multi-Task Learning under a Mixture of Distributions},
 url = {https://proceedings.neurips.cc/paper/2021/file/82599a4ec94aca066873c99b4c741ed8-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
Neurips,2021,Few-Shot Object Detection via Association and DIscrimination,https://proceedings.neurips.cc//paper/2021/hash/8a1e808b55fde9455cb3d8857ed88389-Abstract.html,"['Yuhang Cao', ' Jiaqi Wang', ' Ying Jin', ' Tong Wu', ' Kai Chen', ' Ziwei Liu', ' Dahua Lin']",24,"Object detection has achieved substantial progress in the last decade. However, detecting novel classes with only few samples remains challenging, since deep learning under low data regime usually leads to a degraded feature space. Existing works employ a holistic fine-tuning paradigm to tackle this problem, where the model is first pre-trained on all base classes with abundant samples, and then it is used to carve the novel class feature space. Nonetheless, this paradigm is still imperfect. Durning fine-tuning, a novel class may implicitly leverage the knowledge of multiple base classes to construct its feature space, which induces a scattered feature space, hence violating the inter-class separability. To overcome these obstacles, we propose a two-step fine-tuning framework, Few-shot object detection via Association and DIscrimination (FADI), which builds up a discriminative feature space for each novel class with two integral steps. 1) In the association step, in contrast to implicitly leveraging multiple base classes, we construct a compact novel class feature space via explicitly imitating a specific base class feature space. Specifically, we associate each novel class with a base class according to their semantic similarity. After that, the feature space of a novel class can readily imitate the well-trained feature space of the associated base class. 2) In the discrimination step, to ensure the separability between the novel classes and associated base classes, we disentangle the classification branches for base and novel classes. To further enlarge the inter-class separability between all classes, a set-specialized margin loss is imposed. Extensive experiments on standard Pascal VOC and MS-COCO datasets demonstrate that FADI achieves new state-of-the-art performance, significantly improving the baseline in any shot/split by +18.7. Notably, the advantage of FADI is most announced on extremely few-shot scenarios (e.g. 1- and 3- shot).",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2021_8a1e808b,
 author = {Cao, Yuhang and Wang, Jiaqi and Jin, Ying and Wu, Tong and Chen, Kai and Liu, Ziwei and Lin, Dahua},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {16570--16581},
 publisher = {Curran Associates, Inc.},
 title = {Few-Shot Object Detection via Association and DIscrimination},
 url = {https://proceedings.neurips.cc/paper/2021/file/8a1e808b55fde9455cb3d8857ed88389-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
Neurips,2021,Training Over-parameterized Models with Non-decomposable Objectives,https://proceedings.neurips.cc//paper/2021/hash/9713faa264b94e2bf346a1bb52587fd8-Abstract.html,"['Harikrishna Narasimhan', ' Aditya K. Menon']",5,"Many modern machine learning applications come with complex and nuanced design goals such as minimizing the worst-case error, satisfying a given precision or recall target, or enforcing group-fairness constraints. Popular techniques for optimizing such non-decomposable objectives reduce the problem into a sequence of cost-sensitive learning tasks, each of which is then solved by re-weighting the training loss with example-specific costs. We point out that the standard approach of re-weighting the loss to incorporate label costs can produce unsatisfactory results when used to train over-parameterized models. As a remedy, we propose new cost- sensitive losses that extend the classical idea of logit adjustment to handle more general cost matrices. Our losses are calibrated, and can be further improved with distilled labels from a teacher model. Through experiments on benchmark image datasets, we showcase the effectiveness of our approach in training ResNet models with common robust and constrained optimization objectives.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2021_9713faa2,
 author = {Narasimhan, Harikrishna and Menon, Aditya K},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {18165--18181},
 publisher = {Curran Associates, Inc.},
 title = {Training Over-parameterized Models with Non-decomposable Objectives},
 url = {https://proceedings.neurips.cc/paper/2021/file/9713faa264b94e2bf346a1bb52587fd8-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
Neurips,2021,"Efficient First-Order Contextual Bandits: Prediction, Allocation, and Triangular Discrimination",https://proceedings.neurips.cc//paper/2021/hash/9d684c589d67031a627ad33d59db65e5-Abstract.html,"['Dylan J. Foster', ' Akshay Krishnamurthy']",10,"A recurring theme in statistical learning, online learning, and beyond is that faster convergence rates are possible for problems with low noise, often quantified by the performance of the best hypothesis; such results are known as first-order or small-loss guarantees. While first-order guarantees are relatively well understood in statistical and online learning, adapting to low noise in contextual bandits (and more broadly, decision making) presents major algorithmic challenges. In a COLT 2017 open problem, Agarwal, Krishnamurthy, Langford, Luo, and Schapire asked whether first-order guarantees are even possible for contextual bandits and---if so---whether they can be attained by efficient algorithms. We give a resolution to this question by providing an optimal and efficient reduction from contextual bandits to online regression with the logarithmic (or, cross-entropy) loss. Our algorithm is simple and practical, readily accommodates rich function classes, and requires no distributional assumptions beyond realizability. In a large-scale empirical evaluation, we find that our approach typically outperforms comparable non-first-order methods.On the technical side, we show that the logarithmic loss and an information-theoretic quantity called the triangular discrimination play a fundamental role in obtaining first-order guarantees, and we combine this observation with new refinements to the regression oracle reduction framework of Foster and Rakhlin (2020). The use of triangular discrimination yields novel results even for the classical statistical learning model, and we anticipate that it will find broader use.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2021_9d684c58,
 author = {Foster, Dylan J and Krishnamurthy, Akshay},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {18907--18919},
 publisher = {Curran Associates, Inc.},
 title = {Efficient First-Order Contextual Bandits: Prediction, Allocation, and Triangular Discrimination},
 url = {https://proceedings.neurips.cc/paper/2021/file/9d684c589d67031a627ad33d59db65e5-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
Neurips,2021,Boosted CVaR Classification,https://proceedings.neurips.cc//paper/2021/hash/b691334ccf10d4ab144d672f7783c8a3-Abstract.html,"['Runtian Zhai', ' Chen Dan', ' Arun Suggala', ' J. Zico Kolter', ' Pradeep Ravikumar']",7,"Many modern machine learning tasks require models with high tail performance, i.e. high performance over the worst-off samples in the dataset. This problem has been widely studied in fields such as algorithmic fairness, class imbalance, and risk-sensitive decision making. A popular approach to maximize the model's tail performance is to minimize the CVaR (Conditional Value at Risk) loss, which computes the average risk over the tails of the loss. However, for classification tasks where models are evaluated by the zero-one loss, we show that if the classifiers are deterministic, then the minimizer of the average zero-one loss also minimizes the CVaR zero-one loss, suggesting that CVaR loss minimization is not helpful without additional assumptions. We circumvent this negative result by minimizing the CVaR loss over randomized classifiers, for which the minimizers of the average zero-one loss and the CVaR zero-one loss are no longer the same, so minimizing the latter can lead to better tail performance. To learn such randomized classifiers, we propose the Boosted CVaR Classification framework which is motivated by a direct relationship between CVaR and a classical boosting algorithm called LPBoost. Based on this framework, we design an algorithm called $\alpha$-AdaLPBoost. We empirically evaluate our proposed algorithm on four benchmark datasets and show that it achieves higher tail performance than deterministic model training methods.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2021_b691334c,
 author = {Zhai, Runtian and Dan, Chen and Suggala, Arun and Kolter, J. Zico and Ravikumar, Pradeep},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {21860--21871},
 publisher = {Curran Associates, Inc.},
 title = { Boosted CVaR Classification},
 url = {https://proceedings.neurips.cc/paper/2021/file/b691334ccf10d4ab144d672f7783c8a3-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
Neurips,2021,Disentangled Contrastive Learning on Graphs,https://proceedings.neurips.cc//paper/2021/hash/b6cda17abb967ed28ec9610137aa45f7-Abstract.html,"['Haoyang Li', ' Xin Wang', ' Ziwei Zhang', ' Zehuan Yuan', ' Hang Li', ' Wenwu Zhu']",18,"Recently, self-supervised learning for graph neural networks (GNNs) has attracted considerable attention because of their notable successes in learning the representation of graph-structure data. However, the formation of a real-world graph typically arises from the highly complex interaction of many latent factors. The existing self-supervised learning methods for GNNs are inherently holistic and neglect the entanglement of the latent factors, resulting in the learned representations suboptimal for downstream tasks and difficult to be interpreted. Learning disentangled graph representations with self-supervised learning poses great challenges and remains largely ignored by the existing literature. In this paper, we introduce the Disentangled Graph Contrastive Learning (DGCL) method, which is able to learn disentangled graph-level representations with self-supervision. In particular, we first identify the latent factors of the input graph and derive its factorized representations. Each of the factorized representations describes a latent and disentangled aspect pertinent to a specific latent factor of the graph. Then we propose a novel factor-wise discrimination objective in a contrastive learning manner, which can force the factorized representations to independently reflect the expressive information from different latent factors. Extensive experiments on both synthetic and real-world datasets demonstrate the superiority of our method against several state-of-the-art baselines.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2021_b6cda17a,
 author = {Li, Haoyang and Wang, Xin and Zhang, Ziwei and Yuan, Zehuan and Li, Hang and Zhu, Wenwu},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {21872--21884},
 publisher = {Curran Associates, Inc.},
 title = {Disentangled Contrastive Learning on Graphs},
 url = {https://proceedings.neurips.cc/paper/2021/file/b6cda17abb967ed28ec9610137aa45f7-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
Neurips,2021,Best-case lower bounds in online learning,https://proceedings.neurips.cc//paper/2021/hash/b7da6669894867f04b8727876a69ffc0-Abstract.html,"['Crist√≥bal Guzm√°n', ' Nishant Mehta', ' Ali Mortazavi']",0,"Much of the work in online learning focuses on the study of sublinear upper bounds on the regret. In this work, we initiate the study of best-case lower bounds in online convex optimization, wherein we bound the largest \emph{improvement} an algorithm can obtain relative to the single best action in hindsight. This problem is motivated by the goal of better understanding the adaptivity of a learning algorithm. Another motivation comes from fairness: it is known that best-case lower bounds are instrumental in obtaining algorithms for decision-theoretic online learning (DTOL) that satisfy a notion of group fairness. Our contributions are a general method to provide best-case lower bounds in Follow The Regularized Leader (FTRL) algorithms with time-varying regularizers, which we use to show that best-case lower bounds are of the same order as existing upper regret bounds: this includes situations with a fixed learning rate, decreasing learning rates, timeless methods, and adaptive gradient methods. In stark contrast, we show that the linearized version of FTRL can attain negative linear regret. Finally, in DTOL with two experts and binary losses, we fully characterize the best-case sequences, which provides a finer understanding of the best-case lower bounds.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2021_b7da6669,
 author = {Guzm\'{a}n, Crist\'{o}bal and Mehta, Nishant and Mortazavi, Ali},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {21923--21934},
 publisher = {Curran Associates, Inc.},
 title = {Best-case lower bounds in online learning},
 url = {https://proceedings.neurips.cc/paper/2021/file/b7da6669894867f04b8727876a69ffc0-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
Neurips,2021,SWAD: Domain Generalization by Seeking Flat Minima,https://proceedings.neurips.cc//paper/2021/hash/bcb41ccdc4363c6848a1d760f26c28a0-Abstract.html,"['Junbum Cha', ' Sanghyuk Chun', ' Kyungjae Lee', ' Han-Cheol Cho', ' Seunghyun Park', ' Yunsung Lee', ' Sungrae Park']",74,"Domain generalization (DG) methods aim to achieve generalizability to an unseen target domain by using only training data from the source domains. Although a variety of DG methods have been proposed, a recent study shows that under a fair evaluation protocol, called DomainBed, the simple empirical risk minimization (ERM) approach works comparable to or even outperforms previous methods. Unfortunately, simply solving ERM on a complex, non-convex loss function can easily lead to sub-optimal generalizability by seeking sharp minima. In this paper, we theoretically show that finding flat minima results in a smaller domain generalization gap. We also propose a simple yet effective method, named Stochastic Weight Averaging Densely (SWAD), to find flat minima. SWAD finds flatter minima and suffers less from overfitting than does the vanilla SWA by a dense and overfit-aware stochastic weight sampling strategy. SWAD shows state-of-the-art performances on five DG benchmarks, namely PACS, VLCS, OfficeHome, TerraIncognita, and DomainNet, with consistent and large margins of +1.6% averagely on out-of-domain accuracy. We also compare SWAD with conventional generalization methods, such as data augmentation and consistency regularization methods, to verify that the remarkable performance improvements are originated from by seeking flat minima, not from better in-domain generalizability. Last but not least, SWAD is readily adaptable to existing DG methods without modification; the combination of SWAD and an existing DG method further improves DG performances. Source code is available at https://github.com/khanrc/swad.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2021_bcb41ccd,
 author = {Cha, Junbum and Chun, Sanghyuk and Lee, Kyungjae and Cho, Han-Cheol and Park, Seunghyun and Lee, Yunsung and Park, Sungrae},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {22405--22418},
 publisher = {Curran Associates, Inc.},
 title = {SWAD: Domain Generalization by Seeking Flat Minima},
 url = {https://proceedings.neurips.cc/paper/2021/file/bcb41ccdc4363c6848a1d760f26c28a0-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
Neurips,2021,Minibatch and Momentum Model-based Methods for Stochastic Weakly Convex Optimization,https://proceedings.neurips.cc//paper/2021/hash/c2c701fe341a7756ca7fd4eaa83ff63f-Abstract.html,"['Qi Deng', ' Wenzhi Gao']",3,"Stochastic model-based methods have received increasing attention lately due to their appealing robustness to the stepsize selection and provable efficiency guarantee. We make two important extensions for improving model-based methods on stochastic weakly convex optimization. First, we propose new minibatch model- based methods by involving a set of samples to approximate the model function in each iteration. For the first time, we show that stochastic algorithms achieve linear speedup over the batch size even for non-smooth and non-convex (particularly, weakly convex) problems. To this end, we develop a novel sensitivity analysis of the proximal mapping involved in each algorithm iteration. Our analysis appears to be of independent interests in more general settings. Second, motivated by the success of momentum stochastic gradient descent, we propose a new stochastic extrapolated model-based method, greatly extending the classic Polyak momentum technique to a wider class of stochastic algorithms for weakly convex optimization. The rate of convergence to some natural stationarity condition is established over a fairly flexible range of extrapolation terms.While mainly focusing on weakly convex optimization, we also extend our work to convex optimization. We apply the minibatch and extrapolated model-based methods to stochastic convex optimization, for which we provide a new complexity bound and promising linear speedup in batch size. Moreover, an accelerated model-based method based on Nesterov‚Äôs momentum is presented, for which we establish an optimal complexity bound for reaching optimality.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2021_c2c701fe,
 author = {Deng, Qi and Gao, Wenzhi},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {23115--23127},
 publisher = {Curran Associates, Inc.},
 title = {Minibatch and Momentum Model-based Methods for Stochastic Weakly Convex Optimization},
 url = {https://proceedings.neurips.cc/paper/2021/file/c2c701fe341a7756ca7fd4eaa83ff63f-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
Neurips,2021,Sampling with Trusthworthy Constraints: A Variational Gradient Framework,https://proceedings.neurips.cc//paper/2021/hash/c61aed648da48aa3893fb3eaadd88a7f-Abstract.html,"['Xingchao Liu', ' Xin Tong', ' Qiang Liu']",4,"Sampling-based inference and learning techniques, especially Bayesian inference, provide an essential approach to handling uncertainty in machine learning (ML). As these techniques are increasingly used in daily life, it becomes essential to safeguard the ML systems with various trustworthy-related constraints, such as fairness, safety, interpretability. Mathematically, enforcing these constraints in probabilistic inference can be cast into sampling from intractable distributions subject to general nonlinear constraints, for which practical efficient algorithms are still largely missing. In this work, we propose a family of constrained sampling algorithms which generalize Langevin Dynamics (LD) and Stein Variational Gradient Descent (SVGD) to incorporate a moment constraint specified by a general nonlinear function. By exploiting the gradient flow structure of LD and SVGD, we derive two types of algorithms for handling constraints, including a primal-dual gradient approach and the constraint controlled gradient descent approach. We investigate the continuous-time mean-field limit of these algorithms and show that they have O(1/t) convergence under mild conditions. Moreover, the LD variant converges linearly assuming that a log Sobolev like inequality holds. Various numerical experiments are conducted to demonstrate the efficiency of our algorithms in trustworthy settings.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2021_c61aed64,
 author = {Liu, Xingchao and Tong, Xin and Liu, Qiang},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {23557--23568},
 publisher = {Curran Associates, Inc.},
 title = {Sampling  with Trusthworthy Constraints:  A Variational Gradient Framework   },
 url = {https://proceedings.neurips.cc/paper/2021/file/c61aed648da48aa3893fb3eaadd88a7f-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
Neurips,2021,Bias and variance of the Bayesian-mean decoder,https://proceedings.neurips.cc//paper/2021/hash/c7c3e78e3c9d26cc1158a8735d548eaa-Abstract.html,"['Arthur Prat-Carrabin', ' Michael Woodford']",3,"Perception, in theoretical neuroscience, has been modeled as the encoding of external stimuli into internal signals, which are then decoded. The Bayesian mean is an important decoder, as it is optimal for purposes of both estimation and discrimination. We present widely-applicable approximations to the bias and to the variance of the Bayesian mean, obtained under the minimal and biologically-relevant assumption that the encoding results from a series of independent, though not necessarily identically-distributed, signals. Simulations substantiate the accuracy of our approximations in the small-noise regime. The bias of the Bayesian mean comprises two components: one driven by the prior, and one driven by the precision of the encoding. If the encoding is 'efficient', the two components have opposite effects; their relative strengths are determined by the objective that the encoding optimizes. The experimental literature on perception reports both 'Bayesian' biases directed towards prior expectations, and opposite, 'anti-Bayesian' biases. We show that different tasks are indeed predicted to yield such contradictory biases, under a consistently-optimal encoding-decoding model. Moreover, we recover Wei and Stocker's ""law of human perception"", a relation between the bias of the Bayesian mean and the derivative of its variance, and show how the coefficient of proportionality in this law depends on the task at hand. Our results provide a parsimonious theory of optimal perception under constraints, in which encoding and decoding are adapted both to the prior and to the task faced by the observer.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2021_c7c3e78e,
 author = {Prat-Carrabin, Arthur and Woodford, Michael},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {23793--23805},
 publisher = {Curran Associates, Inc.},
 title = {Bias and variance of the Bayesian-mean decoder},
 url = {https://proceedings.neurips.cc/paper/2021/file/c7c3e78e3c9d26cc1158a8735d548eaa-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
Neurips,2021,Charting and Navigating the Space of Solutions for Recurrent Neural Networks,https://proceedings.neurips.cc//paper/2021/hash/d530d454337fb09964237fecb4bea6ce-Abstract.html,"['Elia Turner', ' Kabir V Dabholkar', ' Omri Barak']",3,"In recent years Recurrent Neural Networks (RNNs) were successfully used to model the way neural activity drives task-related behavior in animals, operating under the implicit assumption that the obtained solutions are universal. Observations in both neuroscience and machine learning challenge this assumption. Animals can approach a given task with a variety of strategies, and training machine learning algorithms introduces the phenomenon of underspecification. These observations imply that every task is associated with a space of solutions. To date, the structure of this space is not understood, limiting the approach of comparing RNNs with neural data.Here, we characterize the space of solutions associated with various tasks. We first study a simple two-neuron network on a task that leads to multiple solutions. We trace the nature of the final solution back to the network‚Äôs initial connectivity and identify discrete dynamical regimes that underlie this diversity. We then examine three neuroscience-inspired tasks: Delayed discrimination, Interval discrimination, and Time reproduction. For each task, we find a rich set of solutions. One layer of variability can be found directly in the neural activity of the networks. An additional layer is uncovered by testing the trained networks' ability to extrapolate, as a perturbation to a system often reveals hidden structure. Furthermore, we relate extrapolation patterns to specific dynamical objects and effective algorithms found by the networks. We introduce a tool to derive the reduced dynamics of networks by generating a compact directed graph describing the essence of the dynamics with regards to behavioral inputs and outputs. Using this representation, we can partition the solutions to each task into a handful of types and show that neural features can partially predict them.Taken together, our results shed light on the concept of the space of solutions and its uses both in Machine learning and in Neuroscience.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2021_d530d454,
 author = {Turner, Elia and Dabholkar, Kabir V and Barak, Omri},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {25320--25333},
 publisher = {Curran Associates, Inc.},
 title = {Charting and Navigating the Space of Solutions for Recurrent Neural Networks},
 url = {https://proceedings.neurips.cc/paper/2021/file/d530d454337fb09964237fecb4bea6ce-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
Neurips,2021,Fair Sortition Made Transparent,https://proceedings.neurips.cc//paper/2021/hash/d7b431b1a0cc5f032399870ff4710743-Abstract.html,"['Bailey Flanigan', ' Gregory Kehne', ' Ariel D. Procaccia']",3,"Sortition is an age-old democratic paradigm, widely manifested today through the random selection of citizens' assemblies. Recently-deployed algorithms select assemblies \textit{maximally fairly}, meaning that subject to demographic quotas, they give all potential participants as equal a chance as possible of being chosen. While these fairness gains can bolster the legitimacy of citizens' assemblies and facilitate their uptake, existing algorithms remain limited by their lack of transparency. To overcome this hurdle, in this work we focus on panel selection by uniform lottery, which is easy to realize in an observable way. By this approach, the final assembly is selected by uniformly sampling some pre-selected set of $m$ possible assemblies.We provide theoretical guarantees on the fairness attainable via this type of uniform lottery, as compared to the existing maximally fair but opaque algorithms, for two different fairness objectives. We complement these results with experiments on real-world instances that demonstrate the viability of the uniform lottery approach as a method of selecting assemblies both fairly and transparently.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2021_d7b431b1,
 author = {Flanigan, Bailey and Kehne, Gregory and Procaccia, Ariel D},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {25720--25731},
 publisher = {Curran Associates, Inc.},
 title = {Fair Sortition Made Transparent},
 url = {https://proceedings.neurips.cc/paper/2021/file/d7b431b1a0cc5f032399870ff4710743-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
Neurips,2021,Implicit Task-Driven Probability Discrepancy Measure for Unsupervised Domain Adaptation,https://proceedings.neurips.cc//paper/2021/hash/d82f9436247aa0049767b776dceab4ed-Abstract.html,"['Mao Li', ' Kaiqi Jiang', ' Xinhua Zhang']",0,"Probability discrepancy measure is a fundamental construct for numerous machine learning models such as weakly supervised learning and generative modeling. However, most measures overlook the fact that the distributions are not the end-product of learning, but are the basis of downstream predictor. Therefore it is important to warp the probability discrepancy measure towards the end tasks, and we hence propose a new bi-level optimization based approach so that the two distributions are compared not uniformly against the entire hypothesis space, but only with respect to the optimal predictor for the downstream end task. When applied to margin disparity discrepancy and contrastive domain discrepancy, our method significantly improves the performance in unsupervised domain adaptation, and enjoys a much more principled training process.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2021_d82f9436,
 author = {Li, Mao and Jiang, Kaiqi and Zhang, Xinhua},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {25824--25838},
 publisher = {Curran Associates, Inc.},
 title = {Implicit Task-Driven Probability Discrepancy Measure for Unsupervised Domain Adaptation},
 url = {https://proceedings.neurips.cc/paper/2021/file/d82f9436247aa0049767b776dceab4ed-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
Neurips,2021,Unintended Selection: Persistent Qualification Rate Disparities and Interventions,https://proceedings.neurips.cc//paper/2021/hash/db00f1b7fdf48fd26b5fb5f309e9afaf-Abstract.html,"['Reilly Raab', ' Yang Liu']",7,"Realistically---and equitably---modeling the dynamics of group-level disparities in machine learning remains an open problem. In particular, we desire models that do not suppose inherent differences between artificial groups of people---but rather endogenize disparities by appeal to unequal initial conditions of insular subpopulations. In this paper, agents each have a real-valued feature $X$ (e.g., credit score) informed by a ``true'' binary label $Y$ representing qualification (e.g., for a loan). Each agent alternately (1) receives a binary classification label $\hat{Y}$ (e.g., loan approval) from a Bayes-optimal machine learning classifier observing $X$ and (2) may update their qualification $Y$ by imitating successful strategies (e.g., seek a raise) within an isolated group $G$ of agents to which they belong. We consider the disparity of qualification rates $\Pr(Y=1)$ between different groups and how this disparity changes subject to a sequence of Bayes-optimal classifiers repeatedly retrained on the global population. We model the evolving qualification rates of each subpopulation (group) using the replicator equation, which derives from a class of imitation processes. We show that differences in qualification rates between subpopulations can persist indefinitely for a set of non-trivial equilibrium states due to uniformed classifier deployments, even when groups are identical in all aspects except initial qualification densities. We next simulate the effects of commonly proposed fairness interventions on this dynamical system along with a new feedback control mechanism capable of permanently eliminating group-level qualification rate disparities. We conclude by discussing the limitations of our model and findings and by outlining potential future work.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2021_db00f1b7,
 author = {Raab, Reilly and Liu, Yang},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {26053--26065},
 publisher = {Curran Associates, Inc.},
 title = {Unintended Selection: Persistent Qualification Rate Disparities and Interventions},
 url = {https://proceedings.neurips.cc/paper/2021/file/db00f1b7fdf48fd26b5fb5f309e9afaf-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
Neurips,2021,Fast Abductive Learning by Similarity-based Consistency Optimization,https://proceedings.neurips.cc//paper/2021/hash/df7e148cabfd9b608090fa5ee3348bfe-Abstract.html,"['Yu-Xuan Huang', ' Wang-Zhou Dai', ' Le-Wen Cai', ' Stephen H Muggleton', ' Yuan Jiang']",6,"To utilize the raw inputs and symbolic knowledge simultaneously, some recent neuro-symbolic learning methods use abduction, i.e., abductive reasoning, to integrate sub-symbolic perception and logical inference. While the perception model, e.g., a neural network, outputs some facts that are inconsistent with the symbolic background knowledge base, abduction can help revise the incorrect perceived facts by minimizing the inconsistency between them and the background knowledge. However, to enable effective abduction, previous approaches need an initialized perception model that discriminates the input raw instances. This limits the application of these methods, as the discrimination ability is usually acquired from a thorough pre-training when the raw inputs are difficult to classify. In this paper, we propose a novel abduction strategy, which leverages the similarity between samples, rather than the output information by the perceptual neural network, to guide the search in abduction. Based on this principle, we further present ABductive Learning with Similarity (ABLSim) and apply it to some difficult neuro-symbolic learning tasks. Experiments show that the efficiency of ABLSim is significantly higher than the state-of-the-art neuro-symbolic methods, allowing it to achieve better performance with less labeled data and weaker domain knowledge.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2021_df7e148c,
 author = {Huang, Yu-Xuan and Dai, Wang-Zhou and Cai, Le-Wen and Muggleton, Stephen H and Jiang, Yuan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {26574--26584},
 publisher = {Curran Associates, Inc.},
 title = {Fast Abductive Learning by Similarity-based Consistency Optimization},
 url = {https://proceedings.neurips.cc/paper/2021/file/df7e148cabfd9b608090fa5ee3348bfe-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
Neurips,2021,To Beam Or Not To Beam: That is a Question of Cooperation for Language GANs,https://proceedings.neurips.cc//paper/2021/hash/df9028fcb6b065e000ffe8a4f03eeb38-Abstract.html,"['Thomas Scialom', ' Paul-Alexis Dray', ' Jacopo Staiano', ' Sylvain Lamprier', ' Benjamin Piwowarski']",6,"Due to the discrete nature of words, language GANs require to be optimized from rewards provided by discriminator networks, via reinforcement learning methods. This is a much harder setting than for continuous tasks, which enjoy gradient flows from discriminators to generators, usually leading to dramatic learning instabilities. However, we claim that this can be solved by making discriminator and generator networks cooperate to produce output sequences during training. These cooperative outputs, inherently built to obtain higher discrimination scores, not only provide denser rewards for training but also form a more compact artificial set for discriminator training, hence improving its accuracy and stability.In this paper, we show that our SelfGAN framework, built on this cooperative principle, outperforms Teacher Forcing and obtains state-of-the-art results on two challenging tasks, Summarization and Question Generation.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2021_df9028fc,
 author = {Scialom, Thomas and Dray, Paul-Alexis and Staiano, Jacopo and Lamprier, Sylvain and Piwowarski, Benjamin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {26585--26597},
 publisher = {Curran Associates, Inc.},
 title = {To Beam Or Not To Beam: That is a Question of Cooperation for Language GANs},
 url = {https://proceedings.neurips.cc/paper/2021/file/df9028fcb6b065e000ffe8a4f03eeb38-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
Neurips,2021,Are Transformers more robust than CNNs?,https://proceedings.neurips.cc//paper/2021/hash/e19347e1c3ca0c0b97de5fb3b690855a-Abstract.html,"['Yutong Bai', ' Jieru Mei', ' Alan L. Yuille', ' Cihang Xie']",94,"Transformer emerges as a powerful tool for visual recognition. In addition to demonstrating competitive performance on a broad range of visual benchmarks, recent works also argue that Transformers are much more robust than Convolutions Neural Networks (CNNs). Nonetheless, surprisingly, we find these conclusions are drawn from unfair experimental settings, where Transformers and CNNs are compared at different scales and are applied with distinct training frameworks. In this paper, we aim to provide the first fair & in-depth comparisons between Transformers and CNNs, focusing on robustness evaluations. With our unified training setup, we first challenge the previous belief that Transformers outshine CNNs when measuring adversarial robustness. More surprisingly, we find CNNs can easily be as robust as Transformers on defending against adversarial attacks, if they properly adopt Transformers' training recipes. While regarding generalization on out-of-distribution samples, we show pre-training on (external) large-scale datasets is not a fundamental request for enabling Transformers to achieve better performance than CNNs. Moreover, our ablations suggest such stronger generalization is largely benefited by the Transformer's self-attention-like architectures per se, rather than by other training setups. We hope this work can help the community better understand and benchmark the robustness of Transformers and CNNs. The code and models are publicly available at: https://github.com/ytongbai/ViTs-vs-CNNs.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2021_e19347e1,
 author = {Bai, Yutong and Mei, Jieru and Yuille, Alan L and Xie, Cihang},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {26831--26843},
 publisher = {Curran Associates, Inc.},
 title = {Are Transformers more robust than CNNs? },
 url = {https://proceedings.neurips.cc/paper/2021/file/e19347e1c3ca0c0b97de5fb3b690855a-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
Neurips,2021,Passive attention in artificial neural networks predicts human visual selectivity,https://proceedings.neurips.cc//paper/2021/hash/e360367584297ee8d2d5afa709cd440e-Abstract.html,"['Thomas Langlois', ' Haicheng Zhao', ' Erin Grant', ' Ishita Dasgupta', ' Tom Griffiths', ' Nori Jacoby']",8,"Developments in machine learning interpretability techniques over the past decade have provided new tools to observe the image regions that are most informative for classification and localization in artificial neural networks (ANNs). Are the same regions similarly informative to human observers? Using data from 79 new experiments and 7,810 participants, we show that passive attention techniques reveal a significant overlap with human visual selectivity estimates derived from 6 distinct behavioral tasks including visual discrimination, spatial localization, recognizability, free-viewing, cued-object search, and saliency search fixations. We find that input visualizations derived from relatively simple ANN architectures probed using guided backpropagation methods are the best predictors of a shared component in the joint variability of the human measures. We validate these correlational results with causal manipulations using recognition experiments. We show that images masked with ANN attention maps were easier for humans to classify than control masks in a speeded recognition experiment. Similarly, we find that recognition performance in the same ANN models was likewise influenced by masking input images using human visual selectivity maps. This work contributes a new approach to evaluating the biological and psychological validity of leading ANNs as models of human vision: by examining their similarities and differences in terms of their visual selectivity to the information contained in images.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2021_e3603675,
 author = {Langlois, Thomas and Zhao, Haicheng and Grant, Erin and Dasgupta, Ishita and Griffiths, Tom and Jacoby, Nori},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {27094--27106},
 publisher = {Curran Associates, Inc.},
 title = {Passive attention in artificial neural networks predicts human visual selectivity},
 url = {https://proceedings.neurips.cc/paper/2021/file/e360367584297ee8d2d5afa709cd440e-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
Neurips,2021,Balanced Chamfer Distance as a Comprehensive Metric for Point Cloud Completion,https://proceedings.neurips.cc//paper/2021/hash/f3bd5ad57c8389a8a1a541a76be463bf-Abstract.html,"['Tong Wu', ' Liang Pan', ' Junzhe Zhang', ' Tai WANG', ' Ziwei Liu', ' Dahua Lin']",6,"Chamfer Distance (CD) and Earth Mover‚Äôs Distance (EMD) are two broadly adopted metrics for measuring the similarity between two point sets. However, CD is usually insensitive to mismatched local density, and EMD is usually dominated by global distribution while overlooks the fidelity of detailed structures. Besides, their unbounded value range induces a heavy influence from the outliers. These defects prevent them from providing a consistent evaluation. To tackle these problems, we propose a new similarity measure named Density-aware Chamfer Distance (DCD). It is derived from CD and benefits from several desirable properties: 1) it can detect disparity of density distributions and is thus a more intensive measure of similarity compared to CD; 2) it is stricter with detailed structures and significantly more computationally efficient than EMD; 3) the bounded value range encourages a more stable and reasonable evaluation over the whole test set. We adopt DCD to evaluate the point cloud completion task, where experimental results show that DCD pays attention to both the overall structure and local geometric details and provides a more reliable evaluation even when CD and EMD contradict each other. We can also use DCD as the training loss, which outperforms the same model trained with CD loss on all three metrics. In addition, we propose a novel point discriminator module that estimates the priority for another guided down-sampling step, and it achieves noticeable improvements under DCD together with competitive results for both CD and EMD. We hope our work could pave the way for a more comprehensive and practical point cloud similarity evaluation. Our code will be available at https://github.com/wutong16/DensityawareChamfer_Distance.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2021_f3bd5ad5,
 author = {Wu, Tong and Pan, Liang and Zhang, Junzhe and WANG, Tai and Liu, Ziwei and Lin, Dahua},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {29088--29100},
 publisher = {Curran Associates, Inc.},
 title = {Balanced Chamfer Distance as a Comprehensive Metric for Point Cloud Completion},
 url = {https://proceedings.neurips.cc/paper/2021/file/f3bd5ad57c8389a8a1a541a76be463bf-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
Neurips,2021,Are My Deep Learning Systems Fair? An Empirical Study of Fixed-Seed Training,https://proceedings.neurips.cc//paper/2021/hash/fdda6e957f1e5ee2f3b311fe4f145ae1-Abstract.html,"['Shangshu Qian', ' Viet Hung Pham', ' Thibaud Lutellier', ' Zeou Hu', ' Jungwon Kim', ' Lin Tan', ' Yaoliang Yu', ' Jiahao Chen', ' Sameena Shah']",13,"Deep learning (DL) systems have been gaining popularity in critical tasks such as credit evaluation and crime prediction. Such systems demand fairness. Recent work shows that DL software implementations introduce variance: identical DL training runs (i.e., identical network, data, configuration, software, and hardware) with a fixed seed produce different models. Such variance could make DL models and networks violate fairness compliance laws, resulting in negative social impact. In this paper, we conduct the first empirical study to quantify the impact of software implementation on the fairness and its variance of DL systems. Our study of 22 mitigation techniques and five baselines reveals up to 12.6% fairness variance across identical training runs with identical seeds. In addition, most debiasing algorithms have a negative impact on the model such as reducing model accuracy, increasing fairness variance, or increasing accuracy variance. Our literature survey shows that while fairness is gaining popularity in artificial intelligence (AI) related conferences, only 34.4% of the papers use multiple identical training runs to evaluate their approach, raising concerns about their results‚Äô validity. We call for better fairness evaluation and testing protocols to improve fairness and fairness variance of DL systems as well as DL research validity and reproducibility at large.",NOT,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2021_fdda6e95,
 author = {Qian, Shangshu and Pham, Viet Hung and Lutellier, Thibaud and Hu, Zeou and Kim, Jungwon and Tan, Lin and Yu, Yaoliang and Chen, Jiahao and Shah, Sameena},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {30211--30227},
 publisher = {Curran Associates, Inc.},
 title = {Are My Deep Learning Systems Fair? An Empirical Study of Fixed-Seed Training},
 url = {https://proceedings.neurips.cc/paper/2021/file/fdda6e957f1e5ee2f3b311fe4f145ae1-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
ICLR,2017,Central Moment Discrepancy (CMD) for Domain-Invariant Representation Learning,https://openreview.net/forum?id=SkB-_mcel,"['Werner Zellinger', 'Thomas Grubinger', 'Edwin Lughofer', 'Thomas Natschl√§ger', 'Susanne Saminger-Platz']",454,"The learning of domain-invariant representations in the context of domain adaptation with neural networks is considered. We propose a new regularization method that minimizes the domain-specific latent feature representations directly in the hidden activation space. Although some standard distribution matching approaches exist that can be interpreted as the matching of weighted sums of moments, e.g. Maximum Mean Discrepancy (MMD), an explicit order-wise matching of higher order moments has not been considered before.
 We propose to match the higher order central moments of probability distributions by means of order-wise moment differences. Our model does not require computationally expensive distance and kernel matrix computations. We utilize the equivalent representation of probability distributions by moment sequences to define a new distance function, called Central Moment Discrepancy (CMD). We prove that CMD is a metric on the set of probability distributions on a compact interval. We further prove that convergence of probability distributions on compact intervals w.r.t. the new metric implies convergence in distribution of the respective random variables.
 We test our approach on two different benchmark data sets for object recognition (Office) and sentiment analysis of product reviews (Amazon reviews). CMD achieves a new state-of-the-art performance on most domain adaptation tasks of Office and outperforms networks trained with MMD, Variational Fair Autoencoders and Domain Adversarial Neural Networks on Amazon reviews. In addition, a post-hoc parameter sensitivity analysis shows that the new approach is stable w. r. t. parameter changes in a certain interval. The source code of the experiments is publicly available.",NOT,,,,,,,,,,,,,,,,"@inproceedings{
zellinger2017central,
title={Central Moment Discrepancy ({CMD}) for Domain-Invariant Representation Learning},
author={Werner Zellinger and Thomas Grubinger and Edwin Lughofer and Thomas Natschl{\""a}ger and Susanne Saminger-Platz},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=SkB-_mcel}
}"
ICLR,2017,Adversarial Feature Learning,https://openreview.net/forum?id=BJtNZAFgg,"['Jeff Donahue', 'Philipp Kr√§henb√ºhl', 'Trevor Darrell']",556,"The ability of the Generative Adversarial Networks (GANs) framework to learn generative models mapping from simple latent distributions to arbitrarily complex data distributions has been demonstrated empirically, with compelling results showing generators learn to ""linearize semantics"" in the latent space of such models. Intuitively, such latent spaces may serve as useful feature representations for auxiliary problems where semantics are relevant. However, in their existing form, GANs have no means of learning the inverse mapping -- projecting data back into the latent space. We propose Bidirectional Generative Adversarial Networks (BiGANs) as a means of learning this inverse mapping, and demonstrate that the resulting learned feature representation is useful for auxiliary supervised discrimination tasks, competitive with contemporary approaches to unsupervised and self-supervised feature learning.",NOT,,,,,,,,,,,,,,,,"@inproceedings{
donahue2017adversarial,
title={Adversarial Feature Learning},
author={Jeff Donahue and Philipp Kr{\""a}henb{\""u}hl and Trevor Darrell},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=BJtNZAFgg}
}"
ICLR,2017,Mode Regularized Generative Adversarial Networks,https://openreview.net/forum?id=HJKkY35le,"['Tong Che', 'Yanran Li', 'Athul Jacob', 'Yoshua Bengio', 'Wenjie Li']",564,"Although Generative Adversarial Networks achieve state-of-the-art results on a
 variety of generative tasks, they are regarded as highly unstable and prone to miss
 modes. We argue that these bad behaviors of GANs are due to the very particular
 functional shape of the trained discriminators in high dimensional spaces, which
 can easily make training stuck or push probability mass in the wrong direction,
 towards that of higher concentration than that of the data generating distribution.
 We introduce several ways of regularizing the objective, which can dramatically
 stabilize the training of GAN models. We also show that our regularizers can help
 the fair distribution of probability mass across the modes of the data generating
 distribution during the early phases of training, thus providing a unified solution
 to the missing modes problem.",NOT,,,,,,,,,,,,,,,,"@inproceedings{
che2017mode,
title={Mode Regularized Generative Adversarial Networks},
author={Tong Che and Yanran Li and Athul Jacob and Yoshua Bengio and Wenjie Li},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=HJKkY35le}
}"
ICLR,2017,Multi-view Recurrent Neural Acoustic Word Embeddings,https://openreview.net/forum?id=rJxDkvqee,"['Wanjia He', 'Weiran Wang', 'Karen Livescu']",82,"Recent work has begun exploring neural acoustic word embeddings‚Äìfixed dimensional vector representations of arbitrary-length speech segments corresponding to words. Such embeddings are applicable to speech retrieval and recognition tasks, where reasoning about whole words may make it possible to avoid ambiguous sub-word representations. The main idea is to map acoustic sequences to fixed-dimensional vectors such that examples of the same word are mapped to similar vectors, while different-word examples are mapped to very different vectors. In this work we take a multi-view approach to learning acoustic word embeddings, in which we jointly learn to embed acoustic sequences and their corresponding character sequences. We use deep bidirectional LSTM embedding models and multi-view contrastive losses. We study the effect of different loss variants, including fixed-margin and cost-sensitive losses. Our acoustic word embeddings improve over previous approaches for the task of word discrimination. We also present results on other tasks that are enabled by the multi-view approach, including cross-view word discrimination and word similarity.",NOT,,,,,,,,,,,,,,,,"@inproceedings{
he2017multiview,
title={Multi-view Recurrent Neural Acoustic Word Embeddings},
author={Wanjia He and Weiran Wang and Karen Livescu},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=rJxDkvqee}
}"
ICLR,2018,Unsupervised Cipher Cracking Using Discrete GANs,https://openreview.net/forum?id=BkeqO7x0-,"['Aidan N. Gomez', 'Sicong Huang', 'Ivan Zhang', 'Bryan M. Li', 'Muhammad Osama', 'Lukasz Kaiser']",64,"This work details CipherGAN, an architecture inspired by CycleGAN used for inferring the underlying cipher mapping given banks of unpaired ciphertext and plaintext. We demonstrate that CipherGAN is capable of cracking language data enciphered using shift and Vigenere ciphers to a high degree of fidelity and for vocabularies much larger than previously achieved. We present how CycleGAN can be made compatible with discrete data and train in a stable way. We then prove that the technique used in CipherGAN avoids the common problem of uninformative discrimination associated with GANs applied to discrete data.",NOT,,,,,,,,,,,,,,,,"@inproceedings{
n.2018unsupervised,
title={Unsupervised Cipher Cracking Using Discrete {GAN}s},
author={Aidan N. Gomez and Sicong Huang and Ivan Zhang and Bryan M. Li and Muhammad Osama and Lukasz Kaiser},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=BkeqO7x0-},
}"
ICLR,2018,Do GANs learn the distribution? Some Theory and Empirics,https://openreview.net/forum?id=BJehNfW0-,"['Sanjeev Arora', 'Andrej Risteski', 'Yi Zhang']",130,"Do GANS (Generative Adversarial Nets) actually learn the target distribution? The foundational paper of Goodfellow et al. (2014) suggested they do, if they were given sufficiently large deep nets, sample size, and computation time. A recent theoretical analysis in Arora et al. (2017) raised doubts whether the same holds when discriminator has bounded size. It showed that the training objective can approach its optimum value even if the generated distribution has very low support. In other words, the training objective is unable to prevent mode collapse. The current paper makes two contributions. (1) It proposes a novel test for estimating support size using the birthday paradox of discrete probability. Using this evidence is presented that well-known GANs approaches do learn distributions of fairly low support. (2) It theoretically studies encoder-decoder GANs architectures (e.g., BiGAN/ALI), which were proposed to learn more meaningful features via GANs, and consequently to also solve the mode-collapse issue. Our result shows that such encoder-decoder training objectives also cannot guarantee learning of the full distribution because they cannot prevent serious mode collapse. More seriously, they cannot prevent learning meaningless codes for data, contrary to usual intuition.",NOT,,,,,,,,,,,,,,,,"@inproceedings{
arora2018do,
title={Do {GAN}s learn the distribution? Some Theory and Empirics},
author={Sanjeev Arora and Andrej Risteski and Yi Zhang},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=BJehNfW0-},
}"
ICLR,2018,Emergent Communication through Negotiation,https://openreview.net/forum?id=Hk6WhagRW,"['Kris Cao', 'Angeliki Lazaridou', 'Marc Lanctot', 'Joel Z Leibo', 'Karl Tuyls', 'Stephen Clark']",142,"Multi-agent reinforcement learning offers a way to study how communication could emerge in communities of agents needing to solve specific problems. In this paper, we study the emergence of communication in the negotiation environment, a semi-cooperative model of agent interaction. We introduce two communication protocols - one grounded in the semantics of the game, and one which is a priori ungrounded. We show that self-interested agents can use the pre-grounded communication channel to negotiate fairly, but are unable to effectively use the ungrounded, cheap talk channel to do the same. However, prosocial agents do learn to use cheap talk to find an optimal negotiating strategy, suggesting that cooperation is necessary for language to emerge. We also study communication behaviour in a setting where one agent interacts with agents in a community with different levels of prosociality and show how agent identifiability can aid negotiation.",NOT,,,,,,,,,,,,,,,,"@inproceedings{
cao2018emergent,
title={Emergent Communication through Negotiation},
author={Kris Cao and Angeliki Lazaridou and Marc Lanctot and Joel Z Leibo and Karl Tuyls and Stephen Clark},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=Hk6WhagRW},
}"
ICLR,2018,On the Discrimination-Generalization Tradeoff in GANs,https://openreview.net/forum?id=Hk9Xc_lR-,"['Pengchuan Zhang', 'Qiang Liu', 'Dengyong Zhou', 'Tao Xu', 'Xiaodong He']",100,"Generative adversarial training can be generally understood as minimizing certain moment matching loss defined by a set of discriminator functions, typically neural networks. The discriminator set should be large enough to be able to uniquely identify the true distribution (discriminative), and also be small enough to go beyond memorizing samples (generalizable). In this paper, we show that a discriminator set is guaranteed to be discriminative whenever its linear span is dense in the set of bounded continuous functions. This is a very mild condition satisfied even by neural networks with a single neuron. Further, we develop generalization bounds between the learned distribution and true distribution under different evaluation metrics. When evaluated with neural distance, our bounds show that generalization is guaranteed as long as the discriminator set is small enough, regardless of the size of the generator or hypothesis set. When evaluated with KL divergence, our bound provides an explanation on the counter-intuitive behaviors of testing likelihood in GAN training. Our analysis sheds lights on understanding the practical performance of GANs.",NOT,,,,,,,,,,,,,,,,"@inproceedings{
zhang2018on,
title={On the Discrimination-Generalization Tradeoff in {GAN}s},
author={Pengchuan Zhang and Qiang Liu and Dengyong Zhou and Tao Xu and Xiaodong He},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=Hk9Xc_lR-},
}"
ICLR,2019,Robustness May Be at Odds with Accuracy,https://openreview.net/forum?id=SyxAb30cY7,"['Dimitris Tsipras', 'Shibani Santurkar', 'Logan Engstrom', 'Alexander Turner', 'Aleksander Madry']",1211,"We show that there exists an inherent tension between the goal of adversarial robustness and that of standard generalization. 
 Specifically, training robust models may not only be more resource-consuming, but also lead to a reduction of standard accuracy. We demonstrate that this trade-off between the standard accuracy of a model and its robustness to adversarial perturbations provably exists even in a fairly simple and natural setting. These findings also corroborate a similar phenomenon observed in practice. Further, we argue that this phenomenon is a consequence of robust classifiers learning fundamentally different feature representations than standard classifiers. These differences, in particular, seem to result in unexpected benefits: the features learned by robust models tend to align better with salient data characteristics and human perception.",NOT,,,,,,,,,,,,,,,,"@inproceedings{
tsipras2018robustness,
title={Robustness May Be at Odds with Accuracy},
author={Dimitris Tsipras and Shibani Santurkar and Logan Engstrom and Alexander Turner and Aleksander Madry},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=SyxAb30cY7},
}"
ICLR,2019,L-Shapley and C-Shapley: Efficient Model Interpretation for Structured Data,https://openreview.net/forum?id=S1E3Ko09F7,"['Jianbo Chen', 'Le Song', 'Martin J. Wainwright', 'Michael I. Jordan']",150,"Instancewise feature scoring is a method for model interpretation, which yields, for each test instance, a vector of importance scores associated with features. Methods based on the Shapley score have been proposed as a fair way of computing feature attributions, but incur an exponential complexity in the number of features. This combinatorial explosion arises from the definition of Shapley value and prevents these methods from being scalable to large data sets and complex models. We focus on settings in which the data have a graph structure, and the contribution of features to the target variable is well-approximated by a graph-structured factorization. In such settings, we develop two algorithms with linear complexity for instancewise feature importance scoring on black-box models. We establish the relationship of our methods to the Shapley value and a closely related concept known as the Myerson value from cooperative game theory. We demonstrate on both language and image data that our algorithms compare favorably with other methods using both quantitative metrics and human evaluation.",NOT,,,,,,,,,,,,,,,,"@inproceedings{
chen2018lshapley,
title={L-Shapley and C-Shapley: Efficient Model Interpretation for Structured Data},
author={Jianbo Chen and Le Song and Martin J. Wainwright and Michael I. Jordan},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=S1E3Ko09F7},
}"
ICLR,2019,DeepOBS: A Deep Learning Optimizer Benchmark Suite,https://openreview.net/forum?id=rJg6ssC5Y7,"['Frank Schneider', 'Lukas Balles', 'Philipp Hennig']",39,"Because the choice and tuning of the optimizer affects the speed, and ultimately the performance of deep learning, there is significant past and recent research in this area. Yet, perhaps surprisingly, there is no generally agreed-upon protocol for the quantitative and reproducible evaluation of optimization strategies for deep learning. We suggest routines and benchmarks for stochastic optimization, with special focus on the unique aspects of deep learning, such as stochasticity, tunability and generalization. As the primary contribution, we present DeepOBS, a Python package of deep learning optimization benchmarks. The package addresses key challenges in the quantitative assessment of stochastic optimizers, and automates most steps of benchmarking. The library includes a wide and extensible set of ready-to-use realistic optimization problems, such as training Residual Networks for image classification on ImageNet or character-level language prediction models, as well as popular classics like MNIST and CIFAR-10. The package also provides realistic baseline results for the most popular optimizers on these test problems, ensuring a fair comparison to the competition when benchmarking new optimizers, and without having to run costly experiments. It comes with output back-ends that directly produce LaTeX code for inclusion in academic publications. It supports TensorFlow and is available open source.",NOT,,,,,,,,,,,,,,,,"@inproceedings{
schneider2018deepobs,
title={Deep{OBS}: A Deep Learning Optimizer Benchmark Suite},
author={Frank Schneider and Lukas Balles and Philipp Hennig},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=rJg6ssC5Y7},
}"
ICLR,2019,A Closer Look at Few-shot Classification,https://openreview.net/forum?id=HkxLXnAcFQ,"['Wei-Yu Chen', 'Yen-Cheng Liu', 'Zsolt Kira', 'Yu-Chiang Frank Wang', 'Jia-Bin Huang']",49,"Few-shot classiÔ¨Åcation aims to learn a classiÔ¨Åer to recognize unseen classes during training with limited labeled examples. While signiÔ¨Åcant progress has been made, the growing complexity of network designs, meta-learning algorithms, and differences in implementation details make a fair comparison difÔ¨Åcult. In this paper, we present 1) a consistent comparative analysis of several representative few-shot classiÔ¨Åcation algorithms, with results showing that deeper backbones signiÔ¨Åcantly reduce the gap across methods including the baseline, 2) a slightly modiÔ¨Åed baseline method that surprisingly achieves competitive performance when compared with the state-of-the-art on both the mini-ImageNet and the CUB datasets, and 3) a new experimental setting for evaluating the cross-domain generalization ability for few-shot classiÔ¨Åcation algorithms. Our results reveal that reducing intra-class variation is an important factor when the feature backbone is shallow, but not as critical when using deeper backbones. In a realistic, cross-domain evaluation setting, we show that a baseline method with a standard Ô¨Åne-tuning practice compares favorably against other state-of-the-art few-shot learning algorithms.",NOT,,,,,,,,,,,,,,,,"@inproceedings{
chen2018a,
title={A Closer Look at Few-shot Classification},
author={Wei-Yu Chen and Yen-Cheng Liu and Zsolt Kira and Yu-Chiang Frank Wang and Jia-Bin Huang},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=HkxLXnAcFQ},
}"
ICLR,2020,Data-dependent Gaussian Prior Objective for Language Generation,https://openreview.net/forum?id=S1efxTVYDr,"['Zuchao Li', 'Rui Wang', 'Kehai Chen', 'Masso Utiyama', 'Eiichiro Sumita', 'Zhuosheng Zhang', 'Hai Zhao']",41,"For typical sequence prediction problems like language generation, maximum likelihood estimation (MLE) has been commonly adopted as it encourages the predicted sequence most consistent with the ground-truth sequence to have the highest probability of occurring. However, MLE focuses on a once-for-all matching between the predicted sequence and gold-standard consequently, treating all incorrect predictions as being equally incorrect. We call such a drawback {\it negative diversity ignorance} in this paper. Treating all incorrect predictions as equal unfairly downplays the nuance of these sequences' detailed token-wise structure. To counteract this, we augment the MLE loss by introducing an extra KL divergence term which is derived from comparing a data-dependent Gaussian prior and the detailed training prediction. The proposed data-dependent Gaussian prior objective (D2GPo) is defined over a prior topological order of tokens, poles apart from the data-independent Gaussian prior (L2 regularization) commonly adopted for smoothing the training of MLE. Experimental results show that the proposed method can effectively make use of more detailed prior in the data and significantly improve the performance of typical language generation tasks, including supervised and unsupervised machine translation, text summarization, storytelling, and image caption.",NOT,,,,,,,,,,,,,,,,"@inproceedings{
Li2020Data-dependent,
title={Data-dependent Gaussian Prior Objective for Language Generation},
author={Zuchao Li and Rui Wang and Kehai Chen and Masso Utiyama and Eiichiro Sumita and Zhuosheng Zhang and Hai Zhao},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=S1efxTVYDr}
}"
ICLR,2020,Self-Adversarial Learning with Comparative Discrimination for Text Generation,https://openreview.net/forum?id=B1l8L6EtDS,"['Wangchunshu Zhou', 'Tao Ge', 'Ke Xu', 'Furu Wei', 'Ming Zhou']",18,"Conventional Generative Adversarial Networks (GANs) for text generation tend to have issues of reward sparsity and mode collapse that affect the quality and diversity of generated samples. To address the issues, we propose a novel self-adversarial learning (SAL) paradigm for improving GANs' performance in text generation. In contrast to standard GANs that use a binary classifier as its discriminator to predict whether a sample is real or generated, SAL employs a comparative discriminator which is a pairwise classifier for comparing the text quality between a pair of samples. During training, SAL rewards the generator when its currently generated sentence is found to be better than its previously generated samples. This self-improvement reward mechanism allows the model to receive credits more easily and avoid collapsing towards the limited number of real samples, which not only helps alleviate the reward sparsity issue but also reduces the risk of mode collapse. Experiments on text generation benchmark datasets show that our proposed approach substantially improves both the quality and the diversity, and yields more stable performance compared to the previous GANs for text generation.",NOT,,,,,,,,,,,,,,,,"@inproceedings{
Zhou2020Self-Adversarial,
title={Self-Adversarial Learning with Comparative Discrimination for Text Generation},
author={Wangchunshu Zhou and Tao Ge and Ke Xu and Furu Wei and Ming Zhou},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=B1l8L6EtDS}
}"
ICLR,2020,Theory and Evaluation Metrics for Learning Disentangled Representations,https://openreview.net/forum?id=HJgK0h4Ywr,"['Kien Do', 'Truyen Tran']",57,"We make two theoretical contributions to disentanglement learning by (a) defining precise semantics of disentangled representations, and (b) establishing robust metrics for evaluation. First, we characterize the concept ‚Äúdisentangled representations‚Äù used in supervised and unsupervised methods along three dimensions‚Äìinformativeness, separability and interpretability‚Äìwhich can be expressed and quantified explicitly using information-theoretic constructs. This helps explain the behaviors of several well-known disentanglement learning models. We then propose robust metrics for measuring informativeness, separability and interpretability. Through a comprehensive suite of experiments, we show that our metrics correctly characterize the representations learned by different methods and are consistent with qualitative (visual) results. Thus, the metrics allow disentanglement learning methods to be compared on a fair ground. We also empirically uncovered new interesting properties of VAE-based methods and interpreted them with our formulation. These findings are promising and hopefully will encourage the design of more theoretically driven models for learning disentangled representations.",NOT,,,,,,,,,,,,,,,,"@inproceedings{
Do2020Theory,
title={Theory and Evaluation Metrics for Learning Disentangled Representations},
author={Kien Do and Truyen Tran},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=HJgK0h4Ywr}
}"
ICLR,2020,Unsupervised Model Selection for Variational Disentangled Representation Learning,https://openreview.net/forum?id=SyxL2TNtvr,"['Sunny Duan', 'Nicholas Watters', 'Loic Matthey', 'Christopher P. Burgess', 'Alexander Lerchner', 'Irina Higgins']",46,"Disentangled representations have recently been shown to improve fairness, data efficiency and generalisation in simple supervised and reinforcement learning tasks. To extend the benefits of disentangled representations to more complex domains and practical applications, it is important to enable hyperparameter tuning and model selection of existing unsupervised approaches without requiring access to ground truth attribute labels, which are not available for most datasets. This paper addresses this problem by introducing a simple yet robust and reliable method for unsupervised disentangled model selection. We show that our approach performs comparably to the existing supervised alternatives across 5400 models from six state of the art unsupervised disentangled representation learning model classes. Furthermore, we show that the ranking produced by our approach correlates well with the final task performance on two different domains.",NOT,,,,,,,,,,,,,,,,"@inproceedings{
Duan2020Unsupervised,
title={Unsupervised Model Selection for Variational Disentangled Representation Learning},
author={Sunny Duan and Loic Matthey and Andre Saraiva and Nick Watters and Chris Burgess and Alexander Lerchner and Irina Higgins},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=SyxL2TNtvr}
}"
ICLR,2020,Fair Resource Allocation in Federated Learning,https://openreview.net/forum?id=ByexElSYDr,"['Tian Li', 'Maziar Sanjabi', 'Ahmad Beirami', 'Virginia Smith']",460,"Federated learning involves training statistical models in massive, heterogeneous networks. Naively minimizing an aggregate loss function in such a network may disproportionately advantage or disadvantage some of the devices. In this work, we propose q-Fair Federated Learning (q-FFL), a novel optimization objective inspired by fair resource allocation in wireless networks that encourages a more fair (i.e., more uniform) accuracy distribution across devices in federated networks. To solve q-FFL, we devise a communication-efficient method, q-FedAvg, that is suited to federated networks. We validate both the effectiveness of q-FFL and the efficiency of q-FedAvg on a suite of federated datasets with both convex and non-convex models, and show that q-FFL (along with q-FedAvg) outperforms existing baselines in terms of the resulting fairness, flexibility, and efficiency.",NOT,,,,,,,,,,,,,,,,"@inproceedings{
Li2020Fair,
title={Fair Resource Allocation in Federated Learning},
author={Tian Li and Maziar Sanjabi and Ahmad Beirami and Virginia Smith},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=ByexElSYDr}
}"
ICLR,2021,The Risks of Invariant Risk Minimization,https://openreview.net/forum?id=BbNIbVPJ-42,"['Elan Rosenfeld', ' Pradeep Kumar Ravikumar', ' Andrej Risteski']",151,"Invariant Causal Prediction (Peters et al., 2016) is a technique for out-of-distribution generalization which assumes that some aspects of the data distribution vary across the training set but that the underlying causal mechanisms remain constant. Recently, Arjovsky et al. (2019) proposed Invariant Risk Minimization (IRM), an objective based on this idea for learning deep, invariant features of data which are a complex function of latent variables; many alternatives have subsequently been suggested. However, formal guarantees for all of these works are severely lacking. In this paper, we present the first analysis of classification under the IRM objective‚Äîas well as these recently proposed alternatives‚Äîunder a fairly natural and general model. In the linear case, we show simple conditions under which the optimal solution succeeds or, more often, fails to recover the optimal invariant predictor. We furthermore present the very first results in the non-linear regime: we demonstrate that IRM can fail catastrophically unless the test data is sufficiently similar to the training distribution‚Äîthis is precisely the issue that it was intended to solve. Thus, in this setting we find that IRM and its alternatives fundamentally do not improve over standard Empirical Risk Minimization.",NOT,,,,,,,,,,,,,,,,"@inproceedings{
rosenfeld2021the,
title={The Risks of Invariant Risk Minimization},
author={Elan Rosenfeld and Pradeep Kumar Ravikumar and Andrej Risteski},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=BbNIbVPJ-42}
}"
ICLR,2021,Prototypical Contrastive Learning of Unsupervised Representations,https://openreview.net/forum?id=KmykpuSrjcq,"['Junnan Li', ' Pan Zhou', ' Caiming Xiong', ' Steven Hoi']",449,"This paper presents Prototypical Contrastive Learning (PCL), an unsupervised representation learning method that bridges contrastive learning with clustering. PCL not only learns low-level features for the task of instance discrimination, but more importantly, it implicitly encodes semantic structures of the data into the learned embedding space. Specifically, we introduce prototypes as latent variables to help find the maximum-likelihood estimation of the network parameters in an Expectation-Maximization framework. We iteratively perform E-step as finding the distribution of prototypes via clustering and M-step as optimizing the network via contrastive learning. We propose ProtoNCE loss, a generalized version of the InfoNCE loss for contrastive learning, which encourages representations to be closer to their assigned prototypes. PCL outperforms state-of-the-art instance-wise contrastive learning methods on multiple benchmarks with substantial improvement in low-resource transfer learning. Code and pretrained models are available at https://github.com/salesforce/PCL.",NOT,,,,,,,,,,,,,,,,"@inproceedings{
li2021prototypical,
title={Prototypical Contrastive Learning of Unsupervised Representations},
author={Junnan Li and Pan Zhou and Caiming Xiong and Steven Hoi},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=KmykpuSrjcq}
}"
ICLR,2021,CaPC Learning: Confidential and Private Collaborative Learning,https://openreview.net/forum?id=h2EbJ4_wMVq,"['Christopher A. Choquette-Choo', ' Natalie Dullerud', ' Adam Dziedzic', ' Yunxiang Zhang', ' Somesh Jha', ' Nicolas Papernot', ' Xiao Wang']",29,"Machine learning benefits from large training datasets, which may not always be possible to collect by any single entity, especially when using privacy-sensitive data. In many contexts, such as healthcare and finance, separate parties may wish to collaborate and learn from each other's data but are prevented from doing so due to privacy regulations. Some regulations prevent explicit sharing of data between parties by joining datasets in a central location (confidentiality). Others also limit implicit sharing of data, e.g., through model predictions (privacy). There is currently no method that enables machine learning in such a setting, where both confidentiality and privacy need to be preserved, to prevent both explicit and implicit sharing of data. Federated learning only provides confidentiality, not privacy, since gradients shared still contain private information. Differentially private learning assumes unreasonably large datasets. Furthermore, both of these learning paradigms produce a central model whose architecture was previously agreed upon by all parties rather than enabling collaborative learning where each party learns and improves their own local model. We introduce Confidential and Private Collaborative (CaPC) learning, the first method provably achieving both confidentiality and privacy in a collaborative setting. We leverage secure multi-party computation (MPC), homomorphic encryption (HE), and other techniques in combination with privately aggregated teacher models. We demonstrate how CaPC allows participants to collaborate without having to explicitly join their training sets or train a central model. Each party is able to improve the accuracy and fairness of their model, even in settings where each party has a model that performs well on their own dataset or when datasets are not IID and model architectures are heterogeneous across parties.",NOT,,,,,,,,,,,,,,,,"@inproceedings{
choquette-choo2021capc,
title={Ca{\{}PC{\}} Learning: Confidential and Private Collaborative Learning},
author={Christopher A. Choquette-Choo and Natalie Dullerud and Adam Dziedzic and Yunxiang Zhang and Somesh Jha and Nicolas Papernot and Xiao Wang},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=h2EbJ4_wMVq}
}"
ICLR,2021,Fuzzy Tiling Activations: A Simple Approach to Learning Sparse Representations Online,https://openreview.net/forum?id=zElset1Klrp,"['Yangchen Pan', ' Kirby Banman', ' Martha White']",5,"Recent work has shown that sparse representations---where only a small percentage of units are active---can significantly reduce interference. Those works, however, relied on relatively complex regularization or meta-learning approaches, that have only been used offline in a pre-training phase. In this work, we pursue a direction that achieves sparsity by design, rather than by learning. Specifically, we design an activation function that produces sparse representations deterministically by construction, and so is more amenable to online training. The idea relies on the simple approach of binning, but overcomes the two key limitations of binning: zero gradients for the flat regions almost everywhere, and lost precision---reduced discrimination---due to coarse aggregation. We introduce a Fuzzy Tiling Activation (FTA) that provides non-negligible gradients and produces overlap between bins that improves discrimination. We first show that FTA is robust under covariate shift in a synthetic online supervised learning problem, where we can vary the level of correlation and drift. Then we move to the deep reinforcement learning setting and investigate both value-based and policy gradient algorithms that use neural networks with FTAs, in classic discrete control and Mujoco continuous control environments. We show that algorithms equipped with FTAs are able to learn a stable policy faster without needing target networks on most domains.",NOT,,,,,,,,,,,,,,,,"@inproceedings{
pan2021fuzzy,
title={Fuzzy Tiling Activations: A Simple Approach to Learning Sparse Representations Online},
author={Yangchen Pan and Kirby Banman and Martha White},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=zElset1Klrp}
}"
ICLR,2021,Clustering-friendly Representation Learning via Instance Discrimination and Feature Decorrelation,https://openreview.net/forum?id=e12NDM7wkEY,"['Yaling Tao', ' Kentaro Takagi', ' Kouta Nakata']",42,"Clustering is one of the most fundamental tasks in machine learning. Recently, deep clustering has become a major trend in clustering techniques. Representation learning often plays an important role in the effectiveness of deep clustering, and thus can be a principal cause of performance degradation. In this paper, we propose a clustering-friendly representation learning method using instance discrimination and feature decorrelation. Our deep-learning-based representation learning method is motivated by the properties of classical spectral clustering. Instance discrimination learns similarities among data and feature decorrelation removes redundant correlation among features. We utilize an instance discrimination method in which learning individual instance classes leads to learning similarity among instances. Through detailed experiments and examination, we show that the approach can be adapted to learning a latent space for clustering. We design novel softmax-formulated decorrelation constraints for learning. In evaluations of image clustering using CIFAR-10 and ImageNet-10, our method achieves accuracy of 81.5% and 95.4%, respectively. We also show that the softmax-formulated constraints are compatible with various neural networks.",NOT,,,,,,,,,,,,,,,,"@inproceedings{
tao2021clusteringfriendly,
title={Clustering-friendly Representation Learning via Instance Discrimination and Feature Decorrelation},
author={Yaling Tao and Kentaro Takagi and Kouta Nakata},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=e12NDM7wkEY}
}"
ICLR,2021,In Search of Lost Domain Generalization,https://openreview.net/forum?id=lQdXeXDoWtI,"['Ishaan Gulrajani', ' David Lopez-Paz']",459,"The goal of domain generalization algorithms is to predict well on distributions different from those seen during training.
 While a myriad of domain generalization algorithms exist, inconsistencies in experimental conditions---datasets, network architectures, and model selection criteria---render fair comparisons difficult.
 The goal of this paper is to understand how useful domain generalization algorithms are in realistic settings.
 As a first step, we realize that model selection is non-trivial for domain generalization tasks, and we argue that algorithms without a model selection criterion remain incomplete.
 Next we implement DomainBed, a testbed for domain generalization including seven benchmarks, fourteen algorithms, and three model selection criteria.
 When conducting extensive experiments using DomainBed we find that when carefully implemented and tuned, ERM outperforms the state-of-the-art in terms of average performance.
 Furthermore, no algorithm included in DomainBed outperforms ERM by more than one point when evaluated under the same experimental conditions.
 We hope that the release of DomainBed, alongside contributions from fellow researchers, will streamline reproducible and rigorous advances in domain generalization.",NOT,,,,,,,,,,,,,,,,"@inproceedings{
gulrajani2021in,
title={In Search of Lost Domain Generalization},
author={Ishaan Gulrajani and David Lopez-Paz},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=lQdXeXDoWtI}
}"
ICLR,2021,What Makes Instance Discrimination Good for Transfer Learning?,https://openreview.net/forum?id=tC6iW2UUbJf,"['Nanxuan Zhao', ' Zhirong Wu', ' Rynson W. H. Lau', ' Stephen Lin']",115,"Contrastive visual pretraining based on the instance discrimination pretext task has made significant progress. Notably, recent work on unsupervised pretraining has shown to surpass the supervised counterpart for finetuning downstream applications such as object detection and segmentation. It comes as a surprise that image annotations would be better left unused for transfer learning. In this work, we investigate the following problems: What makes instance discrimination pretraining good for transfer learning? What knowledge is actually learned and transferred from these models? From this understanding of instance discrimination, how can we better exploit human annotation labels for pretraining? Our findings are threefold. First, what truly matters for the transfer is low-level and mid-level representations, not high-level representations. Second, the intra-category invariance enforced by the traditional supervised model weakens transferability by increasing task misalignment. Finally, supervised pretraining can be strengthened by following an exemplar-based approach without explicit constraints among the instances within the same category.",NOT,,,,,,,,,,,,,,,,"@inproceedings{
zhao2021what,
title={What Makes Instance Discrimination Good for Transfer Learning?},
author={Nanxuan Zhao and Zhirong Wu and Rynson W. H. Lau and Stephen Lin},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=tC6iW2UUbJf}
}"
ICLR,2021,Scaling Symbolic Methods using Gradients for Neural Model Explanation,https://openreview.net/forum?id=V5j-jdoDDP,"['Subham Sekhar Sahoo', ' Subhashini Venugopalan', ' Li Li', ' Rishabh Singh', ' Patrick Riley']",5,"Symbolic techniques based on Satisfiability Modulo Theory (SMT) solvers have been proposed for analyzing and verifying neural network properties, but their usage has been fairly limited owing to their poor scalability with larger networks. In this work, we propose a technique for combining gradient-based methods with symbolic techniques to scale such analyses and demonstrate its application for model explanation. In particular, we apply this technique to identify minimal regions in an input that are most relevant for a neural network's prediction. Our approach uses gradient information (based on Integrated Gradients) to focus on a subset of neurons in the first layer, which allows our technique to scale to large networks. The corresponding SMT constraints encode the minimal input mask discovery problem such that after masking the input, the activations of the selected neurons are still above a threshold. After solving for the minimal masks, our approach scores the mask regions to generate a relative ordering of the features within the mask. This produces a saliency map which explains"" where a model is looking"" when making a prediction. We evaluate our technique on three datasets-MNIST, ImageNet, and Beer Reviews, and demonstrate both quantitatively and qualitatively that the regions generated by our approach are sparser and achieve higher saliency scores compared to the gradient-based methods alone. Code and examples are at - https://github.com/google-research/google-research/tree/master/smug_saliency",NOT,,,,,,,,,,,,,,,,"@inproceedings{
sahoo2021scaling,
title={Scaling Symbolic Methods using Gradients for Neural Model Explanation},
author={Subham Sekhar Sahoo and Subhashini Venugopalan and Li Li and Rishabh Singh and Patrick Riley},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=V5j-jdoDDP}
}"
ICLR,2021,Influence Functions in Deep Learning Are Fragile,https://openreview.net/forum?id=xHKVVHGDOEk,"['Samyadeep Basu', ' Phil Pope', ' Soheil Feizi']",77,"Influence functions approximate the effect of training samples in test-time predictions and have a wide variety of applications in machine learning interpretability and uncertainty estimation. A commonly-used (first-order) influence function can be implemented efficiently as a post-hoc method requiring access only to the gradients and Hessian of the model. For linear models, influence functions are well-defined due to the convexity of the underlying loss function and are generally accurate even across difficult settings where model changes are fairly large such as estimating group influences. Influence functions, however, are not well-understood in the context of deep learning with non-convex loss functions. In this paper, we provide a comprehensive and large-scale empirical study of successes and failures of influence functions in neural network models trained on datasets such as Iris, MNIST, CIFAR-10 and ImageNet. Through our extensive experiments, we show that the network architecture, its depth and width, as well as the extent of model parameterization and regularization techniques have strong effects in the accuracy of influence functions. In particular, we find that (i) influence estimates are fairly accurate for shallow networks, while for deeper networks the estimates are often erroneous; (ii) for certain network architectures and datasets, training with weight-decay regularization is important to get high-quality influence estimates; and (iii) the accuracy of influence estimates can vary significantly depending on the examined test points. These results suggest that in general influence functions in deep learning are fragile and call for developing improved influence estimation methods to mitigate these issues in non-convex setups.",NOT,,,,,,,,,,,,,,,,"@inproceedings{
basu2021influence,
title={Influence Functions in Deep Learning Are Fragile},
author={Samyadeep Basu and Phil Pope and Soheil Feizi},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=xHKVVHGDOEk}
}"
ICLR,2021,CO2: Consistent Contrast for Unsupervised Visual Representation Learning,https://openreview.net/forum?id=U4XLJhqwNF1,"['Chen Wei', ' Huiyu Wang', ' Wei Shen', ' Alan Yuille']",35,"Contrastive learning has recently been a core for unsupervised visual representation learning. Without human annotation, the common practice is to perform an instance discrimination task: Given a query image crop, label crops from the same image as positives, and crops from other randomly sampled images as negatives. An important limitation of this label assignment is that it can not reflect the heterogeneous similarity of the query crop to crops from other images, but regarding them as equally negative. To address this issue, inspired by consistency regularization in semi-supervised learning, we propose Consistent Contrast (CO2), which introduces a consistency term into unsupervised contrastive learning framework. The consistency term takes the similarity of the query crop to crops from other images as unlabeled, and the corresponding similarity of a positive crop as a pseudo label. It then encourages consistency between these two similarities. Empirically, CO2 improves Momentum Contrast (MoCo) by 2.9% top-1 accuracy on ImageNet linear protocol, 3.8% and 1.1% top-5 accuracy on 1% and 10% labeled semi-supervised settings. It also transfers to image classification, object detection, and semantic segmentation on PASCAL VOC. This shows that CO2 learns better visual representations for downstream tasks.",NOT,,,,,,,,,,,,,,,,"@inproceedings{
wei2021co,
title={{\{}CO{\}}2: Consistent Contrast for Unsupervised Visual Representation Learning},
author={Chen Wei and Huiyu Wang and Wei Shen and Alan Yuille},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=U4XLJhqwNF1}
}"
ICLR,2021,DARTS-: Robustly Stepping out of Performance Collapse Without Indicators,https://openreview.net/forum?id=KLH36ELmwIB,"['Xiangxiang Chu', ' Xiaoxing Wang', ' Bo Zhang', ' Shun Lu', ' Xiaolin Wei', ' Junchi Yan']",83,"Despite the fast development of differentiable architecture search (DARTS), it suffers from a standing instability issue regarding searching performance, which extremely limits its application. Existing robustifying methods draw clues from the outcome instead of finding out the causing factor. Various indicators such as Hessian eigenvalues are proposed as a signal of performance collapse, and the searching should be stopped once an indicator reaches a preset threshold.
 However, these methods tend to easily reject good architectures if thresholds are inappropriately set, let alone the searching is intrinsically noisy. In this paper, we undertake a more subtle and direct approach to resolve the collapse. 
 We first demonstrate that skip connections with a learnable architectural coefficient can easily recover from a disadvantageous state and become dominant. We conjecture that skip connections profit too much from this privilege, hence causing the collapse for the derived model. Therefore, we propose to factor out this benefit with an auxiliary skip connection, ensuring a fairer competition for all operations. Extensive experiments on various datasets verify that our approach can substantially improve the robustness of DARTS. Our code is available at https://github.com/Meituan-AutoML/DARTS-",NOT,,,,,,,,,,,,,,,,"@inproceedings{
chu2021darts,
title={{\{}DARTS{\}}-: Robustly Stepping out of Performance Collapse Without Indicators},
author={Xiangxiang Chu and Xiaoxing Wang and Bo Zhang and Shun Lu and Xiaolin Wei and Junchi Yan},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=KLH36ELmwIB}
}"
ICLR,2022,A Tale of Two Flows: Cooperative Learning of Langevin Flow and Normalizing Flow Toward Energy-Based Model,https://openreview.net/forum?id=31d5RLCUuXC,"['Jianwen Xie', ' Yaxuan Zhu', ' Jun Li', ' Ping Li']",4,"This paper studies the cooperative learning of two generative flow models, in which the two models are iteratively updated based on the jointly synthesized examples. The first flow model is a normalizing flow that transforms an initial simple density to a target density by applying a sequence of invertible transformations. The second flow model is a Langevin flow that runs finite steps of gradient-based MCMC toward an energy-based model. We start from proposing a generative framework that trains an energy-based model with a normalizing flow as an amortized sampler to initialize the MCMC chains of the energy-based model. In each learning iteration, we generate synthesized examples by using a normalizing flow initialization followed by a short-run Langevin flow revision toward the current energy-based model. Then we treat the synthesized examples as fair samples from the energy-based model and update the model parameters with the maximum likelihood learning gradient, while the normalizing flow directly learns from the synthesized examples by maximizing the tractable likelihood. Under the short-run non-mixing MCMC scenario, the estimation of the energy-based model is shown to follow the perturbation of maximum likelihood, and the short-run Langevin flow and the normalizing flow form a two-flow generator that we call CoopFlow. We provide an understating of the CoopFlow algorithm by information geometry and show that it is a valid generator as it converges to a moment matching estimator. We demonstrate that the trained CoopFlow is capable of synthesizing realistic images, reconstructing images, and interpolating between images.",NOT,,,,,,,,,,,,,,,,"@inproceedings{
xie2022a,
title={A Tale of Two Flows: Cooperative Learning of Langevin Flow and Normalizing Flow Toward Energy-Based Model},
author={Jianwen Xie and Yaxuan Zhu and Jun Li and Ping Li},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=31d5RLCUuXC}
}"
ICLR,2022,Robust Learning Meets Generative Models: Can Proxy Distributions Improve Adversarial Robustness?,https://openreview.net/forum?id=WVX0NNVBBkV,"['Vikash Sehwag', ' Saeed Mahloujifar', ' Tinashe Handina', ' Sihui Dai', ' Chong Xiang', ' Mung Chiang', ' Prateek Mittal']",30,"While additional training data improves the robustness of deep neural networks against adversarial examples, it presents the challenge of curating a large number of specific real-world samples. We circumvent this challenge by using additional data from proxy distributions learned by advanced generative models. We first seek to formally understand the transfer of robustness from classifiers trained on proxy distributions to the real data distribution. We prove that the difference between the robustness of a classifier on the two distributions is upper bounded by the conditional Wasserstein distance between them. Next we use proxy distributions to significantly improve the performance of adversarial training on five different datasets. For example, we improve robust accuracy by up to 7.5% and 6.7% in ‚Ñì‚àû and ‚Ñì2 threat model over baselines that are not using proxy distributions on the CIFAR-10 dataset. We also improve certified robust accuracy by 7.6% on the CIFAR-10 dataset. We further demonstrate that different generative models brings a disparate improvement in the performance in robust training. We propose a robust discrimination approach to characterize the impact and further provide a deeper understanding of why diffusion-based generative models are a better choice for proxy distribution than generative adversarial networks.",NOT,,,,,,,,,,,,,,,,"@inproceedings{
sehwag2022robust,
title={Robust Learning Meets Generative Models: Can Proxy Distributions Improve Adversarial Robustness?},
author={Vikash Sehwag and Saeed Mahloujifar and Tinashe Handina and Sihui Dai and Chong Xiang and Mung Chiang and Prateek Mittal},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=WVX0NNVBBkV}
}"
ICLR,2022,Imitation Learning from Observations under Transition Model Disparity,https://openreview.net/forum?id=twv2QlJhXzo,"['Tanmay Gangwani', ' Yuan Zhou', ' Jian Peng']",1,"Learning to perform tasks by leveraging a dataset of expert observations, also known as imitation learning from observations (ILO), is an important paradigm for learning skills without access to the expert reward function or the expert actions. We consider ILO in the setting where the expert and the learner agents operate in different environments, with the source of the discrepancy being the transition dynamics model. Recent methods for scalable ILO utilize adversarial learning to match the state-transition distributions of the expert and the learner, an approach that becomes challenging when the dynamics are dissimilar. In this work, we propose an algorithm that trains an intermediary policy in the learner environment and uses it as a surrogate expert for the learner. The intermediary policy is learned such that the state transitions generated by it are close to the state transitions in the expert dataset. To derive a practical and scalable algorithm, we employ concepts from prior work on estimating the support of a probability distribution. Experiments using MuJoCo locomotion tasks highlight that our method compares favorably to the baselines for ILO with transition dynamics mismatch.",NOT,,,,,,,,,,,,,,,,"@inproceedings{
gangwani2022imitation,
title={Imitation Learning from Observations under Transition Model Disparity},
author={Tanmay Gangwani and Yuan Zhou and Jian Peng},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=twv2QlJhXzo}
}"
ICLR,2022,Attacking deep networks with surrogate-based adversarial black-box methods is easy,https://openreview.net/forum?id=Zf4ZdI4OQPV,"['Nicholas A. Lord', ' Romain Mueller', ' Luca Bertinetto']",3,"A recent line of work on black-box adversarial attacks has revived the use of transfer from surrogate models by integrating it into query-based search. However, we find that existing approaches of this type underperform their potential, and can be overly complicated besides. Here, we provide a short and simple algorithm which achieves state-of-the-art results through a search which uses the surrogate network's class-score gradients, with no need for other priors or heuristics. The guiding assumption of the algorithm is that the studied networks are in a fundamental sense learning similar functions, and that a transfer attack from one to the other should thus be fairly ""easy"". This assumption is validated by the extremely low query counts and failure rates achieved: e.g. an untargeted attack on a VGG-16 ImageNet network using a ResNet-152 as the surrogate yields a median query count of 6 at a success rate of 99.9%. Code is available at https://github.com/fiveai/GFCS.",NOT,,,,,,,,,,,,,,,,"@inproceedings{
lord2022attacking,
title={Attacking deep networks with surrogate-based adversarial black-box methods is easy},
author={Nicholas A. Lord and Romain Mueller and Luca Bertinetto},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=Zf4ZdI4OQPV}
}"
ICLR,2022,"Axiomatic Explanations for Visual Search, Retrieval, and Similarity Learning",https://openreview.net/forum?id=TqNsv1TuCX9,"['Mark Hamilton', ' Scott Lundberg', ' Stephanie Fu', ' Lei Zhang', ' William T. Freeman']",1,"Visual search, recommendation, and contrastive similarity learning power technologies that impact billions of users worldwide. Modern model architectures can be complex and difficult to interpret, and there are several competing techniques one can use to explain a search engine's behavior. We show that the theory of fair credit assignment provides a unique axiomatic solution that generalizes several existing recommendation- and metric-explainability techniques in the literature. Using this formalism, we show when existing approaches violate ""fairness"" and derive methods that sidestep these shortcomings and naturally handle counterfactual information. More specifically, we show existing approaches implicitly approximate second-order Shapley-Taylor indices and extend CAM, GradCAM, LIME, SHAP, SBSM, and other methods to search engines. These extensions can extract pairwise correspondences between images from trained opaque-box models. We also introduce a fast kernel-based method for estimating Shapley-Taylor indices that require orders of magnitude fewer function evaluations to converge. Finally, we show that these game-theoretic measures yield more consistent explanations for image similarity architectures.",NOT,,,,,,,,,,,,,,,,"@inproceedings{
hamilton2022axiomatic,
title={Axiomatic Explanations for Visual Search, Retrieval, and Similarity Learning},
author={Mark Hamilton and Scott Lundberg and Stephanie Fu and Lei Zhang and William T. Freeman},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=TqNsv1TuCX9}
}"
ICLR,2022,DISSECT: Disentangled Simultaneous Explanations via Concept Traversals,https://openreview.net/forum?id=qY79G8jGsep,"['Asma Ghandeharioun', ' Been Kim', ' Chun-Liang Li', ' Brendan Jou', ' Brian Eoff', ' Rosalind Picard']",25,"Explaining deep learning model inferences is a promising venue for scientific understanding, improving safety, uncovering hidden biases, evaluating fairness, and beyond, as argued by many scholars. One of the principal benefits of counterfactual explanations is allowing users to explore ""what-if"" scenarios through what does not and cannot exist in the data, a quality that many other forms of explanation such as heatmaps and influence functions are inherently incapable of doing. However, most previous work on generative explainability cannot disentangle important concepts effectively, produces unrealistic examples, or fails to retain relevant information. We propose a novel approach, DISSECT, that jointly trains a generator, a discriminator, and a concept disentangler to overcome such challenges using little supervision. DISSECT generates Concept Traversals (CTs), defined as a sequence of generated examples with increasing degrees of concepts that influence a classifier's decision. By training a generative model from a classifier's signal, DISSECT offers a way to discover a classifier's inherent ""notion"" of distinct concepts automatically rather than rely on user-predefined concepts. We show that DISSECT produces CTs that (1) disentangle several concepts, (2) are influential to a classifier's decision and are coupled to its reasoning due to joint training (3), are realistic, (4) preserve relevant information, and (5) are stable across similar inputs. We validate DISSECT on several challenging synthetic and realistic datasets where previous methods fall short of satisfying desirable criteria for interpretability and show that it performs consistently well. Finally, we present experiments showing applications of DISSECT for detecting potential biases of a classifier and identifying spurious artifacts that impact predictions.",NOT,,,,,,,,,,,,,,,,"@inproceedings{
ghandeharioun2022dissect,
title={{DISSECT}: Disentangled Simultaneous Explanations via Concept Traversals},
author={Asma Ghandeharioun and Been Kim and Chun-Liang Li and Brendan Jou and Brian Eoff and Rosalind Picard},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=qY79G8jGsep}
}"
ICLR,2022,Dual Lottery Ticket Hypothesis,https://openreview.net/forum?id=fOsN52jn25l,"['Yue Bai', ' Huan Wang', ' ZHIQIANG TAO', ' Kunpeng Li', ' Yun Fu']",8,"Fully exploiting the learning capacity of neural networks requires overparameterized dense networks. On the other side, directly training sparse neural networks typically results in unsatisfactory performance. Lottery Ticket Hypothesis (LTH) provides a novel view to investigate sparse network training and maintain its capacity. Concretely, it claims there exist winning tickets from a randomly initialized network found by iterative magnitude pruning and preserving promising trainability (or we say being in trainable condition). In this work, we regard the winning ticket from LTH as the subnetwork which is in trainable condition and its performance as our benchmark, then go from a complementary direction to articulate the Dual Lottery Ticket Hypothesis (DLTH): Randomly selected subnetworks from a randomly initialized dense network can be transformed into a trainable condition and achieve admirable performance compared with LTH --- random tickets in a given lottery pool can be transformed into winning tickets. Specifically, by using uniform-randomly selected subnetworks to represent the general cases, we propose a simple sparse network training strategy, Random Sparse Network Transformation (RST), to substantiate our DLTH. Concretely, we introduce a regularization term to borrow learning capacity and realize information extrusion from the weights which will be masked. After finishing the transformation for the randomly selected subnetworks, we conduct the regular finetuning to evaluate the model using fair comparisons with LTH and other strong baselines. Extensive experiments on several public datasets and comparisons with competitive approaches validate our DLTH as well as the effectiveness of the proposed model RST. Our work is expected to pave a way for inspiring new research directions of sparse network training in the future. Our code is available at https://github.com/yueb17/DLTH.",NOT,,,,,,,,,,,,,,,,"@inproceedings{
bai2022dual,
title={Dual Lottery Ticket Hypothesis},
author={Yue Bai and Huan Wang and ZHIQIANG TAO and Kunpeng Li and Yun Fu},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=fOsN52jn25l}
}"
ICLR,2022,Phase Collapse in Neural Networks,https://openreview.net/forum?id=iPHLcmtietq,"['Florentin Guth', ' John Zarka', ' St√©phane Mallat']",1,"Deep convolutional classifiers linearly separate image classes and improve accuracy as depth increases. They progressively reduce the spatial dimension whereas the number of channels grows with depth. Spatial variability is therefore transformed into variability along channels. A fundamental challenge is to understand the role of non-linearities together with convolutional filters in this transformation. ReLUs with biases are often interpreted as thresholding operators that improve discrimination through sparsity. This paper demonstrates that it is a different mechanism called \emph{phase collapse} which eliminates spatial variability while linearly separating classes. We show that collapsing the phases of complex wavelet coefficients is sufficient to reach the classification accuracy of ResNets of similar depths. However, replacing the phase collapses with thresholding operators that enforce sparsity considerably degrades the performance. We explain these numerical results by showing that the iteration of phase collapses progressively improves separation of classes, as opposed to thresholding non-linearities.",NOT,,,,,,,,,,,,,,,,"@inproceedings{
guth2022phase,
title={Phase Collapse in Neural Networks},
author={Florentin Guth and John Zarka and St{\'e}phane Mallat},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=iPHLcmtietq}
}"
ICLR,2022,Quantitative Performance Assessment of CNN Units via Topological Entropy Calculation,https://openreview.net/forum?id=xFOyMwWPkz,"['Yang Zhao', ' Hao Zhang']",1,"Identifying the status of individual network units is critical for understanding the mechanism of convolutional neural networks (CNNs). However, it is still challenging to reliably give a general indication of unit status, especially for units in different network models. To this end, we propose a novel method for quantitatively clarifying the status of single unit in CNN using algebraic topological tools. Unit status is indicated via the calculation of a defined topological-based entropy, called feature entropy, which measures the degree of chaos of the global spatial pattern hidden in the unit for a category. In this way, feature entropy could provide an accurate indication of status for units in different networks with diverse situations like weight-rescaling operation. Further, we show that feature entropy decreases as the layer goes deeper and shares almost simultaneous trend with loss during training. We show that by investigating the feature entropy of units on only training data, it could give discrimination between networks with different generalization ability from the view of the effectiveness of feature representations.",NOT,,,,,,,,,,,,,,,,"@inproceedings{
zhao2022quantitative,
title={Quantitative Performance Assessment of {CNN} Units via Topological Entropy Calculation},
author={Yang Zhao and Hao Zhang},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=xFOyMwWPkz}
}"
ICLR,2022,Learning Scenario Representation for Solving Two-stage Stochastic Integer Programs,https://openreview.net/forum?id=06Wy2BtxXrz,"['Yaoxin Wu', ' Wen Song', ' Zhiguang Cao', ' Jie Zhang']",2,"Many practical combinatorial optimization problems under uncertainty can be modeled as stochastic integer programs (SIPs), which are extremely challenging to solve due to the high complexity. To solve two-stage SIPs efficiently, we propose a conditional variational autoencoder (CVAE) based method to learn scenario representation for a class of SIP instances. Specifically, we design a graph convolutional network based encoder to embed each scenario with the deterministic part of its instance (i.e. context) into a low-dimensional latent space, from which a decoder reconstructs the scenario from its latent representation conditioned on the context. Such a design effectively captures the dependencies of the scenarios on their corresponding instances. We apply the trained encoder to two tasks in typical SIP solving, i.e. scenario reduction and objective prediction. Experiments on two SIP problems show that the learned latent representation significantly boosts the solving performance to attain high-quality solutions in short computational time, and generalizes fairly well to problems of larger sizes or with more scenarios.",NOT,,,,,,,,,,,,,,,,"@inproceedings{
wu2022learning,
title={Learning Scenario Representation for Solving Two-stage Stochastic Integer Programs},
author={Yaoxin Wu and Wen Song and Zhiguang Cao and Jie Zhang},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=06Wy2BtxXrz}
}"
ICLR,2022,CLEVA-Compass: A Continual Learning Evaluation Assessment Compass to Promote Research Transparency and Comparability,https://openreview.net/forum?id=rHMaBYbkkRJ,"['Martin Mundt', ' Steven Lang', ' Quentin Delfosse', ' Kristian Kersting']",9,"What is the state of the art in continual machine learning? Although a natural question for predominant static benchmarks, the notion to train systems in a lifelong manner entails a plethora of additional challenges with respect to set-up and evaluation. The latter have recently sparked a growing amount of critiques on prominent algorithm-centric perspectives and evaluation protocols being too narrow, resulting in several attempts at constructing guidelines in favor of specific desiderata or arguing against the validity of prevalent assumptions. In this work, we depart from this mindset and argue that the goal of a precise formulation of desiderata is an ill-posed one, as diverse applications may always warrant distinct scenarios. Instead, we introduce the Continual Learning EValuation Assessment Compass: the CLEVA-Compass. The compass provides the visual means to both identify how approaches are practically reported and how works can simultaneously be contextualized in the broader literature landscape. In addition to promoting compact specification in the spirit of recent replication trends, it thus provides an intuitive chart to understand the priorities of individual systems, where they resemble each other, and what elements are missing towards a fair comparison.",NOT,,,,,,,,,,,,,,,,"@inproceedings{
mundt2022clevacompass,
title={{CLEVA}-Compass: A Continual Learning Evaluation Assessment Compass to Promote Research Transparency and Comparability},
author={Martin Mundt and Steven Lang and Quentin Delfosse and Kristian Kersting},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=rHMaBYbkkRJ}
}"
Neurips,2018,The Price of Fair PCA: One Extra dimension,https://proceedings.neurips.cc//paper/2018/hash/cc4af25fa9d2d5c953496579b75f6f6c-Abstract.html,"['Samira Samadi', ' Uthaipon Tantipongpipat', ' Jamie H. Morgenstern', ' Mohit Singh', ' Santosh Vempala']",102,"We investigate whether the standard dimensionality reduction technique of PCA inadvertently produces data representations with different fidelity for two different populations. We show on several real-world data sets, PCA has higher reconstruction error on population A than on B (for example, women versus men or lower- versus higher-educated individuals). This can happen even when the data set has a similar number of samples from A and B. This motivates our study of dimensionality reduction techniques which maintain similar fidelity for A and B. We define the notion of Fair PCA and give a polynomial-time algorithm for finding a low dimensional representation of the data which is nearly-optimal with respect to this measure. Finally, we show on real-world data sets that our algorithm can be used to efficiently generate a fair low dimensional representation of the data.",SCOPE,,Data,Algorithm,Methods,Unsupervised (PCA) fairness,,,,,,,,,,,"@inproceedings{NEURIPS2018_cc4af25f,
 author = {Samadi, Samira and Tantipongpipat, Uthaipon and Morgenstern, Jamie H and Singh, Mohit and Vempala, Santosh},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {The Price of Fair PCA: One Extra dimension},
 url = {https://proceedings.neurips.cc/paper/2018/file/cc4af25fa9d2d5c953496579b75f6f6c-Paper.pdf},
 volume = {31},
 year = {2018}
}

"
Neurips,2019,Learning Fairness in Multi-Agent Systems,https://proceedings.neurips.cc//paper/2019/hash/10493aa88605cad5ab4752b04a63d172-Abstract.html,"['Jiechuan Jiang', ' Zongqing Lu']",30,"Fairness is essential for human society, contributing to stability and productivity. Similarly, fairness is also the key for many multi-agent systems. Taking fairness into multi-agent learning could help multi-agent systems become both efficient and stable. However, learning efficiency and fairness simultaneously is a complex, multi-objective, joint-policy optimization. To tackle these difficulties, we propose FEN, a novel hierarchical reinforcement learning model. We first decompose fairness for each agent and propose fair-efficient reward that each agent learns its own policy to optimize. To avoid multi-objective conflict, we design a hierarchy consisting of a controller and several sub-policies, where the controller maximizes the fair-efficient reward by switching among the sub-policies that provides diverse behaviors to interact with the environment. FEN can be trained in a fully decentralized way, making it easy to be deployed in real-world applications. Empirically, we show that FEN easily learns both fairness and efficiency and significantly outperforms baselines in a variety of multi-agent scenarios.",SCOPE,,Algorithm,,,Fairness in a multi-agent setting,,,,,,,,,,,"@inproceedings{NEURIPS2019_10493aa8,
 author = {Jiang, Jiechuan and Lu, Zongqing},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Learning Fairness in Multi-Agent Systems},
 url = {https://proceedings.neurips.cc/paper/2019/file/10493aa88605cad5ab4752b04a63d172-Paper.pdf},
 volume = {32},
 year = {2019}
}

"
Neurips,2019,Exploring Algorithmic Fairness in Robust Graph Covering Problems,https://proceedings.neurips.cc//paper/2019/hash/1d7c2aae840867027b7edd17b6aaa0e9-Abstract.html,"['Aida Rahmattalabi', ' Phebe Vayanos', ' Anthony Fulginiti', ' Eric Rice', ' Bryan Wilder', ' Amulya Yadav', ' Milind Tambe']",37,"Fueled by algorithmic advances, AI algorithms are increasingly being deployed in settings subject to unanticipated challenges with complex social effects. Motivated by real-world deployment of AI driven, social-network based suicide prevention and landslide risk management interventions, this paper focuses on a robust graph covering problem subject to group fairness constraints. We show that, in the absence of fairness constraints, state-of-the-art algorithms for the robust graph covering problem result in biased node coverage: they tend to discriminate individuals (nodes) based on membership in traditionally marginalized groups. To remediate this issue, we propose a novel formulation of the robust covering problem with fairness constraints and a tractable approximation scheme applicable to real world instances. We provide a formal analysis of the price of group fairness (PoF) for this problem, where we show that uncertainty can lead to greater PoF. We demonstrate the effectiveness of our approach on several real-world social networks. Our method yields competitive node coverage while significantly improving group fairness relative to state-of-the-art methods.",SCOPE,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2019_1d7c2aae,
 author = {Rahmattalabi, Aida and Vayanos, Phebe and Fulginiti, Anthony and Rice, Eric and Wilder, Bryan and Yadav, Amulya and Tambe, Milind},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Exploring Algorithmic Fairness in Robust Graph Covering Problems},
 url = {https://proceedings.neurips.cc/paper/2019/file/1d7c2aae840867027b7edd17b6aaa0e9-Paper.pdf},
 volume = {32},
 year = {2019}
}

"
Neurips,2019,Multi-Criteria Dimensionality Reduction with Applications to Fairness,https://proceedings.neurips.cc//paper/2019/hash/2201611d7a08ffda97e3e8c6b667a1bc-Abstract.html,"['Uthaipon Tantipongpipat', ' Samira Samadi', ' Mohit Singh', ' Jamie H. Morgenstern', ' Santosh Vempala']",24,"Dimensionality reduction is a classical technique widely used for data analysis. One foundational instantiation is Principal Component Analysis (PCA), which minimizes the average reconstruction error. In this paper, we introduce the multi-criteria dimensionality reduction problem where we are given multiple objectives that need to be optimized simultaneously. As an application, our model captures several fairness criteria for dimensionality reduction such as the Fair-PCA problem introduced by Samadi et al. [NeurIPS18] and the Nash Social Welfare (NSW) problem. In the Fair-PCA problem, the input data is divided into k groups, and the goal is to find a single d-dimensional representation for all groups for which the maximum reconstruction error of any one group is minimized. In NSW the goal is to maximize the product of the individual variances of the groups achieved by the common low-dimensinal space. Our main result is an exact polynomial-time algorithm for the two-criteria dimensionality reduction problem when the two criteria are increasing concave functions. As an application of this result, we obtain a polynomial time algorithm for Fair-PCA for k=2 groups, resolving an open problem of Samadi et al.[NeurIPS18], and a polynomial time algorithm for NSW objective for k=2 groups. We also give approximation algorithms for k>2. Our technical contribution in the above results is to prove new low-rank properties of extreme point solutions to semi-definite programs. We conclude with the results of several experiments indicating improved performance and generalized application of our algorithm on real-world datasets.",SCOPE,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2019_2201611d,
 author = {Tantipongpipat, Uthaipon and Samadi, Samira and Singh, Mohit and Morgenstern, Jamie H and Vempala, Santosh},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Multi-Criteria Dimensionality Reduction with Applications to Fairness},
 url = {https://proceedings.neurips.cc/paper/2019/file/2201611d7a08ffda97e3e8c6b667a1bc-Paper.pdf},
 volume = {32},
 year = {2019}
}

"
Neurips,2019,Balancing Efficiency and Fairness in On-Demand Ridesourcing,https://proceedings.neurips.cc//paper/2019/hash/3070e6addcd702cb58de5d7897bfdae1-Abstract.html,"['Nixie S. Lesmana', ' Xuan Zhang', ' Xiaohui Bei']",40,"We investigate the problem of assigning trip requests to available vehicles in on-demand ridesourcing. Much of the literature has focused on maximizing the total value of served requests, achieving efficiency on the passengers‚Äô side. However, such solutions may result in some drivers being assigned to insufficient or undesired trips, therefore losing fairness from the drivers‚Äô perspective. In this paper, we focus on both the system efficiency and the fairness among drivers and quantitatively analyze the trade-offs between these two objectives. In particular, we give an explicit answer to the question of whether there always exists an assignment that achieves any target efficiency and fairness. We also propose a simple reassignment algorithm that can achieve any selected trade-off. Finally, we demonstrate the effectiveness of the algorithms through extensive experiments on real-world datasets.",SCOPE,,Organizational Realities,Algorithm,Methods,Shows how to fairly allocate rides in rideshare systems,,,,,,,,,,,"@inproceedings{NEURIPS2019_3070e6ad,
 author = {Lesmana, Nixie S and Zhang, Xuan and Bei, Xiaohui},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Balancing Efficiency and Fairness in On-Demand Ridesourcing},
 url = {https://proceedings.neurips.cc/paper/2019/file/3070e6addcd702cb58de5d7897bfdae1-Paper.pdf},
 volume = {32},
 year = {2019}
}

"
Neurips,2019,Optimizing Generalized Rate Metrics with Three Players,https://proceedings.neurips.cc//paper/2019/hash/3ce257b311e5acf849992f5a675188e8-Abstract.html,"['Harikrishna Narasimhan', ' Andrew Cotter', ' Maya Gupta']",21,"We present a general framework for solving a large class of learning problems with non-linear functions of classification rates. This includes problems where one wishes to optimize a non-decomposable performance metric such as the F-measure or G-mean, and constrained training problems where the classifier needs to satisfy non-linear rate constraints such as predictive parity fairness, distribution divergences or churn ratios. We extend previous two-player game approaches for constrained optimization to an approach with three players to decouple the classifier rates from the non-linear objective, and seek to find an equilibrium of the game. Our approach generalizes many existing algorithms, and makes possible new algorithms with more flexibility and tighter handling of non-linear rate constraints. We provide convergence guarantees for convex functions of rates, and show how our methodology can be extended to handle sums-of-ratios of rates. Experiments on different fairness tasks confirm the efficacy of our approach.",SCOPE,,,,Methods,Taking into account multiple agents,,,,,,,,,,,"@inproceedings{NEURIPS2019_3ce257b3,
 author = {Narasimhan, Harikrishna and Cotter, Andrew and Gupta, Maya},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Optimizing Generalized Rate Metrics with Three Players},
 url = {https://proceedings.neurips.cc/paper/2019/file/3ce257b311e5acf849992f5a675188e8-Paper.pdf},
 volume = {32},
 year = {2019}
}

"
Neurips,2019,Group Retention when Using Machine Learning in Sequential Decision Making: the Interplay between User Dynamics and Fairness,https://proceedings.neurips.cc//paper/2019/hash/7690dd4db7a92524c684e3191919eb6b-Abstract.html,"['Xueru Zhang', ' Mohammadmahdi Khaliligarekani', ' Cem Tekin', ' mingyan liu']",37,"Machine Learning (ML) models trained on data from multiple demographic groups can inherit representation disparity (Hashimoto et al., 2018) that may exist in the data: the model may be less favorable to groups contributing less to the training process; this in turn can degrade population retention in these groups over time, and exacerbate representation disparity in the long run. In this study, we seek to understand the interplay between ML decisions and the underlying group representation, how they evolve in a sequential framework, and how the use of fairness criteria plays a role in this process. We show that the representation disparity can easily worsen over time under a natural user dynamics (arrival and departure) model when decisions are made based on a commonly used objective and fairness criteria, resulting in some groups diminishing entirely from the sample pool in the long run. It highlights the fact that fairness criteria have to be defined while taking into consideration the impact of decisions on user dynamics. Toward this end, we explain how a proper fairness criterion can be selected based on a general user dynamics model.",SCOPE,,Metrics,,Measurement,"How to measure the impact of implementing a fairness model over time given ""typical user dynamics models""",,,,,,,,,,,"@inproceedings{NEURIPS2019_7690dd4d,
 author = {Zhang, Xueru and Khaliligarekani, Mohammadmahdi and Tekin, Cem and liu, mingyan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Group Retention when Using Machine Learning in Sequential Decision Making: the Interplay between User Dynamics and Fairness},
 url = {https://proceedings.neurips.cc/paper/2019/file/7690dd4db7a92524c684e3191919eb6b-Paper.pdf},
 volume = {32},
 year = {2019}
}

"
Neurips,2019,Coresets for Clustering with Fairness Constraints,https://proceedings.neurips.cc//paper/2019/hash/810dfbbebb17302018ae903e9cb7a483-Abstract.html,"['Lingxiao Huang', ' Shaofeng Jiang', ' Nisheeth Vishnoi']",76,"In a recent work, \cite{chierichetti2017fair} studied the following ``fair'' variants of classical clustering problems such as k-means and k-median: given a set of n data points in R^d and a binary type associated to each data point, the goal is to cluster the points while ensuring that the proportion of each type in each cluster is roughly the same as its underlying proportion. Subsequent work has focused on either extending this setting to when each data point has multiple, non-disjoint sensitive types such as race and gender \cite{bera2019fair}, or to address the problem that the clustering algorithms in the above work do not scale well. The main contribution of this paper is an approach to clustering with fairness constraints that involve {\em multiple, non-disjoint} attributes, that is {\em also scalable}. Our approach is based on novel constructions of coresets: for the k-median objective, we construct an \eps-coreset of size O(\Gamma k^2 \eps^{-d}) where \Gamma is the number of distinct collections of groups that a point may belong to, and for the k-means objective, we show how to construct an \eps-coreset of size O(\Gamma k^3\eps^{-d-1}). The former result is the first known coreset construction for the fair clustering problem with the k-median objective, and the latter result removes the dependence on the size of the full dataset as in~\cite{schmidt2018fair} and generalizes it to multiple, non-disjoint attributes. Importantly, plugging our coresets into existing algorithms for fair clustering such as \cite{backurs2019scalable} results in the fastest algorithms for several cases. Empirically, we assess our approach over the \textbf{Adult} and \textbf{Bank} dataset, and show that the coreset sizes are much smaller than the full dataset; applying coresets indeed accelerates the running time of computing the fair clustering objective while ensuring that the resulting objective difference is small.",SCOPE,,Data,,,Unsupervised,,,,,,,,,,,"@inproceedings{NEURIPS2019_810dfbbe,
 author = {Huang, Lingxiao and Jiang, Shaofeng and Vishnoi, Nisheeth},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Coresets for Clustering with Fairness Constraints},
 url = {https://proceedings.neurips.cc/paper/2019/file/810dfbbebb17302018ae903e9cb7a483-Paper.pdf},
 volume = {32},
 year = {2019}
}

"
Neurips,2019,Policy Learning for Fairness in Ranking,https://proceedings.neurips.cc//paper/2019/hash/9e82757e9a1c12cb710ad680db11f6f1-Abstract.html,"['Ashudeep Singh', ' Thorsten Joachims']",150,"Conventional Learning-to-Rank (LTR) methods optimize the utility of the rankings to the users, but they are oblivious to their impact on the ranked items. However, there has been a growing understanding that the latter is important to consider for a wide range of ranking applications (e.g. online marketplaces, job placement, admissions). To address this need, we propose a general LTR framework that can optimize a wide range of utility metrics (e.g. NDCG) while satisfying fairness of exposure constraints with respect to the items. This framework expands the class of learnable ranking functions to stochastic ranking policies, which provides a language for rigorously expressing fairness specifications. Furthermore, we provide a new LTR algorithm called Fair-PG-Rank for directly searching the space of fair ranking policies via a policy-gradient approach. Beyond the theoretical evidence in deriving the framework and the algorithm, we provide empirical results on simulated and real-world datasets verifying the effectiveness of the approach in individual and group-fairness settings.",SCOPE,,Algorithm,,Methods,Ranking,,,,,,,,,,,"@inproceedings{NEURIPS2019_9e82757e,
 author = {Singh, Ashudeep and Joachims, Thorsten},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Policy Learning for Fairness in Ranking},
 url = {https://proceedings.neurips.cc/paper/2019/file/9e82757e9a1c12cb710ad680db11f6f1-Paper.pdf},
 volume = {32},
 year = {2019}
}

"
Neurips,2019,Equitable Stable Matchings in Quadratic Time,https://proceedings.neurips.cc//paper/2019/hash/cb70ab375662576bd1ac5aaf16b3fca4-Abstract.html,"['Nikolaos Tziavelis', ' Ioannis Giannakopoulos', ' Katerina Doka', ' Nectarios Koziris', ' Panagiotis Karras']",7,"Can a stable matching that achieves high equity among the two sides of a market be reached in quadratic time? The Deferred Acceptance (DA) algorithm finds a stable matching that is biased in favor of one side; optimizing apt equity measures is strongly NP-hard. A proposed approximation algorithm offers a guarantee only with respect to the DA solutions. Recent work introduced Deferred Acceptance with Compensation Chains (DACC), a class of algorithms that can reach any stable matching in O(n^4) time, but did not propose a way to achieve good equity. In this paper, we propose an alternative that is computationally simpler and achieves high equity too. We introduce Monotonic Deferred Acceptance (MDA), a class of algorithms that progresses monotonically towards a stable matching; we couple MDA with a mechanism we call Strongly Deferred Acceptance (SDA), to build an algorithm that reaches an equitable stable matching in quadratic time; we amend this algorithm with a few low-cost local search steps to what we call Deferred Local Search (DLS), and demonstrate experimentally that it outperforms previous solutions in terms of equity measures and matches the most efficient ones in runtime.",SCOPE,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2019_cb70ab37,
 author = {Tziavelis, Nikolaos and Giannakopoulos, Ioannis and Doka, Katerina and Koziris, Nectarios and Karras, Panagiotis},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Equitable Stable Matchings in Quadratic Time},
 url = {https://proceedings.neurips.cc/paper/2019/file/cb70ab375662576bd1ac5aaf16b3fca4-Paper.pdf},
 volume = {32},
 year = {2019}
}

"
Neurips,2019,Discrimination in Online Markets: Effects of Social Bias on Learning from Reviews and Policy Design,https://proceedings.neurips.cc//paper/2019/hash/e00406144c1e7e35240afed70f34166a-Abstract.html,"['Faidra Georgia Monachou', ' Itai Ashlagi']",12,"The increasing popularity of online two-sided markets such as ride-sharing, accommodation and freelance labor platforms, goes hand in hand with new socioeconomic challenges. One major issue remains the existence of bias and discrimination against certain social groups. We study this problem using a two-sided large market model with employers and workers mediated by a platform. Employers who seek to hire workers face uncertainty about a candidate worker's skill level. Therefore, they base their hiring decision on learning from past reviews about an individual worker as well as on their (possibly misspecified) prior beliefs about the ability level of the social group the worker belongs to. Drawing upon the social learning literature with bounded rationality and limited information, uncertainty combined with social bias leads to unequal hiring opportunities between workers of different social groups. Although the effect of social bias decreases as the number of reviews increases (consistent with empirical findings), minority workers still receive lower expected payoffs. Finally, we consider a simple directed matching policy (DM), which combines learning and matching to make better matching decisions for minority workers. Under this policy, there exists a steady-state equilibrium, in which DM reduces the discrimination gap.",SCOPE,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2019_e0040614,
 author = {Monachou, Faidra Georgia and Ashlagi, Itai},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Discrimination in Online Markets: Effects of Social Bias on Learning from Reviews and Policy Design},
 url = {https://proceedings.neurips.cc/paper/2019/file/e00406144c1e7e35240afed70f34166a-Paper.pdf},
 volume = {32},
 year = {2019}
}

"
Neurips,2019,Fair Algorithms for Clustering,https://proceedings.neurips.cc//paper/2019/hash/fc192b0c0d270dbf41870a63a8c76c2f-Abstract.html,"['Suman Bera', ' Deeparnab Chakrabarty', ' Nicolas Flores', ' Maryam Negahbani']",177,"We study the problem of finding low-cost {\em fair clusterings} in data where each data point may belong to many protected groups. Our work significantly generalizes the seminal work of Chierichetti \etal (NIPS 2017) as follows. - We allow the user to specify the parameters that define fair representation. More precisely, these parameters define the maximum over- and minimum under-representation of any group in any cluster. - Our clustering algorithm works on any $\ell_p$-norm objective (e.g. $k$-means, $k$-median, and $k$-center). Indeed, our algorithm transforms any vanilla clustering solution into a fair one incurring only a slight loss in quality. - Our algorithm also allows individuals to lie in multiple protected groups. In other words, we do not need the protected groups to partition the data and we can maintain fairness across different groups simultaneously. Our experiments show that on established data sets, our algorithm performs much better in practice than what our theoretical results suggest.",SCOPE,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2019_fc192b0c,
 author = {Bera, Suman and Chakrabarty, Deeparnab and Flores, Nicolas and Negahbani, Maryam},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Fair Algorithms for Clustering},
 url = {https://proceedings.neurips.cc/paper/2019/file/fc192b0c0d270dbf41870a63a8c76c2f-Paper.pdf},
 volume = {32},
 year = {2019}
}

"
Neurips,2020,Fairness constraints can help exact inference in structured prediction,https://proceedings.neurips.cc//paper/2020/hash/8248a99e81e752cb9b41da3fc43fbe7f-Abstract.html,"['Kevin Bello', ' Jean Honorio']",4,"Many inference problems in structured prediction can be modeled as maximizing a score function on a space of labels, where graphs are a natural representation to decompose the total score into a sum of unary (nodes) and pairwise (edges) scores. Given a generative model with an undirected connected graph G and true vector of binary labels $\bar{y}$, it has been previously shown that when G has good expansion properties, such as complete graphs or d-regular expanders, one can exactly recover $\bar{y}$ (with high probability and in polynomial time) from a single noisy observation of each edge and node. We analyze the previously studied generative model by Globerson et al. (2015) under a notion of statistical parity. That is, given a fair binary node labeling, we ask the question whether it is possible to recover the fair assignment, with high probability and in polynomial time, from single edge and node observations. We find that, in contrast to the known trade-offs between fairness and model performance, the addition of the fairness constraint improves the probability of exact recovery. We effectively explain this phenomenon and empirically show how graphs with poor expansion properties, such as grids, are now capable of achieving exact recovery. Finally, as a byproduct of our analysis, we provide a tighter minimum-eigenvalue bound than that which can be derived from Weyl's inequality.",SCOPE,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2020_8248a99e,
 author = {Bello, Kevin and Honorio, Jean},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {11322--11332},
 publisher = {Curran Associates, Inc.},
 title = {Fairness constraints can help exact inference in structured prediction},
 url = {https://proceedings.neurips.cc/paper/2020/file/8248a99e81e752cb9b41da3fc43fbe7f-Paper.pdf},
 volume = {33},
 year = {2020}
}

"
Neurips,2020,Probabilistic Fair Clustering,https://proceedings.neurips.cc//paper/2020/hash/95f2b84de5660ddf45c8a34933a2e66f-Abstract.html,"['Seyed Esmaeili', ' Brian Brubach', ' Leonidas Tsepenekas', ' John Dickerson']",21,"In clustering problems, a central decision-maker is given a complete metric graph over vertices and must provide a clustering of vertices that minimizes some objective function. In fair clustering problems, vertices are endowed with a color (e.g., membership in a group), and the requirements of a valid clustering might also include the representation of colors in the solution. Prior work in fair clustering assumes complete knowledge of group membership. In this paper, we generalize this by assuming imperfect knowledge of group membership through probabilistic assignments, and present algorithms in this more general setting with approximation ratio guarantees. We also address the problem of ""metric membership"", where group membership has a notion of order and distance. Experiments are conducted using our proposed algorithms as well as baselines to validate our approach, and also surface nuanced concerns when group membership is not known deterministically.",SCOPE,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2020_95f2b84d,
 author = {Esmaeili, Seyed  and Brubach, Brian and Tsepenekas, Leonidas and Dickerson, John},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {12743--12755},
 publisher = {Curran Associates, Inc.},
 title = {Probabilistic Fair Clustering},
 url = {https://proceedings.neurips.cc/paper/2020/file/95f2b84de5660ddf45c8a34933a2e66f-Paper.pdf},
 volume = {33},
 year = {2020}
}

"
Neurips,2020,Fairness in Streaming Submodular Maximization: Algorithms and Hardness,https://proceedings.neurips.cc//paper/2020/hash/9d752cb08ef466fc480fba981cfa44a1-Abstract.html,"['Marwa El Halabi', ' Slobodan Mitroviƒá', ' Ashkan Norouzi-Fard', ' Jakab Tardos', ' Jakub M. Tarnawski']",18,"Submodular maximization has become established as the method of choice for the task of selecting representative and diverse summaries of data. However, if datapoints have sensitive attributes such as gender or age, such machine learning algorithms, left unchecked, are known to exhibit bias: under- or over-representation of particular groups. This has made the design of fair machine learning algorithms increasingly important. In this work we address the question: Is it possible to create fair summaries for massive datasets? To this end, we develop the first streaming approximation algorithms for submodular maximization under fairness constraints, for both monotone and non-monotone functions. We validate our findings empirically on exemplar-based clustering, movie recommendation, DPP-based summarization, and maximum coverage in social networks, showing that fairness constraints do not significantly impact utility.",SCOPE,,Data,Algorithm,Methods,Looks interesting but probably out of scope,,,,,,,,,,,"@inproceedings{NEURIPS2020_9d752cb0,
 author = {El Halabi, Marwa and Mitrovi\'{c}, Slobodan and Norouzi-Fard, Ashkan and Tardos, Jakab and Tarnawski, Jakub M},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {13609--13622},
 publisher = {Curran Associates, Inc.},
 title = {Fairness in Streaming Submodular Maximization: Algorithms and Hardness},
 url = {https://proceedings.neurips.cc/paper/2020/file/9d752cb08ef466fc480fba981cfa44a1-Paper.pdf},
 volume = {33},
 year = {2020}
}

"
Neurips,2020,Prediction with Corrupted Expert Advice,https://proceedings.neurips.cc//paper/2020/hash/a512294422de868f8474d22344636f16-Abstract.html,"['Idan Amir', ' Idan Attias', ' Tomer Koren', ' Yishay Mansour', ' Roi Livni']",24,"We revisit the fundamental problem of prediction with expert advice, in a setting where the environment is benign and generates losses stochastically, but the feedback observed by the learner is subject to a moderate adversarial corruption. We prove that a variant of the classical Multiplicative Weights algorithm with decreasing step sizes achieves constant regret in this setting and performs optimally in a wide range of environments, regardless of the magnitude of the injected corruption. Our results reveal a surprising disparity between the often comparable Follow the Regularized Leader (FTRL) and Online Mirror Descent (OMD) frameworks: we show that for experts in the corrupted stochastic regime, the regret performance of OMD is in fact strictly inferior to that of FTRL.",SCOPE,,,,,Agents,,,,,,,,,,,"@inproceedings{NEURIPS2020_a5122944,
 author = {Amir, Idan and Attias, Idan and Koren, Tomer and Mansour, Yishay and Livni, Roi},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {14315--14325},
 publisher = {Curran Associates, Inc.},
 title = {Prediction with Corrupted Expert Advice},
 url = {https://proceedings.neurips.cc/paper/2020/file/a512294422de868f8474d22344636f16-Paper.pdf},
 volume = {33},
 year = {2020}
}

"
Neurips,2020,KFC: A Scalable Approximation Algorithm for $k$‚àícenter Fair Clustering,https://proceedings.neurips.cc//paper/2020/hash/a6d259bfbfa2062843ef543e21d7ec8e-Abstract.html,"['Elfarouk Harb', ' Ho Shan Lam']",12,"In this paper, we study the problem of fair clustering on the $k-$center objective. In fair clustering, the input is $N$ points, each belonging to at least one of $l$ protected groups, e.g. male, female, Asian, Hispanic. The objective is to cluster the $N$ points into $k$ clusters to minimize a classical clustering objective function. However, there is an additional constraint that each cluster needs to be fair, under some notion of fairness. This ensures that no group is either ``over-represented'' or ``under-represented'' in any cluster. Our work builds on the work of Chierichetti et al. (NIPS 2017), Bera et al. (NeurIPS 2019), Ahmadian et al. (KDD 2019), and Bercea et al. (APPROX 2019). We obtain a randomized $3-$approximation algorithm for the $k-$center objective function, beating the previous state of the art ($4-$approximation). We test our algorithm on real datasets, and show that our algorithm is effective in finding good clusters without over-representation or under-representation, surpassing the current state of the art in runtime speed, clustering cost, while achieving similar fairness violations.",SCOPE,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2020_a6d259bf,
 author = {Harb, Elfarouk and Lam, Ho Shan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {14509--14519},
 publisher = {Curran Associates, Inc.},
 title = {KFC: A Scalable Approximation Algorithm for k−center Fair Clustering},
 url = {https://proceedings.neurips.cc/paper/2020/file/a6d259bfbfa2062843ef543e21d7ec8e-Paper.pdf},
 volume = {33},
 year = {2020}
}

"
Neurips,2020,"Attack of the Tails: Yes, You Really Can Backdoor Federated Learning",https://proceedings.neurips.cc//paper/2020/hash/b8ffa41d4e492f0fad2f13e29e1762eb-Abstract.html,"['Hongyi Wang', ' Kartik Sreenivasan', ' Shashank Rajput', ' Harit Vishwakarma', ' Saurabh Agarwal', ' Jy-yong Sohn', ' Kangwook Lee', ' Dimitris Papailiopoulos']",215,"Due to its decentralized nature, Federated Learning (FL) lends itself to adversarial attacks in the form of backdoors during training. The goal of a backdoor is to corrupt the performance of the trained model on specific sub-tasks (e.g., by classifying green cars as frogs). A range of FL backdoor attacks have been introduced in the literature, but also methods to defend against them, and it is currently an open question whether FL systems can be tailored to be robust against backdoors. In this work, we provide evidence to the contrary. We first establish that, in the general case, robustness to backdoors implies model robustness to adversarial examples, a major open problem in itself. Furthermore, detecting the presence of a backdoor in a FL model is unlikely assuming first-order oracles or polynomial time. We couple our theoretical results with a new family of backdoor attacks, which we refer to as edge-case backdoors. An edge-case backdoor forces a model to misclassify on seemingly easy inputs that are however unlikely to be part of the training, or test data, i.e., they live on the tail of the input distribution. We explain how these edge-case backdoors can lead to unsavory failures and may have serious repercussions on fairness. We further exhibit that, with careful tuning at the side of the adversary, one can insert them across a range of machine learning tasks (e.g., image classification, OCR, text prediction, sentiment analysis), and bypass state-of-the-art defense mechanisms.",SCOPE,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2020_b8ffa41d,
 author = {Wang, Hongyi and Sreenivasan, Kartik and Rajput, Shashank and Vishwakarma, Harit and Agarwal, Saurabh and Sohn, Jy-yong and Lee, Kangwook and Papailiopoulos, Dimitris},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {16070--16084},
 publisher = {Curran Associates, Inc.},
 title = {Attack of the Tails: Yes, You Really Can Backdoor Federated Learning},
 url = {https://proceedings.neurips.cc/paper/2020/file/b8ffa41d4e492f0fad2f13e29e1762eb-Paper.pdf},
 volume = {33},
 year = {2020}
}

"
Neurips,2020,Fair Hierarchical Clustering,https://proceedings.neurips.cc//paper/2020/hash/f10f2da9a238b746d2bac55759915f0d-Abstract.html,"['Sara Ahmadian', ' Alessandro Epasto', ' Marina Knittel', ' Ravi Kumar', ' Mohammad Mahdian', ' Benjamin Moseley', ' Philip Pham', ' Sergei Vassilvitskii', ' Yuyan Wang']",26,"As machine learning has become more prevalent, researchers have begun to recognize the necessity of ensuring machine learning systems are fair. Recently, there has been an interest in defining a notion of fairness that mitigates over-representation in traditional clustering. In this paper we extend this notion to hierarchical clustering, where the goal is to recursively partition the data to optimize a specific objective. For various natural objectives, we obtain simple, efficient algorithms to find a provably good fair hierarchical clustering. Empirically, we show that our algorithms can find a fair hierarchical clustering, with only a negligible loss in the objective.",SCOPE,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2020_f10f2da9,
 author = {Ahmadian, Sara and Epasto, Alessandro and Knittel, Marina and Kumar, Ravi and Mahdian, Mohammad and Moseley, Benjamin and Pham, Philip and Vassilvitskii, Sergei and Wang, Yuyan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {21050--21060},
 publisher = {Curran Associates, Inc.},
 title = {Fair Hierarchical Clustering},
 url = {https://proceedings.neurips.cc/paper/2020/file/f10f2da9a238b746d2bac55759915f0d-Paper.pdf},
 volume = {33},
 year = {2020}
}

"
Neurips,2021,Learning in Multi-Stage Decentralized Matching Markets,https://proceedings.neurips.cc//paper/2021/hash/6a571fe98a2ba453e84923b447d79cff-Abstract.html,"['Xiaowu Dai', ' Michael Jordan']",8,"Matching markets are often organized in a multi-stage and decentralized manner. Moreover, participants in real-world matching markets often have uncertain preferences. This article develops a framework for learning optimal strategies in such settings, based on a nonparametric statistical approach and variational analysis. We propose an efficient algorithm, built upon concepts of ""lower uncertainty bound"" and ""calibrated decentralized matching,"" for maximizing the participants' expected payoff. We show that there exists a welfare-versus-fairness trade-off that is characterized by the uncertainty level of acceptance. Participants will strategically act in favor of a low uncertainty level to reduce competition and increase expected payoff. We prove that participants can be better off with multi-stage matching compared to single-stage matching. We demonstrate aspects of the theoretical predictions through simulations and an experiment using real data from college admissions.",SCOPE,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2021_6a571fe9,
 author = {Dai, Xiaowu and Jordan, Michael},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {12798--12809},
 publisher = {Curran Associates, Inc.},
 title = {Learning in Multi-Stage Decentralized Matching Markets},
 url = {https://proceedings.neurips.cc/paper/2021/file/6a571fe98a2ba453e84923b447d79cff-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
Neurips,2021,FjORD: Fair and Accurate Federated Learning under heterogeneous targets with Ordered Dropout,https://proceedings.neurips.cc//paper/2021/hash/6aed000af86a084f9cb0264161e29dd3-Abstract.html,"['Samuel Horv√°th', ' Stefanos Laskaridis', ' Mario Almeida', ' Ilias Leontiadis', ' Stylianos Venieris', ' Nicholas Lane']",57,"Federated Learning (FL) has been gaining significant traction across different ML tasks, ranging from vision to keyboard predictions. In large-scale deployments, client heterogeneity is a fact and constitutes a primary problem for fairness, training performance and accuracy. Although significant efforts have been made into tackling statistical data heterogeneity, the diversity in the processing capabilities and network bandwidth of clients, termed system heterogeneity, has remained largely unexplored. Current solutions either disregard a large portion of available devices or set a uniform limit on the model's capacity, restricted by the least capable participants.In this work, we introduce Ordered Dropout, a mechanism that achieves an ordered, nested representation of knowledge in Neural Networks and enables the extraction of lower footprint submodels without the need for retraining. We further show that for linear maps our Ordered Dropout is equivalent to SVD. We employ this technique, along with a self-distillation methodology, in the realm of FL in a framework called FjORD. FjORD alleviates the problem of client system heterogeneity by tailoring the model width to the client's capabilities. Extensive evaluation on both CNNs and RNNs across diverse modalities shows that FjORD consistently leads to significant performance gains over state-of-the-art baselines while maintaining its nested structure.",SCOPE,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2021_6aed000a,
 author = {Horv\'{a}th, Samuel and Laskaridis, Stefanos and Almeida, Mario and Leontiadis, Ilias and Venieris, Stylianos and Lane, Nicholas},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {12876--12889},
 publisher = {Curran Associates, Inc.},
 title = {FjORD: Fair and Accurate Federated Learning under heterogeneous targets with Ordered Dropout},
 url = {https://proceedings.neurips.cc/paper/2021/file/6aed000af86a084f9cb0264161e29dd3-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
Neurips,2021,Better Algorithms for Individually Fair $k$-Clustering,https://proceedings.neurips.cc//paper/2021/hash/6f221fcb5c504fe96789df252123770b-Abstract.html,"['Maryam Negahbani', ' Deeparnab Chakrabarty']",10,"We study data clustering problems with $\ell_p$-norm objectives (e.g. \textsc{$k$-Median} and \textsc{$k$-Means}) in the context of individual fairness. The dataset consists of $n$ points, and we want to find $k$ centers such that (a) the objective is minimized, while (b) respecting the individual fairness constraint that every point $v$ has a center within a distance at most $r(v)$, where $r(v)$ is $v$'s distance to its $(n/k)$th nearest point. Jung, Kannan, and Lutz [FORC 2020] introduced this concept and designed a clustering algorithm with provable (approximate) fairness and objective guarantees for the $\ell_\infty$ or \textsc{$k$-Center} objective. Mahabadi and Vakilian [ICML 2020] revisited this problem to give a local-search algorithm for all $\ell_p$-norms. Empirically, their algorithms outperform Jung et. al.'s by a large margin in terms of cost (for \textsc{$k$-Median} and \textsc{$k$-Means}), but they incur a reasonable loss in fairness. In this paper, our main contribution is to use Linear Programming (LP) techniques to obtain better algorithms for this problem, both in theory and in practice. We prove that by modifying known LP rounding techniques, one gets a worst-case guarantee on the objective which is much better than in MV20, and empirically, this objective is extremely close to the optimal. Furthermore, our theoretical fairness guarantees are comparable with MV20 in theory, and empirically, we obtain noticeably fairer solutions.Although solving the LP {\em exactly} might be prohibitive, we demonstrate that in practice, a simple sparsification technique drastically improves the run-time of our algorithm.",SCOPE,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2021_6f221fcb,
 author = {Negahbani, Maryam and Chakrabarty, Deeparnab},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {13340--13351},
 publisher = {Curran Associates, Inc.},
 title = {Better Algorithms for Individually Fair k-Clustering},
 url = {https://proceedings.neurips.cc/paper/2021/file/6f221fcb5c504fe96789df252123770b-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
Neurips,2021,An Online Riemannian PCA for Stochastic Canonical Correlation Analysis,https://proceedings.neurips.cc//paper/2021/hash/758a06618c69880a6cee5314ee42d52f-Abstract.html,"['Zihang Meng', ' Rudrasis Chakraborty', ' Vikas Singh']",6,"We present an efficient stochastic algorithm (RSG+) for canonical correlation analysis (CCA) using a reparametrization of the projection matrices. We show how this reparametrization (into structured matrices), simple in hindsight, directly presents an opportunity to repurpose/adjust mature techniques for numerical optimization on Riemannian manifolds. Our developments nicely complement existing methods for this problem which either require $O(d^3)$ time complexity per iteration with $O(\frac{1}{\sqrt{t}})$ convergence rate (where $d$ is the dimensionality) or only extract the top $1$ component with $O(\frac{1}{t})$ convergence rate. In contrast, our algorithm offers a strict improvement for this classical problem: it achieves $O(d^2k)$ runtime complexity per iteration for extracting the top $k$ canonical components with $O(\frac{1}{t})$ convergence rate. While the paper primarily focuses on the formulation and technical analysis of its properties, our experiments show that the empirical behavior on common datasets is quite promising, We also explore a potential application in training fair models where the label of protected attribute is missing or otherwise unavailable.",SCOPE,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2021_758a0661,
 author = {Meng, Zihang and Chakraborty, Rudrasis and Singh, Vikas},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {14056--14068},
 publisher = {Curran Associates, Inc.},
 title = {An Online Riemannian PCA for Stochastic Canonical Correlation Analysis},
 url = {https://proceedings.neurips.cc/paper/2021/file/758a06618c69880a6cee5314ee42d52f-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
Neurips,2021,Fair Clustering Under a Bounded Cost,https://proceedings.neurips.cc//paper/2021/hash/781877bda0783aac5f1cf765c128b437-Abstract.html,"['Seyed Esmaeili', ' Brian Brubach', ' Aravind Srinivasan', ' John Dickerson']",10,"Clustering is a fundamental unsupervised learning problem where a dataset is partitioned into clusters that consist of nearby points in a metric space. A recent variant, fair clustering, associates a color with each point representing its group membership and requires that each color has (approximately) equal representation in each cluster to satisfy group fairness. In this model, the cost of the clustering objective increases due to enforcing fairness in the algorithm. The relative increase in the cost, the ```````''price of fairness,'' can indeed be unbounded. Therefore, in this paper we propose to treat an upper bound on the clustering objective as a constraint on the clustering problem, and to maximize equality of representation subject to it. We consider two fairness objectives: the group utilitarian objective and the group egalitarian objective, as well as the group leximin objective which generalizes the group egalitarian objective. We derive fundamental lower bounds on the approximation of the utilitarian and egalitarian objectives and introduce algorithms with provable guarantees for them. For the leximin objective we introduce an effective heuristic algorithm. We further derive impossibility results for other natural fairness objectives. We conclude with experimental results on real-world datasets that demonstrate the validity of our algorithms.",SCOPE,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2021_781877bd,
 author = {Esmaeili, Seyed and Brubach, Brian and Srinivasan, Aravind and Dickerson, John},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {14345--14357},
 publisher = {Curran Associates, Inc.},
 title = {Fair Clustering Under a Bounded Cost},
 url = {https://proceedings.neurips.cc/paper/2021/file/781877bda0783aac5f1cf765c128b437-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
Neurips,2021,On Blame Attribution for Accountable Multi-Agent Sequential Decision Making,https://proceedings.neurips.cc//paper/2021/hash/848c4965359e617d5e16c924b4a85fd9-Abstract.html,"['Stelios Triantafyllou', ' Adish Singla', ' Goran Radanovic']",2,"Blame attribution is one of the key aspects of accountable decision making, as it provides means to quantify the responsibility of an agent for a decision making outcome. In this paper, we study blame attribution in the context of cooperative multi-agent sequential decision making. As a particular setting of interest, we focus on cooperative decision making formalized by Multi-Agent Markov Decision Processes (MMDPs), and we analyze different blame attribution methods derived from or inspired by existing concepts in cooperative game theory. We formalize desirable properties of blame attribution in the setting of interest, and we analyze the relationship between these properties and the studied blame attribution methods. Interestingly, we show that some of the well known blame attribution methods, such as Shapley value, are not performance-incentivizing, while others, such as Banzhaf index, may over-blame agents. To mitigate these value misalignment and fairness issues, we introduce a novel blame attribution method, unique in the set of properties it satisfies, which trade-offs explanatory power (by under-blaming agents) for the aforementioned properties. We further show how to account for uncertainty about agents' decision making policies, and we experimentally: a) validate the qualitative properties of the studied blame attribution methods, and b) analyze their robustness to uncertainty.",SCOPE,,,,,Agents,Might be interesting still,,,,,,,,,,"@inproceedings{NEURIPS2021_848c4965,
 author = {Triantafyllou, Stelios and Singla, Adish and Radanovic, Goran},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {15774--15786},
 publisher = {Curran Associates, Inc.},
 title = {On Blame Attribution for Accountable Multi-Agent Sequential Decision Making},
 url = {https://proceedings.neurips.cc/paper/2021/file/848c4965359e617d5e16c924b4a85fd9-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
Neurips,2021,PreferenceNet: Encoding Human Preferences in Auction Design with Deep Learning,https://proceedings.neurips.cc//paper/2021/hash/92977ae4d2ba21425a59afb269c2a14e-Abstract.html,"['Neehar Peri', ' Michael Curry', ' Samuel Dooley', ' John Dickerson']",9,"The design of optimal auctions is a problem of interest in economics, game theory and computer science. Despite decades of effort, strategyproof, revenue-maximizing auction designs are still not known outside of restricted settings. However, recent methods using deep learning have shown some success in approximating optimal auctions, recovering several known solutions and outperforming strong baselines when optimal auctions are not known. In addition to maximizing revenue, auction mechanisms may also seek to encourage socially desirable constraints such as allocation fairness or diversity. However, these philosophical notions neither have standardization nor do they have widely accepted formal definitions. In this paper, we propose PreferenceNet, an extension of existing neural-network-based auction mechanisms to encode constraints using (potentially human-provided) exemplars of desirable allocations. In addition, we introduce a new metric to evaluate an auction allocations' adherence to such socially desirable constraints and demonstrate that our proposed method is competitive with current state-of-the-art neural-network based auction designs. We validate our approach through human subject research and show that we are able to effectively capture real human preferences.",SCOPE,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2021_92977ae4,
 author = {Peri, Neehar and Curry, Michael and Dooley, Samuel and Dickerson, John},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {17532--17542},
 publisher = {Curran Associates, Inc.},
 title = {PreferenceNet: Encoding Human Preferences in Auction Design with Deep Learning},
 url = {https://proceedings.neurips.cc/paper/2021/file/92977ae4d2ba21425a59afb269c2a14e-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
Neurips,2021,A Unified Approach to Fair Online Learning via Blackwell Approachability,https://proceedings.neurips.cc//paper/2021/hash/97ea3cfb64eeaa1edba65501d0bb3c86-Abstract.html,"['Evgenii Chzhen', ' Christophe Giraud', ' Gilles Stoltz']",3,"We provide a setting and a general approach to fair online learning with stochastic sensitive and non-sensitive contexts.The setting is a repeated game between the Player and Nature, where at each stage both pick actions based on the contexts. Inspired by the notion of unawareness, we assume that the Player can only access the non-sensitive context before making a decision, while we discuss both cases of Nature accessing the sensitive contexts and Nature unaware of the sensitive contexts. Adapting Blackwell's approachability theory to handle the case of an unknown contexts' distribution, we provide a general necessary and sufficient condition for learning objectives to be compatible with some fairness constraints. This condition is instantiated on (group-wise) no-regret and (group-wise) calibration objectives, and on demographic parity as an additional constraint. When the objective is not compatible with the constraint, the provided framework permits to characterise the optimal trade-off between the two.",SCOPE,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2021_97ea3cfb,
 author = {Chzhen, Evgenii and Giraud, Christophe and Stoltz, Gilles},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {18280--18292},
 publisher = {Curran Associates, Inc.},
 title = {A Unified Approach to Fair Online Learning via Blackwell Approachability},
 url = {https://proceedings.neurips.cc/paper/2021/file/97ea3cfb64eeaa1edba65501d0bb3c86-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
Neurips,2021,On Large-Cohort Training for Federated Learning,https://proceedings.neurips.cc//paper/2021/hash/ab9ebd57177b5106ad7879f0896685d4-Abstract.html,"['Zachary Charles', ' Zachary Garrett', ' Zhouyuan Huo', ' Sergei Shmulyian', ' Virginia Smith']",29,"Federated learning methods typically learn a model by iteratively sampling updates from a population of clients. In this work, we explore how the number of clients sampled at each round (the cohort size) impacts the quality of the learned model and the training dynamics of federated learning algorithms. Our work poses three fundamental questions. First, what challenges arise when trying to scale federated learning to larger cohorts? Second, what parallels exist between cohort sizes in federated learning, and batch sizes in centralized learning? Last, how can we design federated learning methods that effectively utilize larger cohort sizes? We give partial answers to these questions based on extensive empirical evaluation. Our work highlights a number of challenges stemming from the use of larger cohorts. While some of these (such as generalization issues and diminishing returns) are analogs of large-batch training challenges, others (including catastrophic training failures and fairness concerns) are unique to federated learning.",SCOPE,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2021_ab9ebd57,
 author = {Charles, Zachary and Garrett, Zachary and Huo, Zhouyuan and Shmulyian, Sergei and Smith, Virginia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {20461--20475},
 publisher = {Curran Associates, Inc.},
 title = {On Large-Cohort Training for Federated Learning},
 url = {https://proceedings.neurips.cc/paper/2021/file/ab9ebd57177b5106ad7879f0896685d4-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
Neurips,2021,Fair Scheduling for Time-dependent Resources,https://proceedings.neurips.cc//paper/2021/hash/b5b1d9ada94bb80609d21eecf7a2ce7a-Abstract.html,"['Bo Li', ' Minming Li', ' Ruilong Zhang']",2,"We study a fair resource scheduling problem, where a set of interval jobs are to be allocated to heterogeneous machines controlled by intellectual agents.Each job is associated with release time, deadline, and processing time such that it can be processed if its complete processing period is between its release time and deadline. The machines gain possibly different utilities by processing different jobs, and all jobs assigned to the same machine should be processed without overlap.We consider two widely studied solution concepts, namely, maximin share fairness and envy-freeness.For both criteria, we discuss the extent to which fair allocations exist and present constant approximation algorithms for various settings.",SCOPE,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2021_b5b1d9ad,
 author = {Li, Bo and Li, Minming and Zhang, Ruilong},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {21744--21756},
 publisher = {Curran Associates, Inc.},
 title = {Fair Scheduling for Time-dependent Resources},
 url = {https://proceedings.neurips.cc/paper/2021/file/b5b1d9ada94bb80609d21eecf7a2ce7a-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
Neurips,2021,Fair Exploration via Axiomatic Bargaining,https://proceedings.neurips.cc//paper/2021/hash/b90c46963248e6d7aab1e0f429743ca0-Abstract.html,"['Jackie Baek', ' Vivek Farias']",9,"Motivated by the consideration of fairly sharing the cost of exploration between multiple groups in learning problems, we develop the Nash bargaining solution in the context of multi-armed bandits. Specifically, the 'grouped' bandit associated with any multi-armed bandit problem associates, with each time step, a single group from some finite set of groups. The utility gained by a given group under some learning policy is naturally viewed as the reduction in that group's regret relative to the regret that group would have incurred 'on its own'. We derive policies that yield the Nash bargaining solution relative to the set of incremental utilities possible under any policy. We show that on the one hand, the 'price of fairness' under such policies is limited, while on the other hand, regret optimal policies are arbitrarily unfair under generic conditions. Our theoretical development is complemented by a case study on contextual bandits for warfarin dosing where we are concerned with the cost of exploration across multiple races and age groups.",SCOPE,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2021_b90c4696,
 author = {Baek, Jackie and Farias, Vivek},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {22034--22045},
 publisher = {Curran Associates, Inc.},
 title = {Fair Exploration via Axiomatic Bargaining},
 url = {https://proceedings.neurips.cc/paper/2021/file/b90c46963248e6d7aab1e0f429743ca0-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
Neurips,2021,Fair Algorithms for Multi-Agent Multi-Armed Bandits,https://proceedings.neurips.cc//paper/2021/hash/c96ebeee051996333b6d70b2da6191b0-Abstract.html,"['Safwan Hossain', ' Evi Micha', ' Nisarg Shah']",12,"We propose a multi-agent variant of the classical multi-armed bandit problem, in which there are $N$ agents and $K$ arms, and pulling an arm generates a (possibly different) stochastic reward for each agent. Unlike the classical multi-armed bandit problem, the goal is not to learn the ""best arm""; indeed, each agent may perceive a different arm to be the best for her personally. Instead, we seek to learn a fair distribution over the arms. Drawing on a long line of research in economics and computer science, we use the Nash social welfare as our notion of fairness. We design multi-agent variants of three classic multi-armed bandit algorithms and show that they achieve sublinear regret, which is now measured in terms of the lost Nash social welfare. We also extend a classical lower bound, establishing the optimality of one of our algorithms.",SCOPE,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2021_c96ebeee,
 author = {Hossain, Safwan and Micha, Evi and Shah, Nisarg},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {24005--24017},
 publisher = {Curran Associates, Inc.},
 title = {Fair Algorithms for Multi-Agent Multi-Armed Bandits},
 url = {https://proceedings.neurips.cc/paper/2021/file/c96ebeee051996333b6d70b2da6191b0-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
Neurips,2021,Addressing Algorithmic Disparity and Performance Inconsistency in Federated Learning,https://proceedings.neurips.cc//paper/2021/hash/db8e1af0cb3aca1ae2d0018624204529-Abstract.html,"['Sen Cui', ' Weishen Pan', ' Jian Liang', ' Changshui Zhang', ' Fei Wang']",15,"Federated learning (FL) has gain growing interests for its capability of learning from distributed data sources collectively without the need of accessing the raw data samples across different sources. So far FL research has mostly focused on improving the performance, how the algorithmic disparity will be impacted for the model learned from FL and the impact of algorithmic disparity on the utility inconsistency are largely unexplored. In this paper, we propose an FL framework to jointly consider performance consistency and algorithmic fairness across different local clients (data sources). We derive our framework from a constrained multi-objective optimization perspective, in which we learn a model satisfying fairness constraints on all clients with consistent performance. Specifically, we treat the algorithm prediction loss at each local client as an objective and maximize the worst-performing client with fairness constraints through optimizing a surrogate maximum function with all objectives involved. A gradient-based procedure is employed to achieve the Pareto optimality of this optimization problem. Theoretical analysis is provided to prove that our method can converge to a Pareto solution that achieves the min-max performance with fairness constraints on all clients. Comprehensive experiments on synthetic and real-world datasets demonstrate the superiority that our approach over baselines and its effectiveness in achieving both fairness and consistency across all local clients.",SCOPE,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2021_db8e1af0,
 author = {Cui, Sen and Pan, Weishen and Liang, Jian and Zhang, Changshui and Wang, Fei},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {26091--26102},
 publisher = {Curran Associates, Inc.},
 title = {Addressing Algorithmic Disparity and Performance Inconsistency in Federated Learning},
 url = {https://proceedings.neurips.cc/paper/2021/file/db8e1af0cb3aca1ae2d0018624204529-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
Neurips,2021,Bandit Learning with Delayed Impact of Actions,https://proceedings.neurips.cc//paper/2021/hash/e17184bcb70dcf3942c54e0b537ffc6d-Abstract.html,"['Wei Tang', ' Chien-Ju Ho', ' Yang Liu']",5,"We consider a stochastic multi-armed bandit (MAB) problem with delayed impact of actions. In our setting, actions taken in the pastimpact the arm rewards in the subsequent future. This delayed impact of actions is prevalent in the real world. For example, the capability to pay back a loan for people in a certain social group might depend on historically how frequently that group has been approved loan applications. If banks keep rejecting loan applications to people in a disadvantaged group, it could create a feedback loop and further damage the chance of getting loans for people in that group. In this paper, we formulate this delayed and long-term impact of actions within the context of multi-armed bandits. We generalize the bandit setting to encode the dependency of this ``bias"" due to the action history during learning. The goal is to maximize the collected utilities over time while taking into account the dynamics created by the delayed impacts of historical actions. We propose an algorithm that achieves a regret of $\tilde{O}(KT^{2/3})$ and show a matching regret lower bound of $\Omega(KT^{2/3})$, where $K$ is the number of arms and $T$ is the learning horizon. Our results complement the bandit literature by adding techniques to deal with actions with long-term impacts and have implications in designing fair algorithms.",SCOPE,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2021_e17184bc,
 author = {Tang, Wei and Ho, Chien-Ju and Liu, Yang},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {26804--26817},
 publisher = {Curran Associates, Inc.},
 title = {Bandit Learning with Delayed Impact of Actions},
 url = {https://proceedings.neurips.cc/paper/2021/file/e17184bcb70dcf3942c54e0b537ffc6d-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
Neurips,2021,Online Market Equilibrium with Application to Fair Division,https://proceedings.neurips.cc//paper/2021/hash/e562cd9c0768d5464b64cf61da7fc6bb-Abstract.html,"['Yuan Gao', ' Alex Peysakhovich', ' Christian Kroer']",5,"Computing market equilibria is a problem of both theoretical and applied interest. Much research to date focuses on the case of static Fisher markets with full information on buyers' utility functions and item supplies. Motivated by real-world markets, we consider an online setting: individuals have linear, additive utility functions; items arrive sequentially and must be allocated and priced irrevocably. We define the notion of an online market equilibrium in such a market as time-indexed allocations and prices which guarantee buyer optimality and market clearance in hindsight. We propose a simple, scalable and interpretable allocation and pricing dynamics termed as PACE. When items are drawn i.i.d. from an unknown distribution (with a possibly continuous support), we show that PACE leads to an online market equilibrium asymptotically. In particular, PACE ensures that buyers' time-averaged utilities converge to the equilibrium utilities w.r.t. a static market with item supplies being the unknown distribution and that buyers' time-averaged expenditures converge to their per-period budget. Hence, many desirable properties of market equilibrium-based fair division such as envy-freeness, Pareto optimality, and the proportional-share guarantee are also attained asymptotically in the online setting. Next, we extend the dynamics to handle quasilinear buyer utilities, which gives the first online algorithm for computing first-price pacing equilibria. Finally, numerical experiments on real and synthetic datasets show that the dynamics converges quickly under various metrics.",SCOPE,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2021_e562cd9c,
 author = {Gao, Yuan and Peysakhovich, Alex and Kroer, Christian},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {27305--27318},
 publisher = {Curran Associates, Inc.},
 title = {Online Market Equilibrium with Application to Fair Division},
 url = {https://proceedings.neurips.cc/paper/2021/file/e562cd9c0768d5464b64cf61da7fc6bb-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
AIES,2018,Fairness in Relational Domains,https://dl.acm.org/doi/10.1145/3278721.3278733,"['Golnoosh Farnadi', 'Behrouz Babaki', 'Lise Getoor']",27,"AI and machine learning tools are being used with increasing frequency for decision making in domains that affect peoples' lives such as employment, education, policing and loan approval. These uses raise concerns about biases of algorithmic discrimination and have motivated the development of fairness-aware machine learning. However, existing fairness approaches are based solely on attributes of individuals. In many cases, discrimination is much more complex, and taking into account the social, organizational, and other connections between individuals is important. We introduce new notions of fairness that are able to capture the relational structure in a domain. We use first-order logic to provide a flexible and expressive language for specifying complex relational patterns of discrimination. Furthermore, we extend an existing statistical relational learning framework, probabilistic soft logic (PSL), to incorporate our definition of relational fairness. We refer to this fairness-aware framework FairPSL. FairPSL makes use of the logical definitions of fairnesss but also supports a probabilistic interpretation. In particular, we show how to perform maximum a posteriori(MAP) inference by exploiting probabilistic dependencies within the domain while avoiding violation of fairness guarantees. Preliminary empirical evaluation shows that we are able to make both accurate and fair decisions.",TRAD,,Metrics,Algos,,,,,,,,,,,,,"@inproceedings{10.1145/3278721.3278733,
author = {Farnadi, Golnoosh and Babaki, Behrouz and Getoor, Lise},
title = {Fairness in Relational Domains},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278733},
doi = {10.1145/3278721.3278733},
abstract = {AI and machine learning tools are being used with increasing frequency for decision making in domains that affect peoples' lives such as employment, education, policing and loan approval. These uses raise concerns about biases of algorithmic discrimination and have motivated the development of fairness-aware machine learning. However, existing fairness approaches are based solely on attributes of individuals. In many cases, discrimination is much more complex, and taking into account the social, organizational, and other connections between individuals is important. We introduce new notions of fairness that are able to capture the relational structure in a domain. We use first-order logic to provide a flexible and expressive language for specifying complex relational patterns of discrimination. Furthermore, we extend an existing statistical relational learning framework, probabilistic soft logic (PSL), to incorporate our definition of relational fairness. We refer to this fairness-aware framework FairPSL. FairPSL makes use of the logical definitions of fairnesss but also supports a probabilistic interpretation. In particular, we show how to perform maximum a posteriori(MAP) inference by exploiting probabilistic dependencies within the domain while avoiding violation of fairness guarantees. Preliminary empirical evaluation shows that we are able to make both accurate and fair decisions.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {108–114},
numpages = {7},
keywords = {fairness, probabilistic soft logic, statistical relational learning},
location = {New Orleans, LA, USA},
series = {AIES '18}
}
  
"
AIES,2018,Non-Discriminatory Machine Learning through Convex Fairness Criteria,https://dl.acm.org/doi/10.1145/3278721.3278722,"['Naman Goel', 'Mohammad Yaghini', 'Boi Faltings']",73,We introduce a novel technique to achieve non-discrimination in machine learning without sacrificing convexity and probabilistic interpretation. We also propose a new notion of fairness for machine learning called the weighted proportional fairness and show that our technique satisfies this subjective fairness criterion.,TRAD,,Algos,Metrics,,,,,,,,,,,,,"@inproceedings{10.1145/3278721.3278722,
author = {Goel, Naman and Yaghini, Mohammad and Faltings, Boi},
title = {Non-Discriminatory Machine Learning through Convex Fairness Criteria},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278722},
doi = {10.1145/3278721.3278722},
abstract = {We introduce a novel technique to achieve non-discrimination in machine learning without sacrificing convexity and probabilistic interpretation. We also propose a new notion of fairness for machine learning called the weighted proportional fairness and show that our technique satisfies this subjective fairness criterion.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {116},
numpages = {1},
keywords = {non-discrimination, machine learning, proportional fairness},
location = {New Orleans, LA, USA},
series = {AIES '18}
}
  
"
AIES,2018,Fair Forests: Regularized Tree Induction to Minimize Model Bias,https://dl.acm.org/doi/10.1145/3278721.3278742,"['Edward Raff', 'Jared Sylvester', 'Steven Mills']",58,"The potential lack of fairness in the outputs of machine learning algorithms has recently gained attention both within the research community as well as in society more broadly. Surprisingly, there is no prior work developing tree-induction algorithms for building fair decision trees or fair random forests. These methods have widespread popularity as they are one of the few to be simultaneously interpretable, non-linear, and easy-to-use. In this paper we develop, to our knowledge, the first technique for the induction of fair decision trees.We show that our ""Fair Forest"" retains the benefits of the tree-based approach, while providing both greater accuracy and fairness than other alternatives, for both ""group fairness'' and ""individual fairness.'' We also introduce new measures for fairness which are able to handle multinomial and continues attributes as well as regression problems, as opposed to binary attributes and labels only. Finally, we demonstrate a new, more robust evaluation procedure for algorithms that considers the dataset in its entirety rather than only a specific protected attribute.",TRAD,,Algorithm,Metrics,,,,,,,,,,,,,"@inproceedings{10.1145/3278721.3278742,
author = {Raff, Edward and Sylvester, Jared and Mills, Steven},
title = {Fair Forests: Regularized Tree Induction to Minimize Model Bias},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278742},
doi = {10.1145/3278721.3278742},
abstract = {The potential lack of fairness in the outputs of machine learning algorithms has recently gained attention both within the research community as well as in society more broadly. Surprisingly, there is no prior work developing tree-induction algorithms for building fair decision trees or fair random forests. These methods have widespread popularity as they are one of the few to be simultaneously interpretable, non-linear, and easy-to-use. In this paper we develop, to our knowledge, the first technique for the induction of fair decision trees.We show that our ""Fair Forest"" retains the benefits of the tree-based approach, while providing both greater accuracy and fairness than other alternatives, for both ""group fairness'' and ""individual fairness.'' We also introduce new measures for fairness which are able to handle multinomial and continues attributes as well as regression problems, as opposed to binary attributes and labels only. Finally, we demonstrate a new, more robust evaluation procedure for algorithms that considers the dataset in its entirety rather than only a specific protected attribute.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {243–250},
numpages = {8},
keywords = {feature importance, random forest, fairness},
location = {New Orleans, LA, USA},
series = {AIES '18}
}
  
"
AIES,2018,Mitigating Unwanted Biases with Adversarial Learning,https://dl.acm.org/doi/10.1145/3278721.3278779,"['Brian Hu Zhang', 'Blake Lemoine', 'Margaret Mitchell']",906,"Machine learning is a tool for building models that accurately represent input training data. When undesired biases concerning demographic groups are in the training data, well-trained models will reflect those biases. We present a framework for mitigating such biases by including a variable for the group of interest and simultaneously learning a predictor and an adversary. The input to the network X, here text or census data, produces a prediction Y, such as an analogy completion or income bracket, while the adversary tries to model a protected variable Z, here gender or zip code. The objective is to maximize the predictor's ability to predict Y while minimizing the adversary's ability to predict Z. Applied to analogy completion, this method results in accurate predictions that exhibit less evidence of stereotyping Z. When applied to a classification task using the UCI Adult (Census) Dataset, it results in a predictive model that does not lose much accuracy while achieving very close to equality of odds (Hardt, et al., 2016). The method is flexible and applicable to multiple definitions of fairness as well as a wide range of gradient-based learning models, including both regression and classification tasks.",TRAD,,Algorithm,Data,,,,,,,,,,,,,"@inproceedings{10.1145/3278721.3278779,
author = {Zhang, Brian Hu and Lemoine, Blake and Mitchell, Margaret},
title = {Mitigating Unwanted Biases with Adversarial Learning},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278779},
doi = {10.1145/3278721.3278779},
abstract = {Machine learning is a tool for building models that accurately represent input training data. When undesired biases concerning demographic groups are in the training data, well-trained models will reflect those biases. We present a framework for mitigating such biases by including a variable for the group of interest and simultaneously learning a predictor and an adversary. The input to the network X, here text or census data, produces a prediction Y, such as an analogy completion or income bracket, while the adversary tries to model a protected variable Z, here gender or zip code. The objective is to maximize the predictor's ability to predict Y while minimizing the adversary's ability to predict Z. Applied to analogy completion, this method results in accurate predictions that exhibit less evidence of stereotyping Z. When applied to a classification task using the UCI Adult (Census) Dataset, it results in a predictive model that does not lose much accuracy while achieving very close to equality of odds (Hardt, et al., 2016). The method is flexible and applicable to multiple definitions of fairness as well as a wide range of gradient-based learning models, including both regression and classification tasks.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {335–340},
numpages = {6},
keywords = {multi-task learning, unbiasing, debiasing, adversarial learning},
location = {New Orleans, LA, USA},
series = {AIES '18}
}
  
"
AIES,2019,Active Fairness in Algorithmic Decision Making,https://dl.acm.org/doi/10.1145/3306618.3314277,"['Alejandro Noriega-Campero', 'Michiel A. Bakker', 'Bernardo Garcia-Bulle', ""Alex 'Sandy' Pentland""]",71,"Society increasingly relies on machine learning models for automated decision making. Yet, efficiency gains from automation have come paired with concern for algorithmic discrimination that can systematize inequality. Recent work has proposed optimal post-processing methods that randomize classification decisions for a fraction of individuals, in order to achieve fairness measures related to parity in errors and calibration. These methods, however, have raised concern due to the information inefficiency, intra-group unfairness, and Pareto sub-optimality they entail. The present work proposes an alternativeactive framework for fair classification, where, in deployment, a decision-maker adaptively acquires information according to the needs of different groups or individuals, towards balancing disparities in classification performance. We propose two such methods, where information collection is adapted to group- and individual-level needs respectively. We show on real-world datasets that these can achieve: 1) calibration and single error parity (e.g.,equal opportunity ); and 2) parity in both false positive and false negative rates (i.e.,equal odds ). Moreover, we show that by leveraging their additional degree of freedom,active approaches can substantially outperform randomization-based classifiers previously considered optimal, while avoiding limitations such as intra-group unfairness.",TRAD,,Algorithm,,,Might need a second look,,,,,,,,,,,"@inproceedings{10.1145/3306618.3314277,
author = {Noriega-Campero, Alejandro and Bakker, Michiel A. and Garcia-Bulle, Bernardo and Pentland, Alex 'Sandy'},
title = {Active Fairness in Algorithmic Decision Making},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314277},
doi = {10.1145/3306618.3314277},
abstract = {Society increasingly relies on machine learning models for automated decision making. Yet, efficiency gains from automation have come paired with concern for algorithmic discrimination that can systematize inequality. Recent work has proposed optimal post-processing methods that randomize classification decisions for a fraction of individuals, in order to achieve fairness measures related to parity in errors and calibration. These methods, however, have raised concern due to the information inefficiency, intra-group unfairness, and Pareto sub-optimality they entail. The present work proposes an alternativeactive framework for fair classification, where, in deployment, a decision-maker adaptively acquires information according to the needs of different groups or individuals, towards balancing disparities in classification performance. We propose two such methods, where information collection is adapted to group- and individual-level needs respectively. We show on real-world datasets that these can achieve: 1) calibration and single error parity (e.g.,equal opportunity ); and 2) parity in both false positive and false negative rates (i.e.,equal odds ). Moreover, we show that by leveraging their additional degree of freedom,active approaches can substantially outperform randomization-based classifiers previously considered optimal, while avoiding limitations such as intra-group unfairness.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {77–83},
numpages = {7},
keywords = {algorithmic fairness, active feature acquisition, adaptive inquiry},
location = {Honolulu, HI, USA},
series = {AIES '19}
}
  
"
AIES,2019,Loss-Aversively Fair Classification,https://dl.acm.org/doi/10.1145/3306618.3314266,"['Junaid Ali', 'Muhammad Bilal Zafar', 'Adish Singla', 'Krishna P. Gummadi']",14,"The use of algorithmic (learning-based) decision making in scenarios that affect human lives has motivated a number of recent studies to investigate such decision making systems for potential unfairness, such as discrimination against subjects based on their sensitive features like gender or race. However, when judging the fairness of a newly designed decision making system, these studies have overlooked an important influence on people's perceptions of fairness, which is how the new algorithm changes the status quo, i.e., decisions of the existing decision making system. Motivated by extensive literature in behavioral economics and behavioral psychology (prospect theory), we propose a notion of fair updates that we refer to as loss-averse updates. Loss-averse updates constrain the updates to yield improved (more beneficial) outcomes to subjects compared to the status quo. We propose tractable proxy measures that would allow this notion to be incorporated in the training of a variety of linear and non-linear classifiers. We show how our proxy measures can be combined with existing measures for training nondiscriminatory classifiers.Our evaluation using synthetic and real-world datasets demonstrates that the proposed proxy measures are effective for their desired tasks.",TRAD,,Metrics,Algorithms,,,,,,,,,,,,,"@inproceedings{10.1145/3306618.3314266,
author = {Ali, Junaid and Zafar, Muhammad Bilal and Singla, Adish and Gummadi, Krishna P.},
title = {Loss-Aversively Fair Classification},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314266},
doi = {10.1145/3306618.3314266},
abstract = {The use of algorithmic (learning-based) decision making in scenarios that affect human lives has motivated a number of recent studies to investigate such decision making systems for potential unfairness, such as discrimination against subjects based on their sensitive features like gender or race. However, when judging the fairness of a newly designed decision making system, these studies have overlooked an important influence on people's perceptions of fairness, which is how the new algorithm changes the status quo, i.e., decisions of the existing decision making system. Motivated by extensive literature in behavioral economics and behavioral psychology (prospect theory), we propose a notion of fair updates that we refer to as loss-averse updates. Loss-averse updates constrain the updates to yield improved (more beneficial) outcomes to subjects compared to the status quo. We propose tractable proxy measures that would allow this notion to be incorporated in the training of a variety of linear and non-linear classifiers. We show how our proxy measures can be combined with existing measures for training nondiscriminatory classifiers.Our evaluation using synthetic and real-world datasets demonstrates that the proposed proxy measures are effective for their desired tasks.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {211–218},
numpages = {8},
keywords = {algorithmic fairness, fairness in machine learning, loss-averse fairness, fair updates},
location = {Honolulu, HI, USA},
series = {AIES '19}
}
  
"
AIES,2019,Counterfactual Fairness in Text Classification through Robustness,https://dl.acm.org/doi/10.1145/3306618.3317950,"['Sahaj Garg', 'Vincent Perot', 'Nicole Limtiaco', 'Ankur Taly', 'Ed H. Chi', 'Alex Beutel']",1216,"In this paper, we study counterfactual fairness in text classification, which asks the question: How would the prediction change if the sensitive attribute referenced in the example were different? Toxicity classifiers demonstrate a counterfactual fairness issue by predicting that ""Some people are gay"" is toxic while ""Some people are straight"" is nontoxic. We offer a metric, counterfactual token fairness (CTF), for measuring this particular form of fairness in text classifiers, and describe its relationship with group fairness. Further, we offer three approaches, blindness, counterfactual augmentation, and counterfactual logit pairing (CLP), for optimizing counterfactual token fairness during training, bridging the robustness and fairness literature. Empirically, we find that blindness and CLP address counterfactual token fairness. The methods do not harm classifier performance, and have varying tradeoffs with group fairness. These approaches, both for measurement and optimization, provide a new path forward for addressing fairness concerns in text classification.",TRAD,,Algorithm,,Enforces counterfactual fairness in text classification prolems,,,,,,,,,,,,"@inproceedings{10.1145/3306618.3317950,
author = {Garg, Sahaj and Perot, Vincent and Limtiaco, Nicole and Taly, Ankur and Chi, Ed H. and Beutel, Alex},
title = {Counterfactual Fairness in Text Classification through Robustness},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3317950},
doi = {10.1145/3306618.3317950},
abstract = {In this paper, we study counterfactual fairness in text classification, which asks the question: How would the prediction change if the sensitive attribute referenced in the example were different? Toxicity classifiers demonstrate a counterfactual fairness issue by predicting that ""Some people are gay"" is toxic while ""Some people are straight"" is nontoxic. We offer a metric, counterfactual token fairness (CTF), for measuring this particular form of fairness in text classifiers, and describe its relationship with group fairness. Further, we offer three approaches, blindness, counterfactual augmentation, and counterfactual logit pairing (CLP), for optimizing counterfactual token fairness during training, bridging the robustness and fairness literature. Empirically, we find that blindness and CLP address counterfactual token fairness. The methods do not harm classifier performance, and have varying tradeoffs with group fairness. These approaches, both for measurement and optimization, provide a new path forward for addressing fairness concerns in text classification.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {219–226},
numpages = {8},
keywords = {counterfactual fairness, text classification, robustness, fairness},
location = {Honolulu, HI, USA},
series = {AIES '19}
}
  
"
AIES,2019,Taking Advantage of Multitask Learning for Fair Classification,https://dl.acm.org/doi/10.1145/3306618.3314255,"['Luca Oneto', 'Michele Doninini', 'Amon Elders', 'Massimiliano Pontil']",51,"A central goal of algorithmic fairness is to reduce bias in automated decision making. An unavoidable tension exists between accuracy gains obtained by using sensitive information as part of a statistical model, and any commitment to protect these characteristics. Often, due to biases present in the data, using the sensitive information in the functional form of a classifier improves classification accuracy. In this paper we show how it is possible to get the best of both worlds: optimize model accuracy and fairness without explicitly using the sensitive feature in the functional form of the model, thereby treating different individuals equally. Our method is based on two key ideas. On the one hand, we propose to use Multitask Learning (MTL), enhanced with fairness constraints, to jointly learn group specific classifiers that leverage information between sensitive groups. On the other hand, since learning group specific models might not be permitted, we propose to first predict the sensitive features by any learning method and then to use the predicted sensitive feature to train MTL with fairness constraints. This enables us to tackle fairness with a three-pronged approach, that is, by increasing accuracy on each group, enforcing measures of fairness during training, and protecting sensitive information during testing. Experimental results on two real datasets support our proposal, showing substantial improvements in both accuracy and fairness.",TRAD,,Algorithm,,,,,,,,,,,,,,"@inproceedings{10.1145/3306618.3314255,
author = {Oneto, Luca and Doninini, Michele and Elders, Amon and Pontil, Massimiliano},
title = {Taking Advantage of Multitask Learning for Fair Classification},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314255},
doi = {10.1145/3306618.3314255},
abstract = {A central goal of algorithmic fairness is to reduce bias in automated decision making. An unavoidable tension exists between accuracy gains obtained by using sensitive information as part of a statistical model, and any commitment to protect these characteristics. Often, due to biases present in the data, using the sensitive information in the functional form of a classifier improves classification accuracy. In this paper we show how it is possible to get the best of both worlds: optimize model accuracy and fairness without explicitly using the sensitive feature in the functional form of the model, thereby treating different individuals equally. Our method is based on two key ideas. On the one hand, we propose to use Multitask Learning (MTL), enhanced with fairness constraints, to jointly learn group specific classifiers that leverage information between sensitive groups. On the other hand, since learning group specific models might not be permitted, we propose to first predict the sensitive features by any learning method and then to use the predicted sensitive feature to train MTL with fairness constraints. This enables us to tackle fairness with a three-pronged approach, that is, by increasing accuracy on each group, enforcing measures of fairness during training, and protecting sensitive information during testing. Experimental results on two real datasets support our proposal, showing substantial improvements in both accuracy and fairness.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {227–237},
numpages = {11},
keywords = {multitask learning, classification, fairness},
location = {Honolulu, HI, USA},
series = {AIES '19}
}
  
"
AIES,2019,Multiaccuracy: Black-Box Post-Processing for Fairness in Classification,https://dl.acm.org/doi/10.1145/3306618.3314287,"['Michael P. Kim', 'Amirata Ghorbani', 'James Zou']",199,"Prediction systems are successfully deployed in applications ranging from disease diagnosis, to predicting credit worthiness, to image recognition. Even when the overall accuracy is high, these systems may exhibit systematic biases that harm specific subpopulations; such biases may arise inadvertently due to underrepresentation in the data used to train a machine-learning model, or as the result of intentional malicious discrimination. We develop a rigorous framework of *multiaccuracy* auditing and post-processing to ensure accurate predictions across *identifiable subgroups*. Our algorithm, MULTIACCURACY-BOOST, works in any setting where we have black-box access to a predictor and a relatively small set of labeled data for auditing; importantly, this black-box framework allows for improved fairness and accountability of predictions, even when the predictor is minimally transparent. We prove that MULTIACCURACY-BOOST converges efficiently and show that if the initial model is accurate on an identifiable subgroup, then the post-processed model will be also. We experimentally demonstrate the effectiveness of the approach to improve the accuracy among minority subgroups in diverse applications (image classification, finance, population health). Interestingly, MULTIACCURACY-BOOST can improve subpopulation accuracy (e.g. for ""black women"") even when the sensitive features (e.g. ""race"", ""gender"") are not given to the algorithm explicitly.",TRAD,,Algorithm,,,,,,,,,,,,,,"@inproceedings{10.1145/3306618.3314287,
author = {Kim, Michael P. and Ghorbani, Amirata and Zou, James},
title = {Multiaccuracy: Black-Box Post-Processing for Fairness in Classification},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314287},
doi = {10.1145/3306618.3314287},
abstract = {Prediction systems are successfully deployed in applications ranging from disease diagnosis, to predicting credit worthiness, to image recognition. Even when the overall accuracy is high, these systems may exhibit systematic biases that harm specific subpopulations; such biases may arise inadvertently due to underrepresentation in the data used to train a machine-learning model, or as the result of intentional malicious discrimination. We develop a rigorous framework of *multiaccuracy* auditing and post-processing to ensure accurate predictions across *identifiable subgroups*. Our algorithm, MULTIACCURACY-BOOST, works in any setting where we have black-box access to a predictor and a relatively small set of labeled data for auditing; importantly, this black-box framework allows for improved fairness and accountability of predictions, even when the predictor is minimally transparent. We prove that MULTIACCURACY-BOOST converges efficiently and show that if the initial model is accurate on an identifiable subgroup, then the post-processed model will be also. We experimentally demonstrate the effectiveness of the approach to improve the accuracy among minority subgroups in diverse applications (image classification, finance, population health). Interestingly, MULTIACCURACY-BOOST can improve subpopulation accuracy (e.g. for ""black women"") even when the sensitive features (e.g. ""race"", ""gender"") are not given to the algorithm explicitly.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {247–254},
numpages = {8},
keywords = {discrimination, post-processing, machine learning, fairness},
location = {Honolulu, HI, USA},
series = {AIES '19}
}
  
"
AIES,2019,"Putting Fairness Principles into Practice: Challenges, Metrics, and Improvements",https://dl.acm.org/doi/10.1145/3306618.3314234,"['Alex Beutel', 'Jilin Chen', 'Tulsee Doshi', 'Hai Qian', 'Allison Woodruff', 'Christine Luu', 'Pierre Kreitmann', 'Jonathan Bischof', 'Ed H. Chi']",97,"As more researchers have become aware of and passionate about algorithmic fairness, there has been an explosion in papers laying out new metrics, suggesting algorithms to address issues, and calling attention to issues in existing applications of machine learning. This research has greatly expanded our understanding of the concerns and challenges in deploying machine learning, but there has been much less work in seeing how the rubber meets the road. In this paper we provide a case-study on the application of fairness in machine learning research to a production classification system, and offer new insights in how to measure and address algorithmic fairness issues. We discuss open questions in implementing equality of opportunity and describe our fairness metric, conditional equality, that takes into account distributional differences. Further, we provide a new approach to improve on the fairness metric during model training and demonstrate its efficacy in improving performance for a real-world product.",TRAD,,Algorithm,Metrics,,,,,,,,,,,,,"@inproceedings{10.1145/3306618.3314234,
author = {Beutel, Alex and Chen, Jilin and Doshi, Tulsee and Qian, Hai and Woodruff, Allison and Luu, Christine and Kreitmann, Pierre and Bischof, Jonathan and Chi, Ed H.},
title = {Putting Fairness Principles into Practice: Challenges, Metrics, and Improvements},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314234},
doi = {10.1145/3306618.3314234},
abstract = {As more researchers have become aware of and passionate about algorithmic fairness, there has been an explosion in papers laying out new metrics, suggesting algorithms to address issues, and calling attention to issues in existing applications of machine learning. This research has greatly expanded our understanding of the concerns and challenges in deploying machine learning, but there has been much less work in seeing how the rubber meets the road. In this paper we provide a case-study on the application of fairness in machine learning research to a production classification system, and offer new insights in how to measure and address algorithmic fairness issues. We discuss open questions in implementing equality of opportunity and describe our fairness metric, conditional equality, that takes into account distributional differences. Further, we provide a new approach to improve on the fairness metric during model training and demonstrate its efficacy in improving performance for a real-world product.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {453–459},
numpages = {7},
keywords = {equality of opportunity, classification, fairness},
location = {Honolulu, HI, USA},
series = {AIES '19}
}
  
"
AIES,2019,Fairness Criteria for Face Recognition Applications,https://dl.acm.org/doi/10.1145/3306618.3314308,['Filip Michalsky'],0,"Nowadays, machine learning algorithms play an important role in our daily lives and it is important to ensure their fairness and transparency. A number of methodologies for evaluating machine learning fairness have been introduced in the literature. In this research we propose a systematic confidence evaluation approach to measure fairness discrepancies of our deep learning architecture for image recognition using UTKFace database.",TRAD,,Metrics,,,,,,,,,,,,,,"@inproceedings{10.1145/3306618.3314308,
author = {Michalsky, Filip},
title = {Fairness Criteria for Face Recognition Applications},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314308},
doi = {10.1145/3306618.3314308},
abstract = {Nowadays, machine learning algorithms play an important role in our daily lives and it is important to ensure their fairness and transparency. A number of methodologies for evaluating machine learning fairness have been introduced in the literature. In this research we propose a systematic confidence evaluation approach to measure fairness discrepancies of our deep learning architecture for image recognition using UTKFace database.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {527–528},
numpages = {2},
keywords = {machine learning, fairness, neural networks, face recognition, datasets},
location = {Honolulu, HI, USA},
series = {AIES '19}
}
  
"
AIES,2020,CERTIFAI: A Common Framework to Provide Explanations and Analyse the Fairness and Robustness of Black-box Models,https://dl.acm.org/doi/10.1145/3375627.3375812,"['Shubham Sharma', 'Jette Henderson', 'Joydeep Ghosh']",73,"Concerns within the machine learning community and external pressures from regulators over the vulnerabilities of machine learning algorithms have spurred on the fields of explainability, robustness, and fairness. Often, issues in explainability, robustness, and fairness are confined to their specific sub-fields and few tools exist for model developers to use to simultaneously build their modeling pipelines in a transparent, accountable, and fair way. This can lead to a bottleneck on the model developer's side as they must juggle multiple methods to evaluate their algorithms. In this paper, we present a single framework for analyzing the robustness, fairness, and explainability of a classifier. The framework, which is based on the generation of counterfactual explanations through a custom genetic algorithm, is flexible, model-agnostic, and does not require access to model internals. The framework allows the user to calculate robustness and fairness scores for individual models and generate explanations for individual predictions which provide a means for actionable recourse (changes to an input to help get a desired outcome). This is the first time that a unified tool has been developed to address three key issues pertaining towards building a responsible artificial intelligence system.",TRAD,,,,Measurement,,,,,,,,,,,,"@inproceedings{10.1145/3375627.3375812,
author = {Sharma, Shubham and Henderson, Jette and Ghosh, Joydeep},
title = {CERTIFAI: A Common Framework to Provide Explanations and Analyse the Fairness and Robustness of Black-Box Models},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375812},
doi = {10.1145/3375627.3375812},
abstract = {Concerns within the machine learning community and external pressures from regulators over the vulnerabilities of machine learning algorithms have spurred on the fields of explainability, robustness, and fairness. Often, issues in explainability, robustness, and fairness are confined to their specific sub-fields and few tools exist for model developers to use to simultaneously build their modeling pipelines in a transparent, accountable, and fair way. This can lead to a bottleneck on the model developer's side as they must juggle multiple methods to evaluate their algorithms. In this paper, we present a single framework for analyzing the robustness, fairness, and explainability of a classifier. The framework, which is based on the generation of counterfactual explanations through a custom genetic algorithm, is flexible, model-agnostic, and does not require access to model internals. The framework allows the user to calculate robustness and fairness scores for individual models and generate explanations for individual predictions which provide a means for actionable recourse (changes to an input to help get a desired outcome). This is the first time that a unified tool has been developed to address three key issues pertaining towards building a responsible artificial intelligence system.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {166–172},
numpages = {7},
keywords = {explainability, fairness, machine learning, robust-ness, responsible artificial intelligence},
location = {New York, NY, USA},
series = {AIES '20}
}
  
"
AIES,2020,A Just Approach Balancing Rawlsian Leximax Fairness and Utilitarianism,https://dl.acm.org/doi/10.1145/3375627.3375844,"['Violet (Xinying) Chen', 'J. N. Hooker']",11,"Numerous AI-assisted resource allocation decisions need to balance the conflicting goals of fairness and efficiency. Our paper studies the challenging task of defining and modeling a proper fairness-efficiency trade off. We define fairness with Rawlsian leximax fairness, which views the lexicographic maximum among all feasible outcomes as the most equitable; and define efficiency with Utilitarianism, which seeks to maximize the sum of utilities received by entities regardless of individual differences. Motivated by a justice-driven trade off principle: prioritize fairness to benefit the less advantaged unless too much efficiency is sacrificed, we propose a sequential optimization procedure to balance leximax fairness and utilitarianism in decision-making. Each iteration of our approach maximizes a social welfare function, and we provide a practical mixed integer/linear programming (MILP) formulation for each maximization problem. We illustrate our method on a budget allocation example. Compared with existing approaches of balancing equity and efficiency, our method is more interpretable in terms of parameter selection, and incorporates a strong equity criterion with a thoroughly balanced perspective.",TRAD,,Metrics,Algorithms,,,,,,,,,,,,,"@inproceedings{10.1145/3375627.3375844,
author = {Chen, Violet (Xinying) and Hooker, J. N.},
title = {A Just Approach Balancing Rawlsian Leximax Fairness and Utilitarianism},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375844},
doi = {10.1145/3375627.3375844},
abstract = {Numerous AI-assisted resource allocation decisions need to balance the conflicting goals of fairness and efficiency. Our paper studies the challenging task of defining and modeling a proper fairness-efficiency trade off. We define fairness with Rawlsian leximax fairness, which views the lexicographic maximum among all feasible outcomes as the most equitable; and define efficiency with Utilitarianism, which seeks to maximize the sum of utilities received by entities regardless of individual differences. Motivated by a justice-driven trade off principle: prioritize fairness to benefit the less advantaged unless too much efficiency is sacrificed, we propose a sequential optimization procedure to balance leximax fairness and utilitarianism in decision-making. Each iteration of our approach maximizes a social welfare function, and we provide a practical mixed integer/linear programming (MILP) formulation for each maximization problem. We illustrate our method on a budget allocation example. Compared with existing approaches of balancing equity and efficiency, our method is more interpretable in terms of parameter selection, and incorporates a strong equity criterion with a thoroughly balanced perspective.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {221–227},
numpages = {7},
keywords = {distributive justice, trade off, utilitarianism, fairness},
location = {New York, NY, USA},
series = {AIES '20}
}
  
"
AIES,2020,A Geometric Solution to Fair Representations,https://dl.acm.org/doi/10.1145/3375627.3375864,"['Yuzi He', 'Keith Burghardt', 'Kristina Lerman']",17,"To reduce human error and prejudice, many high-stakes decisions have been turned over to machine algorithms. However, recent research suggests that this does not remove discrimination, and can perpetuate harmful stereotypes. While algorithms have been developed to improve fairness, they typically face at least one of three shortcomings: they are not interpretable, their prediction quality deteriorates quickly compared to unbiased equivalents, and %the methodology cannot easily extend other algorithms they are not easily transferable across models% (e.g., methods to reduce bias in random forests cannot be extended to neural networks) . To address these shortcomings, we propose a geometric method that removes correlations between data and any number of protected variables. Further, we can control the strength of debiasing through an adjustable parameter to address the trade-off between prediction quality and fairness. The resulting features are interpretable and can be used with many popular models, such as linear regression, random forest, and multilayer perceptrons. The resulting predictions are found to be more accurate and fair compared to several state-of-the-art fair AI algorithms across a variety of benchmark datasets. Our work shows that debiasing data is a simple and effective solution toward improving fairness.",TRAD,,Data,,,Debiasing,,,,,,,,,,,"@inproceedings{10.1145/3375627.3375864,
author = {He, Yuzi and Burghardt, Keith and Lerman, Kristina},
title = {A Geometric Solution to Fair Representations},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375864},
doi = {10.1145/3375627.3375864},
abstract = {To reduce human error and prejudice, many high-stakes decisions have been turned over to machine algorithms. However, recent research suggests that this does not remove discrimination, and can perpetuate harmful stereotypes. While algorithms have been developed to improve fairness, they typically face at least one of three shortcomings: they are not interpretable, their prediction quality deteriorates quickly compared to unbiased equivalents, and %the methodology cannot easily extend other algorithms they are not easily transferable across models% (e.g., methods to reduce bias in random forests cannot be extended to neural networks) . To address these shortcomings, we propose a geometric method that removes correlations between data and any number of protected variables. Further, we can control the strength of debiasing through an adjustable parameter to address the trade-off between prediction quality and fairness. The resulting features are interpretable and can be used with many popular models, such as linear regression, random forest, and multilayer perceptrons. The resulting predictions are found to be more accurate and fair compared to several state-of-the-art fair AI algorithms across a variety of benchmark datasets. Our work shows that debiasing data is a simple and effective solution toward improving fairness.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {279–285},
numpages = {7},
keywords = {interpretable method, fair classification, geometric method, sensitive information, projection, debiased features, fair ai, orthogonal space},
location = {New York, NY, USA},
series = {AIES '20}
}
  
"
AIES,2021,Minimax Group Fairness: Algorithms and Experiments,https://dl.acm.org/doi/10.1145/3461702.3462523,"['Emily Diana', 'Wesley Gill', 'Michael Kearns', 'Krishnaram Kenthapadi', 'Aaron Roth']",34,"We consider a recently introduced framework in which fairness is measured by worst-case outcomes across groups, rather than by the more standard differences between group outcomes. In this framework we provide provably convergent oracle-efficient learning algorithms (or equivalently, reductions to non-fair learning) for minimax group fairness. Here the goal is that of minimizing the maximum loss across all groups, rather than equalizing group losses. Our algorithms apply to both regression and classification settings and support both overall error and false positive or false negative rates as the fairness measure of interest. They also support relaxations of the fairness constraints, thus permitting study of the tradeoff between overall accuracy and minimax fairness. We compare the experimental behavior and performance of our algorithms across a variety of fairness-sensitive data sets and show empirical cases in which minimax fairness is strictly and strongly preferable to equal outcome notions.",TRAD,,Metrics,,,,,,,,,,,,,,"@inproceedings{10.1145/3461702.3462523,
author = {Diana, Emily and Gill, Wesley and Kearns, Michael and Kenthapadi, Krishnaram and Roth, Aaron},
title = {Minimax Group Fairness: Algorithms and Experiments},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462523},
doi = {10.1145/3461702.3462523},
abstract = {We consider a recently introduced framework in which fairness is measured by worst-case outcomes across groups, rather than by the more standard differences between group outcomes. In this framework we provide provably convergent oracle-efficient learning algorithms (or equivalently, reductions to non-fair learning) for minimax group fairness. Here the goal is that of minimizing the maximum loss across all groups, rather than equalizing group losses. Our algorithms apply to both regression and classification settings and support both overall error and false positive or false negative rates as the fairness measure of interest. They also support relaxations of the fairness constraints, thus permitting study of the tradeoff between overall accuracy and minimax fairness. We compare the experimental behavior and performance of our algorithms across a variety of fairness-sensitive data sets and show empirical cases in which minimax fairness is strictly and strongly preferable to equal outcome notions.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {66–76},
numpages = {11},
keywords = {fair machine learning, game theory, minimax fairness},
location = {Virtual Event, USA},
series = {AIES '21}
}
  
"
AIES,2021,Accounting for Model Uncertainty in Algorithmic Discrimination,https://dl.acm.org/doi/10.1145/3461702.3462630,"['Junaid Ali', 'Preethi Lahoti', 'Krishna P. Gummadi']",4,"Traditional approaches to ensure group fairness in algorithmic decision making aim to equalize ""total"" error rates for different subgroups in the population. In contrast, we argue that the fairness approaches should instead focus only on equalizing errors arising due to model uncertainty (a.k.a epistemic uncertainty), caused due to lack of knowledge about the best model or due to lack of data. In other words, our proposal calls for ignoring the errors that occur due to uncertainty inherent in the data, i.e., aleatoric uncertainty. We draw a connection between predictive multiplicity and model uncertainty and argue that the techniques from predictive multiplicity could be used to identify errors made due to model uncertainty. We propose scalable convex proxies to come up with classifiers that exhibit predictive multiplicity and empirically show that our methods are comparable in performance and up to four orders of magnitude faster than the current state-of-the-art. We further pro- pose methods to achieve our goal of equalizing group error rates arising due to model uncertainty in algorithmic decision making and demonstrate the effectiveness of these methods using synthetic and real-world datasets",TRAD,,Metrics,Algorithms,,,,,,,,,,,,,"@inproceedings{10.1145/3461702.3462630,
author = {Ali, Junaid and Lahoti, Preethi and Gummadi, Krishna P.},
title = {Accounting for Model Uncertainty in Algorithmic Discrimination},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462630},
doi = {10.1145/3461702.3462630},
abstract = {Traditional approaches to ensure group fairness in algorithmic decision making aim to equalize ""total"" error rates for different subgroups in the population. In contrast, we argue that the fairness approaches should instead focus only on equalizing errors arising due to model uncertainty (a.k.a epistemic uncertainty), caused due to lack of knowledge about the best model or due to lack of data. In other words, our proposal calls for ignoring the errors that occur due to uncertainty inherent in the data, i.e., aleatoric uncertainty. We draw a connection between predictive multiplicity and model uncertainty and argue that the techniques from predictive multiplicity could be used to identify errors made due to model uncertainty. We propose scalable convex proxies to come up with classifiers that exhibit predictive multiplicity and empirically show that our methods are comparable in performance and up to four orders of magnitude faster than the current state-of-the-art. We further pro- pose methods to achieve our goal of equalizing group error rates arising due to model uncertainty in algorithmic decision making and demonstrate the effectiveness of these methods using synthetic and real-world datasets},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {336–345},
numpages = {10},
keywords = {classification, model uncertainty, algorithmic fairness, predictive multiplicity},
location = {Virtual Event, USA},
series = {AIES '21}
}
  
"
AIES,2021,RAWLSNET: Altering Bayesian Networks to Encode Rawlsian Fair Equality of Opportunity,https://dl.acm.org/doi/10.1145/3461702.3462618,"['David Liu', 'Zohair Shafi', 'William Fleisher', 'Tina Eliassi-Rad', 'Scott Alfeld']",4,"We present RAWLSNET, a system for altering Bayesian Network (BN) models to satisfy the Rawlsian principle of fair equality of opportunity (FEO). RAWLSNET's BN models generate aspirational data distributions: data generated to reflect an ideally fair, FEO-satisfying society. FEO states that everyone with the same talent and willingness to use it should have the same chance of achieving advantageous social positions (e.g., employment), regardless of their background circumstances (e.g., socioeconomic status). Satisfying FEO requires alterations to social structures such as school assignments. Our paper describes RAWLSNET, a method which takes as input a BN representation of an FEO application and alters the BN's parameters so as to satisfy FEO when possible, and minimize deviation from FEO otherwise. We also offer guidance for applying RAWLSNET, including on recognizing proper applications of FEO. We demonstrate the use of RAWLSNET with publicly available data sets. RAWLSNET's altered BNs offer the novel capability of generating aspirational data for FEO-relevant tasks. Aspirational data are free from biases of real-world data, and thus are useful for recognizing and detecting sources of unfairness in machine learning algorithms besides biased data.",TRAD,,Algorithm,Metrics,,,,,,,,,,,,,"@inproceedings{10.1145/3461702.3462618,
author = {Liu, David and Shafi, Zohair and Fleisher, William and Eliassi-Rad, Tina and Alfeld, Scott},
title = {RAWLSNET: Altering Bayesian Networks to Encode Rawlsian Fair Equality of Opportunity},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462618},
doi = {10.1145/3461702.3462618},
abstract = {We present RAWLSNET, a system for altering Bayesian Network (BN) models to satisfy the Rawlsian principle of fair equality of opportunity (FEO). RAWLSNET's BN models generate aspirational data distributions: data generated to reflect an ideally fair, FEO-satisfying society. FEO states that everyone with the same talent and willingness to use it should have the same chance of achieving advantageous social positions (e.g., employment), regardless of their background circumstances (e.g., socioeconomic status). Satisfying FEO requires alterations to social structures such as school assignments. Our paper describes RAWLSNET, a method which takes as input a BN representation of an FEO application and alters the BN's parameters so as to satisfy FEO when possible, and minimize deviation from FEO otherwise. We also offer guidance for applying RAWLSNET, including on recognizing proper applications of FEO. We demonstrate the use of RAWLSNET with publicly available data sets. RAWLSNET's altered BNs offer the novel capability of generating aspirational data for FEO-relevant tasks. Aspirational data are free from biases of real-world data, and thus are useful for recognizing and detecting sources of unfairness in machine learning algorithms besides biased data.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {745–755},
numpages = {11},
keywords = {rawlsian fair equality of opportunity, aspirational data, Bayesian networks},
location = {Virtual Event, USA},
series = {AIES '21}
}
  
"
AIES,2021,Causal Multi-level Fairness,https://dl.acm.org/doi/10.1145/3461702.3462587,"['Vishwali Mhasawade', 'Rumi Chunara']",8,"Algorithmic systems are known to impact marginalized groups severely, and more so, if all sources of bias are not considered. While work in algorithmic fairness to-date has primarily focused on addressing discrimination due to individually linked attributes, social science research elucidates how some properties we link to individuals can be conceptualized as having causes at macro (e.g. structural) levels, and it may be important to be fair to attributes at multiple levels. For example, instead of simply considering race as a causal, protected attribute of an individual, the cause may be distilled as perceived racial discrimination an individual experiences, which in turn can be affected by neighborhood-level factors. This multi-level conceptualization is relevant to questions of fairness, as it may not only be important to take into account if the individual belonged to another demographic group, but also if the individual received advantaged treatment at the macro-level. In this paper, we formalize the problem of multi-level fairness using tools from causal inference in a manner that allows one to assess and account for effects of sensitive attributes at multiple levels. We show importance of the problem by illustrating residual unfairness if macro-level sensitive attributes are not accounted for, or included without accounting for their multi-level nature. Further, in the context of a real-world task of predicting income based on macro and individual-level attributes, we demonstrate an approach for mitigating unfairness, a result of multi-level sensitive attributes.",TRAD,,Metrics,,,,,,,,,,,,,,"@inproceedings{10.1145/3461702.3462587,
author = {Mhasawade, Vishwali and Chunara, Rumi},
title = {Causal Multi-Level Fairness},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462587},
doi = {10.1145/3461702.3462587},
abstract = {Algorithmic systems are known to impact marginalized groups severely, and more so, if all sources of bias are not considered. While work in algorithmic fairness to-date has primarily focused on addressing discrimination due to individually linked attributes, social science research elucidates how some properties we link to individuals can be conceptualized as having causes at macro (e.g. structural) levels, and it may be important to be fair to attributes at multiple levels. For example, instead of simply considering race as a causal, protected attribute of an individual, the cause may be distilled as perceived racial discrimination an individual experiences, which in turn can be affected by neighborhood-level factors. This multi-level conceptualization is relevant to questions of fairness, as it may not only be important to take into account if the individual belonged to another demographic group, but also if the individual received advantaged treatment at the macro-level. In this paper, we formalize the problem of multi-level fairness using tools from causal inference in a manner that allows one to assess and account for effects of sensitive attributes at multiple levels. We show importance of the problem by illustrating residual unfairness if macro-level sensitive attributes are not accounted for, or included without accounting for their multi-level nature. Further, in the context of a real-world task of predicting income based on macro and individual-level attributes, we demonstrate an approach for mitigating unfairness, a result of multi-level sensitive attributes.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {784–794},
numpages = {11},
keywords = {social sciences, racial justice, fairness},
location = {Virtual Event, USA},
series = {AIES '21}
}
  
"
AIES,2021,Rawlsian Fair Adaptation of Deep Learning Classifiers,https://dl.acm.org/doi/10.1145/3461702.3462592,"['Kulin Shah', 'Pooja Gupta', 'Amit Deshpande', 'Chiranjib Bhattacharyya']",3,"Group-fairness in classification aims for equality of a predictive utility across different sensitive sub-populations, e.g., race or gender. Equality or near-equality constraints in group-fairness often worsen not only the aggregate utility but also the utility for the least advantaged sub-population. In this paper, we apply the principles of Pareto-efficiency and least-difference to the utility being accuracy, as an illustrative example, and arrive at the Rawls classifier that minimizes the error rate on the worst-off sensitive sub-population. Our mathematical characterization shows that the Rawls classifier uniformly applies a threshold to an ideal score of features, in the spirit of fair equality of opportunity. In practice, such a score or a feature representation is often computed by a black-box model that has been useful but unfair. Our second contribution is practical Rawlsian fair adaptation of any given black-box deep learning model, without changing the score or feature representation it computes. Given any score function or feature representation and only its second-order statistics on the sensitive sub-populations, we seek a threshold classifier on the given score or a linear threshold classifier on the given feature representation that achieves the Rawls error rate restricted to this hypothesis class. Our technical contribution is to formulate the above problems using ambiguous chance constraints, and to provide efficient algorithms for Rawlsian fair adaptation, along with provable upper bounds on the Rawls error rate. Our empirical results show significant improvement over state-of-the-art group-fair algorithms, even without retraining for fairness.",TRAD,,Algorithms,Metrics,,,,,,,,,,,,,"@inproceedings{10.1145/3461702.3462592,
author = {Shah, Kulin and Gupta, Pooja and Deshpande, Amit and Bhattacharyya, Chiranjib},
title = {Rawlsian Fair Adaptation of Deep Learning Classifiers},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462592},
doi = {10.1145/3461702.3462592},
abstract = {Group-fairness in classification aims for equality of a predictive utility across different sensitive sub-populations, e.g., race or gender. Equality or near-equality constraints in group-fairness often worsen not only the aggregate utility but also the utility for the least advantaged sub-population. In this paper, we apply the principles of Pareto-efficiency and least-difference to the utility being accuracy, as an illustrative example, and arrive at the Rawls classifier that minimizes the error rate on the worst-off sensitive sub-population. Our mathematical characterization shows that the Rawls classifier uniformly applies a threshold to an ideal score of features, in the spirit of fair equality of opportunity. In practice, such a score or a feature representation is often computed by a black-box model that has been useful but unfair. Our second contribution is practical Rawlsian fair adaptation of any given black-box deep learning model, without changing the score or feature representation it computes. Given any score function or feature representation and only its second-order statistics on the sensitive sub-populations, we seek a threshold classifier on the given score or a linear threshold classifier on the given feature representation that achieves the Rawls error rate restricted to this hypothesis class. Our technical contribution is to formulate the above problems using ambiguous chance constraints, and to provide efficient algorithms for Rawlsian fair adaptation, along with provable upper bounds on the Rawls error rate. Our empirical results show significant improvement over state-of-the-art group-fair algorithms, even without retraining for fairness.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {936–945},
numpages = {10},
keywords = {fairness for deep learning classifiers, Rawlsian fairness, fair adaptation},
location = {Virtual Event, USA},
series = {AIES '21}
}
  
"
AIES,2021,FaiR-N: Fair and Robust Neural Networks for Structured Data,https://dl.acm.org/doi/10.1145/3461702.3462559,"['Shubham Sharma', 'Alan H. Gee', 'David Paydarfar', 'Joydeep Ghosh']",4,"Fairness and robustness in machine learning are crucial when individuals are subject to automated decisions made by models in high-stake domains. To promote ethical artificial intelligence, fairness metrics that rely on comparing model error rates across subpopulations have been widely investigated for the detection and mitigation of bias. However, fairness measures that rely on comparing the ability to achieve recourse have been relatively unexplored. In this paper, we present a novel formulation for training neural networks that considers the distance of data observations to the decision boundary such that the new objective: (1) reduces the disparity in the average ability of recourse between individuals in each protected group, and (2) increases the average distance of data points to the boundary to promote adversarial robustness. We demonstrate that models trained with this new objective are more fair and adversarially robust neural networks, with similar accuracies, when compared to models without it. We also investigate a trade-off between the recourse-based fairness and robustness objectives. Moreover, we qualitatively motivate and empirically show that reducing recourse disparity across protected groups also improves fairness measures that rely on error rates. To the best of our knowledge, this is the first time that recourse disparity across groups are considered to train fairer neural networks.",TRAD,,Algorithms,,,Really interesting--trains models to have equal recourse ability across groups,,,,,,,,,,,"@inproceedings{10.1145/3461702.3462559,
author = {Sharma, Shubham and Gee, Alan H. and Paydarfar, David and Ghosh, Joydeep},
title = {FaiR-N: Fair and Robust Neural Networks for Structured Data},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462559},
doi = {10.1145/3461702.3462559},
abstract = {Fairness and robustness in machine learning are crucial when individuals are subject to automated decisions made by models in high-stake domains. To promote ethical artificial intelligence, fairness metrics that rely on comparing model error rates across subpopulations have been widely investigated for the detection and mitigation of bias. However, fairness measures that rely on comparing the ability to achieve recourse have been relatively unexplored. In this paper, we present a novel formulation for training neural networks that considers the distance of data observations to the decision boundary such that the new objective: (1) reduces the disparity in the average ability of recourse between individuals in each protected group, and (2) increases the average distance of data points to the boundary to promote adversarial robustness. We demonstrate that models trained with this new objective are more fair and adversarially robust neural networks, with similar accuracies, when compared to models without it. We also investigate a trade-off between the recourse-based fairness and robustness objectives. Moreover, we qualitatively motivate and empirically show that reducing recourse disparity across protected groups also improves fairness measures that rely on error rates. To the best of our knowledge, this is the first time that recourse disparity across groups are considered to train fairer neural networks.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {946–955},
numpages = {10},
keywords = {robustness, neural networks, ethical artificial intelligence, fairness},
location = {Virtual Event, USA},
series = {AIES '21}
}
  
"
AIES,2022,Contrastive Counterfactual Fairness in Algorithmic Decision-Making,https://dl.acm.org/doi/10.1145/3514094.3534143,"['Ece √áiƒüdem Mutlu', 'Niloofar Yousefi', 'Ozlem Ozmen Garibay']",0,"The widespread use of artificial intelligence algorithms and their role in decision-making with consequential decisions for human subjects has resulted in a growing interest in designing AI algorithms accounting for fairness considerations. There have been attempts to account for fairness of AI algorithms without compromising their accuracy to improve poorly designed algorithms that disregard sensitive attributes (e.g., age, race, and gender) at the peril of introducing or increasing bias against specific groups. Although many studies have examined the optimal trade-off between fairness and accuracy, it remains a challenge to understand the sources of unfairness in decision-making and mitigate it effectively. To tackle this problem, researchers have proposed fair causal learning approaches which assist us in modeling cause and effect knowledge structures, discovering bias sources, and refining AI algorithms to make them more transparent and explainable. In this study, we formalize probabilistic interpretations of both contrastive and counterfactual causality as essential features in order to encourage users' trust and to expand the applicability of such automated systems. We use this formalism to define a novel fairness criterion that we call contrastive counterfactual fairness. This paper introduces, to the best of our knowledge, the first probabilistic fairness-aware data augmentation approach that is based on contrastive counterfactual causality. We tested our approach on two well-known fairness-related datasets, UCI Adult and German Credit, and concluded that our proposed method has a promising ability to capture and mitigate unfairness in AI deployment. This model-agnostic approach can be used with any AI model because it is applied in pre-processing.",TRAD,,Metrics,,,Maybe not because it just proposed a new metric,,,,,,,,,,,"@inproceedings{10.1145/3514094.3534143,
author = {Mutlu, Ece \c{C}i\u{g}dem and Yousefi, Niloofar and Ozmen Garibay, Ozlem},
title = {Contrastive Counterfactual Fairness in Algorithmic Decision-Making},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534143},
doi = {10.1145/3514094.3534143},
abstract = {The widespread use of artificial intelligence algorithms and their role in decision-making with consequential decisions for human subjects has resulted in a growing interest in designing AI algorithms accounting for fairness considerations. There have been attempts to account for fairness of AI algorithms without compromising their accuracy to improve poorly designed algorithms that disregard sensitive attributes (e.g., age, race, and gender) at the peril of introducing or increasing bias against specific groups. Although many studies have examined the optimal trade-off between fairness and accuracy, it remains a challenge to understand the sources of unfairness in decision-making and mitigate it effectively. To tackle this problem, researchers have proposed fair causal learning approaches which assist us in modeling cause and effect knowledge structures, discovering bias sources, and refining AI algorithms to make them more transparent and explainable. In this study, we formalize probabilistic interpretations of both contrastive and counterfactual causality as essential features in order to encourage users' trust and to expand the applicability of such automated systems. We use this formalism to define a novel fairness criterion that we call contrastive counterfactual fairness. This paper introduces, to the best of our knowledge, the first probabilistic fairness-aware data augmentation approach that is based on contrastive counterfactual causality. We tested our approach on two well-known fairness-related datasets, UCI Adult and German Credit, and concluded that our proposed method has a promising ability to capture and mitigate unfairness in AI deployment. This model-agnostic approach can be used with any AI model because it is applied in pre-processing.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {499–507},
numpages = {9},
keywords = {fair classification, counterfactual fairness, fair data augmentation, fair causal learning, contrastive fairness},
location = {Oxford, United Kingdom},
series = {AIES '22}
}
  
"
EAAMO,2021,Mitigating Racial Biases in Toxic Language Detection with an Equity-Based Ensemble Framework,https://dl.acm.org/doi/10.1145/3465416.3483299,"['Matan Halevy', 'Camille Harris', 'Amy Bruckman', 'Diyi Yang', 'Ayanna Howard']",12,"Recent research has demonstrated how racial biases against users who write African American English exists in popular toxic language datasets. While previous work has focused on a single fairness criteria, we propose to use additional descriptive fairness metrics to better understand the source of these biases. We demonstrate that different benchmark classifiers, as well as two in-process bias-remediation techniques, propagate racial biases even in a larger corpus. We then propose a novel ensemble-framework that uses a specialized classifier that is fine-tuned to the African American English dialect. We show that our proposed framework substantially reduces the racial biases that the model learns from these datasets. We demonstrate how the ensemble framework improves fairness metrics across all sample datasets with minimal impact on the classification performance, and provide empirical evidence for its ability to unlearn the annotation biases towards authors who use African American English. ** Please note that this work may contain examples of offensive words and phrases.",TRAD,,Algorithm,Metrics,,,,,,,,,,,,,"@inproceedings{10.1145/3465416.3483299,
author = {Halevy, Matan and Harris, Camille and Bruckman, Amy and Yang, Diyi and Howard, Ayanna},
title = {Mitigating Racial Biases in Toxic Language Detection with an Equity-Based Ensemble Framework},
year = {2021},
isbn = {9781450385534},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3465416.3483299},
doi = {10.1145/3465416.3483299},
abstract = {Recent research has demonstrated how racial biases against users who write African American English exists in popular toxic language datasets. While previous work has focused on a single fairness criteria, we propose to use additional descriptive fairness metrics to better understand the source of these biases. We demonstrate that different benchmark classifiers, as well as two in-process bias-remediation techniques, propagate racial biases even in a larger corpus. We then propose a novel ensemble-framework that uses a specialized classifier that is fine-tuned to the African American English dialect. We show that our proposed framework substantially reduces the racial biases that the model learns from these datasets. We demonstrate how the ensemble framework improves fairness metrics across all sample datasets with minimal impact on the classification performance, and provide empirical evidence for its ability to unlearn the annotation biases towards authors who use African American English. ** Please note that this work may contain examples of offensive words and phrases.},
booktitle = {Equity and Access in Algorithms, Mechanisms, and Optimization},
articleno = {7},
numpages = {11},
keywords = {moderation, bias mitigation, AI fairness, hate speech detection},
location = {--, NY, USA},
series = {EAAMO '21}
}
  
"
EAAMO,2021,An Algorithmic Framework for Positive Action,https://dl.acm.org/doi/10.1145/3465416.3483303,"['Oliver Thomas', 'Miri Zilka', 'Adrian Weller', 'Novi Quadrianto']",0,"Positive action is defined within anti-discrimination legislation as voluntary, legal action taken to address an imbalance of opportunity affecting individuals belonging to under-represented groups. Within this theme, we propose a novel algorithmic fairness framework to advance equal representation while respecting anti-discrimination legislation and equal-treatment rights. We use a counterfactual fairness approach to assign one of three outcomes to each candidate: accept; reject; or flagged as a positive action candidate.",TRAD,,Metrics,,,,,,,,,,,,,,"@inproceedings{10.1145/3465416.3483303,
author = {Thomas, Oliver and Zilka, Miri and Weller, Adrian and Quadrianto, Novi},
title = {An Algorithmic Framework for Positive Action},
year = {2021},
isbn = {9781450385534},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3465416.3483303},
doi = {10.1145/3465416.3483303},
abstract = {Positive action is defined within anti-discrimination legislation as voluntary, legal action taken to address an imbalance of opportunity affecting individuals belonging to under-represented groups. Within this theme, we propose a novel algorithmic fairness framework to advance equal representation while respecting anti-discrimination legislation and equal-treatment rights. We use a counterfactual fairness approach to assign one of three outcomes to each candidate: accept; reject; or flagged as a positive action candidate.},
booktitle = {Equity and Access in Algorithms, Mechanisms, and Optimization},
articleno = {18},
numpages = {13},
keywords = {Auditing, Causal Inference, Fair Machine Learning},
location = {--, NY, USA},
series = {EAAMO '21}
}
  
"
EAAMO,2022,FairEGM: Fair Link Prediction and Recommendation via Emulated Graph Modification,https://dl.acm.org/doi/10.1145/3551624.3555287,"['Sean Current', 'Yuntian He', 'Saket Gurukar', 'Srinivasan Parthasarathy']",0,"As machine learning becomes more widely adopted across domains, it is critical that researchers and ML engineers think about the inherent biases in the data that may be perpetuated by the model. Recently, many studies have shown that such biases are also imbibed in Graph Neural Network (GNN) models if the input graph is biased, potentially to the disadvantage of underserved and underrepresented communities. In this work, we aim to mitigate the bias learned by GNNs by jointly optimizing two different loss functions: one for the task of link prediction and one for the task of demographic parity. We further implement three different techniques inspired by graph modification approaches: the Global Fairness Optimization (GFO), Constrained Fairness Optimization (CFO), and Fair Edge Weighting (FEW) models. These techniques mimic the effects of changing underlying graph structures within the GNN and offer a greater degree of interpretability over more integrated neural network methods. Our proposed models emulate microscopic or macroscopic edits to the input graph while training GNNs and learn node embeddings that are both accurate and fair under the context of link recommendations. We demonstrate the effectiveness of our approach on four real world datasets and show that we can improve the recommendation fairness by several factors at negligible cost to link prediction accuracy.",TRAD,,Algorithm,Metrics,Data (GNNS),,,,,,,,,,,,"@inproceedings{10.1145/3551624.3555287,
author = {Current, Sean and He, Yuntian and Gurukar, Saket and Parthasarathy, Srinivasan},
title = {FairEGM: Fair Link Prediction and Recommendation via Emulated Graph Modification},
year = {2022},
isbn = {9781450394772},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551624.3555287},
doi = {10.1145/3551624.3555287},
abstract = {As machine learning becomes more widely adopted across domains, it is critical that researchers and ML engineers think about the inherent biases in the data that may be perpetuated by the model. Recently, many studies have shown that such biases are also imbibed in Graph Neural Network (GNN) models if the input graph is biased, potentially to the disadvantage of underserved and underrepresented communities. In this work, we aim to mitigate the bias learned by GNNs by jointly optimizing two different loss functions: one for the task of link prediction and one for the task of demographic parity. We further implement three different techniques inspired by graph modification approaches: the Global Fairness Optimization (GFO), Constrained Fairness Optimization (CFO), and Fair Edge Weighting (FEW) models. These techniques mimic the effects of changing underlying graph structures within the GNN and offer a greater degree of interpretability over more integrated neural network methods. Our proposed models emulate microscopic or macroscopic edits to the input graph while training GNNs and learn node embeddings that are both accurate and fair under the context of link recommendations. We demonstrate the effectiveness of our approach on four real world datasets and show that we can improve the recommendation fairness by several factors at negligible cost to link prediction accuracy.},
booktitle = {Equity and Access in Algorithms, Mechanisms, and Optimization},
articleno = {3},
numpages = {14},
keywords = {graph convolution neural networks, link prediction, graph neural networks, graph representation learning, demographic parity, group fairness, link recommendation, graph fairness},
location = {Arlington, VA, USA},
series = {EAAMO '22}
}
  
"
EAAMO,2022,"It‚Äôs Not Fairness, and It‚Äôs Not Fair: The Failure of Distributional Equality and the Promise of Relational Equality in Complete-Information Hiring Games",https://dl.acm.org/doi/10.1145/3551624.3555296,"['Benjamin Fish', 'Luke Stark']",0,"Existing efforts to formulate computational definitions of fairness have largely focused on distributional notions of equality, defined through how resources or decisions are divided. Yet existing discrimination is often the result of unequal social relations, rather than simply an unequal distribution of resources. We show how optimizing for existing computational definitions of fairness fails to prevent unequal social relations by providing an example of a self-confirming equilibrium in a simple hiring market that is relationally unequal but satisfies existing distributional notions of fairness. We introduce a notion of blatant relational unfairness for complete-information games, and discuss how this definition helps initiate a new approach to incorporating relational equality into computational systems.",TRAD,,Metrics,,Might need a second look,,,,,,,,,,,,"@inproceedings{10.1145/3551624.3555296,
author = {Fish, Benjamin and Stark, Luke},
title = {It’s Not Fairness, and It’s Not Fair: The Failure of Distributional Equality and the Promise of Relational Equality in Complete-Information Hiring Games},
year = {2022},
isbn = {9781450394772},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551624.3555296},
doi = {10.1145/3551624.3555296},
abstract = {Existing efforts to formulate computational definitions of fairness have largely focused on distributional notions of equality, defined through how resources or decisions are divided. Yet existing discrimination is often the result of unequal social relations, rather than simply an unequal distribution of resources. We show how optimizing for existing computational definitions of fairness fails to prevent unequal social relations by providing an example of a self-confirming equilibrium in a simple hiring market that is relationally unequal but satisfies existing distributional notions of fairness. We introduce a notion of blatant relational unfairness for complete-information games, and discuss how this definition helps initiate a new approach to incorporating relational equality into computational systems.},
booktitle = {Equity and Access in Algorithms, Mechanisms, and Optimization},
articleno = {11},
numpages = {15},
location = {Arlington, VA, USA},
series = {EAAMO '22}
}
  
"
Neurips,2017,"
When Worlds Collide: Integrating Different Counterfactual Assumptions in Fairness
",https://proceedings.neurips.cc//paper/2017/hash/1271a7029c9df08643b631b02cf9e116-Abstract.html,"['Chris Russell', ' Matt J. Kusner', ' Joshua Loftus', ' Ricardo Silva']",146,"Machine learning is now being used to make crucial decisions about people's lives. For nearly all of these decisions there is a risk that individuals of a certain race, gender, sexual orientation, or any other subpopulation are unfairly discriminated against. Our recent method has demonstrated how to use techniques from counterfactual inference to make predictions fair across different subpopulations. This method requires that one provides the causal model that generated the data at hand. In general, validating all causal implications of the model is not possible without further assumptions. Hence, it is desirable to integrate competing causal models to provide counterfactually fair decisions, regardless of which causal ""world"" is the correct one. In this paper, we show how it is possible to make predictions that are approximately fair with respect to multiple possible causal models at once, thus mitigating the problem of exact causal specification. We frame the goal of learning a fair classifier as an optimization problem with fairness constraints entailed by competing causal explanations. We show how this optimization problem can be efficiently solved using gradient-based methods. We demonstrate the flexibility of our model on two real-world fair classification problems. We show that our model can seamlessly balance fairness in multiple worlds with prediction accuracy.",TRAD,,Algorithm,Metrics,,Creates a way to train models which are fair over a range of causal graphs--- is this actually pipeline?,,,,,,,,,,,"@inproceedings{NIPS2017_1271a702,
 author = {Russell, Chris and Kusner, Matt J and Loftus, Joshua and Silva, Ricardo},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {When Worlds Collide: Integrating Different Counterfactual Assumptions in Fairness},
 url = {https://proceedings.neurips.cc/paper/2017/file/1271a7029c9df08643b631b02cf9e116-Paper.pdf},
 volume = {30},
 year = {2017}
}

"
Neurips,2017,Recycling Privileged Learning and Distribution Matching for Fairness,https://proceedings.neurips.cc//paper/2017/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html,"['Novi Quadrianto', ' Viktoriia Sharmanska']",66,"Equipping machine learning models with ethical and legal constraints is a serious issue; without this, the future of machine learning is at risk. This paper takes a step forward in this direction and focuses on ensuring machine learning models deliver fair decisions. In legal scholarships, the notion of fairness itself is evolving and multi-faceted. We set an overarching goal to develop a unified machine learning framework that is able to handle any definitions of fairness, their combinations, and also new definitions that might be stipulated in the future. To achieve our goal, we recycle two well-established machine learning techniques, privileged learning and distribution matching, and harmonize them for satisfying multi-faceted fairness definitions. We consider protected characteristics such as race and gender as privileged information that is available at training but not at test time; this accelerates model training and delivers fairness through unawareness. Further, we cast demographic parity, equalized odds, and equality of opportunity as a classical two-sample problem of conditional distributions, which can be solved in a general form by using distance measures in Hilbert Space. We show several existing models are special cases of ours. Finally, we advocate returning the Pareto frontier of multi-objective minimization of error and unfairness in predictions. This will facilitate decision makers to select an operating point and to be accountable for it.",TRAD,,Algorithm,,,,,,,,,,,,,,"@inproceedings{NIPS2017_250cf8b5,
 author = {Quadrianto, Novi and Sharmanska, Viktoriia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Recycling Privileged Learning and Distribution Matching for Fairness},
 url = {https://proceedings.neurips.cc/paper/2017/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf},
 volume = {30},
 year = {2017}
}

"
Neurips,2017,Learning to Pivot with Adversarial Networks,https://proceedings.neurips.cc//paper/2017/hash/48ab2f9b45957ab574cf005eb8a76760-Abstract.html,"['Gilles Louppe', ' Michael Kagan', ' Kyle Cranmer']",226,"Several techniques for domain adaptation have been proposed to account for differences in the distribution of the data used for training and testing. The majority of this work focuses on a binary domain label. Similar problems occur in a scientific context where there may be a continuous family of plausible data generation processes associated to the presence of systematic uncertainties. Robust inference is possible if it is based on a pivot -- a quantity whose distribution does not depend on the unknown values of the nuisance parameters that parametrize this family of data generation processes. In this work, we introduce and derive theoretical results for a training procedure based on adversarial networks for enforcing the pivotal property (or, equivalently, fairness with respect to continuous attributes) on a predictive model. The method includes a hyperparameter to control the trade-off between accuracy and robustness. We demonstrate the effectiveness of this approach with a toy example and examples from particle physics.",TRAD,,Algorithm,,,,,,,,,,,,,,"@inproceedings{NIPS2017_48ab2f9b,
 author = {Louppe, Gilles and Kagan, Michael and Cranmer, Kyle},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Learning to Pivot with Adversarial Networks},
 url = {https://proceedings.neurips.cc/paper/2017/file/48ab2f9b45957ab574cf005eb8a76760-Paper.pdf},
 volume = {30},
 year = {2017}
}

"
Neurips,2017,From Parity to Preference-based Notions of Fairness in Classification,https://proceedings.neurips.cc//paper/2017/hash/82161242827b703e6acf9c726942a1e4-Abstract.html,"['Muhammad Bilal Zafar', ' Isabel Valera', ' Manuel Rodriguez', ' Krishna Gummadi', ' Adrian Weller']",196,"The adoption of automated, data-driven decision making in an ever expanding range of applications has raised concerns about its potential unfairness towards certain social groups. In this context, a number of recent studies have focused on defining, detecting, and removing unfairness from data-driven decision systems. However, the existing notions of fairness, based on parity (equality) in treatment or outcomes for different social groups, tend to be quite stringent, limiting the overall decision making accuracy. In this paper, we draw inspiration from the fair-division and envy-freeness literature in economics and game theory and propose preference-based notions of fairness -- given the choice between various sets of decision treatments or outcomes, any group of users would collectively prefer its treatment or outcomes, regardless of the (dis)parity as compared to the other groups. Then, we introduce tractable proxies to design margin-based classifiers that satisfy these preference-based notions of fairness. Finally, we experiment with a variety of synthetic and real-world datasets and show that preference-based fairness allows for greater decision accuracy than parity-based fairness.",TRAD,,Metrics,Algorithm,,,,,,,,,,,,,"@inproceedings{NIPS2017_82161242,
 author = {Zafar, Muhammad Bilal and Valera, Isabel and Rodriguez, Manuel and Gummadi, Krishna and Weller, Adrian},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {From Parity to Preference-based Notions of Fairness in Classification},
 url = {https://proceedings.neurips.cc/paper/2017/file/82161242827b703e6acf9c726942a1e4-Paper.pdf},
 volume = {30},
 year = {2017}
}

"
Neurips,2017,Counterfactual Fairness,https://proceedings.neurips.cc//paper/2017/hash/a486cd07e4ac3d270571622f4f316ec5-Abstract.html,"['Matt J. Kusner', ' Joshua Loftus', ' Chris Russell', ' Ricardo Silva']",1216,"Machine learning can impact people with legal or ethical consequences when it is used to automate decisions in areas such as insurance, lending, hiring, and predictive policing. In many of these scenarios, previous decisions have been made that are unfairly biased against certain subpopulations, for example those of a particular race, gender, or sexual orientation. Since this past data may be biased, machine learning predictors must account for this to avoid perpetuating or creating discriminatory practices. In this paper, we develop a framework for modeling fairness using tools from causal inference. Our definition of counterfactual fairness captures the intuition that a decision is fair towards an individual if it the same in (a) the actual world and (b) a counterfactual world where the individual belonged to a different demographic group. We demonstrate our framework on a real-world problem of fair prediction of success in law school.",TRAD,,Metrics,Algorithm,Definition,Is this something that's relevant?,,,,,,,,,,,"@inproceedings{NIPS2017_a486cd07,
 author = {Kusner, Matt J and Loftus, Joshua and Russell, Chris and Silva, Ricardo},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Counterfactual Fairness},
 url = {https://proceedings.neurips.cc/paper/2017/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf},
 volume = {30},
 year = {2017}
}

"
Neurips,2017,Avoiding Discrimination through Causal Reasoning,https://proceedings.neurips.cc//paper/2017/hash/f5f8590cd58a54e94377e6ae2eded4d9-Abstract.html,"['Niki Kilbertus', ' Mateo Rojas Carulla', ' Giambattista Parascandolo', ' Moritz Hardt', ' Dominik Janzing', ' Bernhard Sch√∂lkopf']",530,"Recent work on fairness in machine learning has focused on various statistical discrimination criteria and how they trade off. Most of these criteria are observational: They depend only on the joint distribution of predictor, protected attribute, features, and outcome. While convenient to work with, observational criteria have severe inherent limitations that prevent them from resolving matters of fairness conclusively. Going beyond observational criteria, we frame the problem of discrimination based on protected attributes in the language of causal reasoning. This viewpoint shifts attention from ""What is the right fairness criterion?"" to ""What do we want to assume about our model of the causal data generating process?"" Through the lens of causality, we make several contributions. First, we crisply articulate why and when observational criteria fail, thus formalizing what was before a matter of opinion. Second, our approach exposes previously ignored subtleties and why they are fundamental to the problem. Finally, we put forward natural causal non-discrimination criteria and develop algorithms that satisfy them.",TRAD,,Metrics,Algorithm,,,,,,,,,,,,,"@inproceedings{NIPS2017_f5f8590c,
 author = {Kilbertus, Niki and Rojas Carulla, Mateo and Parascandolo, Giambattista and Hardt, Moritz and Janzing, Dominik and Sch\""{o}lkopf, Bernhard},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Avoiding Discrimination through Causal Reasoning},
 url = {https://proceedings.neurips.cc/paper/2017/file/f5f8590cd58a54e94377e6ae2eded4d9-Paper.pdf},
 volume = {30},
 year = {2017}
}

"
Neurips,2018,Empirical Risk Minimization Under Fairness Constraints,https://proceedings.neurips.cc//paper/2018/hash/83cdcec08fbf90370fcf53bdd56604ff-Abstract.html,"['Michele Donini', ' Luca Oneto', ' Shai Ben-David', ' John S. Shawe-Taylor', ' Massimiliano Pontil']",340,"We address the problem of algorithmic fairness: ensuring that sensitive information does not unfairly influence the outcome of a classifier. We present an approach based on empirical risk minimization, which incorporates a fairness constraint into the learning problem. It encourages the conditional risk of the learned classifier to be approximately constant with respect to the sensitive variable. We derive both risk and fairness bounds that support the statistical consistency of our methodology. We specify our approach to kernel methods and observe that the fairness requirement implies an orthogonality constraint which can be easily added to these methods. We further observe that for linear models the constraint translates into a simple data preprocessing step. Experiments indicate that the method is empirically effective and performs favorably against state-of-the-art approaches.",TRAD,,Algorithm,,Methods,,,,,,,,,,,,"@inproceedings{NEURIPS2018_83cdcec0,
 author = {Donini, Michele and Oneto, Luca and Ben-David, Shai and Shawe-Taylor, John S and Pontil, Massimiliano},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Empirical Risk Minimization Under Fairness Constraints},
 url = {https://proceedings.neurips.cc/paper/2018/file/83cdcec08fbf90370fcf53bdd56604ff-Paper.pdf},
 volume = {31},
 year = {2018}
}

"
Neurips,2018,Fairness Behind a Veil of Ignorance: A Welfare Analysis for Automated Decision Making,https://proceedings.neurips.cc//paper/2018/hash/be3159ad04564bfb90db9e32851ebf9c-Abstract.html,"['Hoda Heidari', ' Claudio Ferrari', ' Krishna Gummadi', ' Andreas Krause']",100,"We draw attention to an important, yet largely overlooked aspect of evaluating fairness for automated decision making systems---namely risk and welfare considerations. Our proposed family of measures corresponds to the long-established formulations of cardinal social welfare in economics, and is justified by the Rawlsian conception of fairness behind a veil of ignorance. The convex formulation of our welfare-based measures of fairness allows us to integrate them as a constraint into any convex loss minimization pipeline. Our empirical analysis reveals interesting trade-offs between our proposal and (a) prediction accuracy, (b) group discrimination, and (c) Dwork et al's notion of individual fairness. Furthermore and perhaps most importantly, our work provides both heuristic justification and empirical evidence suggesting that a lower-bound on our measures often leads to bounded inequality in algorithmic outcomes; hence presenting the first computationally feasible mechanism for bounding individual-level inequality.",TRAD,,Metrics,,,,,,,,,,,,,,"@inproceedings{NEURIPS2018_be3159ad,
 author = {Heidari, Hoda and Ferrari, Claudio and Gummadi, Krishna and Krause, Andreas},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Fairness Behind a Veil of Ignorance: A Welfare Analysis for Automated Decision Making},
 url = {https://proceedings.neurips.cc/paper/2018/file/be3159ad04564bfb90db9e32851ebf9c-Paper.pdf},
 volume = {31},
 year = {2018}
}

"
Neurips,2018,Fairness Through Computationally-Bounded Awareness,https://proceedings.neurips.cc//paper/2018/hash/c8dfece5cc68249206e4690fc4737a8d-Abstract.html,"['Michael Kim', ' Omer Reingold', ' Guy Rothblum']",121,"We study the problem of fair classification within the versatile framework of Dwork et al. [ITCS '12], which assumes the existence of a metric that measures similarity between pairs of individuals. Unlike earlier work, we do not assume that the entire metric is known to the learning algorithm; instead, the learner can query this arbitrary metric a bounded number of times. We propose a new notion of fairness called metric multifairness and show how to achieve this notion in our setting. Metric multifairness is parameterized by a similarity metric d on pairs of individuals to classify and a rich collection C of (possibly overlapping) ""comparison sets"" over pairs of individuals. At a high level, metric multifairness guarantees that similar subpopulations are treated similarly, as long as these subpopulations are identified within the class C.",TRAD,,Metrics,,,,,,,,,,,,,,"@inproceedings{NEURIPS2018_c8dfece5,
 author = {Kim, Michael and Reingold, Omer and Rothblum, Guy},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Fairness Through Computationally-Bounded Awareness},
 url = {https://proceedings.neurips.cc/paper/2018/file/c8dfece5cc68249206e4690fc4737a8d-Paper.pdf},
 volume = {31},
 year = {2018}
}

"
Neurips,2018,Equality of Opportunity in Classification: A Causal Approach,https://proceedings.neurips.cc//paper/2018/hash/ff1418e8cc993fe8abcfe3ce2003e5c5-Abstract.html,"['Junzhe Zhang', ' Elias Bareinboim']",71,"The Equalized Odds (for short, EO) is one of the most popular measures of discrimination used in the supervised learning setting. It ascertains fairness through the balance of the misclassification rates (false positive and negative) across the protected groups -- e.g., in the context of law enforcement, an African-American defendant who would not commit a future crime will have an equal opportunity of being released, compared to a non-recidivating Caucasian defendant. Despite this noble goal, it has been acknowledged in the literature that statistical tests based on the EO are oblivious to the underlying causal mechanisms that generated the disparity in the first place (Hardt et al. 2016). This leads to a critical disconnect between statistical measures readable from the data and the meaning of discrimination in the legal system, where compelling evidence that the observed disparity is tied to a specific causal process deemed unfair by society is required to characterize discrimination. The goal of this paper is to develop a principled approach to connect the statistical disparities characterized by the EO and the underlying, elusive, and frequently unobserved, causal mechanisms that generated such inequality. We start by introducing a new family of counterfactual measures that allows one to explain the misclassification disparities in terms of the underlying mechanisms in an arbitrary, non-parametric structural causal model. This will, in turn, allow legal and data analysts to interpret currently deployed classifiers through causal lens, linking the statistical disparities found in the data to the corresponding causal processes. Leveraging the new family of counterfactual measures, we develop a learning procedure to construct a classifier that is statistically efficient, interpretable, and compatible with the basic human intuition of fairness. We demonstrate our results through experiments in both real (COMPAS) and synthetic datasets.",TRAD,,Metrics,Algorithm,,,,,,,,,,,,,"@inproceedings{NEURIPS2018_ff1418e8,
 author = {Zhang, Junzhe and Bareinboim, Elias},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Equality of Opportunity in Classification: A Causal Approach},
 url = {https://proceedings.neurips.cc/paper/2018/file/ff1418e8cc993fe8abcfe3ce2003e5c5-Paper.pdf},
 volume = {31},
 year = {2018}
}

"
Neurips,2019,"Average Individual Fairness: Algorithms, Generalization and Experiments",https://proceedings.neurips.cc//paper/2019/hash/0e1feae55e360ff05fef58199b3fa521-Abstract.html,"['Saeed Sharifi-Malvajerdi', ' Michael Kearns', ' Aaron Roth']",61,"We propose a new family of fairness definitions for classification problems that combine some of the best properties of both statistical and individual notions of fairness. We posit not only a distribution over individuals, but also a distribution over (or collection of) classification tasks. We then ask that standard statistics (such as error or false positive/negative rates) be (approximately) equalized across individuals, where the rate is defined as an expectation over the classification tasks. Because we are no longer averaging over coarse groups (such as race or gender), this is a semantically meaningful individual-level constraint. Given a sample of individuals and problems, we design an oracle-efficient algorithm (i.e. one that is given access to any standard, fairness-free learning heuristic) for the fair empirical risk minimization task. We also show that given sufficiently many samples, the ERM solution generalizes in two directions: both to new individuals, and to new classification tasks, drawn from their corresponding distributions. Finally we implement our algorithm and empirically verify its effectiveness.",TRAD,,Metrics,Algorithm,,,,,,,,,,,,,"@inproceedings{NEURIPS2019_0e1feae5,
 author = {Sharifi-Malvajerdi, Saeed and Kearns, Michael and Roth, Aaron},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Average Individual Fairness: Algorithms, Generalization and Experiments},
 url = {https://proceedings.neurips.cc/paper/2019/file/0e1feae55e360ff05fef58199b3fa521-Paper.pdf},
 volume = {32},
 year = {2019}
}

"
Neurips,2019,The Fairness of Risk Scores Beyond Classification: Bipartite Ranking and the XAUC Metric,https://proceedings.neurips.cc//paper/2019/hash/73e0f7487b8e5297182c5a711d20bf26-Abstract.html,"['Nathan Kallus', ' Angela Zhou']",50,"Where machine-learned predictive risk scores inform high-stakes decisions, such as bail and sentencing in criminal justice, fairness has been a serious concern. Recent work has characterized the disparate impact that such risk scores can have when used for a binary classification task. This may not account, however, for the more diverse downstream uses of risk scores and their non-binary nature. To better account for this, in this paper, we investigate the fairness of predictive risk scores from the point of view of a bipartite ranking task, where one seeks to rank positive examples higher than negative ones. We introduce the xAUC disparity as a metric to assess the disparate impact of risk scores and define it as the difference in the probabilities of ranking a random positive example from one protected group above a negative one from another group and vice versa. We provide a decomposition of bipartite ranking loss into components that involve the discrepancy and components that involve pure predictive ability within each group. We use xAUC analysis to audit predictive risk scores for recidivism prediction, income prediction, and cardiac arrest prediction, where it describes disparities that are not evident from simply comparing within-group predictive performance.",TRAD,,Metrics,,,,,,,,,,,,,,"@inproceedings{NEURIPS2019_73e0f748,
 author = {Kallus, Nathan and Zhou, Angela},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {The Fairness of Risk Scores Beyond Classification: Bipartite Ranking and the XAUC Metric},
 url = {https://proceedings.neurips.cc/paper/2019/file/73e0f7487b8e5297182c5a711d20bf26-Paper.pdf},
 volume = {32},
 year = {2019}
}

"
Neurips,2019,Near Neighbor: Who is the Fairest of Them All?,https://proceedings.neurips.cc//paper/2019/hash/742141ceda6b8f6786609d31c8ef129f-Abstract.html,"['Sariel Har-Peled', ' Sepideh Mahabadi']",16,"In this work we study a ""fair"" variant of the near neighbor problem. Namely, given a set of $n$ points $P$ and a parameter $r$, the goal is to preprocess the points, such that given a query point $q$, any point in the $r$-neighborhood of the query, i.e., $B(q,r)$, have the same probability of being reported as the near neighbor. We show that LSH based algorithms can be made fair, without a significant loss in efficiency. Specifically, we show an algorithm that reports a point $p$ in the $r$-neighborhood of a query $q$ with almost uniform probability. The time to report such a point is proportional to $O(\dns(q.r) Q(n,c))$, and its space is $O(S(n,c))$, where $Q(n,c)$ and $S(n,c)$ are the query time and space of an LSH algorithm for $c$-approximate near neighbor, and $\dns(q,r)$ is a function of the local density around $q$. Our approach works more generally for sampling uniformly from a sub-collection of sets of a given collection and can be used in a few other applications. Finally, we run experiments to show performance of our approach on real data.",TRAD,,Algorithm,,,,,,,,,,,,,,"@inproceedings{NEURIPS2019_742141ce,
 author = {Har-Peled, Sariel and Mahabadi, Sepideh},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Near Neighbor: Who is the Fairest of Them All?},
 url = {https://proceedings.neurips.cc/paper/2019/file/742141ceda6b8f6786609d31c8ef129f-Paper.pdf},
 volume = {32},
 year = {2019}
}

"
Neurips,2019,Leveraging Labeled and Unlabeled Data for Consistent Fair Binary Classification,https://proceedings.neurips.cc//paper/2019/hash/ba51e6158bcaf80fd0d834950251e693-Abstract.html,"['Evgenii Chzhen', ' Christophe Denis', ' Mohamed Hebiri', ' Luca Oneto', ' Massimiliano Pontil']",55,"We study the problem of fair binary classification using the notion of Equal Opportunity. It requires the true positive rate to distribute equally across the sensitive groups. Within this setting we show that the fair optimal classifier is obtained by recalibrating the Bayes classifier by a group-dependent threshold. We provide a constructive expression for the threshold. This result motivates us to devise a plug-in classification procedure based on both unlabeled and labeled datasets. While the latter is used to learn the output conditional probability, the former is used for calibration. The overall procedure can be computed in polynomial time and it is shown to be statistically consistent both in terms of the classification error and fairness measure. Finally, we present numerical experiments which indicate that our method is often superior or competitive with the state-of-the-art methods on benchmark datasets.",TRAD,,Algorithm,,,,,,,,,,,,,,"@inproceedings{NEURIPS2019_ba51e615,
 author = {Chzhen, Evgenii and Denis, Christophe and Hebiri, Mohamed and Oneto, Luca and Pontil, Massimiliano},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Leveraging Labeled and Unlabeled Data for Consistent Fair Binary Classification},
 url = {https://proceedings.neurips.cc/paper/2019/file/ba51e6158bcaf80fd0d834950251e693-Paper.pdf},
 volume = {32},
 year = {2019}
}

"
Neurips,2019,On Human-Aligned Risk Minimization,https://proceedings.neurips.cc//paper/2019/hash/cd6b73b67c77edeaff94e24b961119dd-Abstract.html,"['Liu Leqi', ' Adarsh Prasad', ' Pradeep K. Ravikumar']",13,"The statistical decision theoretic foundations of modern machine learning have largely focused on the minimization of the expectation of some loss function for a given task. However, seminal results in behavioral economics have shown that human decision-making is based on different risk measures than the expectation of any given loss function. In this paper, we pose the following simple question: in contrast to minimizing expected loss, could we minimize a better human-aligned risk measure? While this might not seem natural at first glance, we analyze the properties of such a revised risk measure, and surprisingly show that it might also better align with additional desiderata like fairness that have attracted considerable recent attention. We focus in particular on a class of human-aligned risk measures inspired by cumulative prospect theory. We empirically study these risk measures, and demonstrate their improved performance on desiderata such as fairness, in contrast to the traditional workhorse of expected loss minimization.",TRAD,,Algorithm,Metrics,,,,,,,,,,,,,"@inproceedings{NEURIPS2019_cd6b73b6,
 author = {Leqi, Liu and Prasad, Adarsh and Ravikumar, Pradeep K},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {On Human-Aligned Risk Minimization},
 url = {https://proceedings.neurips.cc/paper/2019/file/cd6b73b67c77edeaff94e24b961119dd-Paper.pdf},
 volume = {32},
 year = {2019}
}

"
Neurips,2019,Envy-Free Classification,https://proceedings.neurips.cc//paper/2019/hash/e94550c93cd70fe748e6982b3439ad3b-Abstract.html,"['Maria-Florina F. Balcan', ' Travis Dick', ' Ritesh Noothigattu', ' Ariel D. Procaccia']",34,"In classic fair division problems such as cake cutting and rent division, envy-freeness requires that each individual (weakly) prefer his allocation to anyone else's. On a conceptual level, we argue that envy-freeness also provides a compelling notion of fairness for classification tasks, especially when individuals have heterogeneous preferences. Our technical focus is the generalizability of envy-free classification, i.e., understanding whether a classifier that is envy free on a sample would be almost envy free with respect to the underlying distribution with high probability. Our main result establishes that a small sample is sufficient to achieve such guarantees, when the classifier in question is a mixture of deterministic classifiers that belong to a family of low Natarajan dimension.",TRAD,,Metrics,Algorithm,,,,,,,,,,,,,"@inproceedings{NEURIPS2019_e94550c9,
 author = {Balcan, Maria-Florina F and Dick, Travis and Noothigattu, Ritesh and Procaccia, Ariel D},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Envy-Free Classification},
 url = {https://proceedings.neurips.cc/paper/2019/file/e94550c93cd70fe748e6982b3439ad3b-Paper.pdf},
 volume = {32},
 year = {2019}
}

"
Neurips,2020,Achieving Equalized Odds by Resampling Sensitive Attributes,https://proceedings.neurips.cc//paper/2020/hash/03593ce517feac573fdaafa6dcedef61-Abstract.html,"['Yaniv Romano', ' Stephen Bates', ' Emmanuel Candes']",18,"We present a flexible framework for learning predictive models that approximately satisfy the equalized odds notion of fairness. This is achieved by introducing a general discrepancy functional that rigorously quantifies violations of this criterion. This differentiable functional is used as a penalty driving the model parameters towards equalized odds. To rigorously evaluate fitted models, we develop a formal hypothesis test to detect whether a prediction rule violates this property, the first such test in the literature. Both the model fitting and hypothesis testing leverage a resampled version of the sensitive attribute obeying equalized odds, by construction. We demonstrate the applicability and validity of the proposed framework both in regression and multi-class classification problems, reporting improved performance over state-of-the-art methods. Lastly, we show how to incorporate techniques for equitable uncertainty quantification---unbiased for each group under study---to communicate the results of the data analysis in exact terms.",TRAD,,Algorithm,,,,,,,,,,,,,,"@inproceedings{NEURIPS2020_03593ce5,
 author = {Romano, Yaniv and Bates, Stephen and Candes, Emmanuel},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {361--371},
 publisher = {Curran Associates, Inc.},
 title = {Achieving Equalized Odds by Resampling Sensitive Attributes},
 url = {https://proceedings.neurips.cc/paper/2020/file/03593ce517feac573fdaafa6dcedef61-Paper.pdf},
 volume = {33},
 year = {2020}
}

"
Neurips,2020,Fairness without Demographics through Adversarially Reweighted Learning,https://proceedings.neurips.cc//paper/2020/hash/07fc15c9d169ee48573edd749d25945d-Abstract.html,"['Preethi Lahoti', ' Alex Beutel', ' Jilin Chen', ' Kang Lee', ' Flavien Prost', ' Nithum Thain', ' Xuezhi Wang', ' Ed Chi']",152,"Much of the previous machine learning (ML) fairness literature assumes that protected features such as race and sex are present in the dataset, and relies upon them to mitigate fairness concerns. However, in practice factors like privacy and regulation often preclude the collection of protected features, or their use for training or inference, severely limiting the applicability of traditional fairness research. Therefore, we ask: How can we train a ML model to improve fairness when we do not even know the protected group memberships? In this work we address this problem by proposing Adversarially Reweighted Learning (ARL). In particular, we hypothesize that non-protected features and task labels are valuable for identifying fairness issues, and can be used to co-train an adversarial reweighting approach for improving fairness. Our results show that ARL improves Rawlsian Max-Min fairness, with notable AUC improvements for worst-case protected groups in multiple datasets, outperforming state-of-the-art alternatives.",TRAD,,Algorithm,,,Achieves fairness without access to protected attributes,,,,,,,,,,,"@inproceedings{NEURIPS2020_07fc15c9,
 author = {Lahoti, Preethi and Beutel, Alex and Chen, Jilin and Lee, Kang and Prost, Flavien and Thain, Nithum and Wang, Xuezhi and Chi, Ed},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {728--740},
 publisher = {Curran Associates, Inc.},
 title = {Fairness without Demographics through Adversarially Reweighted Learning},
 url = {https://proceedings.neurips.cc/paper/2020/file/07fc15c9d169ee48573edd749d25945d-Paper.pdf},
 volume = {33},
 year = {2020}
}

"
Neurips,2020,Fairness with Overlapping Groups; a Probabilistic Perspective,https://proceedings.neurips.cc//paper/2020/hash/29c0605a3bab4229e46723f89cf59d83-Abstract.html,"['Forest Yang', ' Mouhamadou Cisse', ' Sanmi Koyejo']",41,"In algorithmically fair prediction problems, a standard goal is to ensure the equality of fairness metrics across multiple overlapping groups simultaneously. We reconsider this standard fair classification problem using a probabilistic population analysis, which, in turn, reveals the Bayes-optimal classifier. Our approach unifies a variety of existing group-fair classification methods and enables extensions to a wide range of non-decomposable multiclass performance metrics and fairness measures. The Bayes-optimal classifier further inspires consistent procedures for algorithmically fair classification with overlapping groups. On a variety of real datasets, the proposed approach outperforms baselines in terms of its fairness-performance tradeoff.",TRAD,,Algorithm,Metrics,,,,,,,,,,,,,"@inproceedings{NEURIPS2020_29c0605a,
 author = {Yang, Forest and Cisse, Mouhamadou and Koyejo, Sanmi},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {4067--4078},
 publisher = {Curran Associates, Inc.},
 title = {Fairness with Overlapping Groups; a Probabilistic Perspective},
 url = {https://proceedings.neurips.cc/paper/2020/file/29c0605a3bab4229e46723f89cf59d83-Paper.pdf},
 volume = {33},
 year = {2020}
}

"
Neurips,2020,Fair regression with Wasserstein barycenters,https://proceedings.neurips.cc//paper/2020/hash/51cdbd2611e844ece5d80878eb770436-Abstract.html,"['Evgenii Chzhen', ' Christophe Denis', ' Mohamed Hebiri', ' Luca Oneto', ' Massimiliano Pontil']",52,"We study the problem of learning a real-valued function that satisfies the Demographic Parity constraint. It demands the distribution of the predicted output to be independent of the sensitive attribute. We consider the case that the sensitive attribute is available for prediction. We establish a connection between fair regression and optimal transport theory, based on which we derive a close form expression for the optimal fair predictor. Specifically, we show that the distribution of this optimum is the Wasserstein barycenter of the distributions induced by the standard regression function on the sensitive groups. This result offers an intuitive interpretation of the optimal fair prediction and suggests a simple post-processing algorithm to achieve fairness. We establish risk and distribution-free fairness guarantees for this procedure. Numerical experiments indicate that our method is very effective in learning fair models, with a relative increase in error rate that is inferior to the relative gain in fairness.",TRAD,,Algorithm,,,,,,,,,,,,,,"@inproceedings{NEURIPS2020_51cdbd26,
 author = {Chzhen, Evgenii and Denis, Christophe and Hebiri, Mohamed and Oneto, Luca and Pontil, Massimiliano},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {7321--7331},
 publisher = {Curran Associates, Inc.},
 title = {Fair regression with Wasserstein barycenters},
 url = {https://proceedings.neurips.cc/paper/2020/file/51cdbd2611e844ece5d80878eb770436-Paper.pdf},
 volume = {33},
 year = {2020}
}

"
Neurips,2020,"Classification Under Misspecification: Halfspaces, Generalized Linear Models, and Evolvability",https://proceedings.neurips.cc//paper/2020/hash/5f8b73c0d4b1bf60dd7173b660b87c29-Abstract.html,"['Sitan Chen', ' Frederic Koehler', ' Ankur Moitra', ' Morris Yau']",7,"In this paper, we revisit the problem of distribution-independently learning halfspaces under Massart noise with rate $\eta$. Recent work resolved a long-standing problem in this model of efficiently learning to error $\eta + \epsilon$ for any $\epsilon > 0$, by giving an improper learner that partitions space into $\text{poly}(d,1/\epsilon)$ regions. Here we give a much simpler algorithm and settle a number of outstanding open questions: (1) We give the first \emph{proper} learner for Massart halfspaces that achieves $\eta + \epsilon$. (2) Based on (1), we develop a blackbox knowledge distillation procedure to convert an arbitrarily complex classifier to an equally good proper classifier. (3) By leveraging a simple but overlooked connection to \emph{evolvability}, we show any SQ algorithm requires super-polynomially many queries to achieve $\mathsf{OPT} + \epsilon$. We then zoom out to study generalized linear models and give an efficient algorithm for learning under a challenging new corruption model generalizing Massart noise. Finally we study our algorithm for learning halfspaces under Massart noise empirically and find that it exhibits some appealing fairness properties as a byproduct of its strong provable robustness guarantees.",TRAD,,Algorithm,,,,,,,,,,,,,,"@inproceedings{NEURIPS2020_5f8b73c0,
 author = {Chen, Sitan and Koehler, Frederic and Moitra, Ankur and Yau, Morris},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {8391--8403},
 publisher = {Curran Associates, Inc.},
 title = {Classification Under Misspecification: Halfspaces, Generalized Linear Models, and Evolvability},
 url = {https://proceedings.neurips.cc/paper/2020/file/5f8b73c0d4b1bf60dd7173b660b87c29-Paper.pdf},
 volume = {33},
 year = {2020}
}

"
Neurips,2020,Incorporating Interpretable Output Constraints in Bayesian Neural Networks,https://proceedings.neurips.cc//paper/2020/hash/95c7dfc5538e1ce71301cf92a9a96bd0-Abstract.html,"['Wanqian Yang', ' Lars Lorch', ' Moritz Graule', ' Himabindu Lakkaraju', ' Finale Doshi-Velez']",7,"Domains where supervised models are deployed often come with task-specific constraints, such as prior expert knowledge on the ground-truth function, or desiderata like safety and fairness. We introduce a novel probabilistic framework for reasoning with such constraints and formulate a prior that enables us to effectively incorporate them into Bayesian neural networks (BNNs), including a variant that can be amortized over tasks. The resulting Output-Constrained BNN (OC-BNN) is fully consistent with the Bayesian framework for uncertainty quantification and is amenable to black-box inference. Unlike typical BNN inference in uninterpretable parameter space, OC-BNNs widen the range of functional knowledge that can be incorporated, especially for model users without expertise in machine learning. We demonstrate the efficacy of OC-BNNs on real-world datasets, spanning multiple domains such as healthcare, criminal justice, and credit scoring.",TRAD,,Algorithm,,,,,,,,,,,,,,"@inproceedings{NEURIPS2020_95c7dfc5,
 author = {Yang, Wanqian and Lorch, Lars and Graule, Moritz and Lakkaraju, Himabindu and Doshi-Velez, Finale},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {12721--12731},
 publisher = {Curran Associates, Inc.},
 title = {Incorporating Interpretable Output Constraints in Bayesian Neural Networks},
 url = {https://proceedings.neurips.cc/paper/2020/file/95c7dfc5538e1ce71301cf92a9a96bd0-Paper.pdf},
 volume = {33},
 year = {2020}
}

"
Neurips,2020,A Fair Classifier Using Kernel Density Estimation,https://proceedings.neurips.cc//paper/2020/hash/ac3870fcad1cfc367825cda0101eee62-Abstract.html,"['Jaewoong Cho', ' Gyeongjo Hwang', ' Changho Suh']",35,"As machine learning becomes prevalent in a widening array of sensitive applications such as job hiring and criminal justice, one critical aspect that machine learning classifiers should respect is to ensure fairness: guaranteeing the irrelevancy of a prediction output to sensitive attributes such as gender and race. In this work, we develop a kernel density estimation trick to quantify fairness measures that capture the degree of the irrelevancy. A key feature of our approach is that quantified fairness measures can be expressed as differentiable functions w.r.t. classifier model parameters. This then allows us to enjoy prominent gradient descent to readily solve an interested optimization problem that fully respects fairness constraints. We focus on a binary classification setting and two well-known definitions of group fairness: Demographic Parity (DP) and Equalized Odds (EO). Our experiments both on synthetic and benchmark real datasets demonstrate that our algorithm outperforms prior fair classifiers in accuracy-fairness tradeoff performance both w.r.t. DP and EO.",TRAD,,Algorithm,,,,,,,,,,,,,,"@inproceedings{NEURIPS2020_ac3870fc,
 author = {Cho, Jaewoong and Hwang, Gyeongjo and Suh, Changho},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {15088--15099},
 publisher = {Curran Associates, Inc.},
 title = {A Fair Classifier Using Kernel Density Estimation},
 url = {https://proceedings.neurips.cc/paper/2020/file/ac3870fcad1cfc367825cda0101eee62-Paper.pdf},
 volume = {33},
 year = {2020}
}

"
Neurips,2020,Fair regression via plug-in estimator and recalibration with statistical guarantees,https://proceedings.neurips.cc//paper/2020/hash/ddd808772c035aed516d42ad3559be5f-Abstract.html,"['Evgenii Chzhen', ' Christophe Denis', ' Mohamed Hebiri', ' Luca Oneto', ' Massimiliano Pontil']",24,"We study the problem of learning an optimal regression function subject to a fairness constraint. It requires that, conditionally on the sensitive feature, the distribution of the function output remains the same. This constraint naturally extends the notion of demographic parity, often used in classification, to the regression setting. We tackle this problem by leveraging on a proxy-discretized version, for which we derive an explicit expression of the optimal fair predictor. This result naturally suggests a two stage approach, in which we first estimate the (unconstrained) regression function from a set of labeled data and then we recalibrate it with another set of unlabeled data. The recalibration step can be efficiently performed via a smooth optimization. We derive rates of convergence of the proposed estimator to the optimal fair predictor both in terms of the risk and fairness constraint. Finally, we present numerical experiments illustrating that the proposed method is often superior or competitive with state-of-the-art methods.",TRAD,,Algorithm,Metrics,,Fair regression,,,,,,,,,,,"@inproceedings{NEURIPS2020_ddd80877,
 author = {Chzhen, Evgenii and Denis, Christophe and Hebiri, Mohamed and Oneto, Luca and Pontil, Massimiliano},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {19137--19148},
 publisher = {Curran Associates, Inc.},
 title = {Fair regression via plug-in estimator and recalibration with statistical guarantees},
 url = {https://proceedings.neurips.cc/paper/2020/file/ddd808772c035aed516d42ad3559be5f-Paper.pdf},
 volume = {33},
 year = {2020}
}

"
Neurips,2020,Model Class Reliance for Random Forests,https://proceedings.neurips.cc//paper/2020/hash/fd512441a1a791770a6fa573d688bff5-Abstract.html,"['Gavin Smith', ' Roberto Mansilla', ' James Goulding']",10,"Variable Importance (VI) has traditionally been cast as the process of estimating each variables contribution to a predictive model's overall performance. Analysis of a single model instance, however, guarantees no insight into a variables relevance to underlying generative processes. Recent research has sought to address this concern via analysis of Rashomon sets - sets of alternative model instances that exhibit equivalent predictive performance to some reference model, but which take different functional forms. Measures such as Model Class Reliance (MCR) have been proposed, that are computed against Rashomon sets, in order to ascertain how much a variable must be relied on to make robust predictions, or whether alternatives exist. If MCR range is tight, we have no choice but to use a variable; if range is high then there exists competing, perhaps fairer models, that provide alternative explanations of the phenomena being examined. Applications are wide, from enabling construction of `fairer' models in areas such as recidivism, health analytics and ethical marketing. Tractable estimation of MCR for non-linear models is currently restricted to Kernel Regression under squared loss \cite{fisher2019all}. In this paper we introduce a new technique that extends computation of Model Class Reliance (MCR) to Random Forest classifiers and regressors. The proposed approach addresses a number of open research questions, and in contrast to prior Kernel SVM MCR estimation, runs in linearithmic rather than polynomial time. Taking a fundamentally different approach to previous work, we provide a solution for this important model class, identifying situations where irrelevant covariates do not improve predictions.",TRAD,,Algorithm,Metrics,,,,,,,,,,,,,"@inproceedings{NEURIPS2020_fd512441,
 author = {Smith, Gavin and Mansilla, Roberto and Goulding, James},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {22305--22315},
 publisher = {Curran Associates, Inc.},
 title = {Model Class Reliance for Random Forests},
 url = {https://proceedings.neurips.cc/paper/2020/file/fd512441a1a791770a6fa573d688bff5-Paper.pdf},
 volume = {33},
 year = {2020}
}

"
Neurips,2021,Sample Selection for Fair and Robust Training,https://proceedings.neurips.cc//paper/2021/hash/07563a3fe3bbe7e3ba84431ad9d055af-Abstract.html,"['Yuji Roh', ' Kangwook Lee', ' Steven Whang', ' Changho Suh']",20,"Fairness and robustness are critical elements of Trustworthy AI that need to be addressed together. Fairness is about learning an unbiased model while robustness is about learning from corrupted data, and it is known that addressing only one of them may have an adverse affect on the other. In this work, we propose a sample selection-based algorithm for fair and robust training. To this end, we formulate a combinatorial optimization problem for the unbiased selection of samples in the presence of data corruption. Observing that solving this optimization problem is strongly NP-hard, we propose a greedy algorithm that is efficient and effective in practice. Experiments show that our method obtains fairness and robustness that are better than or comparable to the state-of-the-art technique, both on synthetic and benchmark real datasets. Moreover, unlike other fair and robust training baselines, our algorithm can be used by only modifying the sampling step in batch selection without changing the training algorithm or leveraging additional clean data.",TRAD,,Algorithm,,,Enforces fair AND robust training,,,,,,,,,,,"@inproceedings{NEURIPS2021_07563a3f,
 author = {Roh, Yuji and Lee, Kangwook and Whang, Steven and Suh, Changho},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {815--827},
 publisher = {Curran Associates, Inc.},
 title = {Sample Selection for Fair and Robust Training},
 url = {https://proceedings.neurips.cc/paper/2021/file/07563a3fe3bbe7e3ba84431ad9d055af-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
Neurips,2021,Efficient Mirror Descent Ascent Methods for Nonsmooth Minimax Problems,https://proceedings.neurips.cc//paper/2021/hash/56503192b14190d3826780d47c0d3bf3-Abstract.html,"['Feihu Huang', ' Xidong Wu', ' Heng Huang']",11,"In the paper, we propose a class of efficient mirror descent ascent methods to solve the nonsmooth nonconvex-strongly-concave minimax problems by using dynamic mirror functions, and introduce a convergence analysis framework to conduct rigorous theoretical analysis for our mirror descent ascent methods. For our stochastic algorithms, we first prove that the mini-batch stochastic mirror descent ascent (SMDA) method obtains a gradient complexity of $O(\kappa^3\epsilon^{-4})$ for finding an $\epsilon$-stationary point, where $\kappa$ denotes the condition number. Further, we propose an accelerated stochastic mirror descent ascent (VR-SMDA) method based on the variance reduced technique. We prove that our VR-SMDA method achieves a lower gradient complexity of $O(\kappa^3\epsilon^{-3})$. For our deterministic algorithm, we prove that our deterministic mirror descent ascent (MDA) achieves a lower gradient complexity of $O(\sqrt{\kappa}\epsilon^{-2})$ under mild conditions, which matches the best known complexity in solving smooth nonconvex-strongly-concave minimax optimization. We conduct the experiments on fair classifier and robust neural network training tasks to demonstrate the efficiency of our new algorithms.",TRAD,,Algorithm,,,,,,,,,,,,,,"@inproceedings{NEURIPS2021_56503192,
 author = {Huang, Feihu and Wu, Xidong and Huang, Heng},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {10431--10443},
 publisher = {Curran Associates, Inc.},
 title = {Efficient Mirror Descent Ascent Methods for Nonsmooth Minimax Problems},
 url = {https://proceedings.neurips.cc/paper/2021/file/56503192b14190d3826780d47c0d3bf3-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
Neurips,2021,Adaptive Sampling for Minimax Fair Classification,https://proceedings.neurips.cc//paper/2021/hash/cd7c230fc5deb01ff5f7b1be1acef9cf-Abstract.html,"['Shubhanshu Shekhar', ' Greg Fields', ' Mohammad Ghavamzadeh', ' Tara Javidi']",12,"Machine learning models trained on uncurated datasets can often end up adversely affecting inputs belonging to underrepresented groups. To address this issue, we consider the problem of adaptively constructing training sets which allow us to learn classifiers that are fair in a {\em minimax} sense. We first propose an adaptive sampling algorithm based on the principle of \emph{optimism}, and derive theoretical bounds on its performance. We also propose heuristic extensions of this algorithm suitable for application to large scale, practical problems. Next, by deriving algorithm independent lower-bounds for a specific class of problems, we show that the performance achieved by our adaptive scheme cannot be improved in general. We then validate the benefits of adaptively constructing training sets via experiments on synthetic tasks with logistic regression classifiers, as well as on several real-world tasks using convolutional neural networks (CNNs).",TRAD,,Algorithm,,,,,,,,,,,,,,"@inproceedings{NEURIPS2021_cd7c230f,
 author = {Shekhar, Shubhanshu and Fields, Greg and Ghavamzadeh, Mohammad and Javidi, Tara},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {24535--24544},
 publisher = {Curran Associates, Inc.},
 title = {Adaptive Sampling for Minimax Fair Classification},
 url = {https://proceedings.neurips.cc/paper/2021/file/cd7c230fc5deb01ff5f7b1be1acef9cf-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
Neurips,2021,Post-processing for Individual Fairness,https://proceedings.neurips.cc//paper/2021/hash/d9fea4ca7e4a74c318ec27c1deb0796c-Abstract.html,"['Felix Petersen', ' Debarghya Mukherjee', ' Yuekai Sun', ' Mikhail Yurochkin']",23,"Post-processing in algorithmic fairness is a versatile approach for correcting bias in ML systems that are already used in production. The main appeal of post-processing is that it avoids expensive retraining. In this work, we propose general post-processing algorithms for individual fairness (IF). We consider a setting where the learner only has access to the predictions of the original model and a similarity graph between individuals, guiding the desired fairness constraints. We cast the IF post-processing problem as a graph smoothing problem corresponding to graph Laplacian regularization that preserves the desired ""treat similar individuals similarly"" interpretation. Our theoretical results demonstrate the connection of the new objective function to a local relaxation of the original individual fairness. Empirically, our post-processing algorithms correct individual biases in large-scale NLP models such as BERT, while preserving accuracy.",TRAD,,Algorithm,,,,,,,,,,,,,,"@inproceedings{NEURIPS2021_d9fea4ca,
 author = {Petersen, Felix and Mukherjee, Debarghya and Sun, Yuekai and Yurochkin, Mikhail},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {25944--25955},
 publisher = {Curran Associates, Inc.},
 title = {Post-processing for Individual Fairness},
 url = {https://proceedings.neurips.cc/paper/2021/file/d9fea4ca7e4a74c318ec27c1deb0796c-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
Neurips,2021,Scalable and Stable Surrogates for Flexible Classifiers with Fairness Constraints,https://proceedings.neurips.cc//paper/2021/hash/fc2e6a440b94f64831840137698021e1-Abstract.html,"['Henry C Bendekgey', ' Erik Sudderth']",5,"We investigate how fairness relaxations scale to flexible classifiers like deep neural networks for images and text. We analyze an easy-to-use and robust way of imposing fairness constraints when training, and through this framework prove that some prior fairness surrogates exhibit degeneracies for non-convex models. We resolve these problems via three new surrogates: an adaptive data re-weighting, and two smooth upper-bounds that are provably more robust than some previous methods. Our surrogates perform comparably to the state-of-the-art on low-dimensional fairness benchmarks, while achieving superior accuracy and stability for more complex computer vision and natural language processing tasks.",TRAD,,Algorithm,,,,,,,,,,,,,,"@inproceedings{NEURIPS2021_fc2e6a44,
 author = {Bendekgey, Henry C and Sudderth, Erik},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {30023--30036},
 publisher = {Curran Associates, Inc.},
 title = {Scalable and Stable Surrogates for Flexible Classifiers with Fairness Constraints},
 url = {https://proceedings.neurips.cc/paper/2021/file/fc2e6a440b94f64831840137698021e1-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
ICLR,2020,R√©nyi Fair Inference,https://openreview.net/forum?id=HkgsUJrtDB,"['Sina Baharlouei', 'Maher Nouiehed', 'Ahmad Beirami', 'Meisam Razaviyayn']",51,"Machine learning algorithms have been increasingly deployed in critical automated decision-making systems that directly affect human lives. When these algorithms are solely trained to minimize the training/test error, they could suffer from systematic discrimination against individuals based on their sensitive attributes, such as gender or race. Recently, there has been a surge in machine learning society to develop algorithms for fair machine learning. 
 In particular, several adversarial learning procedures have been proposed to impose fairness. Unfortunately, these algorithms either can only impose fairness up to linear dependence between the variables, or they lack computational convergence guarantees. In this paper, we use R√©nyi correlation as a measure of fairness of machine learning models and develop a general training framework to impose fairness. In particular, we propose a min-max formulation which balances the accuracy and fairness when solved to optimality. For the case of discrete sensitive attributes, we suggest an iterative algorithm with theoretical convergence guarantee for solving the proposed min-max problem. Our algorithm and analysis are then specialized to fair classification and fair clustering problems. To demonstrate the performance of the proposed R√©nyi fair inference framework in practice, we compare it with well-known existing methods on several benchmark datasets. Experiments indicate that the proposed method has favorable empirical performance against state-of-the-art approaches.",TRAD,,Metric,Algorithm,,,,,,,,,,,,,"@inproceedings{
Baharlouei2020Rényi,
title={Rényi Fair Inference},
author={Sina Baharlouei and Maher Nouiehed and Ahmad Beirami and Meisam Razaviyayn},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=HkgsUJrtDB}
}"
ICLR,2020,Training individually fair ML models with sensitive subspace robustness,https://openreview.net/forum?id=B1gdkxHFDH,"['Mikhail Yurochkin', 'Amanda Bower', 'Yuekai Sun']",76,"We propose an approach to training machine learning models that are fair in the sense that their performance is invariant under certain perturbations to the features. For example, the performance of a resume screening system should be invariant under changes to the name of the applicant. We formalize this intuitive notion of fairness by connecting it to the original notion of individual fairness put forth by Dwork et al and show that the proposed approach achieves this notion of fairness. We also demonstrate the effectiveness of the approach on two machine learning tasks that are susceptible to gender and racial biases.",TRAD,,Algorithm,,,,,,,,,,,,,,"@inproceedings{
Yurochkin2020Training,
title={Training individually fair ML models with sensitive subspace robustness},
author={Mikhail Yurochkin and Amanda Bower and Yuekai Sun},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=B1gdkxHFDH}
}"
ICLR,2020,Conditional Learning of Fair Representations,https://openreview.net/forum?id=Hkekl0NFPr,"['Han Zhao', 'Amanda Coston', 'Tameem Adel', 'Geoffrey J. Gordon']",61,"We propose a novel algorithm for learning fair representations that can simultaneously mitigate two notions of disparity among different demographic subgroups. Two key components underpinning the design of our algorithm are balanced error rate and conditional alignment of representations. We show how these two components contribute to ensuring accuracy parity and equalized false-positive and false-negative rates across groups without impacting demographic parity. Furthermore, we also demonstrate both in theory and on two real-world experiments that the proposed algorithm leads to a better utility-fairness trade-off on balanced datasets compared with existing algorithms on learning fair representations.",TRAD,,Data,Algorithm,,,,,,,,,,,,,"@inproceedings{
Zhao2020Conditional,
title={Conditional Learning of Fair Representations},
author={Han Zhao and Amanda Coston and Tameem Adel and Geoffrey J. Gordon},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=Hkekl0NFPr}
}"
ICLR,2021,Individually Fair Rankings,https://openreview.net/forum?id=71zCSP_HuBN,"['Amanda Bower', ' Hamid Eftekhari', ' Mikhail Yurochkin', ' Yuekai Sun']",15,We develop an algorithm to train individually fair learning-to-rank (LTR) models. The proposed approach ensures items from minority groups appear alongside similar items from majority groups. This notion of fair ranking is based on the definition of individual fairness from supervised learning and is more nuanced than prior fair LTR approaches that simply ensure the ranking model provides underrepresented items with a basic level of exposure. The crux of our method is an optimal transport-based regularizer that enforces individual fairness and an efficient algorithm for optimizing the regularizer. We show that our approach leads to certifiably individually fair LTR models and demonstrate the efficacy of our method on ranking tasks subject to demographic biases.,TRAD,,Algorithm,,,,,,,,,,,,,,"@inproceedings{
bower2021individually,
title={Individually Fair Rankings},
author={Amanda Bower and Hamid Eftekhari and Mikhail Yurochkin and Yuekai Sun},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=71zCSP_HuBN}
}"
ICLR,2021,Individually Fair Gradient Boosting,https://openreview.net/forum?id=JBAa9we1AL,"['Alexander Vargo', ' Fan Zhang', ' Mikhail Yurochkin', ' Yuekai Sun']",8,"We consider the task of enforcing individual fairness in gradient boosting. Gradient boosting is a popular method for machine learning from tabular data, which arise often in applications where algorithmic fairness is a concern. At a high level, our approach is a functional gradient descent on a (distributionally) robust loss function that encodes our intuition of algorithmic fairness for the ML task at hand. Unlike prior approaches to individual fairness that only work with smooth ML models, our approach also works with non-smooth models such as decision trees. We show that our algorithm converges globally and generalizes. We also demonstrate the efficacy of our algorithm on three ML problems susceptible to algorithmic bias.",TRAD,,Algorithm,,,,,,,,,,,,,,"@inproceedings{
vargo2021individually,
title={Individually Fair Gradient Boosting},
author={Alexander Vargo and Fan Zhang and Mikhail Yurochkin and Yuekai Sun},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=JBAa9we1AL}
}"
ICLR,2021,SenSeI: Sensitive Set Invariance for Enforcing Individual Fairness,https://openreview.net/forum?id=DktZb97_Fx,"['Mikhail Yurochkin', ' Yuekai Sun']",31,"In this paper, we cast fair machine learning as invariant machine learning. We first formulate a version of individual fairness that enforces invariance on certain sensitive sets. We then design a transport-based regularizer that enforces this version of individual fairness and develop an algorithm to minimize the regularizer efficiently. Our theoretical results guarantee the proposed approach trains certifiably fair ML models. Finally, in the experimental studies we demonstrate improved fairness metrics in comparison to several recent fair training procedures on three ML tasks that are susceptible to algorithmic bias.",TRAD,,Algorithm,,,,,,,,,,,,,,"@inproceedings{
yurochkin2021sensei,
title={SenSeI: Sensitive Set Invariance for Enforcing Individual Fairness},
author={Mikhail Yurochkin and Yuekai Sun},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=DktZb97_Fx}
}"
ICLR,2021,Statistical inference for individual fairness,https://openreview.net/forum?id=z9k8BWL-_2u,"['Subha Maity', ' Songkai Xue', ' Mikhail Yurochkin', ' Yuekai Sun']",12,"As we rely on machine learning (ML) models to make more consequential decisions, the issue of ML models perpetuating or even exacerbating undesirable historical biases (e.g., gender and racial biases) has come to the fore of the public's attention. In this paper, we focus on the problem of detecting violations of individual fairness in ML models. We formalize the problem as measuring the susceptibility of ML models against a form of adversarial attack and develop a suite of inference tools for the adversarial cost function. The tools allow auditors to assess the individual fairness of ML models in a statistically-principled way: form confidence intervals for the worst-case performance differential between similar individuals and test hypotheses of model fairness with (asymptotic) non-coverage/Type I error rate control. We demonstrate the utility of our tools in a real-world case study.",TRAD,,Testing,,,,,,,,,,,,,,"@inproceedings{
maity2021statistical,
title={Statistical inference for individual fairness},
author={Subha Maity and Songkai Xue and Mikhail Yurochkin and Yuekai Sun},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=z9k8BWL-_2u}
}"
ICLR,2021,FairBatch: Batch Selection for Model Fairness,https://openreview.net/forum?id=YNnpaAKeCfx,"['Yuji Roh', ' Kangwook Lee', ' Steven Euijong Whang', ' Changho Suh']",43,"Training a fair machine learning model is essential to prevent demographic disparity. Existing techniques for improving model fairness require broad changes in either data preprocessing or model training, rendering themselves difficult-to-adopt for potentially already complex machine learning systems. We address this problem via the lens of bilevel optimization. While keeping the standard training algorithm as an inner optimizer, we incorporate an outer optimizer so as to equip the inner problem with an additional functionality: Adaptively selecting minibatch sizes for the purpose of improving model fairness. Our batch selection algorithm, which we call FairBatch, implements this optimization and supports prominent fairness measures: equal opportunity, equalized odds, and demographic parity. FairBatch comes with a significant implementation benefit -- it does not require any modification to data preprocessing or model training. For instance, a single-line change of PyTorch code for replacing batch selection part of model training suffices to employ FairBatch. Our experiments conducted both on synthetic and benchmark real data demonstrate that FairBatch can provide such functionalities while achieving comparable (or even greater) performances against the state of the arts. Furthermore, FairBatch can readily improve fairness of any pre-trained model simply via fine-tuning. It is also compatible with existing batch selection techniques intended for different purposes, such as faster convergence, thus gracefully achieving multiple purposes.",TRAD,,Algorithm,,,,,,,,,,,,,,"@inproceedings{
roh2021fairbatch,
title={FairBatch: Batch Selection for Model Fairness},
author={Yuji Roh and Kangwook Lee and Steven Euijong Whang and Changho Suh},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=YNnpaAKeCfx}
}"
ICLR,2021,Fair Mixup: Fairness via Interpolation,https://openreview.net/forum?id=DNl5s5BXeBn,"['Ching-Yao Chuang', ' Youssef Mroueh']",53,"Training classifiers under fairness constraints such as group fairness, regularizes the disparities of predictions between the groups. Nevertheless, even though the constraints are satisfied during training, they might not generalize at evaluation time. To improve the generalizability of fair classifiers, we propose fair mixup, a new data augmentation strategy for imposing the fairness constraint. In particular, we show that fairness can be achieved by regularizing the models on paths of interpolated samples between the groups. We use mixup, a powerful data augmentation strategy to generate these interpolates. We analyze fair mixup and empirically show that it ensures a better generalization for both accuracy and fairness measurement in tabular, vision, and language benchmarks.",TRAD,,Algorithm,,,,,,,,,,,,,,"@inproceedings{
chuang2021fair,
title={Fair Mixup: Fairness via Interpolation},
author={Ching-Yao Chuang and Youssef Mroueh},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=DNl5s5BXeBn}
}"
ICLR,2021,Tilted Empirical Risk Minimization,https://openreview.net/forum?id=K5YasWXZT3O,"['Tian Li', ' Ahmad Beirami', ' Maziar Sanjabi', ' Virginia Smith']",64,"Empirical risk minimization (ERM) is typically designed to perform well on the average loss, which can result in estimators that are sensitive to outliers, generalize poorly, or treat subgroups unfairly. While many methods aim to address these problems individually, in this work, we explore them through a unified framework---tilted empirical risk minimization (TERM). In particular, we show that it is possible to flexibly tune the impact of individual losses through a straightforward extension to ERM using a hyperparameter called the tilt. We provide several interpretations of the resulting framework: We show that TERM can increase or decrease the influence of outliers, respectively, to enable fairness or robustness; has variance-reduction properties that can benefit generalization; and can be viewed as a smooth approximation to a superquantile method. We develop batch and stochastic first-order optimization methods for solving TERM, and show that the problem can be efficiently solved relative to common alternatives. Finally, we demonstrate that TERM can be used for a multitude of applications, such as enforcing fairness between subgroups, mitigating the effect of outliers, and handling class imbalance. TERM is not only competitive with existing solutions tailored to these individual problems, but can also enable entirely new applications, such as simultaneously addressing outliers and promoting fairness.",TRAD,,Algorithm,,,,,,,,,,,,,,"@inproceedings{
li2021tilted,
title={Tilted Empirical Risk Minimization},
author={Tian Li and Ahmad Beirami and Maziar Sanjabi and Virginia Smith},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=K5YasWXZT3O}
}"
ICLR,2022,Controlling Directions Orthogonal to a Classifier,https://openreview.net/forum?id=DIjCrlsu6Z,"['Yilun Xu', ' Hao He', ' Tianxiao Shen', ' Tommi S. Jaakkola']",5,"We propose to identify directions invariant to a given classifier so that these directions can be controlled in tasks such as style transfer. While orthogonal decomposition is directly identifiable when the given classifier is linear, we formally define a notion of orthogonality in the non-linear case. We also provide a surprisingly simple method for constructing the orthogonal classifier (a classifier utilizing directions other than those of the given classifier). Empirically, we present three use cases where controlling orthogonal variation is important: style transfer, domain adaptation, and fairness. The orthogonal classifier enables desired style transfer when domains vary in multiple aspects, improves domain adaptation with label shifts and mitigates the unfairness as a predictor. The code is available at https://github.com/Newbeeer/orthogonal_classifier",TRAD,,Algorithm,,,,,,,,,,,,,,"@inproceedings{
xu2022controlling,
title={Controlling Directions Orthogonal to a Classifier},
author={Yilun Xu and Hao He and Tianxiao Shen and Tommi S. Jaakkola},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=DIjCrlsu6Z}
}"
ICLR,2022,MaGNET: Uniform Sampling from Deep Generative Network Manifolds Without Retraining,https://openreview.net/forum?id=r5qumLiYwf9,"['Ahmed Imtiaz Humayun', ' Randall Balestriero', ' Richard Baraniuk']",8,"Deep Generative Networks (DGNs) are extensively employed in Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and their variants to approximate the data manifold, and data distribution on that manifold. However, training samples are often obtained based on preferences, costs, or convenience producing artifacts in the empirical data distribution e.g. the large fraction of smiling faces in the CelebA dataset or the large fraction of dark-haired individuals in FFHQ). {\em These inconsistencies will be reproduced when sampling from the trained DGN, which has far-reaching potential implications for fairness, data augmentation, anomaly detection, domain adaptation, and beyond.} In response, we develop a differential geometry based sampler -coined MaGNET- that, given any trained DGN, produces samples that are uniformly distributed on the learned manifold. We prove theoretically and empirically that our technique produces a uniform distribution on the manifold regardless of the training set distribution. We perform a range of experiments on various datasets and DGNs. One of them considers the state-of-the-art StyleGAN2 trained on FFHQ dataset, where uniform sampling via MaGNET increases distribution precision \& recall by 4.12\% \& 3.01\% and decreases gender bias by 41.2\%, without requiring labels or retraining.",TRAD,,Algorithm,,,,,,,,,,,,,,"@inproceedings{
humayun2022magnet,
title={Ma{GNET}: Uniform Sampling from Deep Generative Network Manifolds Without Retraining},
author={Ahmed Imtiaz Humayun and Randall Balestriero and Richard Baraniuk},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=r5qumLiYwf9}
}"
ICLR,2022,FairCal: Fairness Calibration for Face Verification,https://openreview.net/forum?id=nRj0NcmSuxb,"['Tiago Salvador', ' Stephanie Cairns', ' Vikram Voleti', ' Noah Marshall', ' Adam M Oberman']",4,"Despite being widely used, face recognition models suffer from bias: the probability of a false positive (incorrect face match) strongly depends on sensitive attributes such as the ethnicity of the face. As a result, these models can disproportionately and negatively impact minority groups, particularly when used by law enforcement. The majority of bias reduction methods have several drawbacks: they use an end-to-end retraining approach, may not be feasible due to privacy issues, and often reduce accuracy. An alternative approach is post-processing methods that build fairer decision classifiers using the features of pre-trained models, thus avoiding the cost of retraining. However, they still have drawbacks: they reduce accuracy (AGENDA, FTC), or require retuning for different false positive rates (FSN). In this work, we introduce the Fairness Calibration (FairCal) method, a post-training approach that simultaneously: (i) increases model accuracy (improving the state-of-the-art), (ii) produces fairly-calibrated probabilities, (iii) significantly reduces the gap in the false positive rates, (iv) does not require knowledge of the sensitive attribute, and (v) does not require retraining, training an additional model or retuning. We apply it to the task of Face Verification, and obtain state-of-the-art results with all the above advantages.",TRAD,,Algorithm,,,,,,,,,,,,,,"@inproceedings{
salvador2022faircal,
title={FairCal: Fairness Calibration for Face Verification},
author={Tiago Salvador and Stephanie Cairns and Vikram Voleti and Noah Marshall and Adam M Oberman},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=nRj0NcmSuxb}
}"
ICLR,2022,Generalized Demographic Parity for Group Fairness,https://openreview.net/forum?id=YigKlMJwjye,"['Zhimeng Jiang', ' Xiaotian Han', ' Chao Fan', ' Fan Yang', ' Ali Mostafavi', ' Xia Hu']",5,"This work aims to generalize demographic parity to continuous sensitive attributes while preserving tractable computation. Current fairness metrics for continuous sensitive attributes largely rely on intractable statistical independence between variables, such as Hirschfeld-Gebelein-Renyi (HGR) and mutual information. Statistical fairness metrics estimation relying on either tractable bounds or neural network approximation, however, are not sufficiently trustful to rank algorithms prediction bias due to lack of estimation accuracy guarantee. To make fairness metrics trustable, we propose \textit{\underline{G}eneralized \underline{D}emographic \underline{P}arity} (GDP), a group fairness metric for continuous and discrete attributes. We show the understanding of GDP from the probability perspective and theoretically reveal the connection between GDP regularizer and adversarial debiasing. To estimate GDP, we adopt hard and soft group strategies via the one-hot or the soft group indicator, representing the membership of each sample in different groups of the sensitive attribute. We provably and numerically show that the soft group strategy achieves a faster estimation error convergence rate. Experiments show the better bias mitigation performance of GDP regularizer, compared with adversarial debiasing, for regression and classification tasks in tabular and graph benchmarks.",TRAD,,Metric,,,,,,,,,,,,,,"@inproceedings{
jiang2022generalized,
title={Generalized Demographic Parity for Group Fairness},
author={Zhimeng Jiang and Xiaotian Han and Chao Fan and Fan Yang and Ali Mostafavi and Xia Hu},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=YigKlMJwjye}
}"
AIES,2020,Measuring Fairness in an Unfair World,https://dl.acm.org/doi/10.1145/3375627.3375854,['Jonathan Herington'],10,"Computer scientists have made great strides in characterizing different measures of algorithmic fairness, and showing that certain measures of fairness cannot be jointly satisfied. In this paper, I argue that the three most popular families of measures - unconditional independence, target-conditional independence and classification-conditional independence - make assumptions that are unsustainable in the context of an unjust world. I begin by introducing the measures and the implicit idealizations they make about the underlying causal structure of the contexts in which they are deployed. I then discuss how these idealizations fall apart in the context of historical injustice, ongoing unmodeled oppression, and the permissibility of using sensitive attributes to rectify injustice. In the final section, I suggest an alternative framework for measuring fairness in the context of existing injustice: distributive fairness.",TRAD/NOT,,Metrics,,,Maybe not even relevant,,,,,,,,,,,"@inproceedings{10.1145/3375627.3375854,
author = {Herington, Jonathan},
title = {Measuring Fairness in an Unfair World},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375854},
doi = {10.1145/3375627.3375854},
abstract = {Computer scientists have made great strides in characterizing different measures of algorithmic fairness, and showing that certain measures of fairness cannot be jointly satisfied. In this paper, I argue that the three most popular families of measures - unconditional independence, target-conditional independence and classification-conditional independence - make assumptions that are unsustainable in the context of an unjust world. I begin by introducing the measures and the implicit idealizations they make about the underlying causal structure of the contexts in which they are deployed. I then discuss how these idealizations fall apart in the context of historical injustice, ongoing unmodeled oppression, and the permissibility of using sensitive attributes to rectify injustice. In the final section, I suggest an alternative framework for measuring fairness in the context of existing injustice: distributive fairness.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {286–292},
numpages = {7},
keywords = {algorithmic decision-making, discrimination, causal inference, fairness, distributive justice},
location = {New York, NY, USA},
series = {AIES '20}
}
  
"
Neurips,2021,An Axiomatic Theory of Provably-Fair Welfare-Centric Machine Learning,https://proceedings.neurips.cc//paper/2021/hash/8b0bb3eff8c1e5bf7f206125959921d7-Abstract.html,['Cyrus Cousins'],8,"We address an inherent difficulty in welfare-theoretic fair machine learning (ML), by proposing an equivalently-axiomatically justified alternative setting, and studying the resulting computational and statistical learning questions. Welfare metrics quantify overall wellbeing across a population of groups, and welfare-based objectives and constraints have recently been proposed to incentivize fair ML methods to satisfy their diverse needs. However, many ML problems are cast as loss minimization tasks, rather than utility maximization, and thus require nontrivial modeling to construct utility functions. We define a complementary metric, termed malfare, measuring overall societal harm, with axiomatic justification via the standard axioms of cardinal welfare, and cast fair ML as malfare minimization over the risk values (expected losses) of each group. Surprisingly, the axioms of cardinal welfare (malfare) dictate that this is not equivalent to simply defining utility as negative loss and maximizing welfare. Building upon these concepts, we define fair-PAC learning, where a fair-PAC learner is an algorithm that learns an Œµ-Œ¥ malfare-optimal model with bounded sample complexity, for any data distribution and (axiomatically justified) malfare concept. Finally, we show conditions under which many standard PAC-learners may be converted to fair-PAC learners, which places fair-PAC learning on firm theoretical ground, as it yields statistical ‚Äî and in some cases computational ‚Äî efficiency guarantees for many well-studied ML models. Fair-PAC learning is also practically relevant, as it democratizes fair ML by providing concrete training algorithms with rigorous generalization guarantees.",TRAD/NOT,,Metric,,,,,,,,,,,,,,"@inproceedings{NEURIPS2021_8b0bb3ef,
 author = {Cousins, Cyrus},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {16610--16621},
 publisher = {Curran Associates, Inc.},
 title = {An Axiomatic Theory of Provably-Fair Welfare-Centric Machine Learning},
 url = {https://proceedings.neurips.cc/paper/2021/file/8b0bb3eff8c1e5bf7f206125959921d7-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
Neurips,2019,Offline Contextual Bandits with High Probability Fairness Guarantees,https://proceedings.neurips.cc//paper/2019/hash/d69768b3da745b77e82cdbddcc8bac98-Abstract.html,"['Blossom Metevier', ' Stephen Giguere', ' Sarah Brockman', ' Ari Kobren', ' Yuriy Brun', ' Emma Brunskill', ' Philip S. Thomas']",32,"We present RobinHood, an ofÔ¨Çine contextual bandit algorithm designed to satisfy a broad family of fairness constraints. Our algorithm accepts multiple fairness deÔ¨Ånitions and allows users to construct their own unique fairness deÔ¨Ånitions for the problem at hand. We provide a theoretical analysis of RobinHood, which includes a proof that it will not return an unfair solution with probability greater than a user-speciÔ¨Åed threshold. We validate our algorithm on three applications: a tutoring system in which we conduct a user study and consider multiple unique fairness deÔ¨Ånitions; a loan approval setting (using the Statlog German credit data set) in which well-known fairness deÔ¨Ånitions are applied; and criminal recidivism (using data released by ProPublica). In each setting, our algorithm is able to produce fair policies that achieve performance competitive with other ofÔ¨Çine and online contextual bandit algorithms.",TRAD/SCOPE,,Algorithm,,Methods,IS a method of fair decision making but is with a bandit approach so maybe out of scope,,,,,,,,,,,"@inproceedings{NEURIPS2019_d69768b3,
 author = {Metevier, Blossom and Giguere, Stephen and Brockman, Sarah and Kobren, Ari and Brun, Yuriy and Brunskill, Emma and Thomas, Philip S.},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Offline Contextual Bandits with High Probability Fairness Guarantees},
 url = {https://proceedings.neurips.cc/paper/2019/file/d69768b3da745b77e82cdbddcc8bac98-Paper.pdf},
 volume = {32},
 year = {2019}
}

"
Neurips,2020,Approximate Heavily-Constrained Learning with Lagrange Multiplier Models,https://proceedings.neurips.cc//paper/2020/hash/62db9e3397c76207a687c360e0243317-Abstract.html,"['Harikrishna Narasimhan', ' Andrew Cotter', ' Yichen Zhou', ' Serena Wang', ' Wenshuo Guo']",6,"In machine learning applications such as ranking fairness or fairness over intersectional groups, one often encounters optimization problems with an extremely large number of constraints. In particular, with ranking fairness tasks, there may even be a variable number of constraints, e.g. one for each query in the training set. In these cases, the standard approach of optimizing a Lagrangian while maintaining one Lagrange multiplier per constraint may no longer be practical. Our proposal is to associate a feature vector with each constraint, and to learn a ``multiplier model‚Äô‚Äô that maps each such vector to the corresponding Lagrange multiplier. We prove optimality, approximate feasibility and generalization guarantees under assumptions on the flexibility of the multiplier model, and empirically demonstrate that our method is effective on real-world case studies.",TRAD/SCOPE,,Algorithm,,,Fair ranking algo,,,,,,,,,,,"@inproceedings{NEURIPS2020_62db9e33,
 author = {Narasimhan, Harikrishna and Cotter, Andrew and Zhou, Yichen and Wang, Serena and Guo, Wenshuo},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {8693--8703},
 publisher = {Curran Associates, Inc.},
 title = {Approximate Heavily-Constrained Learning with Lagrange Multiplier Models},
 url = {https://proceedings.neurips.cc/paper/2020/file/62db9e3397c76207a687c360e0243317-Paper.pdf},
 volume = {33},
 year = {2020}
}

"
Neurips,2020,Metric-Free Individual Fairness in Online Learning,https://proceedings.neurips.cc//paper/2020/hash/80b618ebcac7aa97a6dac2ba65cb7e36-Abstract.html,"['Yahav Bechavod', ' Christopher Jung', ' Steven Z. Wu']",27,"We study an online learning problem subject to the constraint of individual fairness, which requires that similar individuals are treated similarly. Unlike prior work on individual fairness, we do not assume the similarity measure among individuals is known, nor do we assume that such measure takes a certain parametric form. Instead, we leverage the existence of an auditor who detects fairness violations without enunciating the quantitative measure. In each round, the auditor examines the learner's decisions and attempts to identify a pair of individuals that are treated unfairly by the learner. We provide a general reduction framework that reduces online classification in our model to standard online classification, which allows us to leverage existing online learning algorithms to achieve sub-linear regret and number of fairness violations. Surprisingly, in the stochastic setting where the data are drawn independently from a distribution, we are also able to establish PAC-style fairness and accuracy generalization guarantees (Rothblum and Yona (2018)), despite only having access to a very restricted form of fairness feedback. Our fairness generalization bound qualitatively matches the uniform convergence bound of Rothblum and Yona (2018), while also providing a meaningful accuracy generalization guarantee. Our results resolve an open question by Gillen et al. (2018) by showing that online learning under an unknown individual fairness constraint is possible even without assuming a strong parametric form of the underlying similarity measure.",TRAD/SCOPE,,,,,,,,,,,,,,,,"@inproceedings{NEURIPS2020_80b618eb,
 author = {Bechavod, Yahav and Jung, Christopher and Wu, Steven Z.},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {11214--11225},
 publisher = {Curran Associates, Inc.},
 title = {Metric-Free Individual Fairness in Online Learning},
 url = {https://proceedings.neurips.cc/paper/2020/file/80b618ebcac7aa97a6dac2ba65cb7e36-Paper.pdf},
 volume = {33},
 year = {2020}
}

"
Neurips,2020,Group-Fair Online Allocation in Continuous Time,https://proceedings.neurips.cc//paper/2020/hash/9ec0cfdc84044494e10582436e013e64-Abstract.html,"['Semih Cayci', ' Swati Gupta', ' Atilla Eryilmaz']",11,"The theory of discrete-time online learning has been successfully applied in many problems that involve sequential decision-making under uncertainty. However, in many applications including contractual hiring in online freelancing platforms and server allocation in cloud computing systems, the outcome of each action is observed only after a random and action-dependent time. Furthermore, as a consequence of certain ethical and economic concerns, the controller may impose deadlines on the completion of each task, and require fairness across different groups in the allocation of total time budget $B$. In order to address these applications, we consider continuous-time online learning problem with fairness considerations, and present a novel framework based on continuous-time utility maximization. We show that this formulation recovers reward-maximizing, max-min fair and proportionally fair allocation rules across different groups as special cases. We characterize the optimal offline policy, which allocates the total time between different actions in an optimally fair way (as defined by the utility function), and impose deadlines to maximize time-efficiency. In the absence of any statistical knowledge, we propose a novel online learning algorithm based on dual ascent optimization for time averages, and prove that it achieves $\tilde{O}(B^{-1/2})$ regret bound.",TRAD/SCOPE,,Algorithm,Metrics,,,,,,,,,,,,,"@inproceedings{NEURIPS2020_9ec0cfdc,
 author = {Cayci, Semih and Gupta, Swati and Eryilmaz, Atilla},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {13750--13761},
 publisher = {Curran Associates, Inc.},
 title = {Group-Fair Online Allocation in Continuous Time},
 url = {https://proceedings.neurips.cc/paper/2020/file/9ec0cfdc84044494e10582436e013e64-Paper.pdf},
 volume = {33},
 year = {2020}
}

"
Neurips,2021,Two-sided fairness in rankings via Lorenz dominance,https://proceedings.neurips.cc//paper/2021/hash/48259990138bc03361556fb3f94c5d45-Abstract.html,"['Virginie Do', ' Sam Corbett-Davies', ' Jamal Atif', ' Nicolas Usunier']",13,"We consider the problem of generating rankings that are fair towards both users and item producers in recommender systems. We address both usual recommendation (e.g., of music or movies) and reciprocal recommendation (e.g., dating). Following concepts of distributive justice in welfare economics, our notion of fairness aims at increasing the utility of the worse-off individuals, which we formalize using the criterion of Lorenz efficiency. It guarantees that rankings are Pareto efficient, and that they maximally redistribute utility from better-off to worse-off, at a given level of overall utility. We propose to generate rankings by maximizing concave welfare functions, and develop an efficient inference procedure based on the Frank-Wolfe algorithm. We prove that unlike existing approaches based on fairness constraints, our approach always produces fair rankings. Our experiments also show that it increases the utility of the worse-off at lower costs in terms of overall utility.",TRAD/SCOPE,,Algorithm,Metrics,Methods,Fair ranking algo,,,,,,,,,,,"@inproceedings{NEURIPS2021_48259990,
 author = {Do, Virginie and Corbett-Davies, Sam and Atif, Jamal and Usunier, Nicolas},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {8596--8608},
 publisher = {Curran Associates, Inc.},
 title = {Two-sided fairness in rankings via Lorenz dominance},
 url = {https://proceedings.neurips.cc/paper/2021/file/48259990138bc03361556fb3f94c5d45-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
Neurips,2021,Fairness in Ranking under Uncertainty,https://proceedings.neurips.cc//paper/2021/hash/63c3ddcc7b23daa1e42dc41f9a44a873-Abstract.html,"['Ashudeep Singh', ' David Kempe', ' Thorsten Joachims']",12,"Fairness has emerged as an important consideration in algorithmic decision making. Unfairness occurs when an agent with higher merit obtains a worse outcome than an agent with lower merit. Our central point is that a primary cause of unfairness is uncertainty. A principal or algorithm making decisions never has access to the agents' true merit, and instead uses proxy features that only imperfectly predict merit (e.g., GPA, star ratings, recommendation letters). None of these ever fully capture an agent's merit; yet existing approaches have mostly been defining fairness notions directly based on observed features and outcomes.Our primary point is that it is more principled to acknowledge and model the uncertainty explicitly. The role of observed features is to give rise to a posterior distribution of the agents' merits. We use this viewpoint to define a notion of approximate fairness in ranking. We call an algorithm $\phi$-fair (for $\phi \in [0,1]$) if it has the following property for all agents $x$ and all $k$: if agent $x$ is among the top $k$ agents with respect to merit with probability at least $\rho$ (according to the posterior merit distribution), then the algorithm places the agent among the top $k$ agents in its ranking with probability at least $\phi \rho$.We show how to compute rankings that optimally trade off approximate fairness against utility to the principal. In addition to the theoretical characterization, we present an empirical analysis of the potential impact of the approach in simulation studies. For real-world validation, we applied the approach in the context of a paper recommendation system that we built and fielded at the KDD 2020 conference.",TRAD/SCOPE,,Algorithm,Metrics,,Fair ranking algo,,,,,,,,,,,"@inproceedings{NEURIPS2021_63c3ddcc,
 author = {Singh, Ashudeep and Kempe, David and Joachims, Thorsten},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {11896--11908},
 publisher = {Curran Associates, Inc.},
 title = {Fairness in Ranking under Uncertainty},
 url = {https://proceedings.neurips.cc/paper/2021/file/63c3ddcc7b23daa1e42dc41f9a44a873-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
Neurips,2021,Gradient Driven Rewards to Guarantee Fairness in Collaborative Machine Learning,https://proceedings.neurips.cc//paper/2021/hash/8682cc30db9c025ecd3fee433f8ab54c-Abstract.html,"['Xinyi Xu', ' Lingjuan Lyu', ' Xingjun Ma', ' Chenglin Miao', ' Chuan Sheng Foo', ' Bryan Kian Hsiang Low']",14,"In collaborative machine learning(CML), multiple agents pool their resources(e.g., data) together for a common learning task. In realistic CML settings where the agents are self-interested and not altruistic, they may be unwilling to share data or model information without adequate rewards. Furthermore, as the data/model information shared by the agents may differ in quality, designing rewards which are fair to them is important so that they would not feel exploited nor discouraged from sharing. In this paper, we adopt federated learning as the CML paradigm, propose a novel cosine gradient Shapley value(CGSV) to fairly evaluate the expected marginal contribution of each agent‚Äôs uploaded model parameter update/gradient without needing an auxiliary validation dataset, and based on the CGSV, design a novel training-time gradient reward mechanism with a fairness guarantee by sparsifying the aggregated parameter update/gradient downloaded from the server as reward to each agent such that its resulting quality is commensurate to that of the agent‚Äôs uploaded parameter update/gradient. We empirically demonstrate the effectiveness of our fair gradient reward mechanism on multiple benchmark datasets in terms of fairness, predictive performance, and time overhead.",TRAD/SCOPE,,,Algorithm,,Federated learning,,,,,,,,,,,"@inproceedings{NEURIPS2021_8682cc30,
 author = {Xu, Xinyi and Lyu, Lingjuan and Ma, Xingjun and Miao, Chenglin and Foo, Chuan Sheng and Low, Bryan Kian Hsiang},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {16104--16117},
 publisher = {Curran Associates, Inc.},
 title = {Gradient Driven Rewards to Guarantee Fairness in Collaborative Machine Learning},
 url = {https://proceedings.neurips.cc/paper/2021/file/8682cc30db9c025ecd3fee433f8ab54c-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
Neurips,2021,Fair Sparse Regression with Clustering: An Invex Relaxation for a Combinatorial Problem,https://proceedings.neurips.cc//paper/2021/hash/c39b9a47811f1eaf3244a63ae8c22734-Abstract.html,"['Adarsh Barik', ' Jean Honorio']",4,"In this paper, we study the problem of fair sparse regression on a biased dataset where bias depends upon a hidden binary attribute. The presence of a hidden attribute adds an extra layer of complexity to the problem by combining sparse regression and clustering with unknown binary labels. The corresponding optimization problem is combinatorial, but we propose a novel relaxation of it as an invex optimization problem. To the best of our knowledge, this is the first invex relaxation for a combinatorial problem. We show that the inclusion of the debiasing/fairness constraint in our model has no adverse effect on the performance. Rather, it enables the recovery of the hidden attribute. The support of our recovered regression parameter vector matches exactly with the true parameter vector. Moreover, we simultaneously solve the clustering problem by recovering the exact value of the hidden attribute for each sample. Our method uses carefully constructed primal dual witnesses to provide theoretical guarantees for the combinatorial problem. To that end, we show that the sample complexity of our method is logarithmic in terms of the dimension of the regression parameter vector.",TRAD/SCOPE,,Algorithm,,,Clustering,,,,,,,,,,,"@inproceedings{NEURIPS2021_c39b9a47,
 author = {Barik, Adarsh and Honorio, Jean},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {23245--23257},
 publisher = {Curran Associates, Inc.},
 title = {Fair Sparse Regression with Clustering: An Invex Relaxation for a Combinatorial Problem},
 url = {https://proceedings.neurips.cc/paper/2021/file/c39b9a47811f1eaf3244a63ae8c22734-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
AIES,2018,Fairness in Deceased Organ Matching,https://dl.acm.org/doi/10.1145/3278721.3278749,"['Nicholas Mattei', 'Abdallah Saffidine', 'Toby Walsh']",17,"As algorithms are given responsibility to make decisions that impact our lives, there is increasing awareness of the need to ensure the fairness of these decisions. One of the first challenges then is to decide what fairness means in a particular context. We consider here fairness in deciding how to match organs donated by deceased donors to patients. Due to the increasing age of patients on the waiting list, and of organs being donated, the current ""first come, first served'' mechanism used in Australia is under review to take account of age of patients and of organs. We consider how to revise the mechanism to take account of age fairly. We identify a number of different types of fairness, such as to patients, to regions and to blood types and consider how they can be achieved.",Contextual,,Problem Formulation,Definitions,????,Very contextualized--might actually be contextualized fairness and not pipeline fairness,,,,,,,,,,,"@inproceedings{10.1145/3278721.3278749,
author = {Mattei, Nicholas and Saffidine, Abdallah and Walsh, Toby},
title = {Fairness in Deceased Organ Matching},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278749},
doi = {10.1145/3278721.3278749},
abstract = {As algorithms are given responsibility to make decisions that impact our lives, there is increasing awareness of the need to ensure the fairness of these decisions. One of the first challenges then is to decide what fairness means in a particular context. We consider here fairness in deciding how to match organs donated by deceased donors to patients. Due to the increasing age of patients on the waiting list, and of organs being donated, the current ""first come, first served'' mechanism used in Australia is under review to take account of age of patients and of organs. We consider how to revise the mechanism to take account of age fairly. We identify a number of different types of fairness, such as to patients, to regions and to blood types and consider how they can be achieved.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {236–242},
numpages = {7},
keywords = {ethical principles, preferences, decision making, kidney allocation},
location = {New Orleans, LA, USA},
series = {AIES '18}
}
  
"
AIES,2019,Paradoxes in Fair Computer-Aided Decision Making,https://dl.acm.org/doi/10.1145/3306618.3314242,"['Andrew Morgan', 'Rafael Pass']",9,"Computer-aided decision making--where a human decision-maker is aided by a computational classifier in making a decision--is becoming increasingly prevalent. For instance, judges in at least nine states make use of algorithmic tools meant to determine ""recidivism risk scores"" for criminal defendants in sentencing, parole, or bail decisions. A subject of much recent debate is whether such algorithmic tools are ""fair"" in the sense that they do not discriminate against certain groups (e.g., races) of people. Our main result shows that for ""non-trivial"" computer-aided decision making, either the classifier must be discriminatory, or a rational decision-maker using the output of the classifier is forced to be discriminatory. We further provide a complete characterization of situations where fair computer-aided decision making is possible.",YES,,Organizational Realities,,,,,,,,,,,,,,"@inproceedings{10.1145/3306618.3314242,
author = {Morgan, Andrew and Pass, Rafael},
title = {Paradoxes in Fair Computer-Aided Decision Making},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314242},
doi = {10.1145/3306618.3314242},
abstract = {Computer-aided decision making--where a human decision-maker is aided by a computational classifier in making a decision--is becoming increasingly prevalent. For instance, judges in at least nine states make use of algorithmic tools meant to determine ""recidivism risk scores"" for criminal defendants in sentencing, parole, or bail decisions. A subject of much recent debate is whether such algorithmic tools are ""fair"" in the sense that they do not discriminate against certain groups (e.g., races) of people. Our main result shows that for ""non-trivial"" computer-aided decision making, either the classifier must be discriminatory, or a rational decision-maker using the output of the classifier is forced to be discriminatory. We further provide a complete characterization of situations where fair computer-aided decision making is possible.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {85–90},
numpages = {6},
keywords = {game theory, impossibility, algorithmic fairness, fairness in classification},
location = {Honolulu, HI, USA},
series = {AIES '19}
}
  
"
AIES,2019,Fair Transfer Learning with Missing Protected Attributes,https://dl.acm.org/doi/10.1145/3306618.3314236,"['Amanda Coston', 'Karthikeyan Natesan Ramamurthy', 'Dennis Wei', 'Kush R. Varshney', 'Skyler Speakman', 'Zairah Mustahsan', 'Supriyo Chakraborty']",66,"Risk assessment is a growing use for machine learning models. When used in high-stakes applications, especially ones regulated by anti-discrimination laws or governed by societal norms for fairness, it is important to ensure that learned models do not propagate and scale any biases that may exist in training data. In this paper, we add on an additional challenge beyond fairness: unsupervised domain adaptation to covariate shift between a source and target distribution. Motivated by the real-world problem of risk assessment in new markets for health insurance in the United States and mobile money-based loans in East Africa, we provide a precise formulation of the machine learning with covariate shift and score parity problem. Our formulation focuses on situations in which protected attributes are not available in either the source or target domain. We propose two new weighting methods: prevalence-constrained covariate shift (PCCS) which does not require protected attributes in the target domain and target-fair covariate shift (TFCS) which does not require protected attributes in the source domain. We empirically demonstrate their efficacy in two applications.",YES,,Testing and Validation,Algorithms,Mitigation,Distribution shift,,,,,,,,,,,"@inproceedings{10.1145/3306618.3314236,
author = {Coston, Amanda and Ramamurthy, Karthikeyan Natesan and Wei, Dennis and Varshney, Kush R. and Speakman, Skyler and Mustahsan, Zairah and Chakraborty, Supriyo},
title = {Fair Transfer Learning with Missing Protected Attributes},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314236},
doi = {10.1145/3306618.3314236},
abstract = {Risk assessment is a growing use for machine learning models. When used in high-stakes applications, especially ones regulated by anti-discrimination laws or governed by societal norms for fairness, it is important to ensure that learned models do not propagate and scale any biases that may exist in training data. In this paper, we add on an additional challenge beyond fairness: unsupervised domain adaptation to covariate shift between a source and target distribution. Motivated by the real-world problem of risk assessment in new markets for health insurance in the United States and mobile money-based loans in East Africa, we provide a precise formulation of the machine learning with covariate shift and score parity problem. Our formulation focuses on situations in which protected attributes are not available in either the source or target domain. We propose two new weighting methods: prevalence-constrained covariate shift (PCCS) which does not require protected attributes in the target domain and target-fair covariate shift (TFCS) which does not require protected attributes in the source domain. We empirically demonstrate their efficacy in two applications.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {91–98},
numpages = {8},
keywords = {fairness, transfer learning, risk assessments},
location = {Honolulu, HI, USA},
series = {AIES '19}
}
  
"
AIES,2019,The Role and Limits of Principles in AI Ethics: Towards a Focus on Tensions,https://dl.acm.org/doi/10.1145/3306618.3314289,"['Jess Whittlestone', 'Rune Nyrup', 'Anna Alexandrova', 'Stephen Cave']",152,"The last few years have seen a proliferation of principles for AI ethics. There is substantial overlap between different sets of principles, with widespread agreement that AI should be used for the common good, should not be used to harm people or undermine their rights, and should respect widely held values such as fairness, privacy, and autonomy. While articulating and agreeing on principles is important, it is only a starting point. Drawing on comparisons with the field of bioethics, we highlight some of the limitations of principles: in particular, they are often too broad and high-level to guide ethics in practice. We suggest that an important next step for the field of AI ethics is to focus on exploring the tensions that inevitably arise as we try to implement principles in practice. By explicitly recognising these tensions we can begin to make decisions about how they should be resolved in specific cases, and develop frameworks and guidelines for AI ethics that are rigorous and practically relevant. We discuss some different specific ways that tensions arise in AI ethics, and what processes might be needed to resolve them.",YES,,General,,,,,,,,,,,,,,"@inproceedings{10.1145/3306618.3314289,
author = {Whittlestone, Jess and Nyrup, Rune and Alexandrova, Anna and Cave, Stephen},
title = {The Role and Limits of Principles in AI Ethics: Towards a Focus on Tensions},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314289},
doi = {10.1145/3306618.3314289},
abstract = {The last few years have seen a proliferation of principles for AI ethics. There is substantial overlap between different sets of principles, with widespread agreement that AI should be used for the common good, should not be used to harm people or undermine their rights, and should respect widely held values such as fairness, privacy, and autonomy. While articulating and agreeing on principles is important, it is only a starting point. Drawing on comparisons with the field of bioethics, we highlight some of the limitations of principles: in particular, they are often too broad and high-level to guide ethics in practice. We suggest that an important next step for the field of AI ethics is to focus on exploring the tensions that inevitably arise as we try to implement principles in practice. By explicitly recognising these tensions we can begin to make decisions about how they should be resolved in specific cases, and develop frameworks and guidelines for AI ethics that are rigorous and practically relevant. We discuss some different specific ways that tensions arise in AI ethics, and what processes might be needed to resolve them.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {195–200},
numpages = {6},
keywords = {artificial intelligence, ethics, principles},
location = {Honolulu, HI, USA},
series = {AIES '19}
}
  
"
AIES,2019,"Crowdsourcing with Fairness, Diversity and Budget Constraints",https://dl.acm.org/doi/10.1145/3306618.3314282,"['Naman Goel', 'Boi Faltings']",14,"Recent studies have shown that the labels collected from crowdworkers can be discriminatory with respect to sensitive attributes such as gender and race. This raises questions about the suitability of using crowdsourced data for further use, such as for training machine learning algorithms. In this work, we address the problem of fair and diverse data collection from a crowd under budget constraints. We propose a novel algorithm which maximizes the expected accuracy of the collected data, while ensuring that the errors satisfy desired notions of fairness. We provide guarantees on the performance of our algorithm and show that the algorithm performs well in practice through experiments on a real dataset.",YES,,Data,Labeling,Mitigation,,,,,,,,,,,,"@inproceedings{10.1145/3306618.3314282,
author = {Goel, Naman and Faltings, Boi},
title = {Crowdsourcing with Fairness, Diversity and Budget Constraints},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314282},
doi = {10.1145/3306618.3314282},
abstract = {Recent studies have shown that the labels collected from crowdworkers can be discriminatory with respect to sensitive attributes such as gender and race. This raises questions about the suitability of using crowdsourced data for further use, such as for training machine learning algorithms. In this work, we address the problem of fair and diverse data collection from a crowd under budget constraints. We propose a novel algorithm which maximizes the expected accuracy of the collected data, while ensuring that the errors satisfy desired notions of fairness. We provide guarantees on the performance of our algorithm and show that the algorithm performs well in practice through experiments on a real dataset.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {297–304},
numpages = {8},
keywords = {data quality, crowdsourcing, bias, fairness},
location = {Honolulu, HI, USA},
series = {AIES '19}
}
  
"
AIES,2019,What are the Biases in My Word Embedding?,https://dl.acm.org/doi/10.1145/3306618.3314270,"['Nathaniel Swinger', 'Maria De-Arteaga', 'Neil Thomas Heffernan IV', 'Mark DM Leiserson', 'Adam Tauman Kalai']",87,"This paper presents an algorithm for enumerating biases in word embeddings. The algorithm exposes a large number of offensive associations related to sensitive features such as race and gender on publicly available embeddings, including a supposedly ""debiased"" embedding. These biases are concerning in light of the widespread use of word embeddings. The associations are identified by geometric patterns in word embeddings that run parallel between people's names and common lower-case tokens. The algorithm is highly unsupervised: it does not even require the sensitive features to be pre-specified. This is desirable because: (a) many forms of discrimination?such as racial discrimination-are linked to social constructs that may vary depending on the context, rather than to categories with fixed definitions; and (b) it makes it easier to identify biases against intersectional groups, which depend on combinations of sensitive features. The inputs to our algorithm are a list of target tokens, e.g. names, and a word embedding. It outputs a number of Word Embedding Association Tests (WEATs) that capture various biases present in the data. We illustrate the utility of our approach on publicly available word embeddings and lists of names, and evaluate its output using crowdsourcing. We also show how removing names may not remove potential proxy bias.",YES,,Data?,Measurement,Methods,Might need a second look,,,,,,,,,,,"@inproceedings{10.1145/3306618.3314270,
author = {Swinger, Nathaniel and De-Arteaga, Maria and Heffernan IV, Neil Thomas and Leiserson, Mark DM and Kalai, Adam Tauman},
title = {What Are the Biases in My Word Embedding?},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314270},
doi = {10.1145/3306618.3314270},
abstract = {This paper presents an algorithm for enumerating biases in word embeddings. The algorithm exposes a large number of offensive associations related to sensitive features such as race and gender on publicly available embeddings, including a supposedly ""debiased"" embedding. These biases are concerning in light of the widespread use of word embeddings. The associations are identified by geometric patterns in word embeddings that run parallel between people's names and common lower-case tokens. The algorithm is highly unsupervised: it does not even require the sensitive features to be pre-specified. This is desirable because: (a) many forms of discrimination?such as racial discrimination-are linked to social constructs that may vary depending on the context, rather than to categories with fixed definitions; and (b) it makes it easier to identify biases against intersectional groups, which depend on combinations of sensitive features. The inputs to our algorithm are a list of target tokens, e.g. names, and a word embedding. It outputs a number of Word Embedding Association Tests (WEATs) that capture various biases present in the data. We illustrate the utility of our approach on publicly available word embeddings and lists of names, and evaluate its output using crowdsourcing. We also show how removing names may not remove potential proxy bias.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {305–311},
numpages = {7},
keywords = {fairness, bias, word embeddings},
location = {Honolulu, HI, USA},
series = {AIES '19}
}
  
"
AIES,2019,Modeling Risk and Achieving Algorithmic Fairness Using Potential Outcomes,https://dl.acm.org/doi/10.1145/3306618.3314323,['Alan Mishler'],7,"Predictive models and algorithms are increasingly used to support human decision makers, raising concerns about how to ensure that these algorithms are fair. Additionally, these tools are generally designed to predict observable outcomes, but this is problematic when the treatment or exposure is confounded with the outcome. I argue that in most cases, what is actually of interest are potential outcomes. I contrast modeling approaches built around observable vs. potential outcomes, and I recharacterize error rate-based algorithmic fairness metrics in terms of potential outcomes. I also aim to formally model the consequences of using confounded observable predictions to drive interventions.",YES,,Data,Sampling,Measurement,Don't really get it,,,,,,,,,,,"@inproceedings{10.1145/3306618.3314323,
author = {Mishler, Alan},
title = {Modeling Risk and Achieving Algorithmic Fairness Using Potential Outcomes},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314323},
doi = {10.1145/3306618.3314323},
abstract = {Predictive models and algorithms are increasingly used to support human decision makers, raising concerns about how to ensure that these algorithms are fair. Additionally, these tools are generally designed to predict observable outcomes, but this is problematic when the treatment or exposure is confounded with the outcome. I argue that in most cases, what is actually of interest are potential outcomes. I contrast modeling approaches built around observable vs. potential outcomes, and I recharacterize error rate-based algorithmic fairness metrics in terms of potential outcomes. I also aim to formally model the consequences of using confounded observable predictions to drive interventions.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {555–556},
numpages = {2},
keywords = {algorithmic fairness, risk assessment, recidivism, causal inference, bias, potential outcomes, machine learning},
location = {Honolulu, HI, USA},
series = {AIES '19}
}
  
"
AIES,2020,Fair Allocation through Selective Information Acquisition,https://dl.acm.org/doi/10.1145/3375627.3375823,"['William Cai', 'Johann Gaebler', 'Nikhil Garg', 'Sharad Goel']",17,"Public and private institutions must often allocate scarce resources under uncertainty. Banks, for example, extend credit to loan applicants based in part on their estimated likelihood of repaying a loan. But when the quality of information differs across candidates (e.g., if some applicants lack traditional credit histories), common lending strategies can lead to disparities across groups. Here we consider a setting in which decision makers---before allocating resources---can choose to spend some of their limited budget further screening select individuals. We present a computationally efficient algorithm for deciding whom to screen that maximizes a standard measure of social welfare. Intuitively, decision makers should screen candidates on the margin, for whom the additional information could plausibly alter the allocation. We formalize this idea by showing the problem can be reduced to solving a series of linear programs. Both on synthetic and real-world datasets, this strategy improves utility, illustrating the value of targeted information acquisition in such decisions. Further, when there is social value for distributing resources to groups for whom we have a priori poor information---like those without credit scores---our approach can substantially improve the allocation of limited assets.",YES,,Data,Sampling,Decides how to aquire new info to get best social welfare,,,,,,,,,,,,"@inproceedings{10.1145/3375627.3375823,
author = {Cai, William and Gaebler, Johann and Garg, Nikhil and Goel, Sharad},
title = {Fair Allocation through Selective Information Acquisition},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375823},
doi = {10.1145/3375627.3375823},
abstract = {Public and private institutions must often allocate scarce resources under uncertainty. Banks, for example, extend credit to loan applicants based in part on their estimated likelihood of repaying a loan. But when the quality of information differs across candidates (e.g., if some applicants lack traditional credit histories), common lending strategies can lead to disparities across groups. Here we consider a setting in which decision makers---before allocating resources---can choose to spend some of their limited budget further screening select individuals. We present a computationally efficient algorithm for deciding whom to screen that maximizes a standard measure of social welfare. Intuitively, decision makers should screen candidates on the margin, for whom the additional information could plausibly alter the allocation. We formalize this idea by showing the problem can be reduced to solving a series of linear programs. Both on synthetic and real-world datasets, this strategy improves utility, illustrating the value of targeted information acquisition in such decisions. Further, when there is social value for distributing resources to groups for whom we have a priori poor information---like those without credit scores---our approach can substantially improve the allocation of limited assets.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {22–28},
numpages = {7},
keywords = {algorithmic fairness, lending, information acquisition},
location = {New York, NY, USA},
series = {AIES '20}
}
  
"
AIES,2020,When Trusted Black Boxes Don't Agree: Incentivizing Iterative Improvement and Accountability in Critical Software Systems,https://dl.acm.org/doi/10.1145/3375627.3375807,"['Jeanna Neefe Matthews', 'Graham Northup', 'Isabella Grasso', 'Stephen Lorenz', 'Marzieh Babaeianjelodar', 'Hunter Bashaw', 'Sumona Mondal', 'Abigail Matthews', 'Mariama Njie', 'Jessica Goldthwaite']",8,"Software increasingly plays a key role in regulated areas like housing, hiring, and credit, as well as major public functions such as criminal justice and elections. It is easy for there to be unintended defects with a large impact on the lives of individuals and society as a whole. Preventing, finding, and fixing software defects is a key focus of both industrial software development efforts as well as academic research in software engineering. In this paper, we discuss flaws in the larger socio-technical decision-making processes in which critical black-box software systems are developed, deployed, and trusted. We use criminal justice software, specifically probabilistic genotyping (PG) software, as a concrete example. We describe how PG software systems, designed to do the same job, produce different results. We highlight the under-appreciated impact of changes in key parameters and the disparate impact that one such parameter can have on different racial/ethnic groups. We propose concrete changes to the socio-technical decision-making processes surrounding the use of PG software that could be used to incentivize iterative improvements in the accuracy, fairness, reliability, and accountability of these systems.",YES,,Modeling,Hyperparams,Problem Identification,,,,,,,,,,,,"@inproceedings{10.1145/3375627.3375807,
author = {Matthews, Jeanna Neefe and Northup, Graham and Grasso, Isabella and Lorenz, Stephen and Babaeianjelodar, Marzieh and Bashaw, Hunter and Mondal, Sumona and Matthews, Abigail and Njie, Mariama and Goldthwaite, Jessica},
title = {When Trusted Black Boxes Don't Agree: Incentivizing Iterative Improvement and Accountability in Critical Software Systems},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375807},
doi = {10.1145/3375627.3375807},
abstract = {Software increasingly plays a key role in regulated areas like housing, hiring, and credit, as well as major public functions such as criminal justice and elections. It is easy for there to be unintended defects with a large impact on the lives of individuals and society as a whole. Preventing, finding, and fixing software defects is a key focus of both industrial software development efforts as well as academic research in software engineering. In this paper, we discuss flaws in the larger socio-technical decision-making processes in which critical black-box software systems are developed, deployed, and trusted. We use criminal justice software, specifically probabilistic genotyping (PG) software, as a concrete example. We describe how PG software systems, designed to do the same job, produce different results. We highlight the under-appreciated impact of changes in key parameters and the disparate impact that one such parameter can have on different racial/ethnic groups. We propose concrete changes to the socio-technical decision-making processes surrounding the use of PG software that could be used to incentivize iterative improvements in the accuracy, fairness, reliability, and accountability of these systems.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {102–108},
numpages = {7},
keywords = {disparate impact, criminal justice software, software verification, probabilistic genotyping, algorithmic accountability},
location = {New York, NY, USA},
series = {AIES '20}
}
  
"
AIES,2020,An Invitation to System-wide Algorithmic Fairness,https://dl.acm.org/doi/10.1145/3375627.3375860,"['Efr√©n Cruz Cort√©s', 'Debashis Ghosh']",7,"We propose a framework for analyzing and evaluating system-wide algorithmic fairness. The core idea is to use simulation techniques in order to extend the scope of current fairness assessments by incorporating context and feedback to a phenomenon of interest. By doing so, we expect to better understand the interaction among the social behavior giving rise to discrimination, automated decision making tools, and fairness-inspired statistical constraints. In particular, we invite the community to use agent based models as an explanatory tool for causal mechanisms of population level properties. We also propose embedding these into a reinforcement learning algorithm to find optimal actions for meaningful change. As an incentive for taking a system-wide approach , we show through a simple model of predictive policing and trials that if we limit our attention to one portion of the system, we may determine some blatantly unfair practices as fair, and be blind to overall unfairness.",YES,,Organizational Realities,Data,Methods,,,,,,,,,,,,"@inproceedings{10.1145/3375627.3375860,
author = {Cruz Cort\'{e}s, Efr\'{e}n and Ghosh, Debashis},
title = {An Invitation to System-Wide Algorithmic Fairness},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375860},
doi = {10.1145/3375627.3375860},
abstract = {We propose a framework for analyzing and evaluating system-wide algorithmic fairness. The core idea is to use simulation techniques in order to extend the scope of current fairness assessments by incorporating context and feedback to a phenomenon of interest. By doing so, we expect to better understand the interaction among the social behavior giving rise to discrimination, automated decision making tools, and fairness-inspired statistical constraints. In particular, we invite the community to use agent based models as an explanatory tool for causal mechanisms of population level properties. We also propose embedding these into a reinforcement learning algorithm to find optimal actions for meaningful change. As an incentive for taking a system-wide approach , we show through a simple model of predictive policing and trials that if we limit our attention to one portion of the system, we may determine some blatantly unfair practices as fair, and be blind to overall unfairness.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {235–241},
numpages = {7},
keywords = {recidivism, ethical ai, fairness, agent based modeling},
location = {New York, NY, USA},
series = {AIES '20}
}
  
"
AIES,2020,Steps Towards Value-Aligned Systems,https://dl.acm.org/doi/10.1145/3375627.3375872,"['Osonde A. Osoba', 'Benjamin Boudreaux', 'Douglas Yeung']",8,"Algorithmic (including AI/ML) decision-making artifacts are an established and growing part of our decision-making ecosystem. They are now indispensable tools that help us manage the flood of information we use to try to make effective decisions in a complex world. The current literature is full of examples of how individual artifacts violate societal norms and expectations (e.g. violations of fairness, privacy, or safety norms). Against this backdrop, this discussion highlights an under-emphasized perspective in the body of research focused on assessing value misalignment in AI-equipped sociotechnical systems. The research on value misalignment so far has a strong focus on the behavior of individual tech artifacts. This discussion argues for a more structured systems-level approach for assessing value-alignment in sociotechnical systems. We rely primarily on the research on fairness to make our arguments more concrete. And we use the opportunity to highlight how adopting a system perspective improves our ability to explain and address value misalignments better. Our discussion ends with an exploration of priority questions that demand attention if we are to assure the value alignment of whole systems, not just individual artifacts.",Yes,,General,,Detection,"Maybe goes in related work--calls for looking at overall system in which system is embedded, but doesn't really give a concrete takeaway",,,,,,,,,,,"@inproceedings{10.1145/3375627.3375872,
author = {Osoba, Osonde A. and Boudreaux, Benjamin and Yeung, Douglas},
title = {Steps Towards Value-Aligned Systems},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375872},
doi = {10.1145/3375627.3375872},
abstract = {Algorithmic (including AI/ML) decision-making artifacts are an established and growing part of our decision-making ecosystem. They are now indispensable tools that help us manage the flood of information we use to try to make effective decisions in a complex world. The current literature is full of examples of how individual artifacts violate societal norms and expectations (e.g. violations of fairness, privacy, or safety norms). Against this backdrop, this discussion highlights an under-emphasized perspective in the body of research focused on assessing value misalignment in AI-equipped sociotechnical systems. The research on value misalignment so far has a strong focus on the behavior of individual tech artifacts. This discussion argues for a more structured systems-level approach for assessing value-alignment in sociotechnical systems. We rely primarily on the research on fairness to make our arguments more concrete. And we use the opportunity to highlight how adopting a system perspective improves our ability to explain and address value misalignments better. Our discussion ends with an exploration of priority questions that demand attention if we are to assure the value alignment of whole systems, not just individual artifacts.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {332–336},
numpages = {5},
keywords = {sociotechnical systems, ml fairness, value alignment, systems analysis},
location = {New York, NY, USA},
series = {AIES '20}
}
  
"
AIES,2020,Data Augmentation for Discrimination Prevention and Bias Disambiguation,https://dl.acm.org/doi/10.1145/3375627.3375865,"['Shubham Sharma', 'Yunfeng Zhang', 'Jes√∫s M. R√≠os Aliaga', 'Djallel Bouneffouf', 'Vinod Muthusamy', 'Kush R. Varshney']",43,"Machine learning models are prone to biased decisions due to biases in the datasets they are trained on. In this paper, we introduce a novel data augmentation technique to create a fairer dataset for model training that could also lend itself to understanding the type of bias existing in the dataset i.e. if bias arises from a lack of representation for a particular group (sampling bias) or if it arises because of human bias reflected in the labels (prejudice based bias). Given a dataset involving a protected attribute with a privileged and unprivileged group, we create an ""ideal world'' dataset: for every data sample, we create a new sample having the same features (except the protected attribute(s)) and label as the original sample but with the opposite protected attribute value. The synthetic data points are sorted in order of their proximity to the original training distribution and added successively to the real dataset to create intermediate datasets. We theoretically show that two different notions of fairness: statistical parity difference (independence) and average odds difference (separation) always change in the same direction using such an augmentation. We also show submodularity of the proposed fairness-aware augmentation approach that enables an efficient greedy algorithm. We empirically study the effect of training models on the intermediate datasets and show that this technique reduces the two bias measures while keeping the accuracy nearly constant for three datasets. We then discuss the implications of this study on the disambiguation of sample bias and prejudice based bias and discuss how pre-processing techniques should be evaluated in general. The proposed method can be used by policy makers who want to use unbiased datasets to train machine learning models for their applications to add a subset of synthetic points to an extent that they are comfortable with to mitigate unwanted bias.",YES,,Data,Sampling,Methods,,,,,,,,,,,,"@inproceedings{10.1145/3375627.3375865,
author = {Sharma, Shubham and Zhang, Yunfeng and R\'{\i}os Aliaga, Jes\'{u}s M. and Bouneffouf, Djallel and Muthusamy, Vinod and Varshney, Kush R.},
title = {Data Augmentation for Discrimination Prevention and Bias Disambiguation},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375865},
doi = {10.1145/3375627.3375865},
abstract = {Machine learning models are prone to biased decisions due to biases in the datasets they are trained on. In this paper, we introduce a novel data augmentation technique to create a fairer dataset for model training that could also lend itself to understanding the type of bias existing in the dataset i.e. if bias arises from a lack of representation for a particular group (sampling bias) or if it arises because of human bias reflected in the labels (prejudice based bias). Given a dataset involving a protected attribute with a privileged and unprivileged group, we create an ""ideal world'' dataset: for every data sample, we create a new sample having the same features (except the protected attribute(s)) and label as the original sample but with the opposite protected attribute value. The synthetic data points are sorted in order of their proximity to the original training distribution and added successively to the real dataset to create intermediate datasets. We theoretically show that two different notions of fairness: statistical parity difference (independence) and average odds difference (separation) always change in the same direction using such an augmentation. We also show submodularity of the proposed fairness-aware augmentation approach that enables an efficient greedy algorithm. We empirically study the effect of training models on the intermediate datasets and show that this technique reduces the two bias measures while keeping the accuracy nearly constant for three datasets. We then discuss the implications of this study on the disambiguation of sample bias and prejudice based bias and discuss how pre-processing techniques should be evaluated in general. The proposed method can be used by policy makers who want to use unbiased datasets to train machine learning models for their applications to add a subset of synthetic points to an extent that they are comfortable with to mitigate unwanted bias.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {358–364},
numpages = {7},
keywords = {discrimination prevention, fairness in machine learning, responsible artificial intelligence},
location = {New York, NY, USA},
series = {AIES '20}
}
  
"
AIES,2021,Towards Unbiased and Accurate Deferral to Multiple Experts,https://dl.acm.org/doi/10.1145/3461702.3462516,"['Vijay Keswani', 'Matthew Lease', 'Krishnaram Kenthapadi']",23,"Machine learning models are often implemented in cohort with humans in the pipeline, with the model having an option to defer to a domain expert in cases where it has low confidence in its inference. Our goal is to design mechanisms for ensuring accuracy and fairness in such prediction systems that combine machine learning model inferences and domain expert predictions. Prior work on ""deferral systems"" in classification settings has focused on the setting of a pipeline with a single expert and aimed to accommodate the inaccuracies and biases of this expert to simultaneously learn an inference model and a deferral system. Our work extends this framework to settings where multiple experts are available, with each expert having their own domain of expertise and biases. We propose a framework that simultaneously learns a classifier and a deferral system, with the deferral system choosing to defer to one or more human experts in cases of input where the classifier has low confidence. We test our framework on a synthetic dataset and a content moderation dataset with biased synthetic experts, and show that it significantly improves the accuracy and fairness of the final predictions, compared to the baselines. We also collect crowdsourced labels for the content moderation task to construct a real-world dataset for the evaluation of hybrid machine-human frameworks and show that our proposed framework outperforms baselines on this real-world dataset as well.",YES,,Algorithm,Human Handoff,Methods,Looks at how to design fair systems where algorithms are allowed to defer certain decisions to models,,,,,,,,,,,"@inproceedings{10.1145/3461702.3462516,
author = {Keswani, Vijay and Lease, Matthew and Kenthapadi, Krishnaram},
title = {Towards Unbiased and Accurate Deferral to Multiple Experts},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462516},
doi = {10.1145/3461702.3462516},
abstract = {Machine learning models are often implemented in cohort with humans in the pipeline, with the model having an option to defer to a domain expert in cases where it has low confidence in its inference. Our goal is to design mechanisms for ensuring accuracy and fairness in such prediction systems that combine machine learning model inferences and domain expert predictions. Prior work on ""deferral systems"" in classification settings has focused on the setting of a pipeline with a single expert and aimed to accommodate the inaccuracies and biases of this expert to simultaneously learn an inference model and a deferral system. Our work extends this framework to settings where multiple experts are available, with each expert having their own domain of expertise and biases. We propose a framework that simultaneously learns a classifier and a deferral system, with the deferral system choosing to defer to one or more human experts in cases of input where the classifier has low confidence. We test our framework on a synthetic dataset and a content moderation dataset with biased synthetic experts, and show that it significantly improves the accuracy and fairness of the final predictions, compared to the baselines. We also collect crowdsourced labels for the content moderation task to construct a real-world dataset for the evaluation of hybrid machine-human frameworks and show that our proposed framework outperforms baselines on this real-world dataset as well.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {154–165},
numpages = {12},
keywords = {fairness, deferral models, hybrid human-machine frameworks},
location = {Virtual Event, USA},
series = {AIES '21}
}
  
"
AIES,2021,Automating Procedurally Fair Feature Selection in Machine Learning,https://dl.acm.org/doi/10.1145/3461702.3462585,"['Clara Belitz', 'Lan Jiang', 'Nigel Bosch']",5,"In recent years, machine learning has become more common in everyday applications. Consequently, numerous studies have explored issues of unfairness against specific groups or individuals in the context of these applications. Much of the previous work on unfairness in machine learning has focused on the fairness of outcomes rather than process. We propose a feature selection method inspired by fair process (procedural fairness) in addition to fair outcome. Specifically, we introduce the notion of unfairness weight, which indicates how heavily to weight unfairness versus accuracy when measuring the marginal benefit of adding a new feature to a model. Our goal is to maintain accuracy while reducing unfairness, as defined by six common statistical definitions. We show that this approach demonstrably decreases unfairness as the unfairness weight is increased, for most combinations of metrics and classifiers used. A small subset of all the combinations of datasets (4), unfairness metrics (6), and classifiers (3), however, demonstrated relatively low unfairness initially. For these specific combinations, neither unfairness nor accuracy were affected as unfairness weight changed, demonstrating that this method does not reduce accuracy unless there is also an equivalent decrease in unfairness. We also show that this approach selects unfair features and sensitive features for the model less frequently as the unfairness weight increases. As such, this procedure is an effective approach to constructing classifiers that both reduce unfairness and are less likely to include unfair features in the modeling process.",YES,,Feature Selection,,,still has to do with conventional fairness metrics,,,,,,,,,,,"@inproceedings{10.1145/3461702.3462585,
author = {Belitz, Clara and Jiang, Lan and Bosch, Nigel},
title = {Automating Procedurally Fair Feature Selection in Machine Learning},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462585},
doi = {10.1145/3461702.3462585},
abstract = {In recent years, machine learning has become more common in everyday applications. Consequently, numerous studies have explored issues of unfairness against specific groups or individuals in the context of these applications. Much of the previous work on unfairness in machine learning has focused on the fairness of outcomes rather than process. We propose a feature selection method inspired by fair process (procedural fairness) in addition to fair outcome. Specifically, we introduce the notion of unfairness weight, which indicates how heavily to weight unfairness versus accuracy when measuring the marginal benefit of adding a new feature to a model. Our goal is to maintain accuracy while reducing unfairness, as defined by six common statistical definitions. We show that this approach demonstrably decreases unfairness as the unfairness weight is increased, for most combinations of metrics and classifiers used. A small subset of all the combinations of datasets (4), unfairness metrics (6), and classifiers (3), however, demonstrated relatively low unfairness initially. For these specific combinations, neither unfairness nor accuracy were affected as unfairness weight changed, demonstrating that this method does not reduce accuracy unless there is also an equivalent decrease in unfairness. We also show that this approach selects unfair features and sensitive features for the model less frequently as the unfairness weight increases. As such, this procedure is an effective approach to constructing classifiers that both reduce unfairness and are less likely to include unfair features in the modeling process.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {379–389},
numpages = {11},
keywords = {machine learning, bias, feature selection, fairness},
location = {Virtual Event, USA},
series = {AIES '21}
}
  
"
AIES,2021,Can We Obtain Fairness For Free?,https://dl.acm.org/doi/10.1145/3461702.3462614,"['Rashidul Islam', 'Shimei Pan', 'James R. Foulds']",10,"There is growing awareness that AI and machine learning systems can in some cases learn to behave in unfair and discriminatory ways with harmful consequences. However, despite an enormous amount of research, techniques for ensuring AI fairness have yet to see widespread deployment in real systems. One of the main barriers is the conventional wisdom that fairness brings a cost in predictive performance metrics such as accuracy which could affect an organization's bottom-line. In this paper we take a closer look at this concern. Clearly fairness/performance trade-offs exist, but are they inevitable? In contrast to the conventional wisdom, we find that it is frequently possible, indeed straightforward, to improve on a trained model's fairness without sacrificing predictive performance. We systematically study the behavior of fair learning algorithms on a range of benchmark datasets, showing that it is possible to improve fairness to some degree with no loss (or even an improvement) in predictive performance via a sensible hyper-parameter selection strategy. Our results reveal a pathway toward increasing the deployment of fair AI methods, with potentially substantial positive real-world impacts.",YES,,Hyperparameter,,,Not sure,,,,,,,,,,,"@inproceedings{10.1145/3461702.3462614,
author = {Islam, Rashidul and Pan, Shimei and Foulds, James R.},
title = {Can We Obtain Fairness For Free?},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462614},
doi = {10.1145/3461702.3462614},
abstract = {There is growing awareness that AI and machine learning systems can in some cases learn to behave in unfair and discriminatory ways with harmful consequences. However, despite an enormous amount of research, techniques for ensuring AI fairness have yet to see widespread deployment in real systems. One of the main barriers is the conventional wisdom that fairness brings a cost in predictive performance metrics such as accuracy which could affect an organization's bottom-line. In this paper we take a closer look at this concern. Clearly fairness/performance trade-offs exist, but are they inevitable? In contrast to the conventional wisdom, we find that it is frequently possible, indeed straightforward, to improve on a trained model's fairness without sacrificing predictive performance. We systematically study the behavior of fair learning algorithms on a range of benchmark datasets, showing that it is possible to improve fairness to some degree with no loss (or even an improvement) in predictive performance via a sensible hyper-parameter selection strategy. Our results reveal a pathway toward increasing the deployment of fair AI methods, with potentially substantial positive real-world impacts.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {586–596},
numpages = {11},
keywords = {deployment of fairness techniques, fairness/performance trade-offs, fairness in AI, practical barriers, AI and society},
location = {Virtual Event, USA},
series = {AIES '21}
}
  
"
AIES,2021,Towards Equity and Algorithmic Fairness in Student Grade Prediction,https://dl.acm.org/doi/10.1145/3461702.3462623,"['Weijie Jiang', 'Zachary A. Pardos']",10,"Equity of educational outcome and fairness of AI with respect to race have been topics of increasing importance in education. In this work, we address both with empirical evaluations of grade prediction in higher education, an important task to improve curriculum design, plan interventions for academic support, and offer course guidance to students. With fairness as the aim, we trial several strategies for both label and instance balancing to attempt to minimize differences in algorithm performance with respect to race. We find that an adversarial learning approach, combined with grade label balancing, achieved by far the fairest results. With equity of educational outcome as the aim, we trial strategies for boosting predictive performance on historically underserved groups and find success in sampling those groups in inverse proportion to their historic outcomes. With AI-infused technology supports increasingly prevalent on campuses, our methodologies fill a need for frameworks to consider performance trade-offs with respect to sensitive student attributes and allow institutions to instrument their AI resources in ways that are attentive to equity and fairness.",YES,,Data,,,Case Study,,,,,,,,,,,"@inproceedings{10.1145/3461702.3462623,
author = {Jiang, Weijie and Pardos, Zachary A.},
title = {Towards Equity and Algorithmic Fairness in Student Grade Prediction},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462623},
doi = {10.1145/3461702.3462623},
abstract = {Equity of educational outcome and fairness of AI with respect to race have been topics of increasing importance in education. In this work, we address both with empirical evaluations of grade prediction in higher education, an important task to improve curriculum design, plan interventions for academic support, and offer course guidance to students. With fairness as the aim, we trial several strategies for both label and instance balancing to attempt to minimize differences in algorithm performance with respect to race. We find that an adversarial learning approach, combined with grade label balancing, achieved by far the fairest results. With equity of educational outcome as the aim, we trial strategies for boosting predictive performance on historically underserved groups and find success in sampling those groups in inverse proportion to their historic outcomes. With AI-infused technology supports increasingly prevalent on campuses, our methodologies fill a need for frameworks to consider performance trade-offs with respect to sensitive student attributes and allow institutions to instrument their AI resources in ways that are attentive to equity and fairness.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {608–617},
numpages = {10},
keywords = {grade prediction, equity, higher education, fairness},
location = {Virtual Event, USA},
series = {AIES '21}
}
  
"
AIES,2021,Risk Identification Questionnaire for Detecting Unintended Bias in the Machine Learning Development Lifecycle,https://dl.acm.org/doi/10.1145/3461702.3462572,"['Michelle Seng Ah Lee', 'Jatinder Singh']",11,"Unintended biases in machine learning (ML) models have the potential to introduce undue discrimination and exacerbate social inequalities. The research community has proposed various technical and qualitative methods intended to assist practitioners in assessing these biases. While frameworks for identifying the risks of harm due to unintended biases have been proposed, they have not yet been operationalised into practical tools to assist industry practitioners. In this paper, we link prior work on bias assessment methods to phases of a standard organisational risk management process (RMP), noting a gap in measures for helping practitioners identify bias- related risks. Targeting this gap, we introduce a bias identification methodology and questionnaire, illustrating its application through a real-world, practitioner-led use case. We validate the need and usefulness of the questionnaire through a survey of industry practitioners, which provides insights into their practical requirements and preferences. Our results indicate that such a questionnaire is helpful for proactively uncovering unexpected bias concerns, particularly where it is easy to integrate into existing processes, and facilitates communication with non-technical stakeholders. Ultimately, the effective end-to-end management of ML risks requires a more targeted identification of potential harm and its sources, so that appropriate mitigation strategies can be formulated. Towards this, our questionnaire provides a practical means to assist practitioners in identifying bias-related risks.",YES,,General,,,Related Work,,,,,,,,,,,"@inproceedings{10.1145/3461702.3462572,
author = {Lee, Michelle Seng Ah and Singh, Jatinder},
title = {Risk Identification Questionnaire for Detecting Unintended Bias in the Machine Learning Development Lifecycle},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462572},
doi = {10.1145/3461702.3462572},
abstract = {Unintended biases in machine learning (ML) models have the potential to introduce undue discrimination and exacerbate social inequalities. The research community has proposed various technical and qualitative methods intended to assist practitioners in assessing these biases. While frameworks for identifying the risks of harm due to unintended biases have been proposed, they have not yet been operationalised into practical tools to assist industry practitioners.In this paper, we link prior work on bias assessment methods to phases of a standard organisational risk management process (RMP), noting a gap in measures for helping practitioners identify bias- related risks. Targeting this gap, we introduce a bias identification methodology and questionnaire, illustrating its application through a real-world, practitioner-led use case. We validate the need and usefulness of the questionnaire through a survey of industry practitioners, which provides insights into their practical requirements and preferences. Our results indicate that such a questionnaire is helpful for proactively uncovering unexpected bias concerns, particularly where it is easy to integrate into existing processes, and facilitates communication with non-technical stakeholders. Ultimately, the effective end-to-end management of ML risks requires a more targeted identification of potential harm and its sources, so that appropriate mitigation strategies can be formulated. Towards this, our questionnaire provides a practical means to assist practitioners in identifying bias-related risks.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {704–714},
numpages = {11},
keywords = {fair ML, questionnaire, risk identification, risk management, algorithmic bias, algorithmic fairness},
location = {Virtual Event, USA},
series = {AIES '21}
}
  
"
AIES,2021,Fair Bayesian Optimization,https://dl.acm.org/doi/10.1145/3461702.3462629,"['Valerio Perrone', 'Michele Donini', 'Muhammad Bilal Zafar', 'Robin Schmucker', 'Krishnaram Kenthapadi', 'C√©dric Archambeau']",42,"Given the increasing importance of machine learning (ML) in our lives, several algorithmic fairness techniques have been proposed to mitigate biases in the outcomes of the ML models. However, most of these techniques are specialized to cater to a single family of ML models and a specific definition of fairness, limiting their adaptibility in practice. We introduce a general constrained Bayesian optimization (BO) framework to optimize the performance of any ML model while enforcing one or multiple fairness constraints. BO is a model-agnostic optimization method that has been successfully applied to automatically tune the hyperparameters of ML models. We apply BO with fairness constraints to a range of popular models, including random forests, gradient boosting, and neural networks, showing that we can obtain accurate and fair solutions by acting solely on the hyperparameters. We also show empirically that our approach is competitive with specialized techniques that enforce model-specific fairness constraints, and outperforms preprocessing methods that learn fair representations of the input data. Moreover, our method can be used in synergy with such specialized fairness techniques to tune their hyperparameters. Finally, we study the relationship between fairness and the hyperparameters selected by BO. We observe a correlation between regularization and unbiased models, explaining why acting on the hyperparameters leads to ML models that generalize well and are fair.",YES,,Hyperparameter,,Methods,,,,,,,,,,,,"@inproceedings{10.1145/3461702.3462629,
author = {Perrone, Valerio and Donini, Michele and Zafar, Muhammad Bilal and Schmucker, Robin and Kenthapadi, Krishnaram and Archambeau, C\'{e}dric},
title = {Fair Bayesian Optimization},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462629},
doi = {10.1145/3461702.3462629},
abstract = {Given the increasing importance of machine learning (ML) in our lives, several algorithmic fairness techniques have been proposed to mitigate biases in the outcomes of the ML models. However, most of these techniques are specialized to cater to a single family of ML models and a specific definition of fairness, limiting their adaptibility in practice. We introduce a general constrained Bayesian optimization (BO) framework to optimize the performance of any ML model while enforcing one or multiple fairness constraints. BO is a model-agnostic optimization method that has been successfully applied to automatically tune the hyperparameters of ML models. We apply BO with fairness constraints to a range of popular models, including random forests, gradient boosting, and neural networks, showing that we can obtain accurate and fair solutions by acting solely on the hyperparameters. We also show empirically that our approach is competitive with specialized techniques that enforce model-specific fairness constraints, and outperforms preprocessing methods that learn fair representations of the input data. Moreover, our method can be used in synergy with such specialized fairness techniques to tune their hyperparameters. Finally, we study the relationship between fairness and the hyperparameters selected by BO. We observe a correlation between regularization and unbiased models, explaining why acting on the hyperparameters leads to ML models that generalize well and are fair.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {854–863},
numpages = {10},
keywords = {Bayesian optimization, fairness, bias, autoML, hyperparameter tuning},
location = {Virtual Event, USA},
series = {AIES '21}
}
  
"
AIES,2021,Measuring Model Fairness under Noisy Covariates: A Theoretical Perspective,https://dl.acm.org/doi/10.1145/3461702.3462603,"['Flavien Prost', 'Pranjal Awasthi', 'Nick Blumm', 'Aditee Kumthekar', 'Trevor Potter', 'Li Wei', 'Xuezhi Wang', 'Ed H. Chi', 'Jilin Chen', 'Alex Beutel']",4,"In this work we study the problem of measuring the fairness of a machine learning model under noisy information. Focusing on group fairness metrics, we investigate the particular but common situation when the evaluation requires controlling for the confounding effect of covariate variables. In a practical setting, we might not be able to jointly observe the covariate and group information, and a standard workaround is to then use proxies for one or more of these variables. Prior works have demonstrated the challenges with using a proxy for sensitive attributes, and strong independence assumptions are needed to provide guarantees on the accuracy of the noisy estimates. In contrast, in this work we study using a proxy for the covariate variable and present a theoretical analysis that aims to characterize weaker conditions under which accurate fairness evaluation is possible. Furthermore, our theory identifies potential sources of errors and decouples them into two interpretable parts y and E. The first part y depends solely on the performance of the proxy such as precision and recall, whereas the second part E captures correlations between all the variables of interest. We show that in many scenarios the error in the estimates is dominated by y via a linear dependence, whereas the dependence on the correlations E only constitutes a lower order term. As a result we expand the understanding of scenarios where measuring model fairness via proxies can be an effective approach. Finally, we compare, via simulations, the theoretical upper-bounds to the distribution of simulated estimation errors and show that assuming some structure on the data, even weak, is key to significantly improve both theoretical guarantees and empirical results.",YES,,Data,Noise,Methods,,,,,,,,,,,,"@inproceedings{10.1145/3461702.3462603,
author = {Prost, Flavien and Awasthi, Pranjal and Blumm, Nick and Kumthekar, Aditee and Potter, Trevor and Wei, Li and Wang, Xuezhi and Chi, Ed H. and Chen, Jilin and Beutel, Alex},
title = {Measuring Model Fairness under Noisy Covariates: A Theoretical Perspective},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462603},
doi = {10.1145/3461702.3462603},
abstract = {In this work we study the problem of measuring the fairness of a machine learning model under noisy information. Focusing on group fairness metrics, we investigate the particular but common situation when the evaluation requires controlling for the confounding effect of covariate variables. In a practical setting, we might not be able to jointly observe the covariate and group information, and a standard workaround is to then use proxies for one or more of these variables. Prior works have demonstrated the challenges with using a proxy for sensitive attributes, and strong independence assumptions are needed to provide guarantees on the accuracy of the noisy estimates. In contrast, in this work we study using a proxy for the covariate variable and present a theoretical analysis that aims to characterize weaker conditions under which accurate fairness evaluation is possible. Furthermore, our theory identifies potential sources of errors and decouples them into two interpretable parts y and E. The first part y depends solely on the performance of the proxy such as precision and recall, whereas the second part E captures correlations between all the variables of interest. We show that in many scenarios the error in the estimates is dominated by y via a linear dependence, whereas the dependence on the correlations E only constitutes a lower order term. As a result we expand the understanding of scenarios where measuring model fairness via proxies can be an effective approach. Finally, we compare, via simulations, the theoretical upper-bounds to the distribution of simulated estimation errors and show that assuming some structure on the data, even weak, is key to significantly improve both theoretical guarantees and empirical results.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {873–883},
numpages = {11},
keywords = {ml fairness, statistical parity, noisy covariates},
location = {Virtual Event, USA},
series = {AIES '21}
}
  
"
AIES,2021,A Step Toward More Inclusive People Annotations for Fairness,https://dl.acm.org/doi/10.1145/3461702.3462594,"['Candice Schumann', 'Susanna Ricco', 'Utsav Prabhu', 'Vittorio Ferrari', 'Caroline Pantofaru']",15,"The Open Images Dataset contains approximately 9 million images and is a widely accepted dataset for computer vision research. As is common practice for large datasets, the annotations are not exhaustive, with bounding boxes and attribute labels for only a subset of the classes in each image. In this paper, we present a new set of annotations on a subset of the Open Images dataset called the MIAP (More Inclusive Annotations for People) subset, containing bounding boxes and attributes for all of the people visible in those images. The attributes and labeling methodology for the MIAP subset were designed to enable research into model fairness. In addition, we analyze the original annotation methodology for the person class and its subclasses, discussing the resulting patterns in order to inform future annotation efforts. By considering both the original and exhaustive annotation sets, researchers can also now study how systematic patterns in training annotations affect modeling.",YES,,Data,Annotation,,,,,,,,,,,,,"@inproceedings{10.1145/3461702.3462594,
author = {Schumann, Candice and Ricco, Susanna and Prabhu, Utsav and Ferrari, Vittorio and Pantofaru, Caroline},
title = {A Step Toward More Inclusive People Annotations for Fairness},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462594},
doi = {10.1145/3461702.3462594},
abstract = {The Open Images Dataset contains approximately 9 million images and is a widely accepted dataset for computer vision research. As is common practice for large datasets, the annotations are not exhaustive, with bounding boxes and attribute labels for only a subset of the classes in each image. In this paper, we present a new set of annotations on a subset of the Open Images dataset called the MIAP (More Inclusive Annotations for People) subset, containing bounding boxes and attributes for all of the people visible in those images. The attributes and labeling methodology for the MIAP subset were designed to enable research into model fairness. In addition, we analyze the original annotation methodology for the person class and its subclasses, discussing the resulting patterns in order to inform future annotation efforts. By considering both the original and exhaustive annotation sets, researchers can also now study how systematic patterns in training annotations affect modeling.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {916–925},
numpages = {10},
keywords = {fairness, datasets, computer vision},
location = {Virtual Event, USA},
series = {AIES '21}
}
  
"
AIES,2022,Data-Centric Factors in Algorithmic Fairness,https://dl.acm.org/doi/10.1145/3514094.3534147,"['Nianyun Li', 'Naman Goel', 'Elliott Ash']",0,"Notwithstanding the widely held view that data generation and data curation processes are prominent sources of bias in machine learning algorithms, there is little empirical research seeking to document and understand the specific data dimensions affecting algorithmic unfairness. Contra the previous work, which has focused on modeling using simple, small-scale benchmark datasets, we hold the model constant and methodically intervene on relevant dimensions of a much larger, more diverse dataset. For this purpose, we introduce a new dataset on recidivism in 1.5 million criminal cases from courts in the U.S. state of Wisconsin, 2000-2018. From this main dataset, we generate multiple auxiliary datasets to simulate different kinds of biases in the data. Focusing on algorithmic bias toward different race/ethnicity groups, we assess the relevance of training data size, base rate difference between groups, representation of groups in the training data, temporal aspects of data curation, including race/ethnicity or neighborhood characteristics as features, and training separate classifiers by race/ethnicity or crime type. We find that these factors often do influence fairness metrics holding the classifier specification constant, without having a corresponding effect on accuracy metrics. The methodology and the results in the paper provide a useful reference point for a data-centric approach to studying algorithmic fairness in recidivism prediction and beyond.",YES,,Data,,Detection,Creates a new dataset for finding biases,,,,,,,,,,,"@inproceedings{10.1145/3514094.3534147,
author = {Li, Nianyun and Goel, Naman and Ash, Elliott},
title = {Data-Centric Factors in Algorithmic Fairness},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534147},
doi = {10.1145/3514094.3534147},
abstract = {Notwithstanding the widely held view that data generation and data curation processes are prominent sources of bias in machine learning algorithms, there is little empirical research seeking to document and understand the specific data dimensions affecting algorithmic unfairness. Contra the previous work, which has focused on modeling using simple, small-scale benchmark datasets, we hold the model constant and methodically intervene on relevant dimensions of a much larger, more diverse dataset. For this purpose, we introduce a new dataset on recidivism in 1.5 million criminal cases from courts in the U.S. state of Wisconsin, 2000-2018. From this main dataset, we generate multiple auxiliary datasets to simulate different kinds of biases in the data. Focusing on algorithmic bias toward different race/ethnicity groups, we assess the relevance of training data size, base rate difference between groups, representation of groups in the training data, temporal aspects of data curation, including race/ethnicity or neighborhood characteristics as features, and training separate classifiers by race/ethnicity or crime type. We find that these factors often do influence fairness metrics holding the classifier specification constant, without having a corresponding effect on accuracy metrics. The methodology and the results in the paper provide a useful reference point for a data-centric approach to studying algorithmic fairness in recidivism prediction and beyond.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {396–410},
numpages = {15},
keywords = {algorithmic fairness, datasets, recidivism prediction, machine learning},
location = {Oxford, United Kingdom},
series = {AIES '22}
}
  
"
AIES,2022,A Bio-Inspired Framework for Machine Bias Interpretation,https://dl.acm.org/doi/10.1145/3514094.3534126,"['Jake Robertson', 'Catherine Stinson', 'Ting Hu']",0,"Machine learning algorithms use the past and the present to predict the future. But when given biased historical data, these algorithms can quickly become discriminatory. The area of machine learning fairness has emerged to detect and de-bias these algorithms, but has received widespread criticism for its one-size-fits-all approach, which allows certain cases of bias to slip through the cracks. In this study, we take a deeper look at the mechanisms by which machine learning algorithms develop harmful bias. We introduce a new method to interpret discriminatory systems, an Evolutionary algorithm for Feature Interaction (EFI), which we apply to several commonly used machine learning algorithms in two real-world problem instances: violent crime and median house price prediction. In the results, we discover several complex forms of bias including the encoding of race through other seemingly unrelated attributes. Ultimately we suggest that more informative interpretation tools such as EFI can be used to not only explain machine learning outcomes, but supplement and improve existing machine bias detection approaches to provide a more robust and in-depth ethical evaluation of machine learning algorithms.",YES,,Feature Selection,,Detection,Where is the line between explanation methods and pipeline fairness stuff?,,,,,,,,,,,"@inproceedings{10.1145/3514094.3534126,
author = {Robertson, Jake and Stinson, Catherine and Hu, Ting},
title = {A Bio-Inspired Framework for Machine Bias Interpretation},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534126},
doi = {10.1145/3514094.3534126},
abstract = {Machine learning algorithms use the past and the present to predict the future. But when given biased historical data, these algorithms can quickly become discriminatory. The area of machine learning fairness has emerged to detect and de-bias these algorithms, but has received widespread criticism for its one-size-fits-all approach, which allows certain cases of bias to slip through the cracks. In this study, we take a deeper look at the mechanisms by which machine learning algorithms develop harmful bias. We introduce a new method to interpret discriminatory systems, an Evolutionary algorithm for Feature Interaction (EFI), which we apply to several commonly used machine learning algorithms in two real-world problem instances: violent crime and median house price prediction. In the results, we discover several complex forms of bias including the encoding of race through other seemingly unrelated attributes. Ultimately we suggest that more informative interpretation tools such as EFI can be used to not only explain machine learning outcomes, but supplement and improve existing machine bias detection approaches to provide a more robust and in-depth ethical evaluation of machine learning algorithms.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {588–598},
numpages = {11},
keywords = {feature interaction, interpretability, fairness, machine bias, feature importance},
location = {Oxford, United Kingdom},
series = {AIES '22}
}
  
"
AIES,2022,Bias in Artificial Intelligence Models in Financial Services,https://dl.acm.org/doi/10.1145/3514094.3539561,['√Ångel Pav√≥n P√©rez'],0,"Nowadays, artificial intelligence models are widely used in financial services, from credit scoring to fraud detection, having a direct impact on our daily lives. Although such models have been developed to try to reduce human bias and thus bring greater fairness to financial services decisions, studies have found that there is still significant discrimination by both face-to-face and algorithmic lenders. In fact, Apple has recently been investigated for gender discrimination in assigning a credit limit to its users, demonstrating that there may still be inherent biases in the development of such algorithms and models. Furthermore, biases in financial services models may not only lead to unfair discrimination but were also linked to health problems and recovery prospects. This project aims to analyse and identify the different types of biases found in AI models and data used in the financial services industry. We propose a method using data analysis and explainable models to explain how these biases emerge throughout the process of developing AI models as well as applying state-of-the-art bias dealing techniques to avoid and mitigate them. Finally, we propose how to evaluate these models according to the business objectives and consider possible trade-offs between different definitions of fairness. Thus, the main questions that this project will try to answer are as follows: - What are the current biases in credit risk and fraud detection models and how to identify them? - In what ways understanding how biases emerge from the data can help us in bias mitigation? - To what extent could credit risk and fraud detection models bias be mitigated, and what are the implications of those mitigation techniques? Answering these questions, we hope to create a pipeline for building these models by understanding the key points where bias can emerge and the appropriate methods to avoid it.",YES,,Data,,Detection,Case Study,,,,,,,,,,,"@inproceedings{10.1145/3514094.3539561,
author = {Pav\'{o}n P\'{e}rez, \'{A}ngel},
title = {Bias in Artificial Intelligence Models in Financial Services},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3539561},
doi = {10.1145/3514094.3539561},
abstract = {Nowadays, artificial intelligence models are widely used in financial services, from credit scoring to fraud detection, having a direct impact on our daily lives. Although such models have been developed to try to reduce human bias and thus bring greater fairness to financial services decisions, studies have found that there is still significant discrimination by both face-to-face and algorithmic lenders. In fact, Apple has recently been investigated for gender discrimination in assigning a credit limit to its users, demonstrating that there may still be inherent biases in the development of such algorithms and models. Furthermore, biases in financial services models may not only lead to unfair discrimination but were also linked to health problems and recovery prospects.This project aims to analyse and identify the different types of biases found in AI models and data used in the financial services industry. We propose a method using data analysis and explainable models to explain how these biases emerge throughout the process of developing AI models as well as applying state-of-the-art bias dealing techniques to avoid and mitigate them. Finally, we propose how to evaluate these models according to the business objectives and consider possible trade-offs between different definitions of fairness. Thus, the main questions that this project will try to answer are as follows: - What are the current biases in credit risk and fraud detection models and how to identify them? - In what ways understanding how biases emerge from the data can help us in bias mitigation? - To what extent could credit risk and fraud detection models bias be mitigated, and what are the implications of those mitigation techniques?Answering these questions, we hope to create a pipeline for building these models by understanding the key points where bias can emerge and the appropriate methods to avoid it.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {908},
numpages = {1},
keywords = {bias, machine learning, financial services, fairness},
location = {Oxford, United Kingdom},
series = {AIES '22}
}
  
"
EAAMO,2022,Fair Decision-Making for Food Inspections,https://dl.acm.org/doi/10.1145/3551624.3555289,"['Shubham Singh', 'Bhuvni Shah', 'Chris Kanich', 'Ian A. Kash']",2,"We revisit the application of predictive models by the Chicago Department of Public Health to schedule restaurant inspections and prioritize the detection of critical food code violations. We perform the first analysis of the model‚Äôs fairness to the population served by the restaurants in terms of average time to find a critical violation. We find that the model treats inspections unequally based on the sanitarian who conducted the inspection and that, in turn, there are geographic disparities in the benefits of the model. We examine four alternate methods of model training and two alternative ways of scheduling using the model and find that the latter generate more desirable results. The challenges from this application point to important directions for future work around fairness with collective entities rather than individuals, the use of critical violations as a proxy, and the disconnect between fair classification and fairness in dynamic scheduling systems.",YES,,Theme,,,,,,,,,,,,,,"@inproceedings{10.1145/3551624.3555289,
author = {Singh, Shubham and Shah, Bhuvni and Kanich, Chris and Kash, Ian A.},
title = {Fair Decision-Making for Food Inspections},
year = {2022},
isbn = {9781450394772},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551624.3555289},
doi = {10.1145/3551624.3555289},
abstract = {We revisit the application of predictive models by the Chicago Department of Public Health to schedule restaurant inspections and prioritize the detection of critical food code violations. We perform the first analysis of the model’s fairness to the population served by the restaurants in terms of average time to find a critical violation. We find that the model treats inspections unequally based on the sanitarian who conducted the inspection and that, in turn, there are geographic disparities in the benefits of the model. We examine four alternate methods of model training and two alternative ways of scheduling using the model and find that the latter generate more desirable results. The challenges from this application point to important directions for future work around fairness with collective entities rather than individuals, the use of critical violations as a proxy, and the disconnect between fair classification and fairness in dynamic scheduling systems.},
booktitle = {Equity and Access in Algorithms, Mechanisms, and Optimization},
articleno = {5},
numpages = {11},
keywords = {fairness, food inspections, scheduling},
location = {Arlington, VA, USA},
series = {EAAMO '22}
}
  
"
EAAMO,2022,Modeling Access Differences to Reduce Disparity in Resource Allocation,https://dl.acm.org/doi/10.1145/3551624.3555302,"['Kenya Andrews', 'Mesrob Ohannessian', 'Tanya Berger-Wolf']",0,"Motivated by COVID-19 vaccine allocation, where vulnerable subpopulations are simultaneously more impacted in terms of health and more disadvantaged in terms of access to the vaccine, we formalize and study the problem of resource allocation when there are inherent access differences that correlate with advantage and disadvantage. We identify reducing resource disparity as a key goal in this context and show its role as a proxy to more nuanced downstream impacts. We develop a concrete access model that helps quantify how a given allocation translates to resource flow for the advantaged vs. the disadvantaged, based on the access gap between them. We then provide a methodology for access-aware allocation. Intuitively, the resulting allocation leverages more vaccines in locations with higher vulnerable populations to mitigate the access gap and reduce overall disparity. Surprisingly, knowledge of the access gap is often not needed to perform access-aware allocation. To support this formalism, we provide empirical evidence for our access model and show that access-aware allocation can significantly reduce resource disparity and thus improve downstream outcomes. We demonstrate this at various scales, including at county, state, national, and global levels.",YES,,HELP,,,,,,,,,,,,,,"@inproceedings{10.1145/3551624.3555302,
author = {Andrews, Kenya and Ohannessian, Mesrob and Berger-Wolf, Tanya},
title = {Modeling Access Differences to Reduce Disparity in Resource Allocation},
year = {2022},
isbn = {9781450394772},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551624.3555302},
doi = {10.1145/3551624.3555302},
abstract = {Motivated by COVID-19 vaccine allocation, where vulnerable subpopulations are simultaneously more impacted in terms of health and more disadvantaged in terms of access to the vaccine, we formalize and study the problem of resource allocation when there are inherent access differences that correlate with advantage and disadvantage. We identify reducing resource disparity as a key goal in this context and show its role as a proxy to more nuanced downstream impacts. We develop a concrete access model that helps quantify how a given allocation translates to resource flow for the advantaged vs. the disadvantaged, based on the access gap between them. We then provide a methodology for access-aware allocation. Intuitively, the resulting allocation leverages more vaccines in locations with higher vulnerable populations to mitigate the access gap and reduce overall disparity. Surprisingly, knowledge of the access gap is often not needed to perform access-aware allocation. To support this formalism, we provide empirical evidence for our access model and show that access-aware allocation can significantly reduce resource disparity and thus improve downstream outcomes. We demonstrate this at various scales, including at county, state, national, and global levels.},
booktitle = {Equity and Access in Algorithms, Mechanisms, and Optimization},
articleno = {17},
numpages = {11},
location = {Arlington, VA, USA},
series = {EAAMO '22}
}
  
"
Neurips,2018,Predict Responsibly: Improving Fairness and Accuracy by Learning to Defer,https://proceedings.neurips.cc//paper/2018/hash/09d37c08f7b129e96277388757530c72-Abstract.html,"['David Madras', ' Toni Pitassi', ' Richard Zemel']",108,"In many machine learning applications, there are multiple decision-makers involved, both automated and human. The interaction between these agents often goes unaddressed in algorithmic development. In this work, we explore a simple version of this interaction with a two-stage framework containing an automated model and an external decision-maker. The model can choose to say PASS, and pass the decision downstream, as explored in rejection learning. We extend this concept by proposing ""learning to defer"", which generalizes rejection learning by considering the effect of other agents in the decision-making process. We propose a learning algorithm which accounts for potential biases held by external decision-makers in a system. Experiments demonstrate that learning to defer can make systems not only more accurate but also less biased. Even when working with inconsistent or biased users, we show that deferring models still greatly improve the accuracy and/or fairness of the entire system.",YES,,Organizational Realities,Algorithm,Methods,Develops a modeling technique that takes into account the biases of the humans making a decision,,,,,,,,,,,"@inproceedings{NEURIPS2018_09d37c08,
 author = {Madras, David and Pitassi, Toni and Zemel, Richard},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Predict Responsibly: Improving Fairness and Accuracy by Learning to Defer},
 url = {https://proceedings.neurips.cc/paper/2018/file/09d37c08f7b129e96277388757530c72-Paper.pdf},
 volume = {31},
 year = {2018}
}

"
Neurips,2018,Why Is My Classifier Discriminatory?,https://proceedings.neurips.cc//paper/2018/hash/1f1baa5b8edac74eb4eaa329f14a0361-Abstract.html,"['Irene Chen', ' Fredrik D. Johansson', ' David Sontag']",339,"Recent attempts to achieve fairness in predictive models focus on the balance between fairness and accuracy. In sensitive applications such as healthcare or criminal justice, this trade-off is often undesirable as any increase in prediction error could have devastating consequences. In this work, we argue that the fairness of predictions should be evaluated in context of the data, and that unfairness induced by inadequate samples sizes or unmeasured predictive variables should be addressed through data collection, rather than by constraining the model. We decompose cost-based metrics of discrimination into bias, variance, and noise, and propose actions aimed at estimating and reducing each term. Finally, we perform case-studies on prediction of income, mortality, and review ratings, confirming the value of this analysis. We find that data collection is often a means to reduce discrimination without sacrificing accuracy.",YES,,Data,Algorithm,Methods,"Shows how increasing training size reduces bias, and also provides methods for guiding when to aquire new examples and features. I get this less-- shows clustering techniques to find sources of extreme sources of bias in the data",,,,,,,,,,,"@inproceedings{NEURIPS2018_1f1baa5b,
 author = {Chen, Irene and Johansson, Fredrik D and Sontag, David},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Why Is My Classifier Discriminatory?},
 url = {https://proceedings.neurips.cc/paper/2018/file/1f1baa5b8edac74eb4eaa329f14a0361-Paper.pdf},
 volume = {31},
 year = {2018}
}

"
Neurips,2018,Hunting for Discriminatory Proxies in Linear Regression Models,https://proceedings.neurips.cc//paper/2018/hash/6cd9313ed34ef58bad3fdd504355e72c-Abstract.html,"['Samuel Yeom', ' Anupam Datta', ' Matt Fredrikson']",13,"A machine learning model may exhibit discrimination when used to make decisions involving people. One potential cause for such outcomes is that the model uses a statistical proxy for a protected demographic attribute. In this paper we formulate a definition of proxy use for the setting of linear regression and present algorithms for detecting proxies. Our definition follows recent work on proxies in classification models, and characterizes a model's constituent behavior that: 1) correlates closely with a protected random variable, and 2) is causally influential in the overall behavior of the model. We show that proxies in linear regression models can be efficiently identified by solving a second-order cone program, and further extend this result to account for situations where the use of a certain input variable is justified as a ``business necessity''. Finally, we present empirical results on two law enforcement datasets that exhibit varying degrees of racial disparity in prediction outcomes, demonstrating that proxies shed useful light on the causes of discriminatory behavior in models.",YES,,Feature Selection,Algorithm,Methods,,,,,,,,,,,,"@inproceedings{NEURIPS2018_6cd9313e,
 author = {Yeom, Samuel and Datta, Anupam and Fredrikson, Matt},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Hunting for Discriminatory Proxies in Linear Regression Models},
 url = {https://proceedings.neurips.cc/paper/2018/file/6cd9313ed34ef58bad3fdd504355e72c-Paper.pdf},
 volume = {31},
 year = {2018}
}

"
Neurips,2019,Assessing Social and Intersectional Biases in Contextualized Word Representations,https://proceedings.neurips.cc//paper/2019/hash/201d546992726352471cfea6b0df0a48-Abstract.html,"['Yi Chern Tan', ' L. Elisa Celis']",130,"Social bias in machine learning has drawn significant attention, with work ranging from demonstrations of bias in a multitude of applications, curating definitions of fairness for different contexts, to developing algorithms to mitigate bias. In natural language processing, gender bias has been shown to exist in context-free word embeddings. Recently, contextual word representations have outperformed word embeddings in several downstream NLP tasks. These word representations are conditioned on their context within a sentence, and can also be used to encode the entire sentence. In this paper, we analyze the extent to which state-of-the-art models for contextual word representations, such as BERT and GPT-2, encode biases with respect to gender, race, and intersectional identities. Towards this, we propose assessing bias at the contextual word level. This novel approach captures the contextual effects of bias missing in context-free word embeddings, yet avoids confounding effects that underestimate bias at the sentence encoding level. We demonstrate evidence of bias at the corpus level, find varying evidence of bias in embedding association tests, show in particular that racial bias is strongly encoded in contextual word models, and observe that bias effects for intersectional minorities are exacerbated beyond their constituent minority identities. Further, evaluating bias effects at the contextual word level captures biases that are not captured at the sentence level, confirming the need for our novel approach.",YES,,Data,Problem Identification,Measurement,Shows that there is bias in NLP models on the WORD level for contextual word level that is not captured in sentence-based models. They create an approach for bias measurement at the word level.,,,,,,,,,,,"@inproceedings{NEURIPS2019_201d5469,
 author = {Tan, Yi Chern and Celis, L. Elisa},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Assessing Social and Intersectional Biases in Contextualized Word Representations},
 url = {https://proceedings.neurips.cc/paper/2019/file/201d546992726352471cfea6b0df0a48-Paper.pdf},
 volume = {32},
 year = {2019}
}

"
Neurips,2019,Differential Privacy Has Disparate Impact on Model Accuracy,https://proceedings.neurips.cc//paper/2019/hash/fc0de4e0396fff257ea362983c2dda5a-Abstract.html,"['Eugene Bagdasaryan', ' Omid Poursaeed', ' Vitaly Shmatikov']",267,"Differential privacy (DP) is a popular mechanism for training machine learning models with bounded leakage about the presence of specific points in the training data. The cost of differential privacy is a reduction in the model's accuracy. We demonstrate that in the neural networks trained using differentially private stochastic gradient descent (DP-SGD), this cost is not borne equally: accuracy of DP models drops much more for the underrepresented classes and subgroups. For example, a gender classification model trained using DP-SGD exhibits much lower accuracy for black faces than for white faces. Critically, this gap is bigger in the DP model than in the non-DP model, i.e., if the original model is unfair, the unfairness becomes worse once DP is applied. We demonstrate this effect for a variety of tasks and models, including sentiment analysis of text and image classification. We then explain why DP training mechanisms such as gradient clipping and noise addition have disproportionate effect on the underrepresented and more complex subgroups, resulting in a disparate reduction of model accuracy.",YES,,Modeling,,Problem Identification,Shows that DP impacts different group accuracies differently,,,,,,,,,,,"@inproceedings{NEURIPS2019_fc0de4e0,
 author = {Bagdasaryan, Eugene and Poursaeed, Omid and Shmatikov, Vitaly},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Differential Privacy Has Disparate Impact on Model Accuracy},
 url = {https://proceedings.neurips.cc/paper/2019/file/fc0de4e0396fff257ea362983c2dda5a-Paper.pdf},
 volume = {32},
 year = {2019}
}

"
Neurips,2020,Robust Optimization for Fairness with Noisy Protected Groups,https://proceedings.neurips.cc//paper/2020/hash/37d097caf1299d9aa79c2c2b843d2d78-Abstract.html,"['Serena Wang', ' Wenshuo Guo', ' Harikrishna Narasimhan', ' Andrew Cotter', ' Maya Gupta', ' Michael Jordan']",74,"Many existing fairness criteria for machine learning involve equalizing some metric across protected groups such as race or gender. However, practitioners trying to audit or enforce such group-based criteria can easily face the problem of noisy or biased protected group information. First, we study the consequences of naively relying on noisy protected group labels: we provide an upper bound on the fairness violations on the true groups $G$ when the fairness criteria are satisfied on noisy groups $\hat{G}$. Second, we introduce two new approaches using robust optimization that, unlike the naive approach of only relying on $\hat{G}$, are guaranteed to satisfy fairness criteria on the true protected groups $G$ while minimizing a training objective. We provide theoretical guarantees that one such approach converges to an optimal feasible solution. Using two case studies, we show empirically that the robust approaches achieve better true group fairness guarantees than the naive approach.",YES,,Data,Noise,,Shows how naively relying on noisy data can lead to fairness degredation; beyond that is kind of traditional in that it shows how to enforce fairness metrics on noist data,,,,,,,,,,,"@inproceedings{NEURIPS2020_37d097ca,
 author = {Wang, Serena and Guo, Wenshuo and Narasimhan, Harikrishna and Cotter, Andrew and Gupta, Maya and Jordan, Michael},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {5190--5203},
 publisher = {Curran Associates, Inc.},
 title = {Robust Optimization for Fairness with Noisy Protected Groups},
 url = {https://proceedings.neurips.cc/paper/2020/file/37d097caf1299d9aa79c2c2b843d2d78-Paper.pdf},
 volume = {33},
 year = {2020}
}

"
Neurips,2021,Can Information Flows Suggest Targets for Interventions in Neural Circuits?,https://proceedings.neurips.cc//paper/2021/hash/18de4beb01f6a17b6e1dfb9813ba6045-Abstract.html,"['Praveen Venkatesh', ' Sanghamitra Dutta', ' Neil Mehta', ' Pulkit Grover']",2,"Motivated by neuroscientific and clinical applications, we empirically examine whether observational measures of information flow can suggest interventions. We do so by performing experiments on artificial neural networks in the context of fairness in machine learning, where the goal is to induce fairness in the system through interventions. Using our recently developed M-information flow framework, we measure the flow of information about the true label (responsible for accuracy, and hence desirable), and separately, the flow of information about a protected attribute (responsible for bias, and hence undesirable) on the edges of a trained neural network. We then compare the flow magnitudes against the effect of intervening on those edges by pruning. We show that pruning edges that carry larger information flows about the protected attribute reduces bias at the output to a greater extent. This demonstrates that M-information flow can meaningfully suggest targets for interventions, answering the title's question in the affirmative. We also evaluate bias-accuracy tradeoffs for different intervention strategies, to analyze how one might use estimates of desirable and undesirable information flows (here, accuracy and bias flows) to inform interventions that preserve the former while reducing the latter.",YES,,Modeling,Hyperparameters,Methods,Shows how certain pruning techniques can reduce bias (disparity) in model outcomes).,,,,,,,,,,,"@inproceedings{NEURIPS2021_18de4beb,
 author = {Venkatesh, Praveen and Dutta, Sanghamitra and Mehta, Neil and Grover, Pulkit},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {3149--3162},
 publisher = {Curran Associates, Inc.},
 title = {Can Information Flows Suggest Targets for Interventions in Neural Circuits?},
 url = {https://proceedings.neurips.cc/paper/2021/file/18de4beb01f6a17b6e1dfb9813ba6045-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
Neurips,2021,Retiring Adult: New Datasets for Fair Machine Learning,https://proceedings.neurips.cc//paper/2021/hash/32e54441e6382a7fbacbbbaf3c450059-Abstract.html,"['Frances Ding', ' Moritz Hardt', ' John Miller', ' Ludwig Schmidt']",88,"Although the fairness community has recognized the importance of data, researchers in the area primarily rely on UCI Adult when it comes to tabular data. Derived from a 1994 US Census survey, this dataset has appeared in hundreds of research papers where it served as the basis for the development and comparison of many algorithmic fairness interventions. We reconstruct a superset of the UCI Adult data from available US Census sources and reveal idiosyncrasies of the UCI Adult dataset that limit its external validity. Our primary contribution is a suite of new datasets derived from US Census surveys that extend the existing data ecosystem for research on fair machine learning. We create prediction tasks relating to income, employment, health, transportation, and housing. The data span multiple years and all states of the United States, allowing researchers to study temporal shift and geographic variation. We highlight a broad initial sweep of new empirical insights relating to trade-offs between fairness criteria, performance of algorithmic interventions, and the role of distribution shift based on our new datasets. Our findings inform ongoing debates, challenge some existing narratives, and point to future research directions.",YES,,Evaluation,,New Data,Provides new datasets to evaluate fairness systems,,,,,,,,,,,"@inproceedings{NEURIPS2021_32e54441,
 author = {Ding, Frances and Hardt, Moritz and Miller, John and Schmidt, Ludwig},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {6478--6490},
 publisher = {Curran Associates, Inc.},
 title = {Retiring Adult: New Datasets for Fair Machine Learning},
 url = {https://proceedings.neurips.cc/paper/2021/file/32e54441e6382a7fbacbbbaf3c450059-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
Neurips,2021,Assessing Fairness in the Presence of Missing Data,https://proceedings.neurips.cc//paper/2021/hash/85dca1d270f7f9aef00c9d372f114482-Abstract.html,"['Yiliang Zhang', ' Qi Long']",7,"Missing data are prevalent and present daunting challenges in real data analysis. While there is a growing body of literature on fairness in analysis of fully observed data, there has been little theoretical work on investigating fairness in analysis of incomplete data. In practice, a popular analytical approach for dealing with missing data is to use only the set of complete cases, i.e., observations with all features fully observed to train a prediction algorithm. However, depending on the missing data mechanism, the distribution of complete cases and the distribution of the complete data may be substantially different. When the goal is to develop a fair algorithm in the complete data domain where there are no missing values, an algorithm that is fair in the complete case domain may show disproportionate bias towards some marginalized groups in the complete data domain. To fill this significant gap, we study the problem of estimating fairness in the complete data domain for an arbitrary model evaluated merely using complete cases. We provide upper and lower bounds on the fairness estimation error and conduct numerical experiments to assess our theoretical results. Our work provides the first known theoretical results on fairness guarantee in analysis of incomplete data.",YES,,Data,,Measurement,,,,,,,,,,,,"@inproceedings{NEURIPS2021_85dca1d2,
 author = {Zhang, Yiliang and Long, Qi},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {16007--16019},
 publisher = {Curran Associates, Inc.},
 title = {Assessing Fairness in the Presence of Missing Data},
 url = {https://proceedings.neurips.cc/paper/2021/file/85dca1d270f7f9aef00c9d372f114482-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
Neurips,2021,Can Less be More? When Increasing-to-Balancing Label Noise Rates Considered Beneficial,https://proceedings.neurips.cc//paper/2021/hash/91e50fe1e39af2869d3336eaaeebdb43-Abstract.html,"['Yang Liu', ' Jialu Wang']",9,"In this paper, we answer the question of when inserting label noise (less informative labels) can instead return us more accurate and fair models. We are primarily inspired by three observations: 1) In contrast to reducing label noise rates, increasing the noise rates is easy to implement; 2) Increasing a certain class of instances' label noise to balance the noise rates (increasing-to-balancing) results in an easier learning problem; 3) Increasing-to-balancing improves fairness guarantees against label bias. In this paper, we first quantify the trade-offs introduced by increasing a certain group of instances' label noise rate w.r.t. the loss of label informativeness and the lowered learning difficulties. We analytically demonstrate when such an increase is beneficial, in terms of either improved generalization power or the fairness guarantees. Then we present a method to insert label noise properly for the task of learning with noisy labels, either without or with a fairness constraint. The primary technical challenge we face is due to the fact that we would not know which data instances are suffering from higher noise, and we would not have the ground truth labels to verify any possible hypothesis. We propose a detection method that informs us which group of labels might suffer from higher noise without using ground truth labels. We formally establish the effectiveness of the proposed solution and demonstrate it with extensive experiments.",YES,,Data,Algorithm,Methods,"Develops a data processing technique that balances data noise across subgroups by adding MORE noise, and shows that this helps with generalization and fairness guarantees in some instances.",,,,,,,,,,,"@inproceedings{NEURIPS2021_91e50fe1,
 author = {Liu, Yang and Wang, Jialu},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {17467--17479},
 publisher = {Curran Associates, Inc.},
 title = {Can Less be More? When Increasing-to-Balancing Label Noise Rates Considered Beneficial},
 url = {https://proceedings.neurips.cc/paper/2021/file/91e50fe1e39af2869d3336eaaeebdb43-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
Neurips,2021,DECAF: Generating Fair Synthetic Data Using Causally-Aware Generative Networks,https://proceedings.neurips.cc//paper/2021/hash/ba9fab001f67381e56e410575874d967-Abstract.html,"['Boris van Breugel', ' Trent Kyono', ' Jeroen Berrevoets', ' Mihaela van der Schaar']",10,"Machine learning models have been criticized for reflecting unfair biases in the training data. Instead of solving for this by introducing fair learning algorithms directly, we focus on generating fair synthetic data, such that any downstream learner is fair. Generating fair synthetic data from unfair data - while remaining truthful to the underlying data-generating process (DGP) - is non-trivial. In this paper, we introduce DECAF: a GAN-based fair synthetic data generator for tabular data. With DECAF we embed the DGP explicitly as a structural causal model in the input layers of the generator, allowing each variable to be reconstructed conditioned on its causal parents. This procedure enables inference time debiasing, where biased edges can be strategically removed for satisfying user-defined fairness requirements. The DECAF framework is versatile and compatible with several popular definitions of fairness. In our experiments, we show that DECAF successfully removes undesired bias and - in contrast to existing methods - is capable of generating high-quality synthetic data. Furthermore, we provide theoretical guarantees on the generator's convergence and the fairness of downstream models.",YES,,Data,,Methods,Creates a GAN which generates unbiased data to aid with biased data problems… at prediciton time? Needs a bit more of a read,,,,,,,,,,,"@inproceedings{NEURIPS2021_ba9fab00,
 author = {van Breugel, Boris and Kyono, Trent and Berrevoets, Jeroen and van der Schaar, Mihaela},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {22221--22233},
 publisher = {Curran Associates, Inc.},
 title = {DECAF:  Generating Fair Synthetic Data Using Causally-Aware Generative Networks},
 url = {https://proceedings.neurips.cc/paper/2021/file/ba9fab001f67381e56e410575874d967-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
Neurips,2021,Differentially Private Empirical Risk Minimization under the Fairness Lens,https://proceedings.neurips.cc//paper/2021/hash/e7e8f8e5982b3298c8addedf6811d500-Abstract.html,"['Cuong Tran', ' My Dinh', ' Ferdinando Fioretto']",10,"Differential Privacy (DP) is an important privacy-enhancing technology for private machine learning systems. It allows to measure and bound the risk associated with an individual participation in a computation. However, it was recently observed that DP learning systems may exacerbate bias and unfairness for different groups of individuals. This paper builds on these important observations and sheds light on the causes of the disparate impacts arising in the problem of differentially private empirical risk minimization. It focuses on the accuracy disparity arising among groups of individuals in two well-studied DP learning methods: output perturbation and differentially private stochastic gradient descent. The paper analyzes which data and model properties are responsible for the disproportionate impacts, why these aspects are affecting different groups disproportionately, and proposes guidelines to mitigate these effects. The proposed approach is evaluated on several datasets and settings.",YES,,Modeling,,Problem Detection,Shows WHY DP algorithms make fairness more difficult due to their modeling practices,,,,,,,,,,,"@inproceedings{NEURIPS2021_e7e8f8e5,
 author = {Tran, Cuong and Dinh, My and Fioretto, Ferdinando},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {27555--27565},
 publisher = {Curran Associates, Inc.},
 title = {Differentially Private Empirical Risk Minimization under the Fairness Lens},
 url = {https://proceedings.neurips.cc/paper/2021/file/e7e8f8e5982b3298c8addedf6811d500-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
ICLR,2019,Feature-Wise Bias Amplification,https://openreview.net/forum?id=S1ecm2C9K7,"['Klas Leino', 'Emily Black', 'Matt Fredrikson', 'Shayak Sen', 'Anupam Datta']",32,"We study the phenomenon of bias amplification in classifiers, wherein a machine learning model learns to predict classes with a greater disparity than the underlying ground truth. We demonstrate that bias amplification can arise via inductive bias in gradient descent methods resulting in overestimation of importance of moderately-predictive ``weak'' features if insufficient training data is available. This overestimation gives rise to feature-wise bias amplification -- a previously unreported form of bias that can be traced back to the features of a trained model. Through analysis and experiments, we show that the while some bias cannot be mitigated without sacrificing accuracy, feature-wise bias amplification can be mitigated through targeted feature selection. We present two new feature selection algorithms for mitigating bias amplification in linear models, and show how they can be adapted to convolutional neural networks efficiently. Our experiments on synthetic and real data demonstrate that these algorithms consistently lead to reduced bias without harming accuracy, in some cases eliminating predictive bias altogether while providing modest gains in accuracy.",YES,,Feature Selection,Optimization Algo Choice,,,,,,,,,,,,,"@inproceedings{
leino2018featurewise,
title={Feature-Wise Bias Amplification},
author={Klas Leino and Matt Fredrikson and Emily Black and Shayak Sen and Anupam Datta},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=S1ecm2C9K7},
}"
ICLR,2022,The Rich Get Richer: Disparate Impact of Semi-Supervised Learning,https://openreview.net/forum?id=DXPftn5kjQK,"['Zhaowei Zhu', ' Tianyi Luo', ' Yang Liu']",14,"Semi-supervised learning (SSL) has demonstrated its potential to improve the model accuracy for a variety of learning tasks when the high-quality supervised data is severely limited. Although it is often established that the average accuracy for the entire population of data is improved, it is unclear how SSL fares with different sub-populations. Understanding the above question has substantial fairness implications when different sub-populations are defined by the demographic groups that we aim to treat fairly. In this paper, we reveal the disparate impacts of deploying SSL: the sub-population who has a higher baseline accuracy without using SSL (the ""rich"" one) tends to benefit more from SSL; while the sub-population who suffers from a low baseline accuracy (the ""poor"" one) might even observe a performance drop after adding the SSL module. We theoretically and empirically establish the above observation for a broad family of SSL algorithms, which either explicitly or implicitly use an auxiliary ""pseudo-label"". Experiments on a set of image and text classification tasks confirm our claims. We introduce a new metric, Benefit Ratio, and promote the evaluation of the fairness of SSL (Equalized Benefit Ratio). We further discuss how the disparate impact can be mitigated. We hope our paper will alarm the potential pitfall of using SSL and encourage a multifaceted evaluation of future SSL algorithms.",YES,,Modeling Choice,Metrics,,,,,,,,,,,,,"@inproceedings{
zhu2022the,
title={The Rich Get Richer: Disparate Impact of Semi-Supervised Learning},
author={Zhaowei Zhu and Tianyi Luo and Yang Liu},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=DXPftn5kjQK}
}"
ICLR,2022,Learning with Noisy Labels Revisited: A Study Using Real-World Human Annotations,https://openreview.net/forum?id=TBWA6PLJZQm,"['Jiaheng Wei', ' Zhaowei Zhu', ' Hao Cheng', ' Tongliang Liu', ' Gang Niu', ' Yang Liu']",37,"Existing research on learning with noisy labels mainly focuses on synthetic label noise. The synthetic noise, though has clean structures which greatly enabled statistical analyses, often fails to model the real-world noise patterns. The recent literature has observed several efforts to offer real-world noisy datasets, e.g., Food-101N, WebVision, and Clothing1M. Yet the existing efforts suffer from two caveats: firstly, the lack of ground-truth verification makes it hard to theoretically study the property and treatment of real-world label noise. Secondly, these efforts are often of large scales, which may result in unfair comparisons of robust methods within reasonable and accessible computation power. To better understand real-world label noise, it is important to establish controllable, easy-to-use, and moderate-sized real-world noisy datasets with both ground-truth and noisy labels. This work presents two new benchmark datasets, which we name as CIFAR-10N, CIFAR-100N (jointly we call them CIFAR-N), equipping the training datasets of CIFAR-10 and CIFAR-100 with human-annotated real-world noisy labels we collected from Amazon Mechanical Turk. We quantitatively and qualitatively show that real-world noisy labels follow an instance-dependent pattern rather than the classically assumed and adopted ones (e.g., class-dependent label noise). We then initiate an effort to benchmarking a subset of the existing solutions using CIFAR-10N and CIFAR-100N. We further proceed to study the memorization of correct and wrong predictions, which further illustrates the difference between human noise and class-dependent synthetic noise. We show indeed the real-world noise patterns impose new and outstanding challenges as compared to synthetic label noise. These observations require us to rethink the treatment of noisy labels, and we hope the availability of these two datasets would facilitate the development and evaluation of future learning with noisy label solutions. The corresponding datasets and the leaderboard are available at http://noisylabels.com.",YES,,Data,,,,,,,,,,,,,,"@inproceedings{
wei2022learning,
title={Learning with Noisy Labels Revisited: A Study Using Real-World Human Annotations},
author={Jiaheng Wei and Zhaowei Zhu and Hao Cheng and Tongliang Liu and Gang Niu and Yang Liu},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=TBWA6PLJZQm}
}"
ICLR,2022,Fairness in Representation for Multilingual NLP: Insights from Controlled Experiments on Conditional Language Modeling,https://openreview.net/forum?id=-llS6TiOew,['Ada Wan'],1,"We perform systematically and fairly controlled experiments with the 6-layer Transformer to investigate the hardness in conditional-language-modeling languages which have been traditionally considered morphologically rich (AR and RU) and poor (ZH). We evaluate through statistical comparisons across 30 possible language directions from the 6 languages of the United Nations Parallel Corpus across 5 data sizes on 3 representation levels --- character, byte, and word. Results show that performance is relative to the representation granularity of each of the languages, not to the language as a whole. On the character and byte levels, we are able to eliminate statistically significant performance disparity, hence demonstrating that a language cannot be intrinsically hard. The disparity that mirrors the morphological complexity hierarchy is shown to be a byproduct of word segmentation. Evidence from data statistics, along with the fact that word segmentation is qualitatively indeterminate, renders a decades-long debate on morphological complexity (unless it is being intentionally modeled in a word-based, meaning-driven context) irrelevant in the context of computing. The intent of our work is to help effect more objectivity and adequacy in evaluation as well as fairness and inclusivity in experimental setup in the area of language and computing so to uphold diversity in Machine Learning and Artificial Intelligence research. Multilinguality is real and relevant in computing not due to canonical, structural linguistic concepts such as morphology or ""words"" in our minds, but rather standards related to internationalization and localization, such as character encoding --- something which has thus far been sorely overlooked in our discourse and curricula.",YES,,Feature Engineering,Data,,,,,,,,,,,,,"@inproceedings{
wan2022fairness,
title={Fairness in Representation for Multilingual {NLP}: Insights from Controlled Experiments on Conditional Language Modeling},
author={Ada Wan},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=-llS6TiOew}
}"
ICLR,2022,Diverse Client Selection for Federated Learning via Submodular Maximization,https://openreview.net/forum?id=nwKXyFvaUm,"['Ravikumar Balakrishnan', ' Tian Li', ' Tianyi Zhou', ' Nageen Himayat', ' Virginia Smith', ' Jeff Bilmes']",16,"In every communication round of federated learning, a random subset of clients communicate their model updates back to the server which then aggregates them all. The optimal size of this subset is not known and several studies have shown that typically random selection does not perform very well in terms of convergence, learning efficiency and fairness. We, in this paper, propose to select a small diverse subset of clients, namely those carrying representative gradient information, and we transmit only these updates to the server. Our aim is for updating via only a subset to approximate updating via aggregating all client information. We achieve this by choosing a subset that maximizes a submodular facility location function defined over gradient space. We introduce ‚Äúfederated averaging with diverse client selection (DivFL)‚Äù. We provide a thorough analysis of its convergence in the heterogeneous setting and apply it both to synthetic and to real datasets. Empirical results show several benefits to our approach including improved learning efficiency, faster convergence and also more uniform (i.e., fair) performance across clients. We further show a communication-efficient version of DivFL that can still outperform baselines on the above metrics.",YES/EH,,Maybe,Modeling Choice,,,,,,,,,,,,,"@inproceedings{
balakrishnan2022diverse,
title={Diverse Client Selection for Federated Learning via Submodular Maximization},
author={Ravikumar Balakrishnan and Tian Li and Tianyi Zhou and Nageen Himayat and Virginia Smith and Jeff Bilmes},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=nwKXyFvaUm}
}"
ICLR,2022,Distributionally Robust Fair Principal Components via Geodesic Descents,https://openreview.net/forum?id=9NVd-DMtThY,"['Hieu Vu', ' Toan Tran', ' Man-Chung Yue', ' Viet Anh Nguyen']",3,"Principal component analysis is a simple yet useful dimensionality reduction technique in modern machine learning pipelines. In consequential domains such as college admission, healthcare and credit approval, it is imperative to take into account emerging criteria such as the fairness and the robustness of the learned projection. In this paper, we propose a distributionally robust optimization problem for principal component analysis which internalizes a fairness criterion in the objective function. The learned projection thus balances the trade-off between the total reconstruction error and the reconstruction error gap between subgroups, taken in the min-max sense over all distributions in a moment-based ambiguity set. The resulting optimization problem over the Stiefel manifold can be efficiently solved by a Riemannian subgradient descent algorithm with a sub-linear convergence rate. Our experimental results on real-world datasets show the merits of our proposed method over state-of-the-art baselines.",Yes/EH,,Unsupervised,Algorithm,,,,,,,,,,,,,"@inproceedings{
vu2022distributionally,
title={Distributionally Robust Fair Principal Components via Geodesic Descents},
author={Hieu Vu and Toan Tran and Man-Chung Yue and Viet Anh Nguyen},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=9NVd-DMtThY}
}"
AIES,2021,Ensuring Fairness under Prior Probability Shifts,https://dl.acm.org/doi/10.1145/3461702.3462596,"['Arpita Biswas', 'Suvam Mukherjee']",10,"Prior probability shift is a phenomenon where the training and test datasets differ structurally within population subgroups. This phenomenon can be observed in the yearly records of several real-world datasets, for example, recidivism records and medical expenditure surveys. If unaccounted for, such shifts can cause the predictions of a classifier to become unfair towards specific population subgroups. While the fairness notion called Proportional Equality (PE) accounts for such shifts, a procedure to ensure PE-fairness was unknown. In this work, we design an algorithm, called CAPE, that ensures fair classification under such shifts. We introduce a metric, called prevalence difference, which CAPE attempts to minimize in order to achieve fairness under prior probability shifts. We theoretically establish that this metric exhibits several properties that are desirable for a fair classifier. We evaluate the efficacy of CAPE via a thorough empirical evaluation on synthetic datasets. We also compare the performance of CAPE with several state-of-the-art fair classifiers on real-world datasets like COMPAS (criminal risk assessment) and MEPS (medical expenditure panel survey). The results indicate that CAPE ensures a high degree of PE-fairness in its predictions, while performing well on other important metrics.",YES/Maybe,,In-Situ,Metrics,Methods,,,,,,,,,,,,"@inproceedings{10.1145/3461702.3462596,
author = {Biswas, Arpita and Mukherjee, Suvam},
title = {Ensuring Fairness under Prior Probability Shifts},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462596},
doi = {10.1145/3461702.3462596},
abstract = {Prior probability shift is a phenomenon where the training and test datasets differ structurally within population subgroups. This phenomenon can be observed in the yearly records of several real-world datasets, for example, recidivism records and medical expenditure surveys. If unaccounted for, such shifts can cause the predictions of a classifier to become unfair towards specific population subgroups. While the fairness notion called Proportional Equality (PE) accounts for such shifts, a procedure to ensure PE-fairness was unknown. In this work, we design an algorithm, called CAPE, that ensures fair classification under such shifts. We introduce a metric, called prevalence difference, which CAPE attempts to minimize in order to achieve fairness under prior probability shifts. We theoretically establish that this metric exhibits several properties that are desirable for a fair classifier. We evaluate the efficacy of CAPE via a thorough empirical evaluation on synthetic datasets. We also compare the performance of CAPE with several state-of-the-art fair classifiers on real-world datasets like COMPAS (criminal risk assessment) and MEPS (medical expenditure panel survey). The results indicate that CAPE ensures a high degree of PE-fairness in its predictions, while performing well on other important metrics.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {414–424},
numpages = {11},
keywords = {classification, algorithmic fairness, distributional shifts, discrimination},
location = {Virtual Event, USA},
series = {AIES '21}
}
  
"
AIES,2021,Disparate Impact of Artificial Intelligence Bias in Ridehailing Economy's Price Discrimination Algorithms,https://dl.acm.org/doi/10.1145/3461702.3462561,"['Akshat Pandey', 'Aylin Caliskan']",26,"Ridehailing applications that collect mobility data from individuals to inform smart city planning predict each trip's fare pricing with automated algorithms that rely on artificial intelligence (AI). This type of AI algorithm, namely a price discrimination algorithm, is widely used in the industry's black box systems for dynamic individualized pricing. Lacking transparency, studying such AI systems for fairness and disparate impact has not been possible without access to data used in generating the outcomes of price discrimination algorithms. Recently, in an effort to enhance transparency in city planning, the city of Chicago regulation mandated that transportation providers publish anonymized data on ridehailing. As a result, we present the first large-scale measurement of the disparate impact of price discrimination algorithms used by ridehailing applications. The application of random effects models from the meta-analysis literature combines the city-level effects of AI bias on fare pricing from census tract attributes, aggregated from the American Community Survey. An analysis of 100 million ridehailing samples from the city of Chicago indicates a significant disparate impact in fare pricing of neighborhoods due to AI bias learned from ridehailing utilization patterns associated with demographic attributes. Neighborhoods with larger non-white populations, higher poverty levels, younger residents, and high education levels are significantly associated with higher fare prices, with combined effect sizes, measured in Cohen's d, of -0.32, -0.28, 0.69, and 0.24 for each demographic, respectively. Further, our methods hold promise for identifying and addressing the sources of disparate impact in AI algorithms learning from datasets that contain U.S. geolocations.",YES/Maybe,,Data,,Measurement,Case Study,,,,,,,,,,,"@inproceedings{10.1145/3461702.3462561,
author = {Pandey, Akshat and Caliskan, Aylin},
title = {Disparate Impact of Artificial Intelligence Bias in Ridehailing Economy's Price Discrimination Algorithms},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462561},
doi = {10.1145/3461702.3462561},
abstract = {Ridehailing applications that collect mobility data from individuals to inform smart city planning predict each trip's fare pricing with automated algorithms that rely on artificial intelligence (AI). This type of AI algorithm, namely a price discrimination algorithm, is widely used in the industry's black box systems for dynamic individualized pricing. Lacking transparency, studying such AI systems for fairness and disparate impact has not been possible without access to data used in generating the outcomes of price discrimination algorithms. Recently, in an effort to enhance transparency in city planning, the city of Chicago regulation mandated that transportation providers publish anonymized data on ridehailing. As a result, we present the first large-scale measurement of the disparate impact of price discrimination algorithms used by ridehailing applications.The application of random effects models from the meta-analysis literature combines the city-level effects of AI bias on fare pricing from census tract attributes, aggregated from the American Community Survey. An analysis of 100 million ridehailing samples from the city of Chicago indicates a significant disparate impact in fare pricing of neighborhoods due to AI bias learned from ridehailing utilization patterns associated with demographic attributes. Neighborhoods with larger non-white populations, higher poverty levels, younger residents, and high education levels are significantly associated with higher fare prices, with combined effect sizes, measured in Cohen's d, of -0.32, -0.28, 0.69, and 0.24 for each demographic, respectively. Further, our methods hold promise for identifying and addressing the sources of disparate impact in AI algorithms learning from datasets that contain U.S. geolocations.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {822–833},
numpages = {12},
keywords = {prediction, algorithmic bias, disparate impact, price discrimination, AI ethics, geolocation},
location = {Virtual Event, USA},
series = {AIES '21}
}
  
"
AIES,2021,Understanding the Representation and Representativeness of Age in AI Data Sets,https://dl.acm.org/doi/10.1145/3461702.3462590,"['Joon Sung Park', 'Michael S. Bernstein', 'Robin N. Brewer', 'Ece Kamar', 'Meredith Ringel Morris']",8,"A diverse representation of different demographic groups in AI training data sets is important in ensuring that the models will work for a large range of users. To this end, recent efforts in AI fairness and inclusion have advocated for creating AI data sets that are well-balanced across race, gender, socioeconomic status, and disability status. In this paper, we contribute to this line of work by focusing on the representation of age by asking whether older adults are represented proportionally to the population at large in AI data sets. We examine publicly-available information about 92 face data sets to understand how they codify age as a case study to investigate how the subjects' ages are recorded and whether older generations are represented. We find that older adults are very under-represented; five data sets in the study that explicitly documented the closed age intervals of their subjects included older adults (defined as older than 65 years), while only one included oldest-old adults (defined as older than 85 years). Additionally, we find that only 24 of the data sets include any age-related information in their documentation or metadata, and that there is no consistent method followed across these data sets to collect and record the subjects' ages. We recognize the unique difficulties in creating representative data sets in terms of age, but raise it as an important dimension that researchers and engineers interested in inclusive AI should consider.",YES/Maybe,,Data,Evaluation Metrics,,"Shows that older people are under-represented, can be a evaluation metrics problem",,,,,,,,,,,"@inproceedings{10.1145/3461702.3462590,
author = {Park, Joon Sung and Bernstein, Michael S. and Brewer, Robin N. and Kamar, Ece and Morris, Meredith Ringel},
title = {Understanding the Representation and Representativeness of Age in AI Data Sets},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462590},
doi = {10.1145/3461702.3462590},
abstract = {A diverse representation of different demographic groups in AI training data sets is important in ensuring that the models will work for a large range of users. To this end, recent efforts in AI fairness and inclusion have advocated for creating AI data sets that are well-balanced across race, gender, socioeconomic status, and disability status. In this paper, we contribute to this line of work by focusing on the representation of age by asking whether older adults are represented proportionally to the population at large in AI data sets. We examine publicly-available information about 92 face data sets to understand how they codify age as a case study to investigate how the subjects' ages are recorded and whether older generations are represented. We find that older adults are very under-represented; five data sets in the study that explicitly documented the closed age intervals of their subjects included older adults (defined as older than 65 years), while only one included oldest-old adults (defined as older than 85 years). Additionally, we find that only 24 of the data sets include any age-related information in their documentation or metadata, and that there is no consistent method followed across these data sets to collect and record the subjects' ages. We recognize the unique difficulties in creating representative data sets in terms of age, but raise it as an important dimension that researchers and engineers interested in inclusive AI should consider.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {834–842},
numpages = {9},
keywords = {AI fate, datasets, accessibility, older adults, representation, inclusion, aging},
location = {Virtual Event, USA},
series = {AIES '21}
}
  
"
AIES,2022,Long-term Dynamics of Fairness Intervention in Connection Recommender Systems,https://dl.acm.org/doi/10.1145/3514094.3534173,"['Nil-Jana Akpinar', 'Cyrus DiCiccio', 'Preetam Nandy', 'Kinjal Basu']",1,"Recommender system fairness has been studied from the perspectives of a variety of stakeholders including content producers, the content itself and recipients of recommendations. Regardless of which type of stakeholders are considered, most works in this area assess the efficacy of fairness intervention by evaluating a single fixed fairness criterion through the lens of a one-shot, static setting. Yet recommender systems constitute dynamical systems with feedback loops from the recommendations to the underlying population distributions which could lead to unforeseen and adverse consequences if not taken into account. In this paper, we study a connection recommender system patterned after the systems employed by web-scale social networks and analyze the long-term effects of intervening on fairness in the recommendations. We find that, although seemingly fair in aggregate, common exposure and utility parity interventions fail to mitigate amplification of biases in the long term. We theoretically characterize how certain fairness interventions impact the bias amplification dynamics in a stylized Polya urn model.",YES/Maybe,,Deployment,,Detection,"Not prediction, ranking",,,,,,,,,,,"@inproceedings{10.1145/3514094.3534173,
author = {Akpinar, Nil-Jana and DiCiccio, Cyrus and Nandy, Preetam and Basu, Kinjal},
title = {Long-Term Dynamics of Fairness Intervention in Connection Recommender Systems},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534173},
doi = {10.1145/3514094.3534173},
abstract = {Recommender system fairness has been studied from the perspectives of a variety of stakeholders including content producers, the content itself and recipients of recommendations. Regardless of which type of stakeholders are considered, most works in this area assess the efficacy of fairness intervention by evaluating a single fixed fairness criterion through the lens of a one-shot, static setting. Yet recommender systems constitute dynamical systems with feedback loops from the recommendations to the underlying population distributions which could lead to unforeseen and adverse consequences if not taken into account. In this paper, we study a connection recommender system patterned after the systems employed by web-scale social networks and analyze the long-term effects of intervening on fairness in the recommendations. We find that, although seemingly fair in aggregate, common exposure and utility parity interventions fail to mitigate amplification of biases in the long term. We theoretically characterize how certain fairness interventions impact the bias amplification dynamics in a stylized Polya urn model.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {22–35},
numpages = {14},
keywords = {recommender systems, fairness, machine learning},
location = {Oxford, United Kingdom},
series = {AIES '22}
}
  
"
AIES,2022,Racial Disparities in the Enforcement of Marijuana Violations in the US,https://dl.acm.org/doi/10.1145/3514094.3534184,"['Bradley Butcher', 'Chris Robinson', 'Miri Zilka', 'Riccardo Fogliato', 'Carolyn Ashurst', 'Adrian Weller']",1,"Racial disparities in US drug arrest rates have been observed for decades, but their causes and policy implications are still contested. Some have argued that the disparities largely reflect differences in drug use between racial groups, while others have hypothesized that discriminatory enforcement policies and police practices play a significant role. In this work, we analyze racial disparities in the enforcement of marijuana violations in the US. Using data from the National Incident-Based Reporting System (NIBRS) and the National Survey on Drug Use and Health (NSDUH) programs, we investigate whether marijuana usage and purchasing behaviors can explain the racial composition of offenders in police records. We examine potential driving mechanisms behind these disparities and the extent to which county-level socioeconomic factors are associated with corresponding disparities. Our results indicate that the significant racial disparities in reported incidents and arrests cannot be explained by differences in marijuana days-of-use alone. Variations in the location where marijuana is purchased and in the frequency of these purchases partially explain the observed disparities. We observe an increase in racial disparities across most counties over the last decade, with the greatest increases in states that legalized the use of marijuana within this timeframe. Income, high school graduation rate, and rate of employment positively correlate with larger racial disparities, while the rate of incarceration is negatively correlated. We conclude with a discussion of the implications of the observed racial disparities in the context of algorithmic fairness.",YES/Maybe,,Data,,Detection,,,,,,,,,,,,"@inproceedings{10.1145/3514094.3534184,
author = {Butcher, Bradley and Robinson, Chris and Zilka, Miri and Fogliato, Riccardo and Ashurst, Carolyn and Weller, Adrian},
title = {Racial Disparities in the Enforcement of Marijuana Violations in the US},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534184},
doi = {10.1145/3514094.3534184},
abstract = {Racial disparities in US drug arrest rates have been observed for decades, but their causes and policy implications are still contested. Some have argued that the disparities largely reflect differences in drug use between racial groups, while others have hypothesized that discriminatory enforcement policies and police practices play a significant role. In this work, we analyze racial disparities in the enforcement of marijuana violations in the US. Using data from the National Incident-Based Reporting System (NIBRS) and the National Survey on Drug Use and Health (NSDUH) programs, we investigate whether marijuana usage and purchasing behaviors can explain the racial composition of offenders in police records. We examine potential driving mechanisms behind these disparities and the extent to which county-level socioeconomic factors are associated with corresponding disparities. Our results indicate that the significant racial disparities in reported incidents and arrests cannot be explained by differences in marijuana days-of-use alone. Variations in the location where marijuana is purchased and in the frequency of these purchases partially explain the observed disparities. We observe an increase in racial disparities across most counties over the last decade, with the greatest increases in states that legalized the use of marijuana within this timeframe. Income, high school graduation rate, and rate of employment positively correlate with larger racial disparities, while the rate of incarceration is negatively correlated. We conclude with a discussion of the implications of the observed racial disparities in the context of algorithmic fairness.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {130–143},
numpages = {14},
keywords = {marijuana, law enforcement, racial disparities},
location = {Oxford, United Kingdom},
series = {AIES '22}
}
  
"
AIES,2022,"Gender Bias in Word Embeddings: A Comprehensive Analysis of Frequency, Syntax, and Semantics",https://dl.acm.org/doi/10.1145/3514094.3534162,"['Aylin Caliskan', 'Pimparkar Parth Ajay', 'Tessa Charlesworth', 'Robert Wolfe', 'Mahzarin R. Banaji']",4,"Word embeddings are numeric representations of meaning derived from word co-occurrence statistics in corpora of human-produced texts. The statistical regularities in language corpora encode well-known social biases into word embeddings (e.g., the word vector for family is closer to the vector women than to men). Although efforts have been made to mitigate bias in word embeddings, with the hope of improving fairness in downstream Natural Language Processing (NLP) applications, these efforts will remain limited until we more deeply understand the multiple (and often subtle) ways that social biases can be reflected in word embeddings. Here, we focus on gender to provide a comprehensive analysis of group-based biases in widely-used static English word embeddings trained on internet corpora (GloVe 2014, fastText 2017). While some previous research has helped uncover biases in specific semantic associations between a group and a target domain (e.g., women - family), using the Single-Category Word Embedding Association Test, we demonstrate the widespread prevalence of gender biases that also show differences in: (1) frequencies of words associated with men versus women; (b) part-of-speech tags in gender-associated words; (c) semantic categories in gender-associated words; and (d) valence, arousal, and dominance in gender-associated words. We leave the analysis of non-binary gender to future work due to the challenges in accurate group representation caused by limitations inherent in data. First, in terms of word frequency: we find that, of the 1,000 most frequent words in the vocabulary, 77% are more associated with men than women, providing direct evidence of a masculine default in the everyday language of the English-speaking world. Second, turning to parts-of-speech: the top male-associated words are typically verbs (e.g., fight, overpower) while the top female-associated words are typically adjectives and adverbs (e.g., giving, emotionally). Gender biases in embeddings also permeate parts-of-speech. Third, for semantic categories: bottom-up, cluster analyses of the top 1,000 words associated with each gender. The top male-associated concepts include roles and domains of big tech, engineering, religion, sports, and violence; in contrast, the top female-associated concepts are less focused on roles, including, instead, female-specific slurs and sexual content, as well as appearance and kitchen terms. Fourth, using human ratings of word valence, arousal, and dominance from a ~20,000 word lexicon, we find that male-associated words are higher on arousal and dominance, while female-associated words are higher on valence. Ultimately, these findings move the study of gender bias in word embeddings beyond the basic investigation of semantic relationships to also study gender differences in multiple manifestations in text. Given the central role of word embeddings in NLP applications, it is essential to more comprehensively document where biases exist and may remain hidden, allowing them to persist without our awareness throughout large text corpora.",YES/Maybe,,Data,,Detection,,,,,,,,,,,,"@inproceedings{10.1145/3514094.3534162,
author = {Caliskan, Aylin and Ajay, Pimparkar Parth and Charlesworth, Tessa and Wolfe, Robert and Banaji, Mahzarin R.},
title = {Gender Bias in Word Embeddings: A Comprehensive Analysis of Frequency, Syntax, and Semantics},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534162},
doi = {10.1145/3514094.3534162},
abstract = {Word embeddings are numeric representations of meaning derived from word co-occurrence statistics in corpora of human-produced texts. The statistical regularities in language corpora encode well-known social biases into word embeddings (e.g., the word vector for family is closer to the vector women than to men). Although efforts have been made to mitigate bias in word embeddings, with the hope of improving fairness in downstream Natural Language Processing (NLP) applications, these efforts will remain limited until we more deeply understand the multiple (and often subtle) ways that social biases can be reflected in word embeddings. Here, we focus on gender to provide a comprehensive analysis of group-based biases in widely-used static English word embeddings trained on internet corpora (GloVe 2014, fastText 2017). While some previous research has helped uncover biases in specific semantic associations between a group and a target domain (e.g., women - family), using the Single-Category Word Embedding Association Test, we demonstrate the widespread prevalence of gender biases that also show differences in: (1) frequencies of words associated with men versus women; (b) part-of-speech tags in gender-associated words; (c) semantic categories in gender-associated words; and (d) valence, arousal, and dominance in gender-associated words. We leave the analysis of non-binary gender to future work due to the challenges in accurate group representation caused by limitations inherent in data.First, in terms of word frequency: we find that, of the 1,000 most frequent words in the vocabulary, 77% are more associated with men than women, providing direct evidence of a masculine default in the everyday language of the English-speaking world. Second, turning to parts-of-speech: the top male-associated words are typically verbs (e.g., fight, overpower) while the top female-associated words are typically adjectives and adverbs (e.g., giving, emotionally). Gender biases in embeddings also permeate parts-of-speech. Third, for semantic categories: bottom-up, cluster analyses of the top 1,000 words associated with each gender. The top male-associated concepts include roles and domains of big tech, engineering, religion, sports, and violence; in contrast, the top female-associated concepts are less focused on roles, including, instead, female-specific slurs and sexual content, as well as appearance and kitchen terms. Fourth, using human ratings of word valence, arousal, and dominance from a ~20,000 word lexicon, we find that male-associated words are higher on arousal and dominance, while female-associated words are higher on valence. Ultimately, these findings move the study of gender bias in word embeddings beyond the basic investigation of semantic relationships to also study gender differences in multiple manifestations in text. Given the central role of word embeddings in NLP applications, it is essential to more comprehensively document where biases exist and may remain hidden, allowing them to persist without our awareness throughout large text corpora.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {156–170},
numpages = {15},
keywords = {word embeddings, ai bias, masculine default, representation, gender bias, psycholinguistics},
location = {Oxford, United Kingdom},
series = {AIES '22}
}
  
"
AIES,2022,FairCanary: Rapid Continuous Explainable Fairness,https://dl.acm.org/doi/10.1145/3514094.3534157,"['Avijit Ghosh', 'Aalok Shanbhag', 'Christo Wilson']",3,"Systems that offer continuous model monitoring have emerged in response to (1) well-documented failures of deployed Machine Learning (ML) and Artificial Intelligence (AI) models and (2) new regulatory requirements impacting these models. Existing monitoring systems continuously track the performance of deployed ML models and compute feature importance (a.k.a. explanations) for each prediction to help developers identify the root causes of emergent model performance problems. We present Quantile Demographic Drift (QDD), a novel model bias quantification metric that uses quantile binning to measure differences in the overall prediction distributions over subgroups. QDD is ideal for continuous monitoring scenarios, does not suffer from the statistical limitations of conventional threshold-based bias metrics, and does not require outcome labels (which may not be available at runtime). We incorporate QDD into a continuous model monitoring system, called FairCanary, that reuses existing explanations computed for each individual prediction to quickly compute explanations for the QDD bias metrics. This optimization makes FairCanary an order of magnitude faster than previous work that has tried to generate feature-level bias explanations.",YES/Maybe,,Deployment,,Measurement,"Really an explanation metric, but?",,,,,,,,,,,"@inproceedings{10.1145/3514094.3534157,
author = {Ghosh, Avijit and Shanbhag, Aalok and Wilson, Christo},
title = {FairCanary: Rapid Continuous Explainable Fairness},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534157},
doi = {10.1145/3514094.3534157},
abstract = {Systems that offer continuous model monitoring have emerged in response to (1) well-documented failures of deployed Machine Learning (ML) and Artificial Intelligence (AI) models and (2) new regulatory requirements impacting these models. Existing monitoring systems continuously track the performance of deployed ML models and compute feature importance (a.k.a. explanations) for each prediction to help developers identify the root causes of emergent model performance problems.We present Quantile Demographic Drift (QDD), a novel model bias quantification metric that uses quantile binning to measure differences in the overall prediction distributions over subgroups. QDD is ideal for continuous monitoring scenarios, does not suffer from the statistical limitations of conventional threshold-based bias metrics, and does not require outcome labels (which may not be available at runtime). We incorporate QDD into a continuous model monitoring system, called FairCanary, that reuses existing explanations computed for each individual prediction to quickly compute explanations for the QDD bias metrics. This optimization makes FairCanary an order of magnitude faster than previous work that has tried to generate feature-level bias explanations.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {307–316},
numpages = {10},
keywords = {fairness, continuous measurement, model explanation, drift},
location = {Oxford, United Kingdom},
series = {AIES '22}
}
  
"
AIES,2022,"Towards Better Detection of Biased Language with Scarce, Noisy, and Biased Annotations",https://dl.acm.org/doi/10.1145/3514094.3534142,"['Zhuoyan Li', 'Zhuoran Lu', 'Ming Yin']",0,"Biased language is prevalent in today's online social media. To reduce the amount of online biased language, one critical first step is to accurately detect such biased language, ideally automatically. This is a challenging problem, however, as the annotated data necessary for training a biased language classifier is either scarce and costly (e.g., when collected from experts), or noisy and potentially biased on their own (e.g., when collected from crowd workers). The biased language classifier built based on these annotations may thus be inaccurate, and sometimes unfair (e.g., have systematic accuracy disparities across texts with different political leanings). In this paper, we propose a novel method, CLEARE, for biased language detection, in which we utilize self-supervised contrastive learning to enhance the biased language classifier---we learn a robust encoder of the textual data through solving a min-max optimization problem, so that the encoder could help achieve the best classification performance even if the worst data augmentation strategy is selected. Extensive evaluations suggest that CLEARE shows substantial improvements compared to the state-of-art biased language detection methods on several benchmark datasets, in terms of improving both the accuracy and the fairness of the detection.",YES/Maybe,,Algorithm,,Mitigation,Creates a technique that you can put on top of an NLP classifier to combat data bias problems,,,,,,,,,,,"@inproceedings{10.1145/3514094.3534142,
author = {Li, Zhuoyan and Lu, Zhuoran and Yin, Ming},
title = {Towards Better Detection of Biased Language with Scarce, Noisy, and Biased Annotations},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534142},
doi = {10.1145/3514094.3534142},
abstract = {Biased language is prevalent in today's online social media. To reduce the amount of online biased language, one critical first step is to accurately detect such biased language, ideally automatically. This is a challenging problem, however, as the annotated data necessary for training a biased language classifier is either scarce and costly (e.g., when collected from experts), or noisy and potentially biased on their own (e.g., when collected from crowd workers). The biased language classifier built based on these annotations may thus be inaccurate, and sometimes unfair (e.g., have systematic accuracy disparities across texts with different political leanings). In this paper, we propose a novel method, CLEARE, for biased language detection, in which we utilize self-supervised contrastive learning to enhance the biased language classifier---we learn a robust encoder of the textual data through solving a min-max optimization problem, so that the encoder could help achieve the best classification performance even if the worst data augmentation strategy is selected. Extensive evaluations suggest that CLEARE shows substantial improvements compared to the state-of-art biased language detection methods on several benchmark datasets, in terms of improving both the accuracy and the fairness of the detection.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {411–423},
numpages = {13},
keywords = {biased language, contrastive learning, bias detection, fairness},
location = {Oxford, United Kingdom},
series = {AIES '22}
}
  
"
AIES,2022,Strategic Best Response Fairness in Fair Machine Learning,https://dl.acm.org/doi/10.1145/3514094.3534194,"['Hajime Shimao', 'Warut Khern-am-nuai', 'Karthik Kannan', 'Maxime C. Cohen']",1,"While artificial intelligence (AI) and machine learning (ML) have been increasingly used for decision-making, issues related to discrimination in AI/ML have become prominent. While several fair algorithms are proposed to alleviate these discrimination issues, most of them provide fairness by imposing constraints to eliminate disparity in prediction results. However, the use of these fair algorithms may change the behavior of prediction subjects. As such, even though the disparity in prediction results might be removed by fair algorithms, behavioral responses to the use of fair algorithms can still create disparity in behavior which may persist across different groups of prediction subjects. To study this issue, we define a notion called ""strategic best-response fairness"" (SBR-fair). It is defined in a context that includes different groups of prediction subjects who are ex-ante identical in terms of abilities and conditional payoffs. We utilize a game-theoretic model to investigate whether different types of fair algorithms lead to identical equilibrium behaviors among different groups of prediction subjects. If yes, such an algorithm is considered SBR-fair. We then demonstrate that many existing fair algorithms are not SBR-fair. As a result, implementing these algorithms may impose fairness on prediction results but actually induce disparity between privileged and unprivileged individuals in the long run.",YES/Maybe,,Organizational Realities,,Detection,This is about finding equilibriums around fair models--- is this a part of the pipeline?,,,,,,,,,,,"@inproceedings{10.1145/3514094.3534194,
author = {Shimao, Hajime and Khern-am-nuai, Warut and Kannan, Karthik and Cohen, Maxime C.},
title = {Strategic Best Response Fairness in Fair Machine Learning},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534194},
doi = {10.1145/3514094.3534194},
abstract = {While artificial intelligence (AI) and machine learning (ML) have been increasingly used for decision-making, issues related to discrimination in AI/ML have become prominent. While several fair algorithms are proposed to alleviate these discrimination issues, most of them provide fairness by imposing constraints to eliminate disparity in prediction results. However, the use of these fair algorithms may change the behavior of prediction subjects. As such, even though the disparity in prediction results might be removed by fair algorithms, behavioral responses to the use of fair algorithms can still create disparity in behavior which may persist across different groups of prediction subjects. To study this issue, we define a notion called ""strategic best-response fairness"" (SBR-fair). It is defined in a context that includes different groups of prediction subjects who are ex-ante identical in terms of abilities and conditional payoffs. We utilize a game-theoretic model to investigate whether different types of fair algorithms lead to identical equilibrium behaviors among different groups of prediction subjects. If yes, such an algorithm is considered SBR-fair. We then demonstrate that many existing fair algorithms are not SBR-fair. As a result, implementing these algorithms may impose fairness on prediction results but actually induce disparity between privileged and unprivileged individuals in the long run.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {664},
numpages = {1},
keywords = {game-theoretic model, strategic best response, fair machine learning},
location = {Oxford, United Kingdom},
series = {AIES '22}
}
  
"
AIES,2022,Enhancing Fairness in Face Detection in Computer Vision Systems by Demographic Bias Mitigation,https://dl.acm.org/doi/10.1145/3514094.3534153,"['Yu Yang', 'Aayush Gupta', 'Jianwei Feng', 'Prateek Singhal', 'Vivek Yadav', 'Yue Wu', 'Pradeep Natarajan', 'Varsha Hedau', 'Jungseock Joo']",2,"Fairness has become an important agenda in computer vision and artificial intelligence. Recent studies have shown that many computer vision models and datasets exhibit demographic biases and proposed mitigation strategies. These works attempt to address accuracy disparity, spurious correlations, or unbalanced representations in datasets in tasks such as face recognition, verification and expression and attribute classification. These tasks, however, all require face detection as the first preprocessing step, and surprisingly, there has been little effort in identifying or mitigating biases in face detection. Biased face detectors themselves pose a threat against fair and ethical AI systems, and their biases may be further passed on to subsequent downstream tasks such as face recognition in a computer vision pipeline. This paper therefore investigates the problem of biases in face detection, focusing on accuracy disparity of detectors between demographic groups including gender, age group, and skin tone. We collect perceived demographic attributes on a popular face detection benchmark dataset, WIDER FACE, report skewed demographic distributions, and compare detection performance between groups. In order to mitigate the biases, we apply three mitigation methods that have been introduced in the recent literature and also propose two novel methods. Experimental results show that these methods are effective in reducing demographic biases. We also discuss how the effectiveness varies by demographic attributes, detection easiness, and multiple detectors, which will shed light on this new topic of addressing face detection bias.",YES/Maybe,,Data,,Detection,Case Study,,,,,,,,,,,"@inproceedings{10.1145/3514094.3534153,
author = {Yang, Yu and Gupta, Aayush and Feng, Jianwei and Singhal, Prateek and Yadav, Vivek and Wu, Yue and Natarajan, Pradeep and Hedau, Varsha and Joo, Jungseock},
title = {Enhancing Fairness in Face Detection in Computer Vision Systems by Demographic Bias Mitigation},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534153},
doi = {10.1145/3514094.3534153},
abstract = {Fairness has become an important agenda in computer vision and artificial intelligence. Recent studies have shown that many computer vision models and datasets exhibit demographic biases and proposed mitigation strategies. These works attempt to address accuracy disparity, spurious correlations, or unbalanced representations in datasets in tasks such as face recognition, verification and expression and attribute classification. These tasks, however, all require face detection as the first preprocessing step, and surprisingly, there has been little effort in identifying or mitigating biases in face detection. Biased face detectors themselves pose a threat against fair and ethical AI systems, and their biases may be further passed on to subsequent downstream tasks such as face recognition in a computer vision pipeline. This paper therefore investigates the problem of biases in face detection, focusing on accuracy disparity of detectors between demographic groups including gender, age group, and skin tone. We collect perceived demographic attributes on a popular face detection benchmark dataset, WIDER FACE, report skewed demographic distributions, and compare detection performance between groups. In order to mitigate the biases, we apply three mitigation methods that have been introduced in the recent literature and also propose two novel methods. Experimental results show that these methods are effective in reducing demographic biases. We also discuss how the effectiveness varies by demographic attributes, detection easiness, and multiple detectors, which will shed light on this new topic of addressing face detection bias.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {813–822},
numpages = {10},
keywords = {fairness in computer vision, bias measurement and mitigation, face detection bias},
location = {Oxford, United Kingdom},
series = {AIES '22}
}
  
"
AIES,2022,Identifying Bias in Data Using Two-Distribution Hypothesis Tests,https://dl.acm.org/doi/10.1145/3514094.3534169,"['William Yik', 'Limnanthes Serafini', 'Timothy Lindsey', 'George D. Monta√±ez']",1,"As machine learning models become more widely used in important decision-making processes, the need for identifying and mitigating potential sources of bias has increased substantially. Using two-distribution (specified complexity) hypothesis tests, we identify biases in training data with respect to proposed distributions and without the need to train a model, distinguishing our methods from common output-based fairness tests. Furthermore, our methods allow us to return a ""closest plausible explanation"" for a given dataset, potentially revealing underlying biases in the processes that generated them. We also show that a binomial variation of this hypothesis test could be used to identify bias in certain directions, or towards certain outcomes, and again return a closest plausible explanation. The benefits of this binomial variation are compared with other hypothesis tests, including the exact binomial. Lastly, potential industrial applications of our methods are shown using two real-world datasets.",YES/Maybe,,Data,,Detection,Needs more look,,,,,,,,,,,"@inproceedings{10.1145/3514094.3534169,
author = {Yik, William and Serafini, Limnanthes and Lindsey, Timothy and Monta\~{n}ez, George D.},
title = {Identifying Bias in Data Using Two-Distribution Hypothesis Tests},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534169},
doi = {10.1145/3514094.3534169},
abstract = {As machine learning models become more widely used in important decision-making processes, the need for identifying and mitigating potential sources of bias has increased substantially. Using two-distribution (specified complexity) hypothesis tests, we identify biases in training data with respect to proposed distributions and without the need to train a model, distinguishing our methods from common output-based fairness tests. Furthermore, our methods allow us to return a ""closest plausible explanation"" for a given dataset, potentially revealing underlying biases in the processes that generated them. We also show that a binomial variation of this hypothesis test could be used to identify bias in certain directions, or towards certain outcomes, and again return a closest plausible explanation. The benefits of this binomial variation are compared with other hypothesis tests, including the exact binomial. Lastly, potential industrial applications of our methods are shown using two real-world datasets.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {831–844},
numpages = {14},
keywords = {bias, statistics, fairness, machine learning, data analysis, hypothesis testing},
location = {Oxford, United Kingdom},
series = {AIES '22}
}
  
"
Neurips,2019,Unlocking Fairness: a Trade-off Revisited,https://proceedings.neurips.cc//paper/2019/hash/373e4c5d8edfa8b74fd4b6791d0cf6dc-Abstract.html,"['Michael Wick', ' swetasudha panda', ' Jean-Baptiste Tristan']",88,"The prevailing wisdom is that a model's fairness and its accuracy are in tension with one another. However, there is a pernicious {\em modeling-evaluating dualism} bedeviling fair machine learning in which phenomena such as label bias are appropriately acknowledged as a source of unfairness when designing fair models, only to be tacitly abandoned when evaluating them. We investigate fairness and accuracy, but this time under a variety of controlled conditions in which we vary the amount and type of bias. We find, under reasonable assumptions, that the tension between fairness and accuracy is illusive, and vanishes as soon as we account for these phenomena during evaluation. Moreover, our results are consistent with an opposing conclusion: fairness and accuracy are sometimes in accord. This raises the question, {\em might there be a way to harness fairness to improve accuracy after all?} Since most notions of fairness are with respect to the model's predictions and not the ground truth labels, this provides an opportunity to see if we can improve accuracy by harnessing appropriate notions of fairness over large quantities of {\em unlabeled} data with techniques like posterior regularization and generalized expectation. Indeed, we find that semi-supervision not only improves fairness, but also accuracy and has advantages over existing in-processing methods that succumb to selection bias on the training set.",YES/Maybe,,Data,Algorithm,Methods,Shows that semisupervised learning in order to change data inputs can significantly dampen the bias accuracy tradeoff,,,,,,,,,,,"@inproceedings{NEURIPS2019_373e4c5d,
 author = {Wick, Michael and panda, swetasudha and Tristan, Jean-Baptiste},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Unlocking Fairness: a Trade-off Revisited},
 url = {https://proceedings.neurips.cc/paper/2019/file/373e4c5d8edfa8b74fd4b6791d0cf6dc-Paper.pdf},
 volume = {32},
 year = {2019}
}

"
Neurips,2020,Fair Performance Metric Elicitation,https://proceedings.neurips.cc//paper/2020/hash/7ec2442aa04c157590b2fa1a7d093a33-Abstract.html,"['Gaurush Hiranandani', ' Harikrishna Narasimhan', ' Sanmi Koyejo']",11,"What is a fair performance metric? We consider the choice of fairness metrics through the lens of metric elicitation -- a principled framework for selecting performance metrics that best reflect implicit preferences. The use of metric elicitation enables a practitioner to tune the performance and fairness metrics to the task, context, and population at hand. Specifically, we propose a novel strategy to elicit group-fair performance metrics for multiclass classification problems with multiple sensitive groups that also includes selecting the trade-off between predictive performance and fairness violation. The proposed elicitation strategy requires only relative preference feedback and is robust to both finite sample and feedback noise.",YES/Maybe,,Evaluation,,Methods,"How to choose a fair performance metric, strategy. Need to look in more detail.",,,,,,,,,,,"@inproceedings{NEURIPS2020_7ec2442a,
 author = {Hiranandani, Gaurush and Narasimhan, Harikrishna and Koyejo, Sanmi},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {11083--11095},
 publisher = {Curran Associates, Inc.},
 title = {Fair Performance Metric Elicitation},
 url = {https://proceedings.neurips.cc/paper/2020/file/7ec2442aa04c157590b2fa1a7d093a33-Paper.pdf},
 volume = {33},
 year = {2020}
}

"
Neurips,2020,Can I Trust My Fairness Metric? Assessing Fairness with Unlabeled Data and Bayesian Inference,https://proceedings.neurips.cc//paper/2020/hash/d83de59e10227072a9c034ce10029c39-Abstract.html,"['Disi Ji', ' Padhraic Smyth', ' Mark Steyvers']",18,"Group fairness is measured via parity of quantitative metrics across different protected demographic groups. In this paper, we investigate the problem of reliably assessing group fairness metrics when labeled examples are few but unlabeled examples are plentiful. We propose a general Bayesian framework that can augment labeled data with unlabeled data to produce more accurate and lower-variance estimates compared to methods based on labeled data alone. Our approach estimates calibrated scores (for unlabeled examples) of each group using a hierarchical latent variable model conditioned on labeled examples. This in turn allows for inference of posterior distributions for an array of group fairness metrics with a notion of uncertainty. We demonstrate that our approach leads to significant and consistent reductions in estimation error across multiple well-known fairness datasets, sensitive attributes, and predictive models. The results clearly show the benefits of using both unlabeled data and Bayesian inference in assessing whether a prediction model is fair or not.",YES/Maybe,,Data,Algorithm,Measurement,Shows how to measure fairness when there is not much labeled data but lots of unlabeled data,,,,,,,,,,,"@inproceedings{NEURIPS2020_d83de59e,
 author = {Ji, Disi and Smyth, Padhraic and Steyvers, Mark},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {18600--18612},
 publisher = {Curran Associates, Inc.},
 title = {Can I Trust My Fairness Metric? Assessing Fairness with Unlabeled Data and Bayesian Inference},
 url = {https://proceedings.neurips.cc/paper/2020/file/d83de59e10227072a9c034ce10029c39-Paper.pdf},
 volume = {33},
 year = {2020}
}

"
Neurips,2021,Refining Language Models with Compositional Explanations,https://proceedings.neurips.cc//paper/2021/hash/4b26dc4663ccf960c8538d595d0a1d3a-Abstract.html,"['Huihan Yao', ' Ying Chen', ' Qinyuan Ye', ' Xisen Jin', ' Xiang Ren']",12,"Pre-trained language models have been successful on text classification tasks, but are prone to learning spurious correlations from biased datasets, and are thus vulnerable when making inferences in a new domain. Prior work reveals such spurious patterns via post-hoc explanation algorithms which compute the importance of input features. Further, the model is regularized to align the importance scores with human knowledge, so that the unintended model behaviors are eliminated. However, such a regularization technique lacks flexibility and coverage, since only importance scores towards a pre-defined list of features are adjusted, while more complex human knowledge such as feature interaction and pattern generalization can hardly be incorporated. In this work, we propose to refine a learned language model for a target domain by collecting human-provided compositional explanations regarding observed biases. By parsing these explanations into executable logic rules, the human-specified refinement advice from a small set of explanations can be generalized to more training examples. We additionally introduce a regularization term allowing adjustments for both importance and interaction of features to better rectify model behavior. We demonstrate the effectiveness of the proposed approach on two text classification tasks by showing improved performance in target domain as well as improved model fairness after refinement.",YES/Maybe,,Algorithm,Features,Methods,Shows how to leverage explanations to overcome reliance on spurious correlations in pre-trained models,,,,,,,,,,,"@inproceedings{NEURIPS2021_4b26dc46,
 author = {Yao, Huihan and Chen, Ying and Ye, Qinyuan and Jin, Xisen and Ren, Xiang},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {8954--8967},
 publisher = {Curran Associates, Inc.},
 title = {Refining Language Models with Compositional Explanations},
 url = {https://proceedings.neurips.cc/paper/2021/file/4b26dc4663ccf960c8538d595d0a1d3a-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
Neurips,2021,Does enforcing fairness mitigate biases caused by subpopulation shift?,https://proceedings.neurips.cc//paper/2021/hash/d800149d2f947ad4d64f34668f8b20f6-Abstract.html,"['Subha Maity', ' Debarghya Mukherjee', ' Mikhail Yurochkin', ' Yuekai Sun']",7,"Many instances of algorithmic bias are caused by subpopulation shifts. For example, ML models often perform worse on demographic groups that are underrepresented in the training data. In this paper, we study whether enforcing algorithmic fairness during training improves the performance of the trained model in the \emph{target domain}. On one hand, we conceive scenarios in which enforcing fairness does not improve performance in the target domain. In fact, it may even harm performance. On the other hand, we derive necessary and sufficient conditions under which enforcing algorithmic fairness leads to the Bayes model in the target domain. We also illustrate the practical implications of our theoretical results in simulations and on real data.",YES/Maybe,,Data,,Problem Identification,Shows under what circimstances of distribution shift will enforcing fairness constraints be useful on the deployment population (ie lead to less discrimination) and when it will be harmful.,,,,,,,,,,,"@inproceedings{NEURIPS2021_d800149d,
 author = {Maity, Subha and Mukherjee, Debarghya and Yurochkin, Mikhail and Sun, Yuekai},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {25773--25784},
 publisher = {Curran Associates, Inc.},
 title = {Does enforcing fairness mitigate biases caused by subpopulation shift?},
 url = {https://proceedings.neurips.cc/paper/2021/file/d800149d2f947ad4d64f34668f8b20f6-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
Neurips,2021,Certifying Robustness to Programmable Data Bias in Decision Trees,https://proceedings.neurips.cc//paper/2021/hash/dcf531edc9b229acfe0f4b87e1e278dd-Abstract.html,"['Anna Meyer', ' Aws Albarghouthi', "" Loris D'Antoni""]",4,"Datasets can be biased due to societal inequities, human biases, under-representation of minorities, etc. Our goal is to certify that models produced by a learning algorithm are pointwise-robust to dataset biases. This is a challenging problem: it entails learning models for a large, or even infinite, number of datasets, ensuring that they all produce the same prediction. We focus on decision-tree learning due to the interpretable nature of the models. Our approach allows programmatically specifying \emph{bias models} across a variety of dimensions (e.g., label-flipping or missing data), composing types of bias, and targeting bias towards a specific group. To certify robustness, we use a novel symbolic technique to evaluate a decision-tree learner on a large, or infinite, number of datasets, certifying that each and every dataset produces the same prediction for a specific test point. We evaluate our approach on datasets that are commonly used in the fairness literature, and demonstrate our approach's viability on a range of bias models.",YES/Maybe,,Algorithm,,Measurement,Shows how to create classifiers which are robust to data biases---i.e. will give the same response if trained on dataset where one subgroup is ill-represented… BUT WHY DO YOU WANT THAT???,,,,,,,,,,,"@inproceedings{NEURIPS2021_dcf531ed,
 author = {Meyer, Anna and Albarghouthi, Aws and D\textquotesingle Antoni, Loris},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {26276--26288},
 publisher = {Curran Associates, Inc.},
 title = {Certifying Robustness to Programmable Data Bias in Decision Trees},
 url = {https://proceedings.neurips.cc/paper/2021/file/dcf531edc9b229acfe0f4b87e1e278dd-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
Neurips,2021,Subgroup Generalization and Fairness of Graph Neural Networks,https://proceedings.neurips.cc//paper/2021/hash/08425b881bcde94a383cd258cea331be-Abstract.html,"['Jiaqi Ma', ' Junwei Deng', ' Qiaozhu Mei']",24,"Despite enormous successful applications of graph neural networks (GNNs), theoretical understanding of their generalization ability, especially for node-level tasks where data are not independent and identically-distributed (IID), has been sparse. The theoretical investigation of the generalization performance is beneficial for understanding fundamental issues (such as fairness) of GNN models and designing better learning methods. In this paper, we present a novel PAC-Bayesian analysis for GNNs under a non-IID semi-supervised learning setup. Moreover, we analyze the generalization performances on different subgroups of unlabeled nodes, which allows us to further study an accuracy-(dis)parity-style (un)fairness of GNNs from a theoretical perspective. Under reasonable assumptions, we demonstrate that the distance between a test subgroup and the training set can be a key factor affecting the GNN performance on that subgroup, which calls special attention to the training node selection for fair learning. Experiments across multiple GNN models and datasets support our theoretical results.",YES/SCOPE,,Data,,Problem Detection,"Shows what may be some of the causes of discrimination in GNNs: in this case, the distance under certain metrics between test and training set for different subgroups affects GNN performace on those subgroups.",,,,,,,,,,,"@inproceedings{NEURIPS2021_08425b88,
 author = {Ma, Jiaqi and Deng, Junwei and Mei, Qiaozhu},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {1048--1061},
 publisher = {Curran Associates, Inc.},
 title = {Subgroup Generalization and Fairness of Graph Neural Networks},
 url = {https://proceedings.neurips.cc/paper/2021/file/08425b881bcde94a383cd258cea331be-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
Neurips,2021,Dynamic COVID risk assessment accounting for community virus exposure from a spatial-temporal transmission model,https://proceedings.neurips.cc//paper/2021/hash/e97a4f04ef1b914f6a1698caa364f693-Abstract.html,"['Yuan Chen', ' Wenbo Fei', ' Qinxia Wang', ' Donglin Zeng', ' Yuanjia Wang']",0,"COVID-19 pandemic has caused unprecedented negative impacts on our society, including further exposing inequity and disparity in public health. To study the impact of socioeconomic factors on COVID transmission, we first propose a spatial-temporal model to examine the socioeconomic heterogeneity and spatial correlation of COVID-19 transmission at the community level. Second, to assess the individual risk of severe COVID-19 outcomes after a positive diagnosis, we propose a dynamic, varying-coefficient model that integrates individual-level risk factors from electronic health records (EHRs) with community-level risk factors. The underlying neighborhood prevalence of infections (both symptomatic and pre-symptomatic) predicted from the previous spatial-temporal model is included in the individual risk assessment so as to better capture the background risk of virus exposure for each individual. We design a weighting scheme to mitigate multiple selection biases inherited in EHRs of COVID patients. We analyze COVID transmission data in New York City (NYC, the epicenter of the first surge in the United States) and EHRs from NYC hospitals, where time-varying effects of community risk factors and significant interactions between individual- and community-level risk factors are detected. By examining the socioeconomic disparity of infection risks and interaction among the risk factors, our methods can assist public health decision-making and facilitate better clinical management of COVID patients.",YES/SCOPE,,Data,,Case Study,Needs another look---shows how to re-weight COVID health data to mitigate socioeconomic disparities in the model,,,,,,,,,,,"@inproceedings{NEURIPS2021_e97a4f04,
 author = {Chen, Yuan and Fei, Wenbo and Wang, Qinxia and Zeng, Donglin and Wang, Yuanjia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {27747--27760},
 publisher = {Curran Associates, Inc.},
 title = {Dynamic COVID risk assessment accounting for community virus exposure from a spatial-temporal transmission model},
 url = {https://proceedings.neurips.cc/paper/2021/file/e97a4f04ef1b914f6a1698caa364f693-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
AIES,2018,Measuring and Mitigating Unintended Bias in Text Classification,https://dl.acm.org/doi/10.1145/3278721.3278729,"['Lucas Dixon', 'John Li', 'Jeffrey Sorensen', 'Nithum Thain', 'Lucy Vasserman']",484,"We introduce and illustrate a new approach to measuring and mitigating unintended bias in machine learning models. Our definition of unintended bias is parameterized by a test set and a subset of input features. We illustrate how this can be used to evaluate text classifiers using a synthetic test set and a public corpus of comments annotated for toxicity from Wikipedia Talk pages. We also demonstrate how imbalances in training data can lead to unintended bias in the resulting models, and therefore potentially unfair applications. We use a set of common demographic identity terms as the subset of input features on which we measure bias. This technique permits analysis in the common scenario where demographic information on authors and readers is unavailable, so that bias mitigation must focus on the content of the text itself. The mitigation method we introduce is an unsupervised approach based on balancing the training dataset. We demonstrate that this approach reduces the unintended bias without compromising overall model quality.",YES/TRAD,,Data,Measurement,Measurement,,,,,,,,,,,,"@inproceedings{10.1145/3278721.3278729,
author = {Dixon, Lucas and Li, John and Sorensen, Jeffrey and Thain, Nithum and Vasserman, Lucy},
title = {Measuring and Mitigating Unintended Bias in Text Classification},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278729},
doi = {10.1145/3278721.3278729},
abstract = {We introduce and illustrate a new approach to measuring and mitigating unintended bias in machine learning models. Our definition of unintended bias is parameterized by a test set and a subset of input features. We illustrate how this can be used to evaluate text classifiers using a synthetic test set and a public corpus of comments annotated for toxicity from Wikipedia Talk pages. We also demonstrate how imbalances in training data can lead to unintended bias in the resulting models, and therefore potentially unfair applications. We use a set of common demographic identity terms as the subset of input features on which we measure bias. This technique permits analysis in the common scenario where demographic information on authors and readers is unavailable, so that bias mitigation must focus on the content of the text itself. The mitigation method we introduce is an unsupervised approach based on balancing the training dataset. We demonstrate that this approach reduces the unintended bias without compromising overall model quality.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {67–73},
numpages = {7},
keywords = {fairness, algorithmic bias, text classification, machine learning, natural language processing},
location = {New Orleans, LA, USA},
series = {AIES '18}
}
  
"
Neurips,2017,Controllable Invariance through Adversarial Feature Learning,https://proceedings.neurips.cc//paper/2017/hash/8cb22bdd0b7ba1ab13d742e22eed8da2-Abstract.html,"['Qizhe Xie', ' Zihang Dai', ' Yulun Du', ' Eduard Hovy', ' Graham Neubig']",1848,"Learning meaningful representations that maintain the content necessary for a particular task while filtering away detrimental variations is a problem of great interest in machine learning. In this paper, we tackle the problem of learning representations invariant to a specific factor or trait of data. The representation learning process is formulated as an adversarial minimax game. We analyze the optimal equilibrium of such a game and find that it amounts to maximizing the uncertainty of inferring the detrimental factor given the representation while maximizing the certainty of making task-specific predictions. On three benchmark tasks, namely fair and bias-free classification, language-independent generation, and lighting-independent image classification, we show that the proposed framework induces an invariant representation, and leads to better generalization evidenced by the improved performance.",YES/TRAD,,Data,Algorithm,Methods,Data debiasing,,,,,,,,,,,"@inproceedings{NIPS2017_8cb22bdd,
 author = {Xie, Qizhe and Dai, Zihang and Du, Yulun and Hovy, Eduard and Neubig, Graham},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Controllable Invariance through Adversarial Feature Learning},
 url = {https://proceedings.neurips.cc/paper/2017/file/8cb22bdd0b7ba1ab13d742e22eed8da2-Paper.pdf},
 volume = {30},
 year = {2017}
}

"
Neurips,2017,Optimized Pre-Processing for Discrimination Prevention,https://proceedings.neurips.cc//paper/2017/hash/9a49a25d845a483fae4be7e341368e36-Abstract.html,"['Flavio Calmon', ' Dennis Wei', ' Bhanukiran Vinzamuri', ' Karthikeyan Natesan Ramamurthy', ' Kush R. Varshney']",630,"Non-discrimination is a recognized objective in algorithmic decision making. In this paper, we introduce a novel probabilistic formulation of data pre-processing for reducing discrimination. We propose a convex optimization for learning a data transformation with three goals: controlling discrimination, limiting distortion in individual data samples, and preserving utility. We characterize the impact of limited sample size in accomplishing this objective. Two instances of the proposed optimization are applied to datasets, including one on real-world criminal recidivism. Results show that discrimination can be greatly reduced at a small cost in classification accuracy.",YES/TRAD,,Data,Algorithm,Methods,"Data debiasing but focused on the outcome, i.e. focuses in nondiscrimination in the eventual outcome as the goal instead of just making things independent",,,,,,,,,,,"@inproceedings{NIPS2017_9a49a25d,
 author = {Calmon, Flavio and Wei, Dennis and Vinzamuri, Bhanukiran and Natesan Ramamurthy, Karthikeyan and Varshney, Kush R},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Optimized Pre-Processing for Discrimination Prevention},
 url = {https://proceedings.neurips.cc/paper/2017/file/9a49a25d845a483fae4be7e341368e36-Paper.pdf},
 volume = {30},
 year = {2017}
}

"
Neurips,2017,Beyond Parity: Fairness Objectives for Collaborative Filtering,https://proceedings.neurips.cc//paper/2017/hash/e6384711491713d29bc63fc5eeb5ba4f-Abstract.html,"['Sirui Yao', ' Bert Huang']",303,"We study fairness in collaborative-filtering recommender systems, which are sensitive to discrimination that exists in historical data. Biased data can lead collaborative-filtering methods to make unfair predictions for users from minority groups. We identify the insufficiency of existing fairness metrics and propose four new metrics that address different forms of unfairness. These fairness metrics can be optimized by adding fairness terms to the learning objective. Experiments on synthetic and real data show that our new metrics can better measure fairness than the baseline, and that the fairness objectives effectively help reduce unfairness.",YES/TRAD,,Metrics,Algorithm,Mitigation,"Show that in filtering systems, regular fairness techniques don't work super well--they *point to bias as a result of sampling problems in the data*, but at the end of the day they do just create new fairness metrics and then",,,,,,,,,,,"@inproceedings{NIPS2017_e6384711,
 author = {Yao, Sirui and Huang, Bert},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Beyond Parity: Fairness Objectives for Collaborative Filtering},
 url = {https://proceedings.neurips.cc/paper/2017/file/e6384711491713d29bc63fc5eeb5ba4f-Paper.pdf},
 volume = {30},
 year = {2017}
}

"
Neurips,2018,Invariant Representations without Adversarial Training,https://proceedings.neurips.cc//paper/2018/hash/415185ea244ea2b2bedeb0449b926802-Abstract.html,"['Daniel Moyer', ' Shuyang Gao', ' Rob Brekelmans', ' Aram Galstyan', ' Greg Ver Steeg']",165,"Representations of data that are invariant to changes in specified factors are useful for a wide range of problems: removing potential biases in prediction problems, controlling the effects of covariates, and disentangling meaningful factors of variation. Unfortunately, learning representations that exhibit invariance to arbitrary nuisance factors yet remain useful for other tasks is challenging. Existing approaches cast the trade-off between task performance and invariance in an adversarial way, using an iterative minimax optimization. We show that adversarial training is unnecessary and sometimes counter-productive; we instead cast invariant representation learning as a single information-theoretic objective that can be directly optimized. We demonstrate that this approach matches or exceeds performance of state-of-the-art adversarial approaches for learning fair representations and for generative modeling with controllable transformations.",YES/TRAD,,Data,Algorithm,Methods,Data debiasing,,,,,,,,,,,"@inproceedings{NEURIPS2018_415185ea,
 author = {Moyer, Daniel and Gao, Shuyang and Brekelmans, Rob and Galstyan, Aram and Ver Steeg, Greg},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Invariant Representations without Adversarial Training},
 url = {https://proceedings.neurips.cc/paper/2018/file/415185ea244ea2b2bedeb0449b926802-Paper.pdf},
 volume = {31},
 year = {2018}
}

"
Neurips,2019,Noise-tolerant fair classification,https://proceedings.neurips.cc//paper/2019/hash/8d5e957f297893487bd98fa830fa6413-Abstract.html,"['Alex Lamy', ' Ziyuan Zhong', ' Aditya K. Menon', ' Nakul Verma']",51,"Fairness-aware learning involves designing algorithms that do not discriminate with respect to some sensitive feature (e.g., race or gender). Existing work on the problem operates under the assumption that the sensitive feature available in one's training sample is perfectly reliable. This assumption may be violated in many real-world cases: for example, respondents to a survey may choose to conceal or obfuscate their group identity out of fear of potential discrimination. This poses the question of whether one can still learn fair classifiers given noisy sensitive features. In this paper, we answer the question in the affirmative: we show that if one measures fairness using the mean-difference score, and sensitive features are subject to noise from the mutually contaminated learning model, then owing to a simple identity we only need to change the desired fairness-tolerance. The requisite tolerance can be estimated by leveraging existing noise-rate estimators from the label noise literature. We finally show that our procedure is empirically effective on two case-studies involving sensitive feature censoring.",YES/TRAD,,Algorithm,Data,Methods,Shows how to enforce fairness metrics on noisy data,,,,,,,,,,,"@inproceedings{NEURIPS2019_8d5e957f,
 author = {Lamy, Alex and Zhong, Ziyuan and Menon, Aditya K and Verma, Nakul},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Noise-tolerant fair classification},
 url = {https://proceedings.neurips.cc/paper/2019/file/8d5e957f297893487bd98fa830fa6413-Paper.pdf},
 volume = {32},
 year = {2019}
}

"
Neurips,2020,Learning Certified Individually Fair Representations,https://proceedings.neurips.cc//paper/2020/hash/55d491cf951b1b920900684d71419282-Abstract.html,"['Anian Ruoss', ' Mislav Balunovic', ' Marc Fischer', ' Martin Vechev']",50,"Fair representation learning provides an effective way of enforcing fairness constraints without compromising utility for downstream users. A desirable family of such fairness constraints, each requiring similar treatment for similar individuals, is known as individual fairness. In this work, we introduce the first method that enables data consumers to obtain certificates of individual fairness for existing and new data points. The key idea is to map similar individuals to close latent representations and leverage this latent proximity to certify individual fairness. That is, our method enables the data producer to learn and certify a representation where for a data point all similar individuals are at l-infinity distance at most epsilon, thus allowing data consumers to certify individual fairness by proving epsilon-robustness of their classifier. Our experimental evaluation on five real-world datasets and several fairness constraints demonstrates the expressivity and scalability of our approach.",YES/TRAD,,Algorithm,,Methods,"Shows how to enforce individual fairness via creating a special data representation where all similar people are within an epislon ball of each other, so when you train a robust model on it, it’s individually fair.",,,,,,,,,,,"@inproceedings{NEURIPS2020_55d491cf,
 author = {Ruoss, Anian and Balunovic, Mislav and Fischer, Marc and Vechev, Martin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {7584--7596},
 publisher = {Curran Associates, Inc.},
 title = {Learning Certified Individually Fair Representations},
 url = {https://proceedings.neurips.cc/paper/2020/file/55d491cf951b1b920900684d71419282-Paper.pdf},
 volume = {33},
 year = {2020}
}

"
Neurips,2020,Exploiting MMD and Sinkhorn Divergences for Fair and Transferable Representation Learning,https://proceedings.neurips.cc//paper/2020/hash/af9c0e0c1dee63e5acad8b7ed1a5be96-Abstract.html,"['Luca Oneto', ' Michele Donini', ' Giulia Luise', ' Carlo Ciliberto', ' Andreas Maurer', ' Massimiliano Pontil']",22,"Developing learning methods which do not discriminate subgroups in the population is a central goal of algorithmic fairness. One way to reach this goal is by modifying the data representation in order to meet certain fairness constraints. In this work we measure fairness according to demographic parity. This requires the probability of the possible model decisions to be independent of the sensitive information. We argue that the goal of imposing demographic parity can be substantially facilitated within a multitask learning setting. We present a method for learning a shared fair representation across multiple tasks, by means of different new constraints based on MMD and Sinkhorn Divergences. We derive learning bounds establishing that the learned representation transfers well to novel tasks. We present experiments on three real world datasets, showing that the proposed method outperforms state-of-the-art approaches by a significant margin.",YES/TRAD,,Data,Algorithm,,Data debiasing,,,,,,,,,,,"@inproceedings{NEURIPS2020_af9c0e0c,
 author = {Oneto, Luca and Donini, Michele and Luise, Giulia and Ciliberto, Carlo and Maurer, Andreas and Pontil, Massimiliano},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {15360--15370},
 publisher = {Curran Associates, Inc.},
 title = {Exploiting MMD and Sinkhorn Divergences for Fair and Transferable Representation Learning},
 url = {https://proceedings.neurips.cc/paper/2020/file/af9c0e0c1dee63e5acad8b7ed1a5be96-Paper.pdf},
 volume = {33},
 year = {2020}
}

"
Neurips,2021,AutoBalance: Optimized Loss Functions for Imbalanced Data,https://proceedings.neurips.cc//paper/2021/hash/191f8f858acda435ae0daf994e2a72c2-Abstract.html,"['Mingchen Li', ' Xuechen Zhang', ' Christos Thrampoulidis', ' Jiasi Chen', ' Samet Oymak']",12,"Imbalanced datasets are commonplace in modern machine learning problems. The presence of under-represented classes or groups with sensitive attributes results in concerns about generalization and fairness. Such concerns are further exacerbated by the fact that large capacity deep nets can perfectly fit the training data and appear to achieve perfect accuracy and fairness during training, but perform poorly during test. To address these challenges, we propose AutoBalance, a bi-level optimization framework that automatically designs a training loss function to optimize a blend of accuracy and fairness-seeking objectives. Specifically, a lower-level problem trains the model weights, and an upper-level problem tunes the loss function by monitoring and optimizing the desired objective over the validation data. Our loss design enables personalized treatment for classes/groups by employing a parametric cross-entropy loss and individualized data augmentation schemes. We evaluate the benefits and performance of our approach for the application scenarios of imbalanced and group-sensitive classification. Extensive empirical evaluations demonstrate the benefits of AutoBalance over state-of-the-art approaches. Our experimental findings are complemented with theoretical insights on loss function design and the benefits of the train-validation split. All code is available open-source.",YES/TRAD,,Data,Algorithm,Methods,Shows method of imposing fairness constraints at training time that takes imbalanced data into account for fairness techniques,,,,,,,,,,,"@inproceedings{NEURIPS2021_191f8f85,
 author = {Li, Mingchen and Zhang, Xuechen and Thrampoulidis, Christos and Chen, Jiasi and Oymak, Samet},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {3163--3177},
 publisher = {Curran Associates, Inc.},
 title = {AutoBalance: Optimized Loss Functions for Imbalanced Data},
 url = {https://proceedings.neurips.cc/paper/2021/file/191f8f858acda435ae0daf994e2a72c2-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
Neurips,2021,Fairness via Representation Neutralization,https://proceedings.neurips.cc//paper/2021/hash/64ff7983a47d331b13a81156e2f4d29d-Abstract.html,"['Mengnan Du', ' Subhabrata Mukherjee', ' Guanchu Wang', ' Ruixiang Tang', ' Ahmed Awadallah', ' Xia Hu']",28,"Existing bias mitigation methods for DNN models primarily work on learning debiased encoders. This process not only requires a lot of instance-level annotations for sensitive attributes, it also does not guarantee that all fairness sensitive information has been removed from the encoder. To address these limitations, we explore the following research question: Can we reduce the discrimination of DNN models by only debiasing the classification head, even with biased representations as inputs? To this end, we propose a new mitigation technique, namely, Representation Neutralization for Fairness (RNF) that achieves fairness by debiasing only the task-specific classification head of DNN models. To this end, we leverage samples with the same ground-truth label but different sensitive attributes, and use their neutralized representations to train the classification head of the DNN model. The key idea of RNF is to discourage the classification head from capturing spurious correlation between fairness sensitive information in encoder representations with specific class labels. To address low-resource settings with no access to sensitive attribute annotations, we leverage a bias-amplified model to generate proxy annotations for sensitive attributes. Experimental results over several benchmark datasets demonstrate our RNF framework to effectively reduce discrimination of DNN models with minimal degradation in task-specific performance.",YES/TRAD,,Data,Algorithm,Methods,Data debiasing,,,,,,,,,,,"@inproceedings{NEURIPS2021_64ff7983,
 author = {Du, Mengnan and Mukherjee, Subhabrata and Wang, Guanchu and Tang, Ruixiang and Awadallah, Ahmed and Hu, Xia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {12091--12103},
 publisher = {Curran Associates, Inc.},
 title = {Fairness via Representation Neutralization},
 url = {https://proceedings.neurips.cc/paper/2021/file/64ff7983a47d331b13a81156e2f4d29d-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
Neurips,2021,Fair Sequential Selection Using Supervised Learning Models,https://proceedings.neurips.cc//paper/2021/hash/ed277964a8959e72a0d987e598dfbe72-Abstract.html,"['Mohammad Mahdi Khalili', ' Xueru Zhang', ' Mahed Abroshan']",5,"We consider a selection problem where sequentially arrived applicants apply for a limited number of positions/jobs. At each time step, a decision maker accepts or rejects the given applicant using a pre-trained supervised learning model until all the vacant positions are filled. In this paper, we discuss whether the fairness notions (e.g., equal opportunity, statistical parity, etc.) that are commonly used in classification problems are suitable for the sequential selection problems. In particular, we show that even with a pre-trained model that satisfies the common fairness notions, the selection outcomes may still be biased against certain demographic groups. This observation implies that the fairness notions used in classification problems are not suitable for a selection problem where the applicants compete for a limited number of positions. We introduce a new fairness notion, ``Equal Selection (ES),'' suitable for sequential selection problems and propose a post-processing approach to satisfy the ES fairness notion. We also consider a setting where the applicants have privacy concerns, and the decision maker only has access to the noisy version of sensitive attributes. In this setting, we can show that the \textit{perfect} ES fairness can still be attained under certain conditions.",YES/TRAD,,Algorithm,Metrics,Methods,"Shows that even fair models do not end up in fair allocations when a) systems have budgets and b) people arrive over time. Is does bring up some aspects of the system that are related to organizational realities, but at the same time I've pushed away a lot of online learning papers. Thoughts?",,,,,,,,,,,"@inproceedings{NEURIPS2021_ed277964,
 author = {Khalili, Mohammad Mahdi and Zhang, Xueru and Abroshan, Mahed},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {28144--28155},
 publisher = {Curran Associates, Inc.},
 title = {Fair Sequential Selection Using Supervised Learning Models},
 url = {https://proceedings.neurips.cc/paper/2021/file/ed277964a8959e72a0d987e598dfbe72-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
ICLR,2021,FairFil: Contrastive Neural Debiasing Method for Pretrained Text Encoders,https://openreview.net/forum?id=N6JECD-PI5w,"['Pengyu Cheng', ' Weituo Hao', ' Siyang Yuan', ' Shijing Si', ' Lawrence Carin']",15,"Pretrained text encoders, such as BERT, have been applied increasingly in various natural language processing (NLP) tasks, and have recently demonstrated significant performance gains. However, recent studies have demonstrated the existence of social bias in these pretrained NLP models. Although prior works have made progress on word-level debiasing, improved sentence-level fairness of pretrained encoders still lacks exploration. In this paper, we proposed the first neural debiasing method for a pretrained sentence encoder, which transforms the pretrained encoder outputs into debiased representations via a fair filter (FairFil) network. To learn the FairFil, we introduce a contrastive learning framework that not only minimizes the correlation between filtered embeddings and bias words but also preserves rich semantic information of the original sentences. On real-world datasets, our FairFil effectively reduces the bias degree of pretrained text encoders, while continuously showing desirable performance on downstream tasks. Moreover, our post hoc method does not require any retraining of the text encoders, further enlarging FairFil's application space.",YES/TRAD,,Data,,,,,,,,,,,,,,"@inproceedings{
cheng2021fairfil,
title={FairFil: Contrastive Neural Debiasing Method for Pretrained Text Encoders},
author={Pengyu Cheng and Weituo Hao and Siyang Yuan and Shijing Si and Lawrence Carin},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=N6JECD-PI5w}
}"
ICLR,2021,Learning the Pareto Front with Hypernetworks,https://openreview.net/forum?id=NjF772F4ZZR,"['Aviv Navon', ' Aviv Shamsian', ' Ethan Fetaya', ' Gal Chechik']",47,"Multi-objective optimization (MOO) problems are prevalent in machine learning. These problems have a set of optimal solutions, called the Pareto front, where each point on the front represents a different trade-off between possibly conflicting objectives. Recent MOO methods can target a specific desired ray in loss space however, most approaches still face two grave limitations: (i) A separate model has to be trained for each point on the front; and (ii) The exact trade-off must be known before the optimization process. Here, we tackle the problem of learning the entire Pareto front, with the capability of selecting a desired operating point on the front after training. We call this new setup Pareto-Front Learning (PFL).
 
 We describe an approach to PFL implemented using HyperNetworks, which we term Pareto HyperNetworks (PHNs). PHN learns the entire Pareto front simultaneously using a single hypernetwork, which receives as input a desired preference vector and returns a Pareto-optimal model whose loss vector is in the desired ray. The unified model is runtime efficient compared to training multiple models and generalizes to new operating points not used during training. We evaluate our method on a wide set of problems, from multi-task regression and classification to fairness. PHNs learn the entire Pareto front at roughly the same time as learning a single point on the front and at the same time reach a better solution set. PFL opens the door to new applications where models are selected based on preferences that are only available at run time.",YES/TRAD,,Algorithm,,,,,,,,,,,,,,"@inproceedings{
navon2021learning,
title={Learning the Pareto Front with Hypernetworks},
author={Aviv Navon and Aviv Shamsian and Ethan Fetaya and Gal Chechik},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=NjF772F4ZZR}
}"
ICLR,2021,On Dyadic Fairness: Exploring and Mitigating Bias in Graph Connections,https://openreview.net/forum?id=xgGS6PmzNq6,"['Peizhao Li', ' Yifei Wang', ' Han Zhao', ' Pengyu Hong', ' Hongfu Liu']",45,"Disparate impact has raised serious concerns in machine learning applications and its societal impacts. In response to the need of mitigating discrimination, fairness has been regarded as a crucial property in algorithmic design. In this work, we study the problem of disparate impact on graph-structured data. Specifically, we focus on dyadic fairness, which articulates a fairness concept that a predictive relationship between two instances should be independent of the sensitive attributes. Based on this, we theoretically relate the graph connections to dyadic fairness on link predictive scores in learning graph neural networks, and reveal that regulating weights on existing edges in a graph contributes to dyadic fairness conditionally. Subsequently, we propose our algorithm, \textbf{FairAdj}, to empirically learn a fair adjacency matrix with proper graph structural constraints for fair link prediction, and in the meanwhile preserve predictive accuracy as much as possible. Empirical validation demonstrates that our method delivers effective dyadic fairness in terms of various statistics, and at the same time enjoys a favorable fairness-utility tradeoff.",YES/TRAD,,Algorithm,Data,,,,,,,,,,,,,"@inproceedings{
li2021on,
title={On Dyadic Fairness: Exploring and Mitigating Bias in Graph Connections},
author={Peizhao Li and Yifei Wang and Han Zhao and Pengyu Hong and Hongfu Liu},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=xgGS6PmzNq6}
}"
ICLR,2022,Fairness Guarantees under Demographic Shift,https://openreview.net/forum?id=wbPObLm6ueA,"['Stephen Giguere', ' Blossom Metevier', ' Bruno Castro da Silva', ' Yuriy Brun', ' Philip S. Thomas', ' Scott Niekum']",5,"Recent studies have demonstrated that using machine learning for social applications can lead to injustice in the form of racist, sexist, and otherwise unfair and discriminatory outcomes. To address this challenge, recent machine learning algorithms have been designed to limit the likelihood such unfair behaviors will occur. However, these approaches typically assume the data used for training is representative of what will be encountered once the model is deployed, thus limiting their usefulness. In particular, if certain subgroups of the population become more or less probable after the model is deployed (a phenomenon we call demographic shift), the fair-ness assurances provided by prior algorithms are often invalid. We consider the impact of demographic shift and present a class of algorithms, called Shifty algorithms, that provide high-confidence behavioral guarantees that hold under demographic shift. Shifty is the first technique of its kind and demonstrates an effective strategy for designing algorithms to overcome the challenges demographic shift poses. We evaluate Shifty-ttest, an implementation of Shifty based on Student‚Äôs ùë°-test, and, using a real-world data set of university entrance exams and subsequent student success, show that the models output by our algorithm avoid unfair bias under demo-graphic shift, unlike existing methods. Our experiments demonstrate that our algorithm‚Äôs high-confidence fairness guarantees are valid in practice and that our algorithm is an effective tool for training models that are fair when demographic shift occurs.",YES/TRAD,,In-situ,Algorithm,,,,,,,,,,,,,"@inproceedings{
giguere2022fairness,
title={Fairness Guarantees under Demographic Shift},
author={Stephen Giguere and Blossom Metevier and Yuriy Brun and Philip S. Thomas and Scott Niekum and Bruno Castro da Silva},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=wbPObLm6ueA}
}"
ICLR,2022,Is Fairness Only Metric Deep? Evaluating and Addressing Subgroup Gaps in Deep Metric Learning,https://openreview.net/forum?id=js62_xuLDDv,"['Natalie Dullerud', ' Karsten Roth', ' Kimia Hamidieh', ' Nicolas Papernot', ' Marzyeh Ghassemi']",4,"Deep metric learning (DML) enables learning with less supervision through its emphasis on the similarity structure of representations. There has been much work on improving generalization of DML in settings like zero-shot retrieval, but little is known about its implications for fairness. In this paper, we are the first to evaluate state-of-the-art DML methods trained on imbalanced data, and to show the negative impact these representations have on minority subgroup performance when used for downstream tasks. In this work, we first define fairness in DML through an analysis of three properties of the representation space -- inter-class alignment, intra-class alignment, and uniformity -- and propose \textit{\textbf{finDML}}, the \textit{\textbf{f}}airness \textit{\textbf{i}}n \textit{\textbf{n}}on-balanced \textit{\textbf{DML}} benchmark to characterize representation fairness. Utilizing \textit{finDML}, we find bias in DML representations to propagate to common downstream classification tasks. Surprisingly, this bias is propagated even when training data in the downstream task is re-balanced. To address this problem, we present Partial Attribute De-correlation (\textit{\textbf{\pad}}) to disentangle feature representations from sensitive attributes and reduce performance gaps between subgroups in both embedding space and downstream metrics.",YES/TRAD,,Data,"Algorithm, Metrics",,,,,,,,,,,,,"@inproceedings{
dullerud2022is,
title={Is Fairness Only Metric Deep? Evaluating and Addressing Subgroup Gaps in Deep Metric Learning},
author={Natalie Dullerud and Karsten Roth and Kimia Hamidieh and Nicolas Papernot and Marzyeh Ghassemi},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=js62_xuLDDv}
}"
ICLR,2022,Conditional Contrastive Learning with Kernel,https://openreview.net/forum?id=AAJLBoGt0XM,"['Yao-Hung Hubert Tsai', ' Tianqin Li', ' Martin Q. Ma', ' Han Zhao', ' Kun Zhang', ' Louis-Philippe Morency', ' Ruslan Salakhutdinov']",18,"Conditional contrastive learning frameworks consider the conditional sampling procedure that constructs positive or negative data pairs conditioned on specific variables. Fair contrastive learning constructs negative pairs, for example, from the same gender (conditioning on sensitive information), which in turn reduces undesirable information from the learned representations; weakly supervised contrastive learning constructs positive pairs with similar annotative attributes (conditioning on auxiliary information), which in turn are incorporated into the representations. Although conditional contrastive learning enables many applications, the conditional sampling procedure can be challenging if we cannot obtain sufficient data pairs for some values of the conditioning variable. This paper presents Conditional Contrastive Learning with Kernel (CCL-K) that converts existing conditional contrastive objectives into alternative forms that mitigate the insufficient data problem. Instead of sampling data according to the value of the conditioning variable, CCL-K uses the Kernel Conditional Embedding Operator that samples data from all available data and assigns weights to each sampled data given the kernel similarity between the values of the conditioning variable. We conduct experiments using weakly supervised, fair, and hard negatives contrastive learning, showing CCL-K outperforms state-of-the-art baselines.",YES/TRAD,,Data,Algorithm,,,,,,,,,,,,,"@inproceedings{
tsai2022conditional,
title={Conditional Contrastive Learning with Kernel},
author={Yao-Hung Hubert Tsai and Tianqin Li and Martin Q. Ma and Han Zhao and Kun Zhang and Louis-Philippe Morency and Ruslan Salakhutdinov},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=AAJLBoGt0XM}
}"
ICLR,2022,Fair Normalizing Flows,https://openreview.net/forum?id=BrFIKuxrZE,"['Mislav Balunovic', ' Anian Ruoss', ' Martin Vechev']",8,"Fair representation learning is an attractive approach that promises fairness of downstream predictors by encoding sensitive data. Unfortunately, recent work has shown that strong adversarial predictors can still exhibit unfairness by recovering sensitive attributes from these representations. In this work, we present Fair Normalizing Flows (FNF), a new approach offering more rigorous fairness guarantees for learned representations. Specifically, we consider a practical setting where we can estimate the probability density for sensitive groups. The key idea is to model the encoder as a normalizing flow trained to minimize the statistical distance between the latent representations of different groups. The main advantage of FNF is that its exact likelihood computation allows us to obtain guarantees on the maximum unfairness of any potentially adversarial downstream predictor. We experimentally demonstrate the effectiveness of FNF in enforcing various group fairness notions, as well as other attractive properties such as interpretability and transfer learning, on a variety of challenging real-world datasets.",YES/TRAD,,Data,Algorithm,,,,,,,,,,,,,"@inproceedings{
balunovic2022fair,
title={Fair Normalizing Flows},
author={Mislav Balunovic and Anian Ruoss and Martin Vechev},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=BrFIKuxrZE}
}"
FACCT,2018,Potential for Discrimination in Online Targeted Advertising,https://proceedings.mlr.press/v81/speicher18a.html,"['Speicher, Till', 'Ali, Muhammad', 'Venkatadri, Giridhari', 'Ribeiro, Filipe Nunes', 'Arvanitakis, George', 'Benevenuto, Fabr√≠cio', 'Gummadi, Krishna P.', 'Loiseau, Patrick', 'Mislove, Alan']",167,"Recently, online targeted advertising platforms like Facebook have been criticized for allowing advertisers to discriminate against users belonging to sensitive groups, i.e., to exclude users belonging to a certain race or gender from receiving their ads. Such criticisms have led, for instance, Facebook to disallow the use of attributes such as ethnic affinity from being used by advertisers when targeting ads related to housing or employment or financial services. In this paper, we show that such measures are far from sufficient and that the problem of discrimination in targeted advertising is much more pernicious. We argue that discrimination measures should be based on the targeted population and not on the attributes used for targeting. We systematically investigate the different targeting methods offered by Facebook for their ability to enable discriminatory advertising. We show that a malicious advertiser can create highly discriminatory ads without using sensitive attributes. Our findings call for exploring fundamentally new methods for mitigating discrimination in online targeted advertising.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v81-speicher18a,
  title = 	 {Potential for Discrimination in Online Targeted Advertising},
  author = 	 {Speicher, Till and Ali, Muhammad and Venkatadri, Giridhari and Ribeiro, Filipe Nunes and Arvanitakis, George and Benevenuto, Fabrício and Gummadi, Krishna P. and Loiseau, Patrick and Mislove, Alan},
  booktitle = 	 {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
  pages = 	 {5--19},
  year = 	 {2018},
  editor = 	 {Friedler, Sorelle A. and Wilson, Christo},
  volume = 	 {81},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--24 Feb},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v81/speicher18a/speicher18a.pdf},
  url = 	 {https://proceedings.mlr.press/v81/speicher18a.html},
  abstract = 	 {Recently, online targeted advertising platforms like Facebook have been criticized for allowing advertisers to discriminate against users belonging to sensitive groups, i.e., to exclude users belonging to a certain race or gender from receiving their ads. Such criticisms have led, for instance, Facebook to disallow the use of attributes such as ethnic affinity from being used by advertisers when targeting ads related to housing or employment or financial services. In this paper, we show that such measures are far from sufficient and that the problem of discrimination in targeted advertising is much more pernicious.  We argue that discrimination measures should be based on the targeted population and not on the attributes used for targeting. We systematically investigate the different targeting methods offered by Facebook for their ability to enable discriminatory advertising.  We show that a malicious advertiser can create highly discriminatory ads without using sensitive attributes. Our findings call for exploring fundamentally new methods for mitigating discrimination in online targeted advertising.}
}
"
FACCT,2018,Discrimination in Online Advertising: A Multidisciplinary Inquiry,https://proceedings.mlr.press/v81/datta18a.html,"['Datta, Amit', 'Datta, Anupam', 'Makagon, Jael', 'Mulligan, Deirdre K.', 'Tschantz, Michael Carl']",65,"We explore ways in which discrimination may arise in the targeting of job-related advertising, noting the potential for multiple parties to contribute to its occurrence. We then examine the statutes and case law interpreting the prohibition on advertisements that indicate a preference based on protected class, and consider its application to online advertising. We focus on its interaction with Section 230 of the Communications Decency Act, which provides interactive computer services with immunity for providing access to information created by a third party. We argue that such services can lose that immunity if they target ads toward or away from protected classes without explicit instructions from advertisers to do so.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v81-datta18a,
  title = 	 {Discrimination in Online Advertising: A Multidisciplinary Inquiry},
  author = 	 {Datta, Amit and Datta, Anupam and Makagon, Jael and Mulligan, Deirdre K. and Tschantz, Michael Carl},
  booktitle = 	 {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
  pages = 	 {20--34},
  year = 	 {2018},
  editor = 	 {Friedler, Sorelle A. and Wilson, Christo},
  volume = 	 {81},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--24 Feb},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v81/datta18a/datta18a.pdf},
  url = 	 {https://proceedings.mlr.press/v81/datta18a.html},
  abstract = 	 {We explore ways in which discrimination may arise in the targeting of job-related advertising, noting the potential for multiple parties to contribute to its occurrence.  We then examine the statutes and case law interpreting the prohibition on advertisements that indicate a preference based on protected class, and consider its application to online advertising.  We focus on its interaction with Section 230 of the Communications Decency Act, which provides interactive computer services with immunity for providing access to  information created by a third party.  We argue that such services can lose that immunity if they target ads toward or away from protected classes without explicit instructions from advertisers to do so.}
}
"
FACCT,2018,Privacy for All: Ensuring Fair and Equitable Privacy Protections,https://proceedings.mlr.press/v81/ekstrand18a.html,"['Ekstrand, Michael D.', 'Joshaghani, Rezvan', 'Mehrpouyan, Hoda']",72,"In this position paper, we argue for applying recent research on ensuring sociotechnical systems are fair and non-discriminatory to the privacy protections those systems may provide. Privacy literature seldom considers whether a proposed privacy scheme protects all persons uniformly, irrespective of membership in protected classes or particular risk in the face of privacy failure. Just as algorithmic decision-making systems may have discriminatory outcomes even without explicit or deliberate discrimination, so also privacy regimes may disproportionately fail to protect vulnerable members of their target population, resulting in disparate impact with respect to the effectiveness of privacy protections.We propose a research agenda that will illuminate this issue, along with related issues in the intersection of fairness and privacy, and present case studies that show how the outcomes of this research may change existing privacy and fairness research. We believe it is important to ensure that technologies and policies intended to protect the users and subjects of information systems provide such protection in an equitable fashion.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v81-ekstrand18a,
  title = 	 {Privacy for All: Ensuring Fair and Equitable Privacy Protections},
  author = 	 {Ekstrand, Michael D. and Joshaghani, Rezvan and Mehrpouyan, Hoda},
  booktitle = 	 {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
  pages = 	 {35--47},
  year = 	 {2018},
  editor = 	 {Friedler, Sorelle A. and Wilson, Christo},
  volume = 	 {81},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--24 Feb},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v81/ekstrand18a/ekstrand18a.pdf},
  url = 	 {https://proceedings.mlr.press/v81/ekstrand18a.html},
  abstract = 	 {In this position paper, we argue for applying recent research on ensuring sociotechnical systems are fair and non-discriminatory to the privacy protections those systems may provide. Privacy literature seldom considers whether a proposed privacy scheme protects all persons uniformly, irrespective of membership in protected classes or particular risk in the face of privacy failure. Just as algorithmic decision-making systems may have discriminatory outcomes even without explicit or deliberate discrimination, so also privacy regimes may disproportionately fail to protect vulnerable members of their target population, resulting in disparate impact with respect to the effectiveness of privacy protections.We propose a research agenda that will illuminate this issue, along with related issues in the intersection of fairness and privacy, and present case studies that show how the outcomes of this research may change existing privacy and fairness research. We believe it is important to ensure that technologies and policies intended to protect the users and subjects of information systems provide such protection in an equitable fashion.}
}
"
FACCT,2018,Interventions over Predictions: Reframing the Ethical Debate for Actuarial Risk Assessment,https://proceedings.mlr.press/v81/barabas18a.html,"['Barabas, Chelsea', 'Virza, Madars', 'Dinakar, Karthik', 'Ito, Joichi', 'Zittrain, Jonathan']",139,"Actuarial risk assessments are frequently touted as a neutral way to counteract implicit bias and increase the fairness of decisions made at almost every juncture of the criminal justice system, from pretrial release to sentencing, parole and probation. In recent times these assessments have come under increased scrutiny, as critics claim that the statistical techniques underlying them might reproduce existing patterns of discrimination and historical biases that are reflected in the data. Much of this debate is centered around competing notions of fairness and predictive accuracy, which seek to problematize the use of variables that act as ‚Äúproxies‚Äù for protected classes, such as race and gender. However, these debates fail to address the core ethical issue at hand - that current risk assessments are ill-equipped to support ethical punishment and rehabilitation practices in the criminal justice system, because they offer only a limited insight into the underlying drivers of criminal behavior. In this paper, we examine the prevailing paradigms of fairness currently under debate and propose an alternative methodology for identifying the underlying social and structural factors that drive criminal behavior. We argue that the core ethical debate surrounding the use of regression in risk assessments is not one of bias or accuracy. Rather, it‚Äôs one of purpose. If machine learning is operationalized merely in the service of predicting future crime, then it becomes difficult to break cycles of criminalization that are driven by the iatrogenic effects of the criminal justice system itself. We posit that machine learning should not be used for prediction, rather it should be used to surface covariates that are fed into a causal model for understanding the social, structural and psychological drivers of crime. We propose an alternative application of machine learning and causal inference away from predicting risk scores to risk mitigation.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v81-barabas18a,
  title = 	 {Interventions over Predictions: Reframing the Ethical Debate for Actuarial Risk Assessment},
  author = 	 {Barabas, Chelsea and Virza, Madars and Dinakar, Karthik and Ito, Joichi and Zittrain, Jonathan},
  booktitle = 	 {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
  pages = 	 {62--76},
  year = 	 {2018},
  editor = 	 {Friedler, Sorelle A. and Wilson, Christo},
  volume = 	 {81},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--24 Feb},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v81/barabas18a/barabas18a.pdf},
  url = 	 {https://proceedings.mlr.press/v81/barabas18a.html},
  abstract = 	 {Actuarial risk assessments are frequently touted as a neutral way to counteract implicit bias and increase the fairness of decisions made at almost every juncture of the criminal justice system, from pretrial release to sentencing, parole and probation. In recent times these assessments have come under increased scrutiny, as critics claim that the statistical techniques underlying them might reproduce existing patterns of discrimination and historical biases that are reflected in the data. Much of this debate is centered around competing notions of fairness and predictive accuracy, which seek to problematize the use of variables that act as “proxies” for protected classes, such as race and gender. However, these debates fail to address the core ethical issue at hand - that current risk assessments are ill-equipped to support ethical punishment and rehabilitation practices in the criminal justice system, because they offer only a limited insight into the underlying drivers of criminal behavior. In this paper, we examine the prevailing paradigms of fairness currently under debate and propose an alternative methodology for identifying the underlying social and structural factors that drive criminal behavior. We argue that the core ethical debate surrounding the use of regression in risk assessments is not one of bias or accuracy. Rather, it’s one of purpose. If machine learning is operationalized merely in the service of predicting future crime, then it becomes difficult to break cycles of criminalization that are driven by the iatrogenic effects of the criminal justice system itself. We posit that machine learning should not be used for prediction, rather it should be used to surface covariates that are fed into a causal model for understanding the social, structural and psychological drivers of crime. We propose an alternative application of machine learning and causal inference away from predicting risk scores to risk mitigation.}
}
"
FACCT,2018,Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification,https://proceedings.mlr.press/v81/buolamwini18a.html,"['Buolamwini, Joy', 'Gebru, Timnit']",3606,"Recent studies demonstrate that machine learning algorithms can discriminate based on classes like race and gender. In this work, we present an approach to evaluate bias present in automated facial analysis algorithms and datasets with respect to phenotypic subgroups. Using the dermatologist approved Fitzpatrick Skin Type classification system, we characterize the gender and skin type distribution of two facial analysis benchmarks, IJB-A and Adience. We find that these datasets are overwhelmingly composed of lighter-skinned subjects (79.6% for IJB-A and 86.2% for Adience) and introduce a new facial analysis dataset which is balanced by gender and skin type. We evaluate 3 commercial gender classification systems using our dataset and show that darker-skinned females are the most misclassified group (with error rates of up to 34.7%). The maximum error rate for lighter-skinned males is 0.8%. The substantial disparities in the accuracy of classifying darker females, lighter females, darker males, and lighter males in gender classification systems require urgent attention if commercial companies are to build genuinely fair, transparent and accountable facial analysis algorithms.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v81-buolamwini18a,
  title =          {Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification},
  author =          {Buolamwini, Joy and Gebru, Timnit},
  booktitle =          {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
  pages =          {77--91},
  year =          {2018},
  editor =          {Friedler, Sorelle A. and Wilson, Christo},
  volume =          {81},
  series =          {Proceedings of Machine Learning Research},
  month =          {23--24 Feb},
  publisher =    {PMLR},
  pdf =          {http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf},
  url =          {https://proceedings.mlr.press/v81/buolamwini18a.html},
  abstract =          {Recent studies demonstrate that machine learning algorithms can discriminate based on classes like race and gender. In this work, we present an approach to evaluate bias present in automated facial analysis algorithms and datasets with respect to phenotypic subgroups. Using the dermatologist  approved Fitzpatrick Skin Type classification system, we characterize the gender and skin type distribution of two facial analysis benchmarks, IJB-A and Adience. We find that these datasets are overwhelmingly composed of lighter-skinned subjects (79.6% for IJB-A and 86.2% for Adience) and introduce a new facial analysis dataset which is balanced by gender and skin type. We evaluate 3 commercial gender classification systems using our dataset and show that darker-skinned females are the most misclassified group (with error rates of up to 34.7%). The maximum error rate for lighter-skinned males is 0.8%. The substantial disparities in the accuracy of classifying darker females, lighter females, darker males, and lighter males in gender classification systems require urgent attention if commercial companies are to build genuinely fair, transparent and accountable facial analysis algorithms.}
}
"
FACCT,2018,The cost of fairness in binary classification,https://proceedings.mlr.press/v81/menon18a.html,"['Menon, Aditya Krishna', 'Williamson, Robert C']",294,"Binary classifiers are often required to possess fairness in the sense of not overly discriminating with respect to a feature deemed sensitive e.g. race. We study the inherent tradeoffs in learning classifiers with a fairness constraint in the form of two questions: what is the best accuracy we can expect for a given level of fairness?, and what is the nature of these optimal fairness-aware classifiers? To answer these questions, we provide three main contributions. First, we relate two existing fairness measures to cost-sensitive risks. Second, we show that for such cost-sensitive fairness measures, the optimal classifier is an instance-dependent thresholding of the class-probability function. Third, we relate the tradeoff between accuracy and fairness to the alignment between the target and sensitive features‚Äô class-probabilities. A practical implication of our analysis is a simple approach to the fairness-aware problem which involves suitably thresholding class-probability estimates.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v81-menon18a,
  title = 	 {The cost of fairness in binary classification},
  author = 	 {Menon, Aditya Krishna and Williamson, Robert C},
  booktitle = 	 {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
  pages = 	 {107--118},
  year = 	 {2018},
  editor = 	 {Friedler, Sorelle A. and Wilson, Christo},
  volume = 	 {81},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--24 Feb},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v81/menon18a/menon18a.pdf},
  url = 	 {https://proceedings.mlr.press/v81/menon18a.html},
  abstract = 	 {Binary classifiers are often required to possess fairness in the sense of not overly discriminating with respect to a feature deemed sensitive e.g. race. We study the inherent tradeoffs in learning classifiers with a fairness constraint in the form of two questions: what is the best accuracy we can expect for a given level of fairness?, and what is the nature of these optimal fairness-aware classifiers? To answer these questions, we provide three main contributions. First, we relate two existing fairness measures to cost-sensitive risks. Second, we show that for such cost-sensitive fairness measures, the optimal classifier is an instance-dependent thresholding of the class-probability function. Third, we relate the tradeoff between accuracy and fairness to the alignment between the target and sensitive features’ class-probabilities. A practical implication of our analysis is a simple approach to the fairness-aware problem which involves suitably thresholding class-probability estimates.}
}
"
FACCT,2018,Decoupled Classifiers for Group Fair and Efficient Machine Learning,https://proceedings.mlr.press/v81/dwork18a.html,"['Dwork, Cynthia', 'Immorlica, Nicole', 'Kalai, Adam Tauman', 'Leiserson, Max']",215,"When it is ethical and legal to use a sensitive attribute (such as gender or race) in machine learning systems, the question remains how to do so. We show that the naive application of machine learning algorithms using sensitive attributes leads to an inherent tradeoff in accuracy between groups. We provide a simple and efficient decoupling technique, that can be added on top of any black-box machine learning algorithm, to learn different classifiers for different groups. Transfer learning is used to mitigate the problem of having too little data on any one group.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v81-dwork18a,
  title =          {Decoupled Classifiers for Group-Fair and Efficient Machine Learning},
  author =          {Dwork, Cynthia and Immorlica, Nicole and Kalai, Adam Tauman and Leiserson, Max},
  booktitle =          {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
  pages =          {119--133},
  year =          {2018},
  editor =          {Friedler, Sorelle A. and Wilson, Christo},
  volume =          {81},
  series =          {Proceedings of Machine Learning Research},
  month =          {23--24 Feb},
  publisher =    {PMLR},
  pdf =          {http://proceedings.mlr.press/v81/dwork18a/dwork18a.pdf},
  url =          {https://proceedings.mlr.press/v81/dwork18a.html},
  abstract =          {When it is ethical and legal to use a sensitive attribute (such as gender or race) in machine learning systems, the question remains how to do so. We show that the naive application of machine learning algorithms using sensitive attributes leads to an inherent tradeoff in accuracy between groups. We provide a simple and efficient decoupling technique, that can be added on top of any black-box machine learning algorithm, to learn different classifiers for different groups. Transfer learning is used to mitigate the problem of having too little data on any one group.}
}
"
FACCT,2018,A case study of algorithm assisted decision making in child maltreatment hotline screening decisions,https://proceedings.mlr.press/v81/chouldechova18a.html,"['Chouldechova, Alexandra', 'Benavides-Prado, Diana', 'Fialko, Oleksandr', 'Vaithianathan, Rhema']",260,"Every year there are more than 3.6 million referrals made to child protection agencies across the US. The practice of screening calls is left to each jurisdiction to follow local practices and policies, potentially leading to large variation in the way in which referrals are treated across the country. Whilst increasing access to linked administrative data is available, it is difficult for welfare workers to make systematic use of historical information about all the children and adults on a single referral call. Risk prediction models that use routinely collected administrative data can help call workers to better identify cases that are likely to result in adverse outcomes. However, the use of predictive analytics in the area of child welfare is contentious. There is a possibility that some communities‚Äîsuch as those in poverty or from particular racial and ethnic groups‚Äîwill be disadvantaged by the reliance on government administrative data. On the other hand, these analytics tools can augment or replace human judgments, which themselves are biased and imperfect. In this paper we describe our work on developing, validating, fairness auditing, and deploying a risk prediction model in Allegheny County, Pennsylvania, USA. We discuss the results of our analysis to-date, and also highlight key problems and data bias issues that present challenges for model evaluation and deployment.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v81-chouldechova18a,
  title = 	 {A case study of algorithm-assisted decision making in child maltreatment hotline screening decisions},
  author = 	 {Chouldechova, Alexandra and Benavides-Prado, Diana and Fialko, Oleksandr and Vaithianathan, Rhema},
  booktitle = 	 {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
  pages = 	 {134--148},
  year = 	 {2018},
  editor = 	 {Friedler, Sorelle A. and Wilson, Christo},
  volume = 	 {81},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--24 Feb},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v81/chouldechova18a/chouldechova18a.pdf},
  url = 	 {https://proceedings.mlr.press/v81/chouldechova18a.html},
  abstract = 	 {Every year there are more than 3.6 million referrals made to child protection agencies across the US. The practice of screening calls is left to each jurisdiction to follow local practices and policies, potentially leading to large variation in the way in which referrals are treated across the country. Whilst increasing access to linked administrative data is available, it is difficult for welfare workers to make systematic use of historical information about all the children and adults on a single referral call. Risk prediction models that use routinely collected administrative data can help call workers to better identify cases that are likely to result in adverse outcomes. However, the use of predictive analytics in the area of child welfare is contentious. There is a possibility that some communities—such as those in poverty or from particular racial and ethnic groups—will be disadvantaged by the reliance on government administrative data. On the other hand, these analytics tools can augment or replace human judgments, which themselves are biased and imperfect. In this paper we describe our work on developing, validating, fairness auditing, and deploying a risk prediction model in Allegheny County, Pennsylvania, USA. We discuss the results of our analysis to-date, and also highlight key problems and data bias issues that present challenges for model evaluation and deployment.}
}
"
FACCT,2018,Fairness in Machine Learning: Lessons from Political Philosophy,https://proceedings.mlr.press/v81/binns18a.html,"['Binns, Reuben']",386,"What does it mean for a machine learning model to be ‚Äòfair‚Äô, in terms which can be operationalised? Should fairness consist of ensuring everyone has an equal probability of obtaining some benefit, or should we aim instead to minimise the harms to the least advantaged? Can the relevant ideal be determined by reference to some alternative state of affairs in which a particular social pattern of discrimination does not exist? Various definitions proposed in recent literature make different assumptions about what terms like discrimination and fairness mean and how they can be defined in mathematical terms. Questions of discrimination, egalitarianism and justice are of significant interest to moral and political philosophers, who have expended significant efforts in formalising and defending these central concepts. It is therefore unsurprising that attempts to formalise ‚Äòfairness‚Äô in machine learning contain echoes of these old philosophical debates. This paper draws on existing work in moral and political philosophy in order to elucidate emerging debates about fair machine learning.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v81-binns18a,
  title = 	 {Fairness in Machine Learning: Lessons from Political Philosophy},
  author = 	 {Binns, Reuben},
  booktitle = 	 {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
  pages = 	 {149--159},
  year = 	 {2018},
  editor = 	 {Friedler, Sorelle A. and Wilson, Christo},
  volume = 	 {81},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--24 Feb},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v81/binns18a/binns18a.pdf},
  url = 	 {https://proceedings.mlr.press/v81/binns18a.html},
  abstract = 	 {What does it mean for a machine learning model to be ‘fair’, in terms which can be operationalised? Should fairness consist of ensuring everyone has an equal probability of obtaining some benefit, or should we aim instead to minimise the harms to the least advantaged? Can the relevant ideal be determined by reference to some alternative state of affairs in which a particular social pattern of discrimination does not exist? Various definitions proposed in recent literature make different assumptions about what terms like discrimination and fairness mean and how they can be defined in mathematical terms. Questions of discrimination, egalitarianism and justice are of significant interest to moral and political philosophers, who have expended significant efforts in formalising and defending these central concepts. It is therefore unsurprising that attempts to formalise ‘fairness’ in machine learning contain echoes of these old philosophical debates. This paper draws on existing work in moral and political philosophy in order to elucidate emerging debates about fair machine learning.}
}
"
FACCT,2018,Runaway Feedback Loops in Predictive Policing,https://proceedings.mlr.press/v81/ensign18a.html,"['Ensign, Danielle', 'Friedler, Sorelle A.', 'Neville, Scott', 'Scheidegger, Carlos', 'Venkatasubramanian, Suresh']",346,"Predictive policing systems are increasingly used to determine how to allocate police across a city in order to best prevent crime. Discovered crime data (e.g., arrest counts) are used to help update the model, and the process is repeated. Such systems have been shown susceptible to runaway feedback loops, where police are repeatedly sent back to the same neighborhoods regardless of the true crime rate. In response, we develop a mathematical model of predictive policing that proves why this feedback loop occurs, show empirically that this model exhibits such problems, and demonstrate how to change the inputs to a predictive policing system (in a black-box manner) so the runaway feedback loop does not occur, allowing the true crime rate to be learned. Our results are quantitative: we can establish a link (in our model) between the degree to which runaway feedback causes problems and the disparity in crime rates between areas. Moreover, we can also demonstrate the way in which reported incidents of crime (those reported by residents) and discovered incidents of crime (i.e those directly observed by police officers dispatched as a result of the predictive policing algorithm) interact: in brief, while reported incidents can attenuate the degree of runaway feedback, they cannot entirely remove it without the interventions we suggest.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v81-ensign18a,
  title = 	 {Runaway Feedback Loops in Predictive Policing},
  author = 	 {Ensign, Danielle and Friedler, Sorelle A. and Neville, Scott and Scheidegger, Carlos and Venkatasubramanian, Suresh},
  booktitle = 	 {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
  pages = 	 {160--171},
  year = 	 {2018},
  editor = 	 {Friedler, Sorelle A. and Wilson, Christo},
  volume = 	 {81},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--24 Feb},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v81/ensign18a/ensign18a.pdf},
  url = 	 {https://proceedings.mlr.press/v81/ensign18a.html},
  abstract = 	 {Predictive policing systems are increasingly used to determine how to allocate police across a city in order to best prevent crime. Discovered crime data (e.g., arrest counts) are used to help update the model, and the process is repeated. Such systems have been shown susceptible to runaway feedback loops, where police are repeatedly sent back to the same neighborhoods regardless of the true crime rate.  In response, we develop a mathematical model of predictive policing that proves why this feedback loop occurs, show empirically that this model exhibits such problems, and demonstrate how to change the inputs to a predictive policing system (in a black-box manner) so the runaway feedback loop does not occur, allowing the true crime rate to be learned.   Our results are quantitative: we can establish a link (in our model) between the degree to which runaway feedback causes problems and the disparity in crime rates between areas. Moreover, we can also demonstrate the way in which reported incidents of crime (those reported by residents) and discovered incidents of crime (i.e those directly observed by police officers dispatched as a result of the predictive policing algorithm) interact: in brief, while reported incidents can attenuate the degree of runaway feedback, they cannot entirely remove it without the interventions we suggest.}
}
"
FACCT,2018,Recommendation Independence,https://proceedings.mlr.press/v81/kamishima18a.html,"['Kamishima, Toshihiro', 'Akaho, Shotaro', 'Asoh, Hideki', 'Sakuma, Jun']",59,"This paper studies a recommendation algorithm whose outcomes are not influenced by specified information. It is useful in contexts potentially unfair decision should be avoided, such as job-applicant recommendations that are not influenced by socially sensitive information. An algorithm that could exclude the influence of sensitive information would thus be useful for job-matching with fairness. We call the condition between a recommendation outcome and a sensitive feature Recommendation Independence, which is formally defined as statistical independence between the outcome and the feature. Our previous independence-enhanced algorithms simply matched the means of predictions between sub-datasets consisting of the same sensitive value. However, this approach could not remove the sensitive information represented by the second or higher moments of distributions. In this paper, we develop new methods that can deal with the second moment, i.e., variance, of recommendation outcomes without increasing the computational complexity. These methods can more strictly remove the sensitive information, and experimental results demonstrate that our new algorithms can more effectively eliminate the factors that undermine fairness. Additionally, we explore potential applications for independence-enhanced recommendation, and discuss its relation to other concepts, such as recommendation diversity.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v81-kamishima18a,
  title =          {Recommendation Independence},
  author =          {Kamishima, Toshihiro and Akaho, Shotaro and Asoh, Hideki and Sakuma, Jun},
  booktitle =          {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
  pages =          {187--201},
  year =          {2018},
  editor =          {Friedler, Sorelle A. and Wilson, Christo},
  volume =          {81},
  series =          {Proceedings of Machine Learning Research},
  month =          {23--24 Feb},
  publisher =    {PMLR},
  pdf =          {http://proceedings.mlr.press/v81/kamishima18a/kamishima18a.pdf},
  url =          {https://proceedings.mlr.press/v81/kamishima18a.html},
  abstract =          {This paper studies a recommendation algorithm whose outcomes are not influenced by specified information. It is useful in contexts potentially unfair decision should be avoided, such as job-applicant recommendations that are not influenced by socially sensitive information. An algorithm that could exclude the influence of sensitive information would thus be useful for job-matching with fairness. We call the condition between a recommendation outcome and a sensitive feature Recommendation Independence, which is formally defined as statistical independence between the outcome and the feature. Our previous independence-enhanced algorithms simply matched the means of predictions between sub-datasets consisting of the same sensitive value. However, this approach could not remove the sensitive information represented by the second or higher moments of distributions. In this paper, we develop new methods that can deal with the second moment, i.e., variance, of recommendation outcomes without increasing the computational complexity. These methods can more strictly remove the sensitive information, and experimental results demonstrate that our new algorithms can more effectively eliminate the factors that undermine fairness. Additionally, we explore potential applications for independence-enhanced recommendation, and discuss its relation to other concepts, such as recommendation diversity.}
}
"
FACCT,2018,Balanced Neighborhoods for Multi sided Fairness in Recommendation,https://proceedings.mlr.press/v81/burke18a.html,"['Burke, Robin', 'Sonboli, Nasim', 'Ordonez-Gauger, Aldo']",153,"Fairness has emerged as an important category of analysis for machine learning systems in some application areas. In extending the concept of fairness to recommender systems, there is an essential tension between the goals of fairness and those of personalization. However, there are contexts in which equity across recommendation outcomes is a desirable goal. It is also the case that in some applications fairness may be a multisided concept, in which the impacts on multiple groups of individuals must be considered. In this paper, we examine two different cases of fairness-aware recommender systems: consumer-centered and provider-centered. We explore the concept of a balanced neighborhood as a mechanism to preserve personalization in recommendation while enhancing the fairness of recommendation outcomes. We show that a modified version of the Sparse Linear Method (SLIM) can be used to improve the balance of user and item neighborhoods, with the result of achieving greater outcome fairness in real-world datasets with minimal loss in ranking performance.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v81-burke18a,
  title =          {Balanced Neighborhoods for Multi-sided Fairness in Recommendation},
  author =          {Burke, Robin and Sonboli, Nasim and Ordonez-Gauger, Aldo},
  booktitle =          {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
  pages =          {202--214},
  year =          {2018},
  editor =          {Friedler, Sorelle A. and Wilson, Christo},
  volume =          {81},
  series =          {Proceedings of Machine Learning Research},
  month =          {23--24 Feb},
  publisher =    {PMLR},
  pdf =          {http://proceedings.mlr.press/v81/burke18a/burke18a.pdf},
  url =          {https://proceedings.mlr.press/v81/burke18a.html},
  abstract =          {Fairness has emerged as an important category of analysis for machine learning systems in some application areas. In extending the concept of fairness to recommender systems, there is an essential tension between the goals of fairness and those of personalization. However, there are contexts in which  equity across recommendation outcomes is a desirable goal. It is also the case that in some applications fairness may be a multisided concept, in which the impacts on multiple groups of individuals must be considered. In this paper, we examine two different cases of fairness-aware recommender systems: consumer-centered and provider-centered. We  explore the concept of a balanced neighborhood as a mechanism to preserve personalization in recommendation while enhancing the fairness of recommendation outcomes. We show that a modified version of the Sparse Linear Method (SLIM) can be used to improve the balance of user and item neighborhoods, with the result of achieving greater outcome fairness in real-world datasets with minimal loss in ranking performance.}
}
"
FACCT,2019,Problem Formulation and Fairness,https://dl.acm.org/doi/10.1145/3287560.3287567,"['Samir Passi', 'Solon Barocas']",128,"Formulating data science problems is an uncertain and difficult process. It requires various forms of discretionary work to translate high-level objectives or strategic goals into tractable problems, necessitating, among other things, the identification of appropriate target variables and proxies. While these choices are rarely self-evident, normative assessments of data science projects often take them for granted, even though different translations can raise profoundly different ethical concerns. Whether we consider a data science project fair often has as much to do with the formulation of the problem as any property of the resulting model. Building on six months of ethnographic fieldwork with a corporate data science team---and channeling ideas from sociology and history of science, critical data studies, and early writing on knowledge discovery in databases---we describe the complex set of actors and activities involved in problem formulation. Our research demonstrates that the specification and operationalization of the problem are always negotiated and elastic, and rarely worked out with explicit normative considerations in mind. In so doing, we show that careful accounts of everyday data science work can help us better understand how and why data science problems are posed in certain ways---and why specific formulations prevail in practice, even in the face of what might seem like normatively preferable alternatives. We conclude by discussing the implications of our findings, arguing that effective normative interventions will require attending to the practical work of problem formulation.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3287560.3287567,
author = {Passi, Samir and Barocas, Solon},
title = {Problem Formulation and Fairness},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287567},
doi = {10.1145/3287560.3287567},
abstract = {Formulating data science problems is an uncertain and difficult process. It requires various forms of discretionary work to translate high-level objectives or strategic goals into tractable problems, necessitating, among other things, the identification of appropriate target variables and proxies. While these choices are rarely self-evident, normative assessments of data science projects often take them for granted, even though different translations can raise profoundly different ethical concerns. Whether we consider a data science project fair often has as much to do with the formulation of the problem as any property of the resulting model. Building on six months of ethnographic fieldwork with a corporate data science team---and channeling ideas from sociology and history of science, critical data studies, and early writing on knowledge discovery in databases---we describe the complex set of actors and activities involved in problem formulation. Our research demonstrates that the specification and operationalization of the problem are always negotiated and elastic, and rarely worked out with explicit normative considerations in mind. In so doing, we show that careful accounts of everyday data science work can help us better understand how and why data science problems are posed in certain ways---and why specific formulations prevail in practice, even in the face of what might seem like normatively preferable alternatives. We conclude by discussing the implications of our findings, arguing that effective normative interventions will require attending to the practical work of problem formulation.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {39–48},
numpages = {10},
keywords = {Fairness, Machine Learning, Data Science, Problem Formulation, Target Variable},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}
  
"
FACCT,2019,50 Years of Test (Un)fairness: Lessons for Machine Learning,https://dl.acm.org/doi/10.1145/3287560.3287600,"['Ben Hutchinson', 'Margaret Mitchell']",253,"Quantitative definitions of what is unfair and what is fair have been introduced in multiple disciplines for well over 50 years, including in education, hiring, and machine learning. We trace how the notion of fairness has been defined within the testing communities of education and hiring over the past half century, exploring the cultural and social context in which different fairness definitions have emerged. In some cases, earlier definitions of fairness are similar or identical to definitions of fairness in current machine learning research, and foreshadow current formal work. In other cases, insights into what fairness means and how to measure it have largely gone overlooked. We compare past and current notions of fairness along several dimensions, including the fairness criteria, the focus of the criteria (e.g., a test, a model, or its use), the relationship of fairness to individuals, groups, and subgroups, and the mathematical method for measuring fairness (e.g., classification, regression). This work points the way towards future research and measurement of (un)fairness that builds from our modern understanding of fairness while incorporating insights from the past.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3287560.3287600,
author = {Hutchinson, Ben and Mitchell, Margaret},
title = {50 Years of Test (Un)Fairness: Lessons for Machine Learning},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287600},
doi = {10.1145/3287560.3287600},
abstract = {Quantitative definitions of what is unfair and what is fair have been introduced in multiple disciplines for well over 50 years, including in education, hiring, and machine learning. We trace how the notion of fairness has been defined within the testing communities of education and hiring over the past half century, exploring the cultural and social context in which different fairness definitions have emerged. In some cases, earlier definitions of fairness are similar or identical to definitions of fairness in current machine learning research, and foreshadow current formal work. In other cases, insights into what fairness means and how to measure it have largely gone overlooked. We compare past and current notions of fairness along several dimensions, including the fairness criteria, the focus of the criteria (e.g., a test, a model, or its use), the relationship of fairness to individuals, groups, and subgroups, and the mathematical method for measuring fairness (e.g., classification, regression). This work points the way towards future research and measurement of (un)fairness that builds from our modern understanding of fairness while incorporating insights from the past.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {49–58},
numpages = {10},
keywords = {ML fairness, fairness, history, psychometrics, test fairness},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}
  
"
FACCT,2019,Fairness and Abstraction in Sociotechnical Systems,https://dl.acm.org/doi/10.1145/3287560.3287598,"['Andrew D. Selbst', 'Danah Boyd', 'Sorelle A. Friedler', 'Suresh Venkatasubramanian', 'Janet Vertesi']",582,"A key goal of the fair-ML community is to develop machine-learning based systems that, once introduced into a social context, can achieve social and legal outcomes such as fairness, justice, and due process. Bedrock concepts in computer science---such as abstraction and modular design---are used to define notions of fairness and discrimination, to produce fairness-aware learning algorithms, and to intervene at different stages of a decision-making pipeline to produce ""fair"" outcomes. In this paper, however, we contend that these concepts render technical interventions ineffective, inaccurate, and sometimes dangerously misguided when they enter the societal context that surrounds decision-making systems. We outline this mismatch with five ""traps"" that fair-ML work can fall into even as it attempts to be more context-aware in comparison to traditional data science. We draw on studies of sociotechnical systems in Science and Technology Studies to explain why such traps occur and how to avoid them. Finally, we suggest ways in which technical designers can mitigate the traps through a refocusing of design in terms of process rather than solutions, and by drawing abstraction boundaries to include social actors rather than purely technical ones.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3287560.3287598,
author = {Selbst, Andrew D. and Boyd, Danah and Friedler, Sorelle A. and Venkatasubramanian, Suresh and Vertesi, Janet},
title = {Fairness and Abstraction in Sociotechnical Systems},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287598},
doi = {10.1145/3287560.3287598},
abstract = {A key goal of the fair-ML community is to develop machine-learning based systems that, once introduced into a social context, can achieve social and legal outcomes such as fairness, justice, and due process. Bedrock concepts in computer science---such as abstraction and modular design---are used to define notions of fairness and discrimination, to produce fairness-aware learning algorithms, and to intervene at different stages of a decision-making pipeline to produce ""fair"" outcomes. In this paper, however, we contend that these concepts render technical interventions ineffective, inaccurate, and sometimes dangerously misguided when they enter the societal context that surrounds decision-making systems. We outline this mismatch with five ""traps"" that fair-ML work can fall into even as it attempts to be more context-aware in comparison to traditional data science. We draw on studies of sociotechnical systems in Science and Technology Studies to explain why such traps occur and how to avoid them. Finally, we suggest ways in which technical designers can mitigate the traps through a refocusing of design in terms of process rather than solutions, and by drawing abstraction boundaries to include social actors rather than purely technical ones.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {59–68},
numpages = {10},
keywords = {Interdisciplinary, Sociotechnical Systems, Fairness-aware Machine Learning},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}
  
"
FACCT,2019,Disparate Interactions: An Algorithm-in-the-Loop Analysis of Fairness in Risk Assessments,https://dl.acm.org/doi/10.1145/3287560.3287563,"['Ben Green', 'Yiling Chen']",198,"Despite vigorous debates about the technical characteristics of risk assessments being deployed in the U.S. criminal justice system, remarkably little research has studied how these tools affect actual decision-making processes. After all, risk assessments do not make definitive decisions---they inform judges, who are the final arbiters. It is therefore essential that considerations of risk assessments be informed by rigorous studies of how judges actually interpret and use them. This paper takes a first step toward such research on human interactions with risk assessments through a controlled experimental study on Amazon Mechanical Turk. We found several behaviors that call into question the supposed efficacy and fairness of risk assessments: our study participants 1) underperformed the risk assessment even when presented with its predictions, 2) could not effectively evaluate the accuracy of their own or the risk assessment's predictions, and 3) exhibited behaviors fraught with ""disparate interactions,"" whereby the use of risk assessments led to higher risk predictions about black defendants and lower risk predictions about white defendants. These results suggest the need for a new ""algorithm-in-the-loop"" framework that places machine learning decision-making aids into the sociotechnical context of improving human decisions rather than the technical context of generating the best prediction in the abstract. If risk assessments are to be used at all, they must be grounded in rigorous evaluations of their real-world impacts instead of in their theoretical potential.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3287560.3287563,
author = {Green, Ben and Chen, Yiling},
title = {Disparate Interactions: An Algorithm-in-the-Loop Analysis of Fairness in Risk Assessments},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287563},
doi = {10.1145/3287560.3287563},
abstract = {Despite vigorous debates about the technical characteristics of risk assessments being deployed in the U.S. criminal justice system, remarkably little research has studied how these tools affect actual decision-making processes. After all, risk assessments do not make definitive decisions---they inform judges, who are the final arbiters. It is therefore essential that considerations of risk assessments be informed by rigorous studies of how judges actually interpret and use them. This paper takes a first step toward such research on human interactions with risk assessments through a controlled experimental study on Amazon Mechanical Turk. We found several behaviors that call into question the supposed efficacy and fairness of risk assessments: our study participants 1) underperformed the risk assessment even when presented with its predictions, 2) could not effectively evaluate the accuracy of their own or the risk assessment's predictions, and 3) exhibited behaviors fraught with ""disparate interactions,"" whereby the use of risk assessments led to higher risk predictions about black defendants and lower risk predictions about white defendants. These results suggest the need for a new ""algorithm-in-the-loop"" framework that places machine learning decision-making aids into the sociotechnical context of improving human decisions rather than the technical context of generating the best prediction in the abstract. If risk assessments are to be used at all, they must be grounded in rigorous evaluations of their real-world impacts instead of in their theoretical potential.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {90–99},
numpages = {10},
keywords = {Mechanical Turk, risk assessment, fairness, behavioral experiment},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}
  
"
FACCT,2019,An Empirical Study of Rich Subgroup Fairness for Machine Learning,https://dl.acm.org/doi/10.1145/3287560.3287592,"['Michael Kearns', 'Seth Neel', 'Aaron Roth', 'Zhiwei Steven Wu']",138,"Kearns, Neel, Roth, and Wu [ICML 2018] recently proposed a notion of rich subgroup fairness intended to bridge the gap between statistical and individual notions of fairness. Rich subgroup fairness picks a statistical fairness constraint (say, equalizing false positive rates across protected groups), but then asks that this constraint hold over an exponentially or infinitely large collection of subgroups defined by a class of functions with bounded VC dimension. They give an algorithm guaranteed to learn subject to this constraint, under the condition that it has access to oracles for perfectly learning absent a fairness constraint. In this paper, we undertake an extensive empirical evaluation of the algorithm of Kearns et al. On four real datasets for which fairness is a concern, we investigate the basic convergence of the algorithm when instantiated with fast heuristics in place of learning oracles, measure the tradeoffs between fairness and accuracy, and compare this approach with the recent algorithm of Agarwal, Beygelzeimer, Dudik, Langford, and Wallach [ICML 2018], which implements weaker and more traditional marginal fairness constraints defined by individual protected attributes. We find that in general, the Kearns et al. algorithm converges quickly, large gains in fairness can be obtained with mild costs to accuracy, and that optimizing accuracy subject only to marginal fairness leads to classifiers with substantial subgroup unfairness. We also provide a number of analyses and visualizations of the dynamics and behavior of the Kearns et al. algorithm. Overall we find this algorithm to be effective on real data, and rich subgroup fairness to be a viable notion in practice.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3287560.3287592,
author = {Kearns, Michael and Neel, Seth and Roth, Aaron and Wu, Zhiwei Steven},
title = {An Empirical Study of Rich Subgroup Fairness for Machine Learning},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287592},
doi = {10.1145/3287560.3287592},
abstract = {Kearns, Neel, Roth, and Wu [ICML 2018] recently proposed a notion of rich subgroup fairness intended to bridge the gap between statistical and individual notions of fairness. Rich subgroup fairness picks a statistical fairness constraint (say, equalizing false positive rates across protected groups), but then asks that this constraint hold over an exponentially or infinitely large collection of subgroups defined by a class of functions with bounded VC dimension. They give an algorithm guaranteed to learn subject to this constraint, under the condition that it has access to oracles for perfectly learning absent a fairness constraint. In this paper, we undertake an extensive empirical evaluation of the algorithm of Kearns et al. On four real datasets for which fairness is a concern, we investigate the basic convergence of the algorithm when instantiated with fast heuristics in place of learning oracles, measure the tradeoffs between fairness and accuracy, and compare this approach with the recent algorithm of Agarwal, Beygelzeimer, Dudik, Langford, and Wallach [ICML 2018], which implements weaker and more traditional marginal fairness constraints defined by individual protected attributes. We find that in general, the Kearns et al. algorithm converges quickly, large gains in fairness can be obtained with mild costs to accuracy, and that optimizing accuracy subject only to marginal fairness leads to classifiers with substantial subgroup unfairness. We also provide a number of analyses and visualizations of the dynamics and behavior of the Kearns et al. algorithm. Overall we find this algorithm to be effective on real data, and rich subgroup fairness to be a viable notion in practice.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {100–109},
numpages = {10},
keywords = {Fair Classification, Fairness Auditing, Algorithmic Bias, Subgroup Fairness},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}
  
"
FACCT,2019,Equality of Voice: Towards Fair Representation in Crowdsourced Top-K Recommendations,https://dl.acm.org/doi/10.1145/3287560.3287570,"['Abhijnan Chakraborty', 'Gourab K. Patro', 'Niloy Ganguly', 'Krishna P. Gummadi', 'Patrick Loiseau']",60,"To help their users to discover important items at a particular time, major websites like Twitter, Yelp, TripAdvisor or NYTimes provide Top-K recommendations (e.g., 10 Trending Topics, Top 5 Hotels in Paris or 10 Most Viewed News Stories), which rely on crowdsourced popularity signals to select the items. However, different sections of a crowd may have different preferences, and there is a large silent majority who do not explicitly express their opinion. Also, the crowd often consists of actors like bots, spammers, or people running orchestrated campaigns. Recommendation algorithms today largely do not consider such nuances, hence are vulnerable to strategic manipulation by small but hyper-active user groups. To fairly aggregate the preferences of all users while recommending top-K items, we borrow ideas from prior research on social choice theory, and identify a voting mechanism called Single Transferable Vote (STV) as having many of the fairness properties we desire in top-K item (s)elections. We develop an innovative mechanism to attribute preferences of silent majority which also make STV completely operational. We show the generalizability of our approach by implementing it on two different real-world datasets. Through extensive experimentation and comparison with state-of-the-art techniques, we show that our proposed approach provides maximum user satisfaction, and cuts down drastically on items disliked by most but hyper-actively promoted by a few users.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3287560.3287570,
author = {Chakraborty, Abhijnan and Patro, Gourab K. and Ganguly, Niloy and Gummadi, Krishna P. and Loiseau, Patrick},
title = {Equality of Voice: Towards Fair Representation in Crowdsourced Top-K Recommendations},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287570},
doi = {10.1145/3287560.3287570},
abstract = {To help their users to discover important items at a particular time, major websites like Twitter, Yelp, TripAdvisor or NYTimes provide Top-K recommendations (e.g., 10 Trending Topics, Top 5 Hotels in Paris or 10 Most Viewed News Stories), which rely on crowdsourced popularity signals to select the items. However, different sections of a crowd may have different preferences, and there is a large silent majority who do not explicitly express their opinion. Also, the crowd often consists of actors like bots, spammers, or people running orchestrated campaigns. Recommendation algorithms today largely do not consider such nuances, hence are vulnerable to strategic manipulation by small but hyper-active user groups.To fairly aggregate the preferences of all users while recommending top-K items, we borrow ideas from prior research on social choice theory, and identify a voting mechanism called Single Transferable Vote (STV) as having many of the fairness properties we desire in top-K item (s)elections. We develop an innovative mechanism to attribute preferences of silent majority which also make STV completely operational. We show the generalizability of our approach by implementing it on two different real-world datasets. Through extensive experimentation and comparison with state-of-the-art techniques, we show that our proposed approach provides maximum user satisfaction, and cuts down drastically on items disliked by most but hyper-actively promoted by a few users.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {129–138},
numpages = {10},
keywords = {Fairness in Recommendation, Twitter Trends, Most Popular News, Fair Representation, Top-K Recommendation},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}
  
"
FACCT,2019,Fair Algorithms for Learning in Allocation Problems,https://dl.acm.org/doi/10.1145/3287560.3287571,"['Hadi Elzayn', 'Shahin Jabbari', 'Christopher Jung', 'Michael Kearns', 'Seth Neel', 'Aaron Roth', 'Zachary Schutzman']",66,"Settings such as lending and policing can be modeled by a centralized agent allocating a scarce resource (e.g. loans or police officers) amongst several groups, in order to maximize some objective (e.g. loans given that are repaid, or criminals that are apprehended). Often in such problems fairness is also a concern. One natural notion of fairness, based on general principles of equality of opportunity, asks that conditional on an individual being a candidate for the resource in question, the probability of actually receiving it is approximately independent of the individual's group. For example, in lending this would mean that equally creditworthy individuals in different racial groups have roughly equal chances of receiving a loan. In policing it would mean that two individuals committing the same crime in different districts would have roughly equal chances of being arrested. In this paper, we formalize this general notion of fairness for allocation problems and investigate its algorithmic consequences. Our main technical results include an efficient learning algorithm that converges to an optimal fair allocation even when the allocator does not know the frequency of candidates (i.e. creditworthy individuals or criminals) in each group. This algorithm operates in a censored feedback model in which only the number of candidates who received the resource in a given allocation can be observed, rather than the true number of candidates in each group. This models the fact that we do not learn the creditworthiness of individuals we do not give loans to and do not learn about crimes committed if the police presence in a district is low. As an application of our framework and algorithm, we consider the predictive policing problem, in which the resource being allocated to each group is the number of police officers assigned to each district. The learning algorithm is trained on arrest data gathered from its own deployments on previous days, resulting in a potential feedback loop that our algorithm provably overcomes. In this case, the fairness constraint asks that the probability that an individual who has committed a crime is arrested should be independent of the district in which they live. We investigate the performance of our learning algorithm on the Philadelphia Crime Incidents dataset.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3287560.3287571,
author = {Elzayn, Hadi and Jabbari, Shahin and Jung, Christopher and Kearns, Michael and Neel, Seth and Roth, Aaron and Schutzman, Zachary},
title = {Fair Algorithms for Learning in Allocation Problems},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287571},
doi = {10.1145/3287560.3287571},
abstract = {Settings such as lending and policing can be modeled by a centralized agent allocating a scarce resource (e.g. loans or police officers) amongst several groups, in order to maximize some objective (e.g. loans given that are repaid, or criminals that are apprehended). Often in such problems fairness is also a concern. One natural notion of fairness, based on general principles of equality of opportunity, asks that conditional on an individual being a candidate for the resource in question, the probability of actually receiving it is approximately independent of the individual's group. For example, in lending this would mean that equally creditworthy individuals in different racial groups have roughly equal chances of receiving a loan. In policing it would mean that two individuals committing the same crime in different districts would have roughly equal chances of being arrested.In this paper, we formalize this general notion of fairness for allocation problems and investigate its algorithmic consequences. Our main technical results include an efficient learning algorithm that converges to an optimal fair allocation even when the allocator does not know the frequency of candidates (i.e. creditworthy individuals or criminals) in each group. This algorithm operates in a censored feedback model in which only the number of candidates who received the resource in a given allocation can be observed, rather than the true number of candidates in each group. This models the fact that we do not learn the creditworthiness of individuals we do not give loans to and do not learn about crimes committed if the police presence in a district is low.As an application of our framework and algorithm, we consider the predictive policing problem, in which the resource being allocated to each group is the number of police officers assigned to each district. The learning algorithm is trained on arrest data gathered from its own deployments on previous days, resulting in a potential feedback loop that our algorithm provably overcomes. In this case, the fairness constraint asks that the probability that an individual who has committed a crime is arrested should be independent of the district in which they live. We investigate the performance of our learning algorithm on the Philadelphia Crime Incidents dataset.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {170–179},
numpages = {10},
keywords = {online learning, censored feedback, algorithmic fairness, resource allocation},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}
  
"
FACCT,2019,Fair Allocation through Competitive Equilibrium from Generic Incomes,https://dl.acm.org/doi/10.1145/3287560.3287582,"['Moshe Babaioff', 'Noam Nisan', 'Inbal Talgam-Cohen']",25,"Two food banks catering to populations of different sizes with different needs must divide among themselves a donation of food items. What constitutes a ""fair"" allocation of the items among them? Competitive equilibrium from equal incomes (CEEI) is a classic solution to the problem of fair and efficient allocation of goods among equally entitled agents [Foley 1967, Varian 1974]. Every agent (foodbank) receives an equal endowment of artificial currency with which to ""purchase"" bundles of goods (food items). Prices for the goods are set high enough such that the agents can simultaneously get their favorite within-budget bundle, and low enough such that all goods are allocated (no waste). A CEEI satisfies mathematical notions of fairness like fair-share, and also has built-in transparency -- prices can be published so the agents can verify they're being treated equally. However, a CEEI is not guaranteed to exist when the items are indivisible. We study competitive equilibrium from generic incomes (CEGI), which is based on the idea of slightly perturbed endowments, and enjoys similar fairness, efficiency and transparency properties as CEEI. We show that when the two agents have almost equal endowments and additive preferences for the items, a CEGI always exists. We then consider agents who are a priori non-equal (like different-sized foodbanks); we formulate a new notion of fair allocation among non-equals satisfied by CEGI, and show existence in cases of interest (like when the agents have identical preferences). Experiments on simulated and Spliddit data (a popular fair division website) indicate more general existence. Our results open opportunities for future research on fairness through generic endowments, and on fair treatment of non-equals.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3287560.3287582,
author = {Babaioff, Moshe and Nisan, Noam and Talgam-Cohen, Inbal},
title = {Fair Allocation through Competitive Equilibrium from Generic Incomes},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287582},
doi = {10.1145/3287560.3287582},
abstract = {Two food banks catering to populations of different sizes with different needs must divide among themselves a donation of food items. What constitutes a ""fair"" allocation of the items among them?Competitive equilibrium from equal incomes (CEEI) is a classic solution to the problem of fair and efficient allocation of goods among equally entitled agents [Foley 1967, Varian 1974]. Every agent (foodbank) receives an equal endowment of artificial currency with which to ""purchase"" bundles of goods (food items). Prices for the goods are set high enough such that the agents can simultaneously get their favorite within-budget bundle, and low enough such that all goods are allocated (no waste). A CEEI satisfies mathematical notions of fairness like fair-share, and also has built-in transparency -- prices can be published so the agents can verify they're being treated equally. However, a CEEI is not guaranteed to exist when the items are indivisible.We study competitive equilibrium from generic incomes (CEGI), which is based on the idea of slightly perturbed endowments, and enjoys similar fairness, efficiency and transparency properties as CEEI. We show that when the two agents have almost equal endowments and additive preferences for the items, a CEGI always exists. We then consider agents who are a priori non-equal (like different-sized foodbanks); we formulate a new notion of fair allocation among non-equals satisfied by CEGI, and show existence in cases of interest (like when the agents have identical preferences). Experiments on simulated and Spliddit data (a popular fair division website) indicate more general existence. Our results open opportunities for future research on fairness through generic endowments, and on fair treatment of non-equals.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {180},
numpages = {1},
keywords = {Fisher markets, additive preferences, Market equilibrium, fairness, unequal entitlements},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}
  
"
FACCT,2019,A Moral Framework for Understanding Fair ML through Economic Models of Equality of Opportunity,https://dl.acm.org/doi/10.1145/3287560.3287584,"['Hoda Heidari', 'Michele Loi', 'Krishna P. Gummadi', 'Andreas Krause']",107,"We map the recently proposed notions of algorithmic fairness to economic models of Equality of opportunity (EOP)---an extensively studied ideal of fairness in political philosophy. We formally show that through our conceptual mapping, many existing definition of algorithmic fairness, such as predictive value parity and equality of odds, can be interpreted as special cases of EOP. In this respect, our work serves as a unifying moral framework for understanding existing notions of algorithmic fairness. Most importantly, this framework allows us to explicitly spell out the moral assumptions underlying each notion of fairness, and interpret recent fairness impossibility results in a new light. Last but not least and inspired by luck egalitarian models of EOP, we propose a new family of measures for algorithmic fairness. We illustrate our proposal empirically and show that employing a measure of algorithmic (un)fairness when its underlying moral assumptions are not satisfied, can have devastating consequences for the disadvantaged group's welfare.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3287560.3287584,
author = {Heidari, Hoda and Loi, Michele and Gummadi, Krishna P. and Krause, Andreas},
title = {A Moral Framework for Understanding Fair ML through Economic Models of Equality of Opportunity},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287584},
doi = {10.1145/3287560.3287584},
abstract = {We map the recently proposed notions of algorithmic fairness to economic models of Equality of opportunity (EOP)---an extensively studied ideal of fairness in political philosophy. We formally show that through our conceptual mapping, many existing definition of algorithmic fairness, such as predictive value parity and equality of odds, can be interpreted as special cases of EOP. In this respect, our work serves as a unifying moral framework for understanding existing notions of algorithmic fairness. Most importantly, this framework allows us to explicitly spell out the moral assumptions underlying each notion of fairness, and interpret recent fairness impossibility results in a new light. Last but not least and inspired by luck egalitarian models of EOP, we propose a new family of measures for algorithmic fairness. We illustrate our proposal empirically and show that employing a measure of algorithmic (un)fairness when its underlying moral assumptions are not satisfied, can have devastating consequences for the disadvantaged group's welfare.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {181–190},
numpages = {10},
keywords = {Equality of Odds, Equality of Opportunity (EOP), Rawlsian and Luck Egalitarian EOP, Statistical Parity, Fairness for Machine Learning, Predictive Value Parity},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}
  
"
FACCT,2019,Beyond Open vs. Closed: Balancing Individual Privacy and Public Accountability in Data Sharing,https://dl.acm.org/doi/10.1145/3287560.3287577,"['Meg Young', 'Luke Rodriguez', 'Emily Keller', 'Feiyang Sun', 'Boyang Sa', 'Jan Whittington', 'Bill Howe']",37,"Data too sensitive to be ""open"" for analysis and re-purposing typically remains ""closed"" as proprietary information. This dichotomy undermines efforts to make algorithmic systems more fair, transparent, and accountable. Access to proprietary data in particular is needed by government agencies to enforce policy, researchers to evaluate methods, and the public to hold agencies accountable; all of these needs must be met while preserving individual privacy and firm competitiveness. In this paper, we describe an integrated legal-technical approach provided by a third-party public-private data trust designed to balance these competing interests. Basic membership allows firms and agencies to enable low-risk access to data for compliance reporting and core methods research, while modular data sharing agreements support a wide array of projects and use cases. Unless specifically stated otherwise in an agreement, all data access is initially provided to end users through customized synthetic datasets that offer a) strong privacy guarantees, b) removal of signals that could expose competitive advantage, and c) removal of biases that could reinforce discriminatory policies, all while maintaining fidelity to the original data. We find that using synthetic data in conjunction with strong legal protections over raw data strikes a balance between transparency, proprietorship, privacy, and research objectives. This legal-technical framework can form the basis for data trusts in a variety of contexts.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3287560.3287577,
author = {Young, Meg and Rodriguez, Luke and Keller, Emily and Sun, Feiyang and Sa, Boyang and Whittington, Jan and Howe, Bill},
title = {Beyond Open vs. Closed: Balancing Individual Privacy and Public Accountability in Data Sharing},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287577},
doi = {10.1145/3287560.3287577},
abstract = {Data too sensitive to be ""open"" for analysis and re-purposing typically remains ""closed"" as proprietary information. This dichotomy undermines efforts to make algorithmic systems more fair, transparent, and accountable. Access to proprietary data in particular is needed by government agencies to enforce policy, researchers to evaluate methods, and the public to hold agencies accountable; all of these needs must be met while preserving individual privacy and firm competitiveness. In this paper, we describe an integrated legal-technical approach provided by a third-party public-private data trust designed to balance these competing interests. Basic membership allows firms and agencies to enable low-risk access to data for compliance reporting and core methods research, while modular data sharing agreements support a wide array of projects and use cases. Unless specifically stated otherwise in an agreement, all data access is initially provided to end users through customized synthetic datasets that offer a) strong privacy guarantees, b) removal of signals that could expose competitive advantage, and c) removal of biases that could reinforce discriminatory policies, all while maintaining fidelity to the original data. We find that using synthetic data in conjunction with strong legal protections over raw data strikes a balance between transparency, proprietorship, privacy, and research objectives. This legal-technical framework can form the basis for data trusts in a variety of contexts.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {191–200},
numpages = {10},
keywords = {data sharing, privacy, data ethics, data governance, algorithmic bias},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}
  
"
FACCT,2019,Who's the Guinea Pig?: Investigating Online A/B/n Tests in-the-Wild,https://dl.acm.org/doi/10.1145/3287560.3287565,"['Shan Jiang', 'John Martin', 'Christo Wilson']",18,"A/B/n testing has been adopted by many technology companies as a data-driven approach to product design and optimization. These tests are often run on their websites without explicit consent from users. In this paper, we investigate such online A/B/n tests by using Optimizely as a lens. First, we provide measurement results of 575 websites that use Optimizely drawn from the Alexa Top-1M, and analyze the distributions of their audiences and experiments. Then, we use three case studies to discuss potential ethical pitfalls of such experiments, including involvement of political content, price discrimination, and advertising campaigns. We conclude with a suggestion for greater awareness of ethical concerns inherent in human experimentation and a call for increased transparency among A/B/n test operators.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3287560.3287565,
author = {Jiang, Shan and Martin, John and Wilson, Christo},
title = {Who's the Guinea Pig? Investigating Online A/B/n Tests in-the-Wild},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287565},
doi = {10.1145/3287560.3287565},
abstract = {A/B/n testing has been adopted by many technology companies as a data-driven approach to product design and optimization. These tests are often run on their websites without explicit consent from users. In this paper, we investigate such online A/B/n tests by using Optimizely as a lens. First, we provide measurement results of 575 websites that use Optimizely drawn from the Alexa Top-1M, and analyze the distributions of their audiences and experiments. Then, we use three case studies to discuss potential ethical pitfalls of such experiments, including involvement of political content, price discrimination, and advertising campaigns. We conclude with a suggestion for greater awareness of ethical concerns inherent in human experimentation and a call for increased transparency among A/B/n test operators.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {201–210},
numpages = {10},
keywords = {personalization, A/B/n testing, online controlled experiments},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}
  
"
FACCT,2019,Fairness-Aware Programming,https://dl.acm.org/doi/10.1145/3287560.3287588,"['Aws Albarghouthi', 'Samuel Vinitsky']",27,"Increasingly, programming tasks involve automating and deploying sensitive decision-making processes that may have adverse impacts on individuals or groups of people. The issue of fairness in automated decision-making has thus become a major problem, attracting interdisciplinary attention. In this work, we aim to make fairness a first-class concern in programming. Specifically, we propose fairness-aware programming, where programmers can state fairness expectations natively in their code, and have a runtime system monitor decision-making and report violations of fairness. We present a rich and general specification language that allows a programmer to specify a range of fairness definitions from the literature, as well as others. As the decision-making program executes, the runtime maintains statistics on the decisions made and incrementally checks whether the fairness definitions have been violated, reporting such violations to the developer. The advantages of this approach are two fold: (i) Enabling declarative mathematical specifications of fairness in the programming language simplifies the process of checking fairness, as the programmer does not have to write ad hoc code for maintaining statistics. (ii) Compared to existing techniques for checking and ensuring fairness, our approach monitors a decision-making program in the wild, which may be running on a distribution that is unlike the dataset on which a classifier was trained and tested. We describe an implementation of our proposed methodology as a library in the Python programming language and illustrate its use on case studies from the algorithmic fairness literature.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3287560.3287588,
author = {Albarghouthi, Aws and Vinitsky, Samuel},
title = {Fairness-Aware Programming},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287588},
doi = {10.1145/3287560.3287588},
abstract = {Increasingly, programming tasks involve automating and deploying sensitive decision-making processes that may have adverse impacts on individuals or groups of people. The issue of fairness in automated decision-making has thus become a major problem, attracting interdisciplinary attention. In this work, we aim to make fairness a first-class concern in programming. Specifically, we propose fairness-aware programming, where programmers can state fairness expectations natively in their code, and have a runtime system monitor decision-making and report violations of fairness.We present a rich and general specification language that allows a programmer to specify a range of fairness definitions from the literature, as well as others. As the decision-making program executes, the runtime maintains statistics on the decisions made and incrementally checks whether the fairness definitions have been violated, reporting such violations to the developer. The advantages of this approach are two fold: (i) Enabling declarative mathematical specifications of fairness in the programming language simplifies the process of checking fairness, as the programmer does not have to write ad hoc code for maintaining statistics. (ii) Compared to existing techniques for checking and ensuring fairness, our approach monitors a decision-making program in the wild, which may be running on a distribution that is unlike the dataset on which a classifier was trained and tested.We describe an implementation of our proposed methodology as a library in the Python programming language and illustrate its use on case studies from the algorithmic fairness literature.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {211–219},
numpages = {9},
keywords = {Runtime verification, Fairness, Assertion languages, Runtime monitoring, Probabilistic specifications},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}
  
"
FACCT,2019,The Social Cost of Strategic Classification,https://dl.acm.org/doi/10.1145/3287560.3287576,"['Smitha Milli', 'John Miller', 'Anca D. Dragan', 'Moritz Hardt']",125,"Consequential decision-making typically incentivizes individuals to behave strategically, tailoring their behavior to the specifics of the decision rule. A long line of work has therefore sought to counteract strategic behavior by designing more conservative decision boundaries in an effort to increase robustness to the effects of strategic covariate shift. We show that these efforts benefit the institutional decision maker at the expense of the individuals being classified. Introducing a notion of social burden, we prove that any increase in institutional utility necessarily leads to a corresponding increase in social burden. Moreover, we show that the negative externalities of strategic classification can disproportionately harm disadvantaged groups in the population. Our results highlight that strategy-robustness must be weighed against considerations of social welfare and fairness.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3287560.3287576,
author = {Milli, Smitha and Miller, John and Dragan, Anca D. and Hardt, Moritz},
title = {The Social Cost of Strategic Classification},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287576},
doi = {10.1145/3287560.3287576},
abstract = {Consequential decision-making typically incentivizes individuals to behave strategically, tailoring their behavior to the specifics of the decision rule. A long line of work has therefore sought to counteract strategic behavior by designing more conservative decision boundaries in an effort to increase robustness to the effects of strategic covariate shift.We show that these efforts benefit the institutional decision maker at the expense of the individuals being classified. Introducing a notion of social burden, we prove that any increase in institutional utility necessarily leads to a corresponding increase in social burden. Moreover, we show that the negative externalities of strategic classification can disproportionately harm disadvantaged groups in the population.Our results highlight that strategy-robustness must be weighed against considerations of social welfare and fairness.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {230–239},
numpages = {10},
keywords = {Strategic classification, fairness, machine learning},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}
  
"
FACCT,2019,Downstream Effects of Affirmative Action,https://dl.acm.org/doi/10.1145/3287560.3287578,"['Sampath Kannan', 'Aaron Roth', 'Juba Ziani']",72,"We study a two-stage model, in which students are 1) admitted to college on the basis of an entrance exam which is a noisy signal about their qualifications (type), and then 2) those students who were admitted to college can be hired by an employer as a function of their college grades, which are an independently drawn noisy signal of their type. Students are drawn from one of two populations, which might have different type distributions. We assume that the employer at the end of the pipeline is rational, in the sense that it computes a posterior distribution on student type conditional on all information that it has available (college admissions, grades, and group membership), and makes a decision based on posterior expectation. We then study what kinds of fairness goals can be achieved by the college by setting its admissions rule and grading policy. For example, the college might have the goal of guaranteeing equal opportunity across populations: that the probability of passing through the pipeline and being hired by the employer should be independent of group membership, conditioned on type. Alternately, the college might have the goal of incentivizing the employer to have a group blind hiring rule. We show that both goals can be achieved when the college does not report grades. On the other hand, we show that under reasonable conditions, these goals are impossible to achieve even in isolation when the college uses an (even minimally) informative grading policy.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3287560.3287578,
author = {Kannan, Sampath and Roth, Aaron and Ziani, Juba},
title = {Downstream Effects of Affirmative Action},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287578},
doi = {10.1145/3287560.3287578},
abstract = {We study a two-stage model, in which students are 1) admitted to college on the basis of an entrance exam which is a noisy signal about their qualifications (type), and then 2) those students who were admitted to college can be hired by an employer as a function of their college grades, which are an independently drawn noisy signal of their type. Students are drawn from one of two populations, which might have different type distributions. We assume that the employer at the end of the pipeline is rational, in the sense that it computes a posterior distribution on student type conditional on all information that it has available (college admissions, grades, and group membership), and makes a decision based on posterior expectation. We then study what kinds of fairness goals can be achieved by the college by setting its admissions rule and grading policy. For example, the college might have the goal of guaranteeing equal opportunity across populations: that the probability of passing through the pipeline and being hired by the employer should be independent of group membership, conditioned on type. Alternately, the college might have the goal of incentivizing the employer to have a group blind hiring rule. We show that both goals can be achieved when the college does not report grades. On the other hand, we show that under reasonable conditions, these goals are impossible to achieve even in isolation when the college uses an (even minimally) informative grading policy.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {240–248},
numpages = {9},
keywords = {affirmative action, job market, Long-term fairness, college admissions},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}
  
"
FACCT,2019,Measuring the Biases that Matter: The Ethical and Casual Foundations for Measures of Fairness in Algorithms,https://dl.acm.org/doi/10.1145/3287560.3287573,"['Bruce Glymour', 'Jonathan Herington']",51,"Measures of algorithmic bias can be roughly classified into four categories, distinguished by the conditional probabilistic dependencies to which they are sensitive. First, measures of ""procedural bias"" diagnose bias when the score returned by an algorithm is probabilistically dependent on a sensitive class variable (e.g. race or sex). Second, measures of ""outcome bias"" capture probabilistic dependence between class variables and the outcome for each subject (e.g. parole granted or loan denied). Third, measures of ""behavior-relative error bias"" capture probabilistic dependence between class variables and the algorithmic score, conditional on target behaviors (e.g. recidivism or loan default). Fourth, measures of ""score-relative error bias"" capture probabilistic dependence between class variables and behavior, conditional on score. Several recent discussions have demonstrated a tradeoff between these different measures of algorithmic bias, and at least one recent paper has suggested conditions under which tradeoffs may be minimized. In this paper we use the machinery of causal graphical models to show that, under standard assumptions, the underlying causal relations among variables forces some tradeoffs. We delineate a number of normative considerations that are encoded in different measures of bias, with reference to the philosophical literature on the wrongfulness of disparate treatment and disparate impact. While both kinds of error bias are nominally motivated by concern to avoid disparate impact, we argue that consideration of causal structures shows that these measures are better understood as complicated and unreliable measures of procedural biases (i.e. disparate treatment). Moreover, while procedural bias is indicative of disparate treatment, we show that the measure of procedural bias one ought to adopt is dependent on the account of the wrongfulness of disparate treatment one endorses. Finally, given that neither score-relative nor behavior-relative measures of error bias capture the relevant normative considerations, we suggest that error bias proper is best measured by score-based measures of accuracy, such as the Brier score.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3287560.3287573,
author = {Glymour, Bruce and Herington, Jonathan},
title = {Measuring the Biases That Matter: The Ethical and Casual Foundations for Measures of Fairness in Algorithms},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287573},
doi = {10.1145/3287560.3287573},
abstract = {Measures of algorithmic bias can be roughly classified into four categories, distinguished by the conditional probabilistic dependencies to which they are sensitive. First, measures of ""procedural bias"" diagnose bias when the score returned by an algorithm is probabilistically dependent on a sensitive class variable (e.g. race or sex). Second, measures of ""outcome bias"" capture probabilistic dependence between class variables and the outcome for each subject (e.g. parole granted or loan denied). Third, measures of ""behavior-relative error bias"" capture probabilistic dependence between class variables and the algorithmic score, conditional on target behaviors (e.g. recidivism or loan default). Fourth, measures of ""score-relative error bias"" capture probabilistic dependence between class variables and behavior, conditional on score. Several recent discussions have demonstrated a tradeoff between these different measures of algorithmic bias, and at least one recent paper has suggested conditions under which tradeoffs may be minimized.In this paper we use the machinery of causal graphical models to show that, under standard assumptions, the underlying causal relations among variables forces some tradeoffs. We delineate a number of normative considerations that are encoded in different measures of bias, with reference to the philosophical literature on the wrongfulness of disparate treatment and disparate impact. While both kinds of error bias are nominally motivated by concern to avoid disparate impact, we argue that consideration of causal structures shows that these measures are better understood as complicated and unreliable measures of procedural biases (i.e. disparate treatment). Moreover, while procedural bias is indicative of disparate treatment, we show that the measure of procedural bias one ought to adopt is dependent on the account of the wrongfulness of disparate treatment one endorses. Finally, given that neither score-relative nor behavior-relative measures of error bias capture the relevant normative considerations, we suggest that error bias proper is best measured by score-based measures of accuracy, such as the Brier score.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {269–278},
numpages = {10},
keywords = {casual inference, Algorithmic decision-making, discrimination, fairness},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}
  
"
FACCT,2019,Racial categories in machine learning,https://dl.acm.org/doi/10.1145/3287560.3287575,"['Sebastian Benthall', 'Bruce D. Haynes']",89,"Controversies around race and machine learning have sparked debate among computer scientists over how to design machine learning systems that guarantee fairness. These debates rarely engage with how racial identity is embedded in our social experience, making for sociological and psychological complexity. This complexity challenges the paradigm of considering fairness to be a formal property of supervised learning with respect to protected personal attributes. Racial identity is not simply a personal subjective quality. For people labeled ""Black"" it is an ascribed political category that has consequences for social differentiation embedded in systemic patterns of social inequality achieved through both social and spatial segregation. In the United States, racial classification can best be understood as a system of inherently unequal status categories that places whites as the most privileged category while signifying the Negro/black category as stigmatized. Social stigma is reinforced through the unequal distribution of societal rewards and goods along racial lines that is reinforced by state, corporate, and civic institutions and practices. This creates a dilemma for society and designers: be blind to racial group disparities and thereby reify racialized social inequality by no longer measuring systemic inequality, or be conscious of racial categories in a way that itself reifies race. We propose a third option. By preceding group fairness interventions with unsupervised learning to dynamically detect patterns of segregation, machine learning systems can mitigate the root cause of social disparities, social segregation and stratification, without further anchoring status categories of disadvantage.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3287560.3287575,
author = {Benthall, Sebastian and Haynes, Bruce D.},
title = {Racial Categories in Machine Learning},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287575},
doi = {10.1145/3287560.3287575},
abstract = {Controversies around race and machine learning have sparked debate among computer scientists over how to design machine learning systems that guarantee fairness. These debates rarely engage with how racial identity is embedded in our social experience, making for sociological and psychological complexity. This complexity challenges the paradigm of considering fairness to be a formal property of supervised learning with respect to protected personal attributes. Racial identity is not simply a personal subjective quality. For people labeled ""Black"" it is an ascribed political category that has consequences for social differentiation embedded in systemic patterns of social inequality achieved through both social and spatial segregation. In the United States, racial classification can best be understood as a system of inherently unequal status categories that places whites as the most privileged category while signifying the Negro/black category as stigmatized. Social stigma is reinforced through the unequal distribution of societal rewards and goods along racial lines that is reinforced by state, corporate, and civic institutions and practices. This creates a dilemma for society and designers: be blind to racial group disparities and thereby reify racialized social inequality by no longer measuring systemic inequality, or be conscious of racial categories in a way that itself reifies race. We propose a third option. By preceding group fairness interventions with unsupervised learning to dynamically detect patterns of segregation, machine learning systems can mitigate the root cause of social disparities, social segregation and stratification, without further anchoring status categories of disadvantage.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {289–298},
numpages = {10},
keywords = {machine learning, racial classification, segregation, fairness},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}
  
"
FACCT,2019,From Soft Classifiers to Hard Decisions: How fair can we be?,https://dl.acm.org/doi/10.1145/3287560.3287561,"['Ran Canetti', 'Aloni Cohen', 'Nishanth Dikkala', 'Govind Ramnarayan', 'Sarah Scheffler', 'Adam Smith']",38,"A popular methodology for building binary decision-making classifiers in the presence of imperfect information is to first construct a calibrated non-binary ""scoring"" classifier, and then to post-process this score to obtain a binary decision. We study various fairness (or, error-balance) properties of this methodology, when the non-binary scores are calibrated over all protected groups, and with a variety of post-processing algorithms. Specifically, we show: First, there does not exist a general way to post-process a calibrated classifier to equalize protected groups' positive or negative predictive value (PPV or NPV). For certain ""nice"" calibrated classifiers, either PPV or NPV can be equalized when the post-processor uses different thresholds across protected groups. Still, when the post-processing consists of a single global threshold across all groups, natural fairness properties, such as equalizing PPV in a nontrivial way, do not hold even for ""nice"" classifiers. Second, when the post-processing stage is allowed to defer on some decisions (that is, to avoid making a decision by handing off some examples to a separate process), then for the non-deferred decisions, the resulting classifier can be made to equalize PPV, NPV, false positive rate (FPR) and false negative rate (FNR) across the protected groups. This suggests a way to partially evade the impossibility results of Chouldechova and Kleinberg et al., which preclude equalizing all of these measures simultaneously. We also present different deferring strategies and show how they affect the fairness properties of the overall system. We evaluate our post-processing techniques using the COMPAS data set from 2016.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3287560.3287561,
author = {Canetti, Ran and Cohen, Aloni and Dikkala, Nishanth and Ramnarayan, Govind and Scheffler, Sarah and Smith, Adam},
title = {From Soft Classifiers to Hard Decisions: How Fair Can We Be?},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287561},
doi = {10.1145/3287560.3287561},
abstract = {A popular methodology for building binary decision-making classifiers in the presence of imperfect information is to first construct a calibrated non-binary ""scoring"" classifier, and then to post-process this score to obtain a binary decision. We study various fairness (or, error-balance) properties of this methodology, when the non-binary scores are calibrated over all protected groups, and with a variety of post-processing algorithms. Specifically, we show:First, there does not exist a general way to post-process a calibrated classifier to equalize protected groups' positive or negative predictive value (PPV or NPV). For certain ""nice"" calibrated classifiers, either PPV or NPV can be equalized when the post-processor uses different thresholds across protected groups. Still, when the post-processing consists of a single global threshold across all groups, natural fairness properties, such as equalizing PPV in a nontrivial way, do not hold even for ""nice"" classifiers.Second, when the post-processing stage is allowed to defer on some decisions (that is, to avoid making a decision by handing off some examples to a separate process), then for the non-deferred decisions, the resulting classifier can be made to equalize PPV, NPV, false positive rate (FPR) and false negative rate (FNR) across the protected groups. This suggests a way to partially evade the impossibility results of Chouldechova and Kleinberg et al., which preclude equalizing all of these measures simultaneously. We also present different deferring strategies and show how they affect the fairness properties of the overall system.We evaluate our post-processing techniques using the COMPAS data set from 2016.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {309–318},
numpages = {10},
keywords = {post-processing, classification, algorithmic fairness},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}
  
"
FACCT,2019,Classification with Fairness Constraints: A Meta-Algorithm with Provable Guarantees,https://dl.acm.org/doi/10.1145/3287560.3287586,"['L. Elisa Celis', 'Lingxiao Huang', 'Vijay Keswani', 'Nisheeth K. Vishnoi']",223,"Developing classification algorithms that are fair with respect to sensitive attributes of the data is an important problem due to the increased deployment of classification algorithms in societal contexts. Several recent works have focused on studying classification with respect to specific fairness metrics, modeled the corresponding fair classification problem as constrained optimization problems, and developed tailored algorithms to solve them. Despite this, there still remain important metrics for which there are no fair classifiers with theoretical guarantees; primarily because the resulting optimization problem is non-convex. The main contribution of this paper is a meta-algorithm for classification that can take as input a general class of fairness constraints with respect to multiple non-disjoint and multi-valued sensitive attributes, and which comes with provable guarantees. In particular, our algorithm can handle non-convex ""linear fractional"" constraints (which includes fairness constraints such as predictive parity) for which no prior algorithm was known. Key to our results is an algorithm for a family of classification problems with convex constraints along with a reduction from classification problems with linear fractional constraints to this family. Empirically, we observe that our algorithm is fast, can achieve near-perfect fairness with respect to various fairness metrics, and the loss in accuracy due to the imposed fairness constraints is often small.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3287560.3287586,
author = {Celis, L. Elisa and Huang, Lingxiao and Keswani, Vijay and Vishnoi, Nisheeth K.},
title = {Classification with Fairness Constraints: A Meta-Algorithm with Provable Guarantees},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287586},
doi = {10.1145/3287560.3287586},
abstract = {Developing classification algorithms that are fair with respect to sensitive attributes of the data is an important problem due to the increased deployment of classification algorithms in societal contexts. Several recent works have focused on studying classification with respect to specific fairness metrics, modeled the corresponding fair classification problem as constrained optimization problems, and developed tailored algorithms to solve them. Despite this, there still remain important metrics for which there are no fair classifiers with theoretical guarantees; primarily because the resulting optimization problem is non-convex. The main contribution of this paper is a meta-algorithm for classification that can take as input a general class of fairness constraints with respect to multiple non-disjoint and multi-valued sensitive attributes, and which comes with provable guarantees. In particular, our algorithm can handle non-convex ""linear fractional"" constraints (which includes fairness constraints such as predictive parity) for which no prior algorithm was known. Key to our results is an algorithm for a family of classification problems with convex constraints along with a reduction from classification problems with linear fractional constraints to this family. Empirically, we observe that our algorithm is fast, can achieve near-perfect fairness with respect to various fairness metrics, and the loss in accuracy due to the imposed fairness constraints is often small.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {319–328},
numpages = {10},
keywords = {Algorithmic Fairness, Classification},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}"
FACCT,2019,A comparative study of fairness-enhancing interventions in machine learning,https://dl.acm.org/doi/10.1145/3287560.3287589,"['Sorelle A. Friedler', 'Carlos Scheidegger', 'Suresh Venkatasubramanian', 'Sonam Choudhary', 'Evan P. Hamilton', 'Derek Roth']",478,"Computers are increasingly used to make decisions that have significant impact on people's lives. Often, these predictions can affect different population subgroups disproportionately. As a result, the issue of fairness has received much recent interest, and a number of fairness-enhanced classifiers have appeared in the literature. This paper seeks to study the following questions: how do these different techniques fundamentally compare to one another, and what accounts for the differences? Specifically, we seek to bring attention to many under-appreciated aspects of such fairness-enhancing interventions that require investigation for these algorithms to receive broad adoption. We present the results of an open benchmark we have developed that lets us compare a number of different algorithms under a variety of fairness measures and existing datasets. We find that although different algorithms tend to prefer specific formulations of fairness preservations, many of these measures strongly correlate with one another. In addition, we find that fairness-preserving algorithms tend to be sensitive to fluctuations in dataset composition (simulated in our benchmark by varying training-test splits) and to different forms of preprocessing, indicating that fairness interventions might be more brittle than previously thought.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3287560.3287589,
author = {Friedler, Sorelle A. and Scheidegger, Carlos and Venkatasubramanian, Suresh and Choudhary, Sonam and Hamilton, Evan P. and Roth, Derek},
title = {A Comparative Study of Fairness-Enhancing Interventions in Machine Learning},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287589},
doi = {10.1145/3287560.3287589},
abstract = {Computers are increasingly used to make decisions that have significant impact on people's lives. Often, these predictions can affect different population subgroups disproportionately. As a result, the issue of fairness has received much recent interest, and a number of fairness-enhanced classifiers have appeared in the literature. This paper seeks to study the following questions: how do these different techniques fundamentally compare to one another, and what accounts for the differences? Specifically, we seek to bring attention to many under-appreciated aspects of such fairness-enhancing interventions that require investigation for these algorithms to receive broad adoption.We present the results of an open benchmark we have developed that lets us compare a number of different algorithms under a variety of fairness measures and existing datasets. We find that although different algorithms tend to prefer specific formulations of fairness preservations, many of these measures strongly correlate with one another. In addition, we find that fairness-preserving algorithms tend to be sensitive to fluctuations in dataset composition (simulated in our benchmark by varying training-test splits) and to different forms of preprocessing, indicating that fairness interventions might be more brittle than previously thought.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {329–338},
numpages = {10},
keywords = {Fairness-aware machine learning, benchmarks},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}
  
"
FACCT,2019,Fairness Under Unawareness: Assessing Disparity When Protected Class Is Unobserved,https://dl.acm.org/doi/10.1145/3287560.3287594,"['Jiahao Chen', 'Nathan Kallus', 'Xiaojie Mao', 'Geoffry Svacha', 'Madeleine Udell']",210,"Assessing the fairness of a decision making system with respect to a protected class, such as gender or race, is challenging when class membership labels are unavailable. Probabilistic models for predicting the protected class based on observable proxies, such as surname and geolocation for race, are sometimes used to impute these missing labels for compliance assessments. Empirically, these methods are observed to exaggerate disparities, but the reason why is unknown. In this paper, we decompose the biases in estimating outcome disparity via threshold-based imputation into multiple interpretable bias sources, allowing us to explain when over- or underestimation occurs. We also propose an alternative weighted estimator that uses soft classification, and show that its bias arises simply from the conditional covariance of the outcome with the true class membership. Finally, we illustrate our results with numerical simulations and a public dataset of mortgage applications, using geolocation as a proxy for race. We confirm that the bias of threshold-based imputation is generally upward, but its magnitude varies strongly with the threshold chosen. Our new weighted estimator tends to have a negative bias that is much simpler to analyze and reason about.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3287560.3287594,
author = {Chen, Jiahao and Kallus, Nathan and Mao, Xiaojie and Svacha, Geoffry and Udell, Madeleine},
title = {Fairness Under Unawareness: Assessing Disparity When Protected Class Is Unobserved},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287594},
doi = {10.1145/3287560.3287594},
abstract = {Assessing the fairness of a decision making system with respect to a protected class, such as gender or race, is challenging when class membership labels are unavailable. Probabilistic models for predicting the protected class based on observable proxies, such as surname and geolocation for race, are sometimes used to impute these missing labels for compliance assessments. Empirically, these methods are observed to exaggerate disparities, but the reason why is unknown. In this paper, we decompose the biases in estimating outcome disparity via threshold-based imputation into multiple interpretable bias sources, allowing us to explain when over- or underestimation occurs. We also propose an alternative weighted estimator that uses soft classification, and show that its bias arises simply from the conditional covariance of the outcome with the true class membership. Finally, we illustrate our results with numerical simulations and a public dataset of mortgage applications, using geolocation as a proxy for race. We confirm that the bias of threshold-based imputation is generally upward, but its magnitude varies strongly with the threshold chosen. Our new weighted estimator tends to have a negative bias that is much simpler to analyze and reason about.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {339–348},
numpages = {10},
keywords = {race imputation, protected class, probablistic proxy model, fair lending, disparate impact, racial discrimination, Bayesian Improved Surname Geocoding},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}
  
"
FACCT,2019,Fairness through Causal Awareness: Learning Causal Latent-Variable Models for Biased Data,https://dl.acm.org/doi/10.1145/3287560.3287564,"['David Madras', 'Elliot Creager', 'Toniann Pitassi', 'Richard Zemel']",98,"How do we learn from biased data? Historical datasets often reflect historical prejudices; sensitive or protected attributes may affect the observed treatments and outcomes. Classification algorithms tasked with predicting outcomes accurately from these datasets tend to replicate these biases. We advocate a causal modeling approach to learning from biased data, exploring the relationship between fair classification and intervention. We propose a causal model in which the sensitive attribute confounds both the treatment and the outcome. Building on prior work in deep learning and generative modeling, we describe how to learn the parameters of this causal model from observational data alone, even in the presence of unobserved confounders. We show experimentally that fairness-aware causal modeling provides better estimates of the causal effects between the sensitive attribute, the treatment, and the outcome. We further present evidence that estimating these causal effects can help learn policies that are both more accurate and fair, when presented with a historically biased dataset.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3287560.3287564,
author = {Madras, David and Creager, Elliot and Pitassi, Toniann and Zemel, Richard},
title = {Fairness through Causal Awareness: Learning Causal Latent-Variable Models for Biased Data},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287564},
doi = {10.1145/3287560.3287564},
abstract = {How do we learn from biased data? Historical datasets often reflect historical prejudices; sensitive or protected attributes may affect the observed treatments and outcomes. Classification algorithms tasked with predicting outcomes accurately from these datasets tend to replicate these biases. We advocate a causal modeling approach to learning from biased data, exploring the relationship between fair classification and intervention. We propose a causal model in which the sensitive attribute confounds both the treatment and the outcome. Building on prior work in deep learning and generative modeling, we describe how to learn the parameters of this causal model from observational data alone, even in the presence of unobserved confounders. We show experimentally that fairness-aware causal modeling provides better estimates of the causal effects between the sensitive attribute, the treatment, and the outcome. We further present evidence that estimating these causal effects can help learn policies that are both more accurate and fair, when presented with a historically biased dataset.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {349–358},
numpages = {10},
keywords = {fairness in machine learning, causal inference, variational inference},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}
  
"
FACCT,2019,From Fair Decision Making To Social Equality,https://dl.acm.org/doi/10.1145/3287560.3287599,"['Hussein Mouzannar', 'Mesrob I. Ohannessian', 'Nathan Srebro']",79,"The study of fairness in intelligent decision systems has mostly ignored long-term influence on the underlying population. Yet fairness considerations (e.g. affirmative action) have often the implicit goal of achieving balance among groups within the population. The most basic notion of balance is eventual equality between the qualifications of the groups. How can we incorporate influence dynamics in decision making? How well do dynamics-oblivious fairness policies fare in terms of reaching equality? In this paper, we propose a simple yet revealing model that encompasses (1) a selection process where an institution chooses from multiple groups according to their qualifications so as to maximize an institutional utility and (2) dynamics that govern the evolution of the groups' qualifications according to the imposed policies. We focus on demographic parity as the formalism of affirmative action. We first give conditions under which an unconstrained policy reaches equality on its own. In this case, surprisingly, imposing demographic parity may break equality. When it doesn't, one would expect the additional constraint to reduce utility, however, we show that utility may in fact increase. In real world scenarios, unconstrained policies do not lead to equality. In such cases, we show that although imposing demographic parity may remedy it, there is a danger that groups settle at a worse set of qualifications. As a silver lining, we also identify when the constraint not only leads to equality, but also improves all groups. These cases and trade-offs are instrumental in determining when and how imposing demographic parity can be beneficial in selection processes, both for the institution and for society on the long run.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3287560.3287599,
author = {Mouzannar, Hussein and Ohannessian, Mesrob I. and Srebro, Nathan},
title = {From Fair Decision Making To Social Equality},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287599},
doi = {10.1145/3287560.3287599},
abstract = {The study of fairness in intelligent decision systems has mostly ignored long-term influence on the underlying population. Yet fairness considerations (e.g. affirmative action) have often the implicit goal of achieving balance among groups within the population. The most basic notion of balance is eventual equality between the qualifications of the groups. How can we incorporate influence dynamics in decision making? How well do dynamics-oblivious fairness policies fare in terms of reaching equality? In this paper, we propose a simple yet revealing model that encompasses (1) a selection process where an institution chooses from multiple groups according to their qualifications so as to maximize an institutional utility and (2) dynamics that govern the evolution of the groups' qualifications according to the imposed policies. We focus on demographic parity as the formalism of affirmative action.We first give conditions under which an unconstrained policy reaches equality on its own. In this case, surprisingly, imposing demographic parity may break equality. When it doesn't, one would expect the additional constraint to reduce utility, however, we show that utility may in fact increase. In real world scenarios, unconstrained policies do not lead to equality. In such cases, we show that although imposing demographic parity may remedy it, there is a danger that groups settle at a worse set of qualifications. As a silver lining, we also identify when the constraint not only leads to equality, but also improves all groups. These cases and trade-offs are instrumental in determining when and how imposing demographic parity can be beneficial in selection processes, both for the institution and for society on the long run.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {359–368},
numpages = {10},
keywords = {social equality, influence on society, demographic parity, selection processes, dynamics, affirmative action, fairness},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}
  
"
FACCT,2020,Toward situated interventions for algorithmic equity: lessons from the field,https://dl.acm.org/doi/10.1145/3351095.3372874,"['Michael Katell', 'Meg Young', 'Dharma Dailey', 'Bernease Herman', 'Vivian Guetler', 'Aaron Tam', 'Corinne Bintz', 'Daniella Raz', 'P. M. Krafft']",75,"Research to date aimed at the fairness, accountability, and transparency of algorithmic systems has largely focused on topics such as identifying failures of current systems and on technical interventions intended to reduce bias in computational processes. Researchers have given less attention to methods that account for the social and political contexts of specific, situated technical systems at their points of use. Co-developing algorithmic accountability interventions in communities supports outcomes that are more likely to address problems in their situated context and re-center power with those most disparately affected by the harms of algorithmic systems. In this paper we report on our experiences using participatory and co-design methods for algorithmic accountability in a project called the Algorithmic Equity Toolkit. The main insights we gleaned from our experiences were: (i) many meaningful interventions toward equitable algorithmic systems are non-technical; (ii) community organizations derive the most value from localized materials as opposed to what is ""scalable"" beyond a particular policy context; (iii) framing harms around algorithmic bias suggests that more accurate data is the solution, at the risk of missing deeper questions about whether some technologies should be used at all. More broadly, we found that community-based methods are important inroads to addressing algorithmic harms in their situated contexts.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3351095.3372874,
author = {Katell, Michael and Young, Meg and Dailey, Dharma and Herman, Bernease and Guetler, Vivian and Tam, Aaron and Bintz, Corinne and Raz, Daniella and Krafft, P. M.},
title = {Toward Situated Interventions for Algorithmic Equity: Lessons from the Field},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372874},
doi = {10.1145/3351095.3372874},
abstract = {Research to date aimed at the fairness, accountability, and transparency of algorithmic systems has largely focused on topics such as identifying failures of current systems and on technical interventions intended to reduce bias in computational processes. Researchers have given less attention to methods that account for the social and political contexts of specific, situated technical systems at their points of use. Co-developing algorithmic accountability interventions in communities supports outcomes that are more likely to address problems in their situated context and re-center power with those most disparately affected by the harms of algorithmic systems. In this paper we report on our experiences using participatory and co-design methods for algorithmic accountability in a project called the Algorithmic Equity Toolkit. The main insights we gleaned from our experiences were: (i) many meaningful interventions toward equitable algorithmic systems are non-technical; (ii) community organizations derive the most value from localized materials as opposed to what is ""scalable"" beyond a particular policy context; (iii) framing harms around algorithmic bias suggests that more accurate data is the solution, at the risk of missing deeper questions about whether some technologies should be used at all. More broadly, we found that community-based methods are important inroads to addressing algorithmic harms in their situated contexts.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {45–55},
numpages = {11},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
FACCT,2020,"""The human body is a black box"": supporting clinical decision-making with deep learning",https://dl.acm.org/doi/10.1145/3351095.3372827,"['Mark Sendak', 'Madeleine Clare Elish', 'Michael Gao', 'Joseph Futoma', 'William Ratliff', 'Marshall Nichols', 'Armando Bedoya', 'Suresh Balu', ""Cara O'Brien""]",103,"Machine learning technologies are increasingly developed for use in healthcare. While research communities have focused on creating state-of-the-art models, there has been less focus on real world implementation and the associated challenges to fairness, transparency, and accountability that come from actual, situated use. Serious questions remain underexamined regarding how to ethically build models, interpret and explain model output, recognize and account for biases, and minimize disruptions to professional expertise and work cultures. We address this gap in the literature and provide a detailed case study covering the development, implementation, and evaluation of Sepsis Watch, a machine learning-driven tool that assists hospital clinicians in the early diagnosis and treatment of sepsis. Sepsis is a severe infection that can lead to organ failure or death if not treated in time and is the leading cause of inpatient deaths in US hospitals. We, the team that developed and evaluated the tool, discuss our conceptualization of the tool not as a model deployed in the world but instead as a socio-technical system requiring integration into existing social and professional contexts. Rather than focusing solely on model interpretability to ensure fair and accountable machine learning, we point toward four key values and practices that should be considered when developing machine learning to support clinical decision-making: rigorously define the problem in context, build relationships with stakeholders, respect professional discretion, and create ongoing feedback loops with stakeholders. Our work has significant implications for future research regarding mechanisms of institutional accountability and considerations for responsibly designing machine learning systems. Our work underscores the limits of model interpretability as a solution to ensure transparency, accuracy, and accountability in practice. Instead, our work demonstrates other means and goals to achieve FATML values in design and in practice.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3351095.3372827,
author = {Sendak, Mark and Elish, Madeleine Clare and Gao, Michael and Futoma, Joseph and Ratliff, William and Nichols, Marshall and Bedoya, Armando and Balu, Suresh and O'Brien, Cara},
title = {""The Human Body is a Black Box"": Supporting Clinical Decision-Making with Deep Learning},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372827},
doi = {10.1145/3351095.3372827},
abstract = {Machine learning technologies are increasingly developed for use in healthcare. While research communities have focused on creating state-of-the-art models, there has been less focus on real world implementation and the associated challenges to fairness, transparency, and accountability that come from actual, situated use. Serious questions remain underexamined regarding how to ethically build models, interpret and explain model output, recognize and account for biases, and minimize disruptions to professional expertise and work cultures. We address this gap in the literature and provide a detailed case study covering the development, implementation, and evaluation of Sepsis Watch, a machine learning-driven tool that assists hospital clinicians in the early diagnosis and treatment of sepsis. Sepsis is a severe infection that can lead to organ failure or death if not treated in time and is the leading cause of inpatient deaths in US hospitals. We, the team that developed and evaluated the tool, discuss our conceptualization of the tool not as a model deployed in the world but instead as a socio-technical system requiring integration into existing social and professional contexts. Rather than focusing solely on model interpretability to ensure fair and accountable machine learning, we point toward four key values and practices that should be considered when developing machine learning to support clinical decision-making: rigorously define the problem in context, build relationships with stakeholders, respect professional discretion, and create ongoing feedback loops with stakeholders. Our work has significant implications for future research regarding mechanisms of institutional accountability and considerations for responsibly designing machine learning systems. Our work underscores the limits of model interpretability as a solution to ensure transparency, accuracy, and accountability in practice. Instead, our work demonstrates other means and goals to achieve FATML values in design and in practice.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {99–109},
numpages = {11},
keywords = {interpretability, trust, deep learning, expertise, medicine},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
FACCT,2020,Assessing algorithmic fairness with unobserved protected class using data combination,https://dl.acm.org/doi/10.1145/3351095.3373154,"['Nathan Kallus', 'Xiaojie Mao', 'Angela Zhou']",95,"The increasing impact of algorithmic decisions on people's lives compels us to scrutinize their fairness and, in particular, the disparate impacts that ostensibly-color-blind algorithms can have on different groups. Examples include credit decisioning, hiring, advertising, criminal justice, personalized medicine, and targeted policymaking, where in some cases legislative or regulatory frameworks for fairness exist and define specific protected classes. In this paper we study a fundamental challenge to assessing disparate impacts, or performance disparities in general, in practice: protected class membership is often not observed in the data. This is particularly a problem in lending and healthcare. We consider the use of an auxiliary dataset, such as the US census, that includes protected class labels but not decisions or outcomes. We show that a variety of common disparity measures are generally unidentifiable aside for some unrealistic cases, providing a new perspective on the documented biases of popular proxy-based methods. We provide exact characterizations of the sharpest-possible partial identification set of disparities either under no assumptions or when we incorporate mild smoothness constraints. We further provide optimization-based algorithms for computing and visualizing these sets of simultaneously achievable pairwise disparties for assessing disparities that arise between multiple groups, which enables reliable and robust assessments - an important tool when disparity assessment can have far-reaching policy implications. We demonstrate this in two case studies with real data: mortgage lending and personalized medicine dosing.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3351095.3373154,
author = {Kallus, Nathan and Mao, Xiaojie and Zhou, Angela},
title = {Assessing Algorithmic Fairness with Unobserved Protected Class Using Data Combination},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3373154},
doi = {10.1145/3351095.3373154},
abstract = {The increasing impact of algorithmic decisions on people's lives compels us to scrutinize their fairness and, in particular, the disparate impacts that ostensibly-color-blind algorithms can have on different groups. Examples include credit decisioning, hiring, advertising, criminal justice, personalized medicine, and targeted policymaking, where in some cases legislative or regulatory frameworks for fairness exist and define specific protected classes. In this paper we study a fundamental challenge to assessing disparate impacts, or performance disparities in general, in practice: protected class membership is often not observed in the data. This is particularly a problem in lending and healthcare. We consider the use of an auxiliary dataset, such as the US census, that includes protected class labels but not decisions or outcomes. We show that a variety of common disparity measures are generally unidentifiable aside for some unrealistic cases, providing a new perspective on the documented biases of popular proxy-based methods. We provide exact characterizations of the sharpest-possible partial identification set of disparities either under no assumptions or when we incorporate mild smoothness constraints. We further provide optimization-based algorithms for computing and visualizing these sets of simultaneously achievable pairwise disparties for assessing disparities that arise between multiple groups, which enables reliable and robust assessments - an important tool when disparity assessment can have far-reaching policy implications. We demonstrate this in two case studies with real data: mortgage lending and personalized medicine dosing.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {110},
numpages = {1},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
FACCT,2020,FlipTest: fairness testing via optimal transport,https://dl.acm.org/doi/10.1145/3351095.3372845,"['Emily Black', 'Samuel Yeom', 'Matt Fredrikson']",59,"We present FlipTest, a black-box technique for uncovering discrimination in classifiers. FlipTest is motivated by the intuitive question: had an individual been of a different protected status, would the model have treated them differently? Rather than relying on causal information to answer this question, FlipTest leverages optimal transport to match individuals in different protected groups, creating similar pairs of in-distribution samples. We show how to use these instances to detect discrimination by constructing a flipset: the set of individuals whose classifier output changes post-translation, which corresponds to the set of people who may be harmed because of their group membership. To shed light on why the model treats a given subgroup differently, FlipTest produces a transparency report: a ranking of features that are most associated with the model's behavior on the flipset. Evaluating the approach on three case studies, we show that this provides a computationally inexpensive way to identify subgroups that may be harmed by model discrimination, including in cases where the model satisfies group fairness criteria.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3351095.3372845,
author = {Black, Emily and Yeom, Samuel and Fredrikson, Matt},
title = {FlipTest: Fairness Testing via Optimal Transport},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372845},
doi = {10.1145/3351095.3372845},
abstract = {We present FlipTest, a black-box technique for uncovering discrimination in classifiers. FlipTest is motivated by the intuitive question: had an individual been of a different protected status, would the model have treated them differently? Rather than relying on causal information to answer this question, FlipTest leverages optimal transport to match individuals in different protected groups, creating similar pairs of in-distribution samples. We show how to use these instances to detect discrimination by constructing a flipset: the set of individuals whose classifier output changes post-translation, which corresponds to the set of people who may be harmed because of their group membership. To shed light on why the model treats a given subgroup differently, FlipTest produces a transparency report: a ranking of features that are most associated with the model's behavior on the flipset. Evaluating the approach on three case studies, we show that this provides a computationally inexpensive way to identify subgroups that may be harmed by model discrimination, including in cases where the model satisfies group fairness criteria.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {111–121},
numpages = {11},
keywords = {disparate impact, machine learning, fairness, optimal transport},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
FACCT,2020,"Implications of AI (un-)fairness in higher education admissions: the effects of perceived AI (un-)fairness on exit, voice and organizational reputation",https://dl.acm.org/doi/10.1145/3351095.3372867,"['Frank Marcinkowski', 'Kimon Kieslich', 'Christopher Starke', 'Marco L√ºnich']",83,"Algorithmic decision-making (ADM) is becoming increasingly important in all areas of social life. In higher education, machine-learning systems have manifold uses because they can efficiently process large amounts of student data and use these data to arrive at effective decisions. Despite the potential upsides of ADM systems, fairness concerns are gaining momentum in academic and public discourses. The criticism largely focuses on the disparate effects of ADM. That is, algorithms may not serve as objective and fair decision-makers but, rather, reproduce biases existing within the respective training data. This study adopted a different approach by focusing on individual perceptions of fairness. Specifically, we looked at two different dimensions of perceived fairness: (i) procedural fairness and (ii) distributive fairness. Using cross-sectional survey data (n = 304) from a large German university, we tested whether students' assessments of fairness differ with respect to algorithmic vs. human decision-making (HDM) within the higher education context. Furthermore, we investigated whether fairness perceptions have subsequent effects on three different outcome variables, which are hugely important for universities: (1) exit, (2) voice, and (3) organizational reputation. The results of our survey suggest that participants evaluated ADM higher than HDM in terms of both procedural and distributive fairness. Concerning the subsequent effects of fairness perceptions, we find that (1) distributive fairness as well as procedural fairness perceptions have a negative impact on the intention to protest against an ADM system, whereas (2) only procedural fairness perceptions negatively affect the likelihood of exiting. Finally, (3) distributive fairness, but not procedural fairness perceptions have a positive effect on organizational reputation. For universities aiming to implement ADM systems, it is crucial, therefore, to take possible fairness issues and their further implications into account.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3351095.3372867,
author = {Marcinkowski, Frank and Kieslich, Kimon and Starke, Christopher and L\""{u}nich, Marco},
title = {Implications of AI (Un-)Fairness in Higher Education Admissions: The Effects of Perceived AI (Un-)Fairness on Exit, Voice and Organizational Reputation},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372867},
doi = {10.1145/3351095.3372867},
abstract = {Algorithmic decision-making (ADM) is becoming increasingly important in all areas of social life. In higher education, machine-learning systems have manifold uses because they can efficiently process large amounts of student data and use these data to arrive at effective decisions. Despite the potential upsides of ADM systems, fairness concerns are gaining momentum in academic and public discourses. The criticism largely focuses on the disparate effects of ADM. That is, algorithms may not serve as objective and fair decision-makers but, rather, reproduce biases existing within the respective training data. This study adopted a different approach by focusing on individual perceptions of fairness. Specifically, we looked at two different dimensions of perceived fairness: (i) procedural fairness and (ii) distributive fairness. Using cross-sectional survey data (n = 304) from a large German university, we tested whether students' assessments of fairness differ with respect to algorithmic vs. human decision-making (HDM) within the higher education context. Furthermore, we investigated whether fairness perceptions have subsequent effects on three different outcome variables, which are hugely important for universities: (1) exit, (2) voice, and (3) organizational reputation. The results of our survey suggest that participants evaluated ADM higher than HDM in terms of both procedural and distributive fairness. Concerning the subsequent effects of fairness perceptions, we find that (1) distributive fairness as well as procedural fairness perceptions have a negative impact on the intention to protest against an ADM system, whereas (2) only procedural fairness perceptions negatively affect the likelihood of exiting. Finally, (3) distributive fairness, but not procedural fairness perceptions have a positive effect on organizational reputation. For universities aiming to implement ADM systems, it is crucial, therefore, to take possible fairness issues and their further implications into account.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {122–130},
numpages = {9},
keywords = {distributive fairness, higher education systems, artificial intelligence, voice, exit, reputation, procedural fairness, algorithmic decision making},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
FACCT,2020,Case study: predictive fairness to reduce misdemeanor recidivism through social service interventions,https://dl.acm.org/doi/10.1145/3351095.3372863,"['Kit T. Rodolfa', 'Erika Salomon', 'Lauren Haynes', 'Iv√°n Higuera Mendieta', 'Jamie Larson', 'Rayid Ghani']",24,"The criminal justice system is currently ill-equipped to improve outcomes of individuals who cycle in and out of the system with a series of misdemeanor offenses. Often due to constraints of caseload and poor record linkage, prior interactions with an individual may not be considered when an individual comes back into the system, let alone in a proactive manner through the application of diversion programs. The Los Angeles City Attorney's Office recently created a new Recidivism Reduction and Drug Diversion unit (R2D2) tasked with reducing recidivism in this population. Here we describe a collaboration with this new unit as a case study for the incorporation of predictive equity into machine learning based decision making in a resource-constrained setting. The program seeks to improve outcomes by developing individually-tailored social service interventions (i.e., diversions, conditional plea agreements, stayed sentencing, or other favorable case disposition based on appropriate social service linkage rather than traditional sentencing methods) for individuals likely to experience subsequent interactions with the criminal justice system, a time and resource-intensive undertaking that necessitates an ability to focus resources on individuals most likely to be involved in a future case. Seeking to achieve both efficiency (through predictive accuracy) and equity (improving outcomes in traditionally under-served communities and working to mitigate existing disparities in criminal justice outcomes), we discuss the equity outcomes we seek to achieve, describe the corresponding choice of a metric for measuring predictive fairness in this context, and explore a set of options for balancing equity and efficiency when building and selecting machine learning models in an operational public policy setting.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3351095.3372863,
author = {Rodolfa, Kit T. and Salomon, Erika and Haynes, Lauren and Mendieta, Iv\'{a}n Higuera and Larson, Jamie and Ghani, Rayid},
title = {Case Study: Predictive Fairness to Reduce Misdemeanor Recidivism through Social Service Interventions},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372863},
doi = {10.1145/3351095.3372863},
abstract = {The criminal justice system is currently ill-equipped to improve outcomes of individuals who cycle in and out of the system with a series of misdemeanor offenses. Often due to constraints of caseload and poor record linkage, prior interactions with an individual may not be considered when an individual comes back into the system, let alone in a proactive manner through the application of diversion programs. The Los Angeles City Attorney's Office recently created a new Recidivism Reduction and Drug Diversion unit (R2D2) tasked with reducing recidivism in this population. Here we describe a collaboration with this new unit as a case study for the incorporation of predictive equity into machine learning based decision making in a resource-constrained setting. The program seeks to improve outcomes by developing individually-tailored social service interventions (i.e., diversions, conditional plea agreements, stayed sentencing, or other favorable case disposition based on appropriate social service linkage rather than traditional sentencing methods) for individuals likely to experience subsequent interactions with the criminal justice system, a time and resource-intensive undertaking that necessitates an ability to focus resources on individuals most likely to be involved in a future case. Seeking to achieve both efficiency (through predictive accuracy) and equity (improving outcomes in traditionally under-served communities and working to mitigate existing disparities in criminal justice outcomes), we discuss the equity outcomes we seek to achieve, describe the corresponding choice of a metric for measuring predictive fairness in this context, and explore a set of options for balancing equity and efficiency when building and selecting machine learning models in an operational public policy setting.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {142–153},
numpages = {12},
keywords = {algorithmic fairness, criminal justice, machine learning disparities, racial bias},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
FACCT,2020,The concept of fairness in the GDPR: a linguistic and contextual interpretation,https://dl.acm.org/doi/10.1145/3351095.3372868,['Gianclaudio Malgieri'],41,"There is a growing attention on the notion of fairness in the GDPR in the European legal literature. However, the principle of fairness in the Data Protection framework is still ambiguous and uncertain, as computer science literature and interpretative guidelines reveal. This paper looks for a better understanding of the concept of fairness in the data protection field through two parallel methodological tools: linguistic comparison and contextual interpretation. In terms of linguistic comparison, the paper analyses all translations of the world ""fair"" in the GDPR in the EU official languages, as the CJEU suggests in CILFIT Case for the interpretation of the EU law. The analysis takes into account also the translation of the notion of fairness in other contiguous fields (e.g. at Article 8 of the EU Charter of fundamental rights or in the Consumer field, e.g. Unfair terms directive or Unfair commercial practice directive). In general, the notion of fairness is translated with several different nuances (in accordance or in discordance with the previous Data protection Directive and with Article 8 of the Charter) In some versions different words are used interchangeably (it is the case of French, Spanish and Portuguese texts), in other versions there seems to be a specific rationale for using different terms in different parts of the GDPR (it is the case of German and Greek version). The analysis reveals three mean semantic notions: correctness (Italian, Swedish, Romanian), loyalty (French, Spanish, Portuguese and the German version of ""Treu und Glaube"") and equitability (French, Spanish and Portuguese). Interestingly, these three notions have common roots in the Western legal history: the Roman law notion of ""bona fide"". Taking into account both the value of ""bona fide"" in the current European legal contexts and also a contextual interpretation of the role of fairness in the GDPR, the preliminary conclusions is that fairness refers to a substantial balancing of interests among data controllers and data subjects. The approach of fairness is effect-based: what is relevant is not the formal respect of procedures (in terms of transparency, lawfulness or accountability), but the substantial mitigation of unfair imbalances that create situations of ""vulnerability"". Building on these reflections, the paper analyses how the notion of fairness and imbalance are related to the idea of vulnerability, within and beyond the GDPR. In sum, the article suggests that the best interpretation of the fairness principles in the GDPR (taking into account both the notion of procedural fairness and of fair balancing) is the mitigation of data subjects' vulnerabilities through specific safeguards and measures.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3351095.3372868,
author = {Malgieri, Gianclaudio},
title = {The Concept of Fairness in the GDPR: A Linguistic and Contextual Interpretation},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372868},
doi = {10.1145/3351095.3372868},
abstract = {There is a growing attention on the notion of fairness in the GDPR in the European legal literature. However, the principle of fairness in the Data Protection framework is still ambiguous and uncertain, as computer science literature and interpretative guidelines reveal.This paper looks for a better understanding of the concept of fairness in the data protection field through two parallel methodological tools: linguistic comparison and contextual interpretation.In terms of linguistic comparison, the paper analyses all translations of the world ""fair"" in the GDPR in the EU official languages, as the CJEU suggests in CILFIT Case for the interpretation of the EU law.The analysis takes into account also the translation of the notion of fairness in other contiguous fields (e.g. at Article 8 of the EU Charter of fundamental rights or in the Consumer field, e.g. Unfair terms directive or Unfair commercial practice directive).In general, the notion of fairness is translated with several different nuances (in accordance or in discordance with the previous Data protection Directive and with Article 8 of the Charter)In some versions different words are used interchangeably (it is the case of French, Spanish and Portuguese texts), in other versions there seems to be a specific rationale for using different terms in different parts of the GDPR (it is the case of German and Greek version).The analysis reveals three mean semantic notions: correctness (Italian, Swedish, Romanian), loyalty (French, Spanish, Portuguese and the German version of ""Treu und Glaube"") and equitability (French, Spanish and Portuguese).Interestingly, these three notions have common roots in the Western legal history: the Roman law notion of ""bona fide"".Taking into account both the value of ""bona fide"" in the current European legal contexts and also a contextual interpretation of the role of fairness in the GDPR, the preliminary conclusions is that fairness refers to a substantial balancing of interests among data controllers and data subjects.The approach of fairness is effect-based: what is relevant is not the formal respect of procedures (in terms of transparency, lawfulness or accountability), but the substantial mitigation of unfair imbalances that create situations of ""vulnerability"". Building on these reflections, the paper analyses how the notion of fairness and imbalance are related to the idea of vulnerability, within and beyond the GDPR.In sum, the article suggests that the best interpretation of the fairness principles in the GDPR (taking into account both the notion of procedural fairness and of fair balancing) is the mitigation of data subjects' vulnerabilities through specific safeguards and measures.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {154–166},
numpages = {13},
keywords = {linguistic comparison, fairness, data protection, GDPR},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
FACCT,2020,Studying up: reorienting the study of algorithmic fairness around issues of power,https://dl.acm.org/doi/10.1145/3351095.3372859,"['Chelsea Barabas', 'Colin Doyle', 'JB Rubinovitz', 'Karthik Dinakar']",58,"Research within the social sciences and humanities has long characterized the work of data science as a sociotechnical process, comprised of a set of logics and techniques that are inseparable from specific social norms, expectations and contexts of development and use. Yet all too often the assumptions and premises underlying data analysis remain unexamined, even in contemporary debates about the fairness of algorithmic systems. This blindspot exists in part because the methodological toolkit used to evaluate the fairness of algorithmic systems remains limited to a narrow set of computational and legal modes of analysis. In this paper, we expand on Elish and Boyd's [17] call for data scientists to develop more robust frameworks for understanding their work as situated practice by examining a specific methodological debate within the field of anthropology, frequently referred to as the practice of ""studying up"". We reflect on the contributions that the call to ""study up"" has made in the field of anthropology before making the case that the field of algorithmic fairness would similarly benefit from a reorientation ""upward"". A case study from our own work illustrates what it looks like to reorient one's research questions ""up"" in a high-profile debate regarding the fairness of an algorithmic system - namely, pretrial risk assessment in American criminal law. We discuss the limitations of contemporary fairness discourse with regard to pretrial risk assessment before highlighting the insights gained when we reframe our research questions to focus on those who inhabit positions of power and authority within the U.S. court system. Finally, we reflect on the challenges we have encountered in implementing data science projects that ""study up"". In the process, we surface new insights and questions about what it means to ethically engage in data science work that directly confronts issues of power and authority.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3351095.3372859,
author = {Barabas, Chelsea and Doyle, Colin and Rubinovitz, JB and Dinakar, Karthik},
title = {Studying up: Reorienting the Study of Algorithmic Fairness around Issues of Power},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372859},
doi = {10.1145/3351095.3372859},
abstract = {Research within the social sciences and humanities has long characterized the work of data science as a sociotechnical process, comprised of a set of logics and techniques that are inseparable from specific social norms, expectations and contexts of development and use. Yet all too often the assumptions and premises underlying data analysis remain unexamined, even in contemporary debates about the fairness of algorithmic systems. This blindspot exists in part because the methodological toolkit used to evaluate the fairness of algorithmic systems remains limited to a narrow set of computational and legal modes of analysis. In this paper, we expand on Elish and Boyd's [17] call for data scientists to develop more robust frameworks for understanding their work as situated practice by examining a specific methodological debate within the field of anthropology, frequently referred to as the practice of ""studying up"". We reflect on the contributions that the call to ""study up"" has made in the field of anthropology before making the case that the field of algorithmic fairness would similarly benefit from a reorientation ""upward"". A case study from our own work illustrates what it looks like to reorient one's research questions ""up"" in a high-profile debate regarding the fairness of an algorithmic system - namely, pretrial risk assessment in American criminal law. We discuss the limitations of contemporary fairness discourse with regard to pretrial risk assessment before highlighting the insights gained when we reframe our research questions to focus on those who inhabit positions of power and authority within the U.S. court system. Finally, we reflect on the challenges we have encountered in implementing data science projects that ""study up"". In the process, we surface new insights and questions about what it means to ethically engage in data science work that directly confronts issues of power and authority.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {167–176},
numpages = {10},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
FACCT,2020,POTs: protective optimization technologies,https://dl.acm.org/doi/10.1145/3351095.3372853,"['Bogdan Kulynych', 'Rebekah Overdorf', 'Carmela Troncoso', 'Seda G√ºrses']",52,"Algorithmic fairness aims to address the economic, moral, social, and political impact that digital systems have on populations through solutions that can be applied by service providers. Fairness frameworks do so, in part, by mapping these problems to a narrow definition and assuming the service providers can be trusted to deploy countermeasures. Not surprisingly, these decisions limit fairness frameworks' ability to capture a variety of harms caused by systems. We characterize fairness limitations using concepts from requirements engineering and from social sciences. We show that the focus on algorithms' inputs and outputs misses harms that arise from systems interacting with the world; that the focus on bias and discrimination omits broader harms on populations and their environments; and that relying on service providers excludes scenarios where they are not cooperative or intentionally adversarial. We propose Protective Optimization Technologies (POTs). POTs, provide means for affected parties to address the negative impacts of systems in the environment, expanding avenues for political contestation. POTs intervene from outside the system, do not require service providers to cooperate, and can serve to correct, shift, or expose harms that systems impose on populations and their environments. We illustrate the potential and limitations of POTs in two case studies: countering road congestion caused by traffic beating applications, and recalibrating credit scoring for loan applicants.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3351095.3372853,
author = {Kulynych, Bogdan and Overdorf, Rebekah and Troncoso, Carmela and G\""{u}rses, Seda},
title = {POTs: Protective Optimization Technologies},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372853},
doi = {10.1145/3351095.3372853},
abstract = {Algorithmic fairness aims to address the economic, moral, social, and political impact that digital systems have on populations through solutions that can be applied by service providers. Fairness frameworks do so, in part, by mapping these problems to a narrow definition and assuming the service providers can be trusted to deploy countermeasures. Not surprisingly, these decisions limit fairness frameworks' ability to capture a variety of harms caused by systems.We characterize fairness limitations using concepts from requirements engineering and from social sciences. We show that the focus on algorithms' inputs and outputs misses harms that arise from systems interacting with the world; that the focus on bias and discrimination omits broader harms on populations and their environments; and that relying on service providers excludes scenarios where they are not cooperative or intentionally adversarial.We propose Protective Optimization Technologies (POTs). POTs, provide means for affected parties to address the negative impacts of systems in the environment, expanding avenues for political contestation. POTs intervene from outside the system, do not require service providers to cooperate, and can serve to correct, shift, or expose harms that systems impose on populations and their environments. We illustrate the potential and limitations of POTs in two case studies: countering road congestion caused by traffic beating applications, and recalibrating credit scoring for loan applicants.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {177–188},
numpages = {12},
keywords = {protective optimization technologies, fairness and accountability},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
FACCT,2020,Fair decision making using privacy-protected data,https://dl.acm.org/doi/10.1145/3351095.3372872,"['David Pujol', 'Ryan McKenna', 'Satya Kuppam', 'Michael Hay', 'Ashwin Machanavajjhala', 'Gerome Miklau']",52,"Data collected about individuals is regularly used to make decisions that impact those same individuals. We consider settings where sensitive personal data is used to decide who will receive resources or benefits. While it is well known that there is a trade-off between protecting privacy and the accuracy of decisions, we initiate a first-of-its-kind study into the impact of formally private mechanisms (based on differential privacy) on fair and equitable decision-making. We empirically investigate novel tradeoffs on two real-world decisions made using U.S. Census data (allocation of federal funds and assignment of voting rights benefits) as well as a classic apportionment problem. Our results show that if decisions are made using an ‚àà-differentially private version of the data, under strict privacy constraints (smaller ‚àà), the noise added to achieve privacy may disproportionately impact some groups over others. We propose novel measures of fairness in the context of randomized differentially private algorithms and identify a range of causes of outcome disparities. We also explore improved algorithms to remedy the unfairness observed.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3351095.3372872,
author = {Pujol, David and McKenna, Ryan and Kuppam, Satya and Hay, Michael and Machanavajjhala, Ashwin and Miklau, Gerome},
title = {Fair Decision Making Using Privacy-Protected Data},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372872},
doi = {10.1145/3351095.3372872},
abstract = {Data collected about individuals is regularly used to make decisions that impact those same individuals. We consider settings where sensitive personal data is used to decide who will receive resources or benefits. While it is well known that there is a trade-off between protecting privacy and the accuracy of decisions, we initiate a first-of-its-kind study into the impact of formally private mechanisms (based on differential privacy) on fair and equitable decision-making. We empirically investigate novel tradeoffs on two real-world decisions made using U.S. Census data (allocation of federal funds and assignment of voting rights benefits) as well as a classic apportionment problem.Our results show that if decisions are made using an ∈-differentially private version of the data, under strict privacy constraints (smaller ∈), the noise added to achieve privacy may disproportionately impact some groups over others. We propose novel measures of fairness in the context of randomized differentially private algorithms and identify a range of causes of outcome disparities. We also explore improved algorithms to remedy the unfairness observed.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {189–199},
numpages = {11},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
FACCT,2020,Fairness warnings and fair-MAML: learning fairly with minimal data,https://dl.acm.org/doi/10.1145/3351095.3372839,"['Dylan Slack', 'Sorelle A. Friedler', 'Emile Givental']",43,"Motivated by concerns surrounding the fairness effects of sharing and transferring fair machine learning tools, we propose two algorithms: Fairness Warnings and Fair-MAML. The first is a model-agnostic algorithm that provides interpretable boundary conditions for when a fairly trained model may not behave fairly on similar but slightly different tasks within a given domain. The second is a fair meta-learning approach to train models that can be quickly fine-tuned to specific tasks from only a few number of sample instances while balancing fairness and accuracy. We demonstrate experimentally the individual utility of each model using relevant baselines and provide the first experiment to our knowledge of K-shot fairness, i.e. training a fair model on a new task with only K data points. Then, we illustrate the usefulness of both algorithms as a combined method for training models from a few data points on new tasks while using Fairness Warnings as interpretable boundary conditions under which the newly trained model may not be fair.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3351095.3372839,
author = {Slack, Dylan and Friedler, Sorelle A. and Givental, Emile},
title = {Fairness Warnings and Fair-MAML: Learning Fairly with Minimal Data},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372839},
doi = {10.1145/3351095.3372839},
abstract = {Motivated by concerns surrounding the fairness effects of sharing and transferring fair machine learning tools, we propose two algorithms: Fairness Warnings and Fair-MAML. The first is a model-agnostic algorithm that provides interpretable boundary conditions for when a fairly trained model may not behave fairly on similar but slightly different tasks within a given domain. The second is a fair meta-learning approach to train models that can be quickly fine-tuned to specific tasks from only a few number of sample instances while balancing fairness and accuracy. We demonstrate experimentally the individual utility of each model using relevant baselines and provide the first experiment to our knowledge of K-shot fairness, i.e. training a fair model on a new task with only K data points. Then, we illustrate the usefulness of both algorithms as a combined method for training models from a few data points on new tasks while using Fairness Warnings as interpretable boundary conditions under which the newly trained model may not be fair.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {200–209},
numpages = {10},
keywords = {covariate shift, meta-learning, fairness, machine learning},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
FACCT,2020,Onward for the freedom of others: marching beyond the AI ethics,https://dl.acm.org/doi/10.1145/3351095.3373152,['Petros Terzis'],13,"The debate on the ethics of Artificial Intelligence brought together different stakeholders including but not limited to academics, policymakers, CEOs, activists, workers' representatives, lobbyists, journalists, and 'moral machines'. Prominent political institutions crafted principles for the 'ethical being' of the AI companies while tech giants were documenting ethics in a series of self-written guidelines. In parallel, a large community started to flourish, focusing on how to technically embed ethical parameters into algorithmic systems. Founded upon the philosophical work of Simone de Beauvoir and Jean-Paul Sartre, this paper explores the philosophical antinomies of the 'AI Ethics' debate as well as the conceptual disorientation of the 'fairness discussion'. By bringing the philosophy of existentialism to the dialogue, this paper attempts to challenge the dialectical monopoly of utilitarianism and sheds fresh light on the -already glaring- AI arena. Why is 'the AI Ethics guidelines' a futile battle doomed to dangerous abstraction? How this battle can harm our sense of collective freedom? Which is the uncomfortable reality that remains obscured by the smoke-gas of the 'AI Ethics' discussion? And eventually, what's the alternative? There seems to be a different pathway for discussing and implementing ethics; A pathway that sets the freedom of others at the epicenter of the battle for a sustainable and open to all future.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3351095.3373152,
author = {Terzis, Petros},
title = {Onward for the Freedom of Others: Marching beyond the AI Ethics},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3373152},
doi = {10.1145/3351095.3373152},
abstract = {The debate on the ethics of Artificial Intelligence brought together different stakeholders including but not limited to academics, policymakers, CEOs, activists, workers' representatives, lobbyists, journalists, and 'moral machines'. Prominent political institutions crafted principles for the 'ethical being' of the AI companies while tech giants were documenting ethics in a series of self-written guidelines. In parallel, a large community started to flourish, focusing on how to technically embed ethical parameters into algorithmic systems. Founded upon the philosophical work of Simone de Beauvoir and Jean-Paul Sartre, this paper explores the philosophical antinomies of the 'AI Ethics' debate as well as the conceptual disorientation of the 'fairness discussion'. By bringing the philosophy of existentialism to the dialogue, this paper attempts to challenge the dialectical monopoly of utilitarianism and sheds fresh light on the -already glaring- AI arena. Why is 'the AI Ethics guidelines' a futile battle doomed to dangerous abstraction? How this battle can harm our sense of collective freedom? Which is the uncomfortable reality that remains obscured by the smoke-gas of the 'AI Ethics' discussion? And eventually, what's the alternative? There seems to be a different pathway for discussing and implementing ethics; A pathway that sets the freedom of others at the epicenter of the battle for a sustainable and open to all future.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {220–229},
numpages = {10},
keywords = {existentialism, philosophy, algorithms, ethics, artificial intelligence},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
FACCT,2020,"Algorithmic targeting of social policies: fairness, accuracy, and distributed governance",https://dl.acm.org/doi/10.1145/3351095.3375784,"['Alejandro Noriega-Campero', 'Bernardo Garcia-Bulle', 'Luis Fernando Cantu', 'Michiel A. Bakker', 'Luis Tejerina', 'Alex Pentland']",22,"Targeted social policies are the main strategy for poverty alleviation across the developing world. These include targeted cash transfers (CTs), as well as targeted subsidies in health, education, housing, energy, childcare, and others. Due to the scale, diversity, and widespread relevance of targeted social policies like CTs, the algorithmic rules that decide who is eligible to benefit from them---and who is not---are among the most important algorithms operating in the world today. Here we report on a year-long engagement towards improving social targeting systems in a couple of developing countries. We demonstrate that a shift towards the use of AI methods in poverty-based targeting can substantially increase accuracy, extending the coverage of the poor by nearly a million people in two countries, without increasing expenditure. However, we also show that, absent explicit parity constraints, both status quo and AI-based systems induce disparities across population subgroups. Moreover, based on qualitative interviews with local social institutions, we find a lack of consensus on normative standards for prioritization and fairness criteria. Hence, we close by proposing a decision-support platform for distributed governance, which enables a diversity of institutions to customize the use of AI-based insights into their targeting decisions.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3351095.3375784,
author = {Noriega-Campero, Alejandro and Garcia-Bulle, Bernardo and Cantu, Luis Fernando and Bakker, Michiel A. and Tejerina, Luis and Pentland, Alex},
title = {Algorithmic Targeting of Social Policies: Fairness, Accuracy, and Distributed Governance},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375784},
doi = {10.1145/3351095.3375784},
abstract = {Targeted social policies are the main strategy for poverty alleviation across the developing world. These include targeted cash transfers (CTs), as well as targeted subsidies in health, education, housing, energy, childcare, and others. Due to the scale, diversity, and widespread relevance of targeted social policies like CTs, the algorithmic rules that decide who is eligible to benefit from them---and who is not---are among the most important algorithms operating in the world today. Here we report on a year-long engagement towards improving social targeting systems in a couple of developing countries. We demonstrate that a shift towards the use of AI methods in poverty-based targeting can substantially increase accuracy, extending the coverage of the poor by nearly a million people in two countries, without increasing expenditure. However, we also show that, absent explicit parity constraints, both status quo and AI-based systems induce disparities across population subgroups. Moreover, based on qualitative interviews with local social institutions, we find a lack of consensus on normative standards for prioritization and fairness criteria. Hence, we close by proposing a decision-support platform for distributed governance, which enables a diversity of institutions to customize the use of AI-based insights into their targeting decisions.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {241–251},
numpages = {11},
keywords = {algorithmic fairness, proxy means tests, AI for social good, cash transfers, targeted social programs},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
FACCT,2020,Roles for computing in social change,https://dl.acm.org/doi/10.1145/3351095.3372871,"['Rediet Abebe', 'Solon Barocas', 'Jon Kleinberg', 'Karen Levy', 'Manish Raghavan', 'David G. Robinson']",169,"A recent normative turn in computer science has brought concerns about fairness, bias, and accountability to the core of the field. Yet recent scholarship has warned that much of this technical work treats problematic features of the status quo as fixed, and fails to address deeper patterns of injustice and inequality. While acknowledging these critiques, we posit that computational research has valuable roles to play in addressing social problems --- roles whose value can be recognized even from a perspective that aspires toward fundamental social change. In this paper, we articulate four such roles, through an analysis that considers the opportunities as well as the significant risks inherent in such work. Computing research can serve as a diagnostic, helping us to understand and measure social problems with precision and clarity. As a formalizer, computing shapes how social problems are explicitly defined --- changing how those problems, and possible responses to them, are understood. Computing serves as rebuttal when it illuminates the boundaries of what is possible through technical means. And computing acts as synecdoche when it makes long-standing social problems newly salient in the public eye. We offer these paths forward as modalities that leverage the particular strengths of computational work in the service of social change, without overclaiming computing's capacity to solve social problems on its own.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3351095.3372871,
author = {Abebe, Rediet and Barocas, Solon and Kleinberg, Jon and Levy, Karen and Raghavan, Manish and Robinson, David G.},
title = {Roles for Computing in Social Change},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372871},
doi = {10.1145/3351095.3372871},
abstract = {A recent normative turn in computer science has brought concerns about fairness, bias, and accountability to the core of the field. Yet recent scholarship has warned that much of this technical work treats problematic features of the status quo as fixed, and fails to address deeper patterns of injustice and inequality. While acknowledging these critiques, we posit that computational research has valuable roles to play in addressing social problems --- roles whose value can be recognized even from a perspective that aspires toward fundamental social change. In this paper, we articulate four such roles, through an analysis that considers the opportunities as well as the significant risks inherent in such work. Computing research can serve as a diagnostic, helping us to understand and measure social problems with precision and clarity. As a formalizer, computing shapes how social problems are explicitly defined --- changing how those problems, and possible responses to them, are understood. Computing serves as rebuttal when it illuminates the boundaries of what is possible through technical means. And computing acts as synecdoche when it makes long-standing social problems newly salient in the public eye. We offer these paths forward as modalities that leverage the particular strengths of computational work in the service of social change, without overclaiming computing's capacity to solve social problems on its own.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {252–260},
numpages = {9},
keywords = {discrimination, societal implications of AI, social change, inequality},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
FACCT,2020,The relationship between trust in AI and trustworthy machine learning technologies,https://dl.acm.org/doi/10.1145/3351095.3372834,"['Ehsan Toreini', 'Mhairi Aitken', 'Kovila Coopamootoo', 'Karen Elliott', 'Carlos Gonzalez Zelaya', 'Aad van Moorsel']",139,"To design and develop AI-based systems that users and the larger public can justifiably trust, one needs to understand how machine learning technologies impact trust. To guide the design and implementation of trusted AI-based systems, this paper provides a systematic approach to relate considerations about trust from the social sciences to trustworthiness technologies proposed for AI-based services and products. We start from the ABI+ (Ability, Benevolence, Integrity, Predictability) framework augmented with a recently proposed mapping of ABI+ on qualities of technologies that support trust. We consider four categories of trustworthiness technologies for machine learning, namely these for Fairness, Explainability, Auditability and Safety (FEAS) and discuss if and how these support the required qualities. Moreover, trust can be impacted throughout the life cycle of AI-based systems, and we therefore introduce the concept of Chain of Trust to discuss trustworthiness technologies in all stages of the life cycle. In so doing we establish the ways in which machine learning technologies support trusted AI-based systems. Finally, FEAS has obvious relations with known frameworks and therefore we relate FEAS to a variety of international 'principled AI' policy and technology frameworks that have emerged in recent years.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3351095.3372834,
author = {Toreini, Ehsan and Aitken, Mhairi and Coopamootoo, Kovila and Elliott, Karen and Zelaya, Carlos Gonzalez and van Moorsel, Aad},
title = {The Relationship between Trust in AI and Trustworthy Machine Learning Technologies},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372834},
doi = {10.1145/3351095.3372834},
abstract = {To design and develop AI-based systems that users and the larger public can justifiably trust, one needs to understand how machine learning technologies impact trust. To guide the design and implementation of trusted AI-based systems, this paper provides a systematic approach to relate considerations about trust from the social sciences to trustworthiness technologies proposed for AI-based services and products. We start from the ABI+ (Ability, Benevolence, Integrity, Predictability) framework augmented with a recently proposed mapping of ABI+ on qualities of technologies that support trust. We consider four categories of trustworthiness technologies for machine learning, namely these for Fairness, Explainability, Auditability and Safety (FEAS) and discuss if and how these support the required qualities. Moreover, trust can be impacted throughout the life cycle of AI-based systems, and we therefore introduce the concept of Chain of Trust to discuss trustworthiness technologies in all stages of the life cycle. In so doing we establish the ways in which machine learning technologies support trusted AI-based systems. Finally, FEAS has obvious relations with known frameworks and therefore we relate FEAS to a variety of international 'principled AI' policy and technology frameworks that have emerged in recent years.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {272–283},
numpages = {12},
keywords = {machine learning, trustworthiness, artificial intelligence, trust},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
FACCT,2020,Lessons from archives: strategies for collecting sociocultural data in machine learning,https://dl.acm.org/doi/10.1145/3351095.3372829,"['Eun Seo Jo', 'Timnit Gebru']",186,"A growing body of work shows that many problems in fairness, accountability, transparency, and ethics in machine learning systems are rooted in decisions surrounding the data collection and annotation process. In spite of its fundamental nature however, data collection remains an overlooked part of the machine learning (ML) pipeline. In this paper, we argue that a new specialization should be formed within ML that is focused on methodologies for data collection and annotation: efforts that require institutional frameworks and procedures. Specifically for sociocultural data, parallels can be drawn from archives and libraries. Archives are the longest standing communal effort to gather human information and archive scholars have already developed the language and procedures to address and discuss many challenges pertaining to data collection such as consent, power, inclusivity, transparency, and ethics & privacy. We discuss these five key approaches in document collection practices in archives that can inform data collection in sociocultural ML. By showing data collection practices from another field, we encourage ML research to be more cognizant and systematic in data collection and draw from interdisciplinary expertise.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3351095.3372829,
author = {Jo, Eun Seo and Gebru, Timnit},
title = {Lessons from Archives: Strategies for Collecting Sociocultural Data in Machine Learning},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372829},
doi = {10.1145/3351095.3372829},
abstract = {A growing body of work shows that many problems in fairness, accountability, transparency, and ethics in machine learning systems are rooted in decisions surrounding the data collection and annotation process. In spite of its fundamental nature however, data collection remains an overlooked part of the machine learning (ML) pipeline. In this paper, we argue that a new specialization should be formed within ML that is focused on methodologies for data collection and annotation: efforts that require institutional frameworks and procedures. Specifically for sociocultural data, parallels can be drawn from archives and libraries. Archives are the longest standing communal effort to gather human information and archive scholars have already developed the language and procedures to address and discuss many challenges pertaining to data collection such as consent, power, inclusivity, transparency, and ethics & privacy. We discuss these five key approaches in document collection practices in archives that can inform data collection in sociocultural ML. By showing data collection practices from another field, we encourage ML research to be more cognizant and systematic in data collection and draw from interdisciplinary expertise.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {306–316},
numpages = {11},
keywords = {data collection, machine learning, archives, datasets, sociocultural data, ML fairness},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
FACCT,2020,Bidding strategies with gender nondiscrimination constraints for online ad auctions,https://dl.acm.org/doi/10.1145/3351095.3375783,"['Milad Nasr', 'Michael Carl Tschantz']",26,"Interactions between bids to show ads online can lead to an advertiser's ad being shown to more men than women even when the advertiser does not target towards men. We design bidding strategies that advertisers can use to avoid such emergent discrimination without having to modify the auction mechanism. We mathematically analyze the strategies to determine the additional cost to the advertiser for avoiding discrimination, proving our strategies to be optimal in some settings. We use simulations to understand other settings.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3351095.3375783,
author = {Nasr, Milad and Tschantz, Michael Carl},
title = {Bidding Strategies with Gender Nondiscrimination Constraints for Online Ad Auctions},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375783},
doi = {10.1145/3351095.3375783},
abstract = {Interactions between bids to show ads online can lead to an advertiser's ad being shown to more men than women even when the advertiser does not target towards men. We design bidding strategies that advertisers can use to avoid such emergent discrimination without having to modify the auction mechanism. We mathematically analyze the strategies to determine the additional cost to the advertiser for avoiding discrimination, proving our strategies to be optimal in some settings. We use simulations to understand other settings.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {337–347},
numpages = {11},
keywords = {targeted advertising, MDPs, online auctions, fairness constraints},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
FACCT,2020,Multi-category fairness in sponsored search auctions,https://dl.acm.org/doi/10.1145/3351095.3372848,"['Christina Ilvento', 'Meena Jagadeesan', 'Shuchi Chawla']",23,"Fairness in advertising is a topic of particular concern motivated by theoretical and empirical observations in both the computer science and economics literature. We examine the problem of fairness in advertising for general purpose platforms that service advertisers from many different categories. First, we propose inter-category and intra-category fairness desiderata that take inspiration from individual fairness and envy-freeness. Second, we investigate the ""platform utility"" (a proxy for the quality of allocation) achievable by mechanisms satisfying these desiderata. More specifically, we compare the utility of fair mechanisms against the unfair optimum, and show by construction that our fairness desiderata are compatible with utility. Our mechanisms also enjoy nice implementation properties including metric-obliviousness, which allows the platform to produce fair allocations without needing to know the specifics of the fairness requirements.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3351095.3372848,
author = {Ilvento, Christina and Jagadeesan, Meena and Chawla, Shuchi},
title = {Multi-Category Fairness in Sponsored Search Auctions},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372848},
doi = {10.1145/3351095.3372848},
abstract = {Fairness in advertising is a topic of particular concern motivated by theoretical and empirical observations in both the computer science and economics literature. We examine the problem of fairness in advertising for general purpose platforms that service advertisers from many different categories. First, we propose inter-category and intra-category fairness desiderata that take inspiration from individual fairness and envy-freeness. Second, we investigate the ""platform utility"" (a proxy for the quality of allocation) achievable by mechanisms satisfying these desiderata. More specifically, we compare the utility of fair mechanisms against the unfair optimum, and show by construction that our fairness desiderata are compatible with utility. Our mechanisms also enjoy nice implementation properties including metric-obliviousness, which allows the platform to produce fair allocations without needing to know the specifics of the fairness requirements.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {348–358},
numpages = {11},
keywords = {envy-freeness, algorithmic fairness, advertisement auctions, individual fairness, utility},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
FACCT,2020,Reducing sentiment polarity for demographic attributes in word embeddings using adversarial learning,https://dl.acm.org/doi/10.1145/3351095.3372837,"['Chris Sweeney', 'Maryam Najafian']",23,"The use of word embedding models in sentiment analysis has gained a lot of traction in the Natural Language Processing (NLP) community. However, many inherently neutral word vectors describing demographic identity have unintended implicit correlations with negative or positive sentiment, resulting in unfair downstream machine learning algorithms. We leverage adversarial learning to decorrelate demographic identity term word vectors with positive or negative sentiment, and re-embed them into the word embeddings. We show that our method effectively minimizes unfair positive/negative sentiment polarity while retaining the semantic accuracy of the word embeddings. Furthermore, we show that our method effectively reduces unfairness in downstream sentiment regression and can be extended to reduce unfairness in toxicity classification tasks.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3351095.3372837,
author = {Sweeney, Chris and Najafian, Maryam},
title = {Reducing Sentiment Polarity for Demographic Attributes in Word Embeddings Using Adversarial Learning},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372837},
doi = {10.1145/3351095.3372837},
abstract = {The use of word embedding models in sentiment analysis has gained a lot of traction in the Natural Language Processing (NLP) community. However, many inherently neutral word vectors describing demographic identity have unintended implicit correlations with negative or positive sentiment, resulting in unfair downstream machine learning algorithms. We leverage adversarial learning to decorrelate demographic identity term word vectors with positive or negative sentiment, and re-embed them into the word embeddings. We show that our method effectively minimizes unfair positive/negative sentiment polarity while retaining the semantic accuracy of the word embeddings. Furthermore, we show that our method effectively reduces unfairness in downstream sentiment regression and can be extended to reduce unfairness in toxicity classification tasks.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {359–368},
numpages = {10},
keywords = {embeddings, fairness, NLP},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
FACCT,2020,"An empirical study on the perceived fairness of realistic, imperfect machine learning models",https://dl.acm.org/doi/10.1145/3351095.3372831,"['Galen Harrison', 'Julia Hanson', 'Christine Jacinto', 'Julio Ramirez', 'Blase Ur']",64,"There are many competing definitions of what statistical properties make a machine learning model fair. Unfortunately, research has shown that some key properties are mutually exclusive. Realistic models are thus necessarily imperfect, choosing one side of a trade-off or the other. To gauge perceptions of the fairness of such realistic, imperfect models, we conducted a between-subjects experiment with 502 Mechanical Turk workers. Each participant compared two models for deciding whether to grant bail to criminal defendants. The first model equalized one potentially desirable model property, with the other property varying across racial groups. The second model did the opposite. We tested pairwise trade-offs between the following four properties: accuracy; false positive rate; outcomes; and the consideration of race. We also varied which racial group the model disadvantaged. We observed a preference among participants for equalizing the false positive rate between groups over equalizing accuracy. Nonetheless, no preferences were overwhelming, and both sides of each trade-off we tested were strongly preferred by a non-trivial fraction of participants. We observed nuanced distinctions between participants considering a model ""unbiased"" and considering it ""fair."" Furthermore, even when a model within a trade-off pair was seen as fair and unbiased by a majority of participants, we did not observe consensus that a machine learning model was preferable to a human judge. Our results highlight challenges for building machine learning models that are perceived as fair and broadly acceptable in realistic situations.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3351095.3372831,
author = {Harrison, Galen and Hanson, Julia and Jacinto, Christine and Ramirez, Julio and Ur, Blase},
title = {An Empirical Study on the Perceived Fairness of Realistic, Imperfect Machine Learning Models},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372831},
doi = {10.1145/3351095.3372831},
abstract = {There are many competing definitions of what statistical properties make a machine learning model fair. Unfortunately, research has shown that some key properties are mutually exclusive. Realistic models are thus necessarily imperfect, choosing one side of a trade-off or the other. To gauge perceptions of the fairness of such realistic, imperfect models, we conducted a between-subjects experiment with 502 Mechanical Turk workers. Each participant compared two models for deciding whether to grant bail to criminal defendants. The first model equalized one potentially desirable model property, with the other property varying across racial groups. The second model did the opposite. We tested pairwise trade-offs between the following four properties: accuracy; false positive rate; outcomes; and the consideration of race. We also varied which racial group the model disadvantaged. We observed a preference among participants for equalizing the false positive rate between groups over equalizing accuracy. Nonetheless, no preferences were overwhelming, and both sides of each trade-off we tested were strongly preferred by a non-trivial fraction of participants. We observed nuanced distinctions between participants considering a model ""unbiased"" and considering it ""fair."" Furthermore, even when a model within a trade-off pair was seen as fair and unbiased by a majority of participants, we did not observe consensus that a machine learning model was preferable to a human judge. Our results highlight challenges for building machine learning models that are perceived as fair and broadly acceptable in realistic situations.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {392–402},
numpages = {11},
keywords = {fairness, machine learning, accountability, survey, data science},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
FACCT,2020,Artificial mental phenomena: psychophysics as a framework to detect perception biases in AI models,https://dl.acm.org/doi/10.1145/3351095.3375623,"['Lizhen Liang', 'Daniel E. Acuna']",10,"Detecting biases in artificial intelligence has become difficult because of the impenetrable nature of deep learning. The central difficulty is in relating unobservable phenomena deep inside models with observable, outside quantities that we can measure from inputs and outputs. For example, can we detect gendered perceptions of occupations (e.g., female librarian, male electrician) using questions to and answers from a word embedding-based system? Current techniques for detecting biases are often customized for a task, dataset, or method, affecting their generalization. In this work, we draw from Psychophysics in Experimental Psychology---meant to relate quantities from the real world (i.e., ""Physics"") into subjective measures in the mind (i.e., ""Psyche"")---to propose an intellectually coherent and generalizable framework to detect biases in AI. Specifically, we adapt the two-alternative forced choice task (2AFC) to estimate potential biases and the strength of those biases in black-box models. We successfully reproduce previously-known biased perceptions in word embeddings and sentiment analysis predictions. We discuss how concepts in experimental psychology can be naturally applied to understanding artificial mental phenomena, and how psychophysics can form a useful methodological foundation to study fairness in AI.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3351095.3375623,
author = {Liang, Lizhen and Acuna, Daniel E.},
title = {Artificial Mental Phenomena: Psychophysics as a Framework to Detect Perception Biases in AI Models},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375623},
doi = {10.1145/3351095.3375623},
abstract = {Detecting biases in artificial intelligence has become difficult because of the impenetrable nature of deep learning. The central difficulty is in relating unobservable phenomena deep inside models with observable, outside quantities that we can measure from inputs and outputs. For example, can we detect gendered perceptions of occupations (e.g., female librarian, male electrician) using questions to and answers from a word embedding-based system? Current techniques for detecting biases are often customized for a task, dataset, or method, affecting their generalization. In this work, we draw from Psychophysics in Experimental Psychology---meant to relate quantities from the real world (i.e., ""Physics"") into subjective measures in the mind (i.e., ""Psyche"")---to propose an intellectually coherent and generalizable framework to detect biases in AI. Specifically, we adapt the two-alternative forced choice task (2AFC) to estimate potential biases and the strength of those biases in black-box models. We successfully reproduce previously-known biased perceptions in word embeddings and sentiment analysis predictions. We discuss how concepts in experimental psychology can be naturally applied to understanding artificial mental phenomena, and how psychophysics can form a useful methodological foundation to study fairness in AI.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {403–412},
numpages = {10},
keywords = {biases in word embeddings, artificial psychophysics, two-alternative forced choice task, biases in sentiment analysis},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
FACCT,2020,The social lives of generative adversarial networks,https://dl.acm.org/doi/10.1145/3351095.3373156,['Michael Castelle'],10,"Generative adversarial networks (GANs) are a genre of deep learning model of significant practical and theoretical interest for their facility in producing photorealistic 'fake' images which are plausibly similar, but not identical, to a corpus of training data. But from the perspective of a sociologist, the distinctive architecture of GANs is highly suggestive. First, a convolutional neural network for classification, on its own, is (at present) popularly considered to be an 'AI'; and a generative neural network is a kind of inversion of such a classification network (i.e. a layered transformation from a vector of numbers to an image, as opposed to a transformation from an image to a vector of numbers). If, then, in the training of GANs, these two 'AIs' interact with each other in a dyadic fashion, shouldn't we consider that form of learning... social? This observation can lead to some surprising associations as we compare and contrast GANs with the theories of the sociologist Pierre Bourdieu, whose concept of the so-called habitus is one which is simultaneously cognitive and social: a productive perception in which classification practices and practical action cannot be fully disentangled. Bourdieu had long been concerned with the reproduction of social stratification: his early works studied formal public schooling in France not as an egalitarian system but instead as one which unintentionally maintained the persistence of class distinctions. It was, he argued, through the cultural inculcation of an embodied and partially unconscious habitus---a ""durably installed generative principle of regulated improvisations""---that, he argued, students from the upper classes are given an advantage which is only further reinforced throughout their educational trajectories. For Bourdieu, institutions of schooling instill ""deeply interiorized master patterns"" of behavior and thought (and classification) which in turn direct the acquisition of subsequent patterns, whose character is determined not simply by this cognitive layering but by their actual use in lived practice, especially early in childhood development. In this work I develop a productive analogy between the GAN architecture and Bourdieu's habitus, in three ways. First, I call attention to the fact that connectionist approaches and Bourdieu's theories were both conceived as revolts against rule-bound paradigms. In the 1980s, Rumelhart and McClelland used a multilayer neural network to learn the phonology of English past-tense verbs because ""sometimes we don't follow the rules... language is full of exceptions to the rules""; and in the case of Bourdieu, the habitus was an answer to a long-standing question: ""how can behaviour be regulated without being the product of obedience to rules?"" Bourdieu strove to transgress what was then seen in the social sciences as a conceptual opposition between structure-based theories of social life and those which emphasized an embodied agency. Second, I suggest that concerns about bias and discrimination in machine learning in recent years can in part be attributed due to the increased use of ML models not just for static classification but for practical action. Similarly, the habitus for Bourdieu is simultaneously durable and transposable: its judgments may be relatively stable, but are capable of being deployed dynamically in novel and varying social situations---or what ML practitioners might call generalizability. We can thus theorize generative models (including GANs) as biased not just in their stereotyped classifications, but through their potential for actively generating new biased data. These generated actions then recursively become part of the social arena Bourdieu called the field, into which new agents are 'born' and for which they may know few alternatives. Finally, it is intriguing that GAN researchers and Bourdieu both extensively use metaphors from game theory. Goodfellow described the GAN architecture as a ""two-player minimax game with value function V(G,D)"", meaning that there is a single abstract function whose output value the discriminator is trying to maximize and which the generator is trying to minimize; but the dynamic nature of the GAN training process means that convergence to Nash equilibrium is nontrivial. But for Bourdieu, such a utility-based approach to artistic creation could not be more crude when compared to the social reality of art worlds: utilitarianism is, for him, ""the degree zero of sociology"", by which he means an isolated, inert, and amodal---and therefore not particularly sociological---starting point. Moreover, 19th-century bohemian culture was characterized primarily by its inversion of financial incentives, in which failure is a kind of success, and ""selling out"" (i.e. maximizing profit) worst of all; and thus the relentless optimization of neural networks may be fundamentally at odds with the ""value functions"" of many human artists. I conclude that deep learning, while primarily understood as a scientific and technical achievement, may also intentionally or unintentionally constitute a nascent, independent reinvention of social theory.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3351095.3373156,
author = {Castelle, Michael},
title = {The Social Lives of Generative Adversarial Networks},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3373156},
doi = {10.1145/3351095.3373156},
abstract = {Generative adversarial networks (GANs) are a genre of deep learning model of significant practical and theoretical interest for their facility in producing photorealistic 'fake' images which are plausibly similar, but not identical, to a corpus of training data. But from the perspective of a sociologist, the distinctive architecture of GANs is highly suggestive. First, a convolutional neural network for classification, on its own, is (at present) popularly considered to be an 'AI'; and a generative neural network is a kind of inversion of such a classification network (i.e. a layered transformation from a vector of numbers to an image, as opposed to a transformation from an image to a vector of numbers). If, then, in the training of GANs, these two 'AIs' interact with each other in a dyadic fashion, shouldn't we consider that form of learning... social? This observation can lead to some surprising associations as we compare and contrast GANs with the theories of the sociologist Pierre Bourdieu, whose concept of the so-called habitus is one which is simultaneously cognitive and social: a productive perception in which classification practices and practical action cannot be fully disentangled.Bourdieu had long been concerned with the reproduction of social stratification: his early works studied formal public schooling in France not as an egalitarian system but instead as one which unintentionally maintained the persistence of class distinctions. It was, he argued, through the cultural inculcation of an embodied and partially unconscious habitus---a ""durably installed generative principle of regulated improvisations""---that, he argued, students from the upper classes are given an advantage which is only further reinforced throughout their educational trajectories. For Bourdieu, institutions of schooling instill ""deeply interiorized master patterns"" of behavior and thought (and classification) which in turn direct the acquisition of subsequent patterns, whose character is determined not simply by this cognitive layering but by their actual use in lived practice, especially early in childhood development.In this work I develop a productive analogy between the GAN architecture and Bourdieu's habitus, in three ways. First, I call attention to the fact that connectionist approaches and Bourdieu's theories were both conceived as revolts against rule-bound paradigms. In the 1980s, Rumelhart and McClelland used a multilayer neural network to learn the phonology of English past-tense verbs because ""sometimes we don't follow the rules... language is full of exceptions to the rules""; and in the case of Bourdieu, the habitus was an answer to a long-standing question: ""how can behaviour be regulated without being the product of obedience to rules?"" Bourdieu strove to transgress what was then seen in the social sciences as a conceptual opposition between structure-based theories of social life and those which emphasized an embodied agency.Second, I suggest that concerns about bias and discrimination in machine learning in recent years can in part be attributed due to the increased use of ML models not just for static classification but for practical action. Similarly, the habitus for Bourdieu is simultaneously durable and transposable: its judgments may be relatively stable, but are capable of being deployed dynamically in novel and varying social situations---or what ML practitioners might call generalizability. We can thus theorize generative models (including GANs) as biased not just in their stereotyped classifications, but through their potential for actively generating new biased data. These generated actions then recursively become part of the social arena Bourdieu called the field, into which new agents are 'born' and for which they may know few alternatives.Finally, it is intriguing that GAN researchers and Bourdieu both extensively use metaphors from game theory. Goodfellow described the GAN architecture as a ""two-player minimax game with value function V(G,D)"", meaning that there is a single abstract function whose output value the discriminator is trying to maximize and which the generator is trying to minimize; but the dynamic nature of the GAN training process means that convergence to Nash equilibrium is nontrivial. But for Bourdieu, such a utility-based approach to artistic creation could not be more crude when compared to the social reality of art worlds: utilitarianism is, for him, ""the degree zero of sociology"", by which he means an isolated, inert, and amodal---and therefore not particularly sociological---starting point. Moreover, 19th-century bohemian culture was characterized primarily by its inversion of financial incentives, in which failure is a kind of success, and ""selling out"" (i.e. maximizing profit) worst of all; and thus the relentless optimization of neural networks may be fundamentally at odds with the ""value functions"" of many human artists. I conclude that deep learning, while primarily understood as a scientific and technical achievement, may also intentionally or unintentionally constitute a nascent, independent reinvention of social theory.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {413},
numpages = {1},
keywords = {generative adversarial networks, sociological theory, habitus, bias, game theory},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
FACCT,2020,Integrating FATE/critical data studies into data science curricula: where are we going and how do we get there?,https://dl.acm.org/doi/10.1145/3351095.3372832,"['Jo Bates', 'David Cameron', 'Alessandro Checco', 'Paul Clough', 'Frank Hopfgartner', 'Suvodeep Mazumdar', 'Laura Sbaffi', 'Peter Stordy', 'Antonio de la Vega de Le√≥n']",40,"There have been multiple calls for integrating topics related to fairness, accountability, transparency, ethics (FATE) and social justice into Data Science curricula, but little exploration of how this might work in practice. This paper presents the findings of a collaborative auto-ethnography (CAE) engaged in by a MSc Data Science teaching team based at University of Sheffield (UK) Information School where FATE/Critical Data Studies (CDS) topics have been a core part of the curriculum since 2015/16. In this paper, we adopt the CAE approach to reflect on our experiences of working at the intersection of disciplines, and our progress and future plans for integrating FATE/CDS into the curriculum. We identify a series of challenges for deeper FATE/CDS integration related to our own competencies and the wider socio-material context of Higher Education in the UK. We conclude with recommendations for ourselves and the wider FATE/CDS orientated Data Science community.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3351095.3372832,
author = {Bates, Jo and Cameron, David and Checco, Alessandro and Clough, Paul and Hopfgartner, Frank and Mazumdar, Suvodeep and Sbaffi, Laura and Stordy, Peter and de la Vega de Le\'{o}n, Antonio},
title = {Integrating FATE/Critical Data Studies into Data Science Curricula: Where Are We Going and How Do We Get There?},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372832},
doi = {10.1145/3351095.3372832},
abstract = {There have been multiple calls for integrating topics related to fairness, accountability, transparency, ethics (FATE) and social justice into Data Science curricula, but little exploration of how this might work in practice. This paper presents the findings of a collaborative auto-ethnography (CAE) engaged in by a MSc Data Science teaching team based at University of Sheffield (UK) Information School where FATE/Critical Data Studies (CDS) topics have been a core part of the curriculum since 2015/16. In this paper, we adopt the CAE approach to reflect on our experiences of working at the intersection of disciplines, and our progress and future plans for integrating FATE/CDS into the curriculum. We identify a series of challenges for deeper FATE/CDS integration related to our own competencies and the wider socio-material context of Higher Education in the UK. We conclude with recommendations for ourselves and the wider FATE/CDS orientated Data Science community.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {425–435},
numpages = {11},
keywords = {critical data studies, data science, higher education, FATE},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
FACCT,2020,Bias in word embeddings,https://dl.acm.org/doi/10.1145/3351095.3372843,"['Orestis Papakyriakopoulos', 'Simon Hegelich', 'Juan Carlos Medina Serrano', 'Fabienne Marco']",63,"Word embeddings are a widely used set of natural language processing techniques that map words to vectors of real numbers. These vectors are used to improve the quality of generative and predictive models. Recent studies demonstrate that word embeddings contain and amplify biases present in data, such as stereotypes and prejudice. In this study, we provide a complete overview of bias in word embeddings. We develop a new technique for bias detection for gendered languages and use it to compare bias in embeddings trained on Wikipedia and on political social media data. We investigate bias diffusion and prove that existing biases are transferred to further machine learning models. We test two techniques for bias mitigation and show that the generally proposed methodology for debiasing models at the embeddings level is insufficient. Finally, we employ biased word embeddings and illustrate that they can be used for the detection of similar biases in new data. Given that word embeddings are widely used by commercial companies, we discuss the challenges and required actions towards fair algorithmic implementations and applications.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3351095.3372843,
author = {Papakyriakopoulos, Orestis and Hegelich, Simon and Serrano, Juan Carlos Medina and Marco, Fabienne},
title = {Bias in Word Embeddings},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372843},
doi = {10.1145/3351095.3372843},
abstract = {Word embeddings are a widely used set of natural language processing techniques that map words to vectors of real numbers. These vectors are used to improve the quality of generative and predictive models. Recent studies demonstrate that word embeddings contain and amplify biases present in data, such as stereotypes and prejudice. In this study, we provide a complete overview of bias in word embeddings. We develop a new technique for bias detection for gendered languages and use it to compare bias in embeddings trained on Wikipedia and on political social media data. We investigate bias diffusion and prove that existing biases are transferred to further machine learning models. We test two techniques for bias mitigation and show that the generally proposed methodology for debiasing models at the embeddings level is insufficient. Finally, we employ biased word embeddings and illustrate that they can be used for the detection of similar biases in new data. Given that word embeddings are widely used by commercial companies, we discuss the challenges and required actions towards fair algorithmic implementations and applications.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {446–457},
numpages = {12},
keywords = {diffusion, word embeddings, sexism, racism, mitigation, bias, homophobia, fairness, detection},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
FACCT,2020,"What does it mean to 'solve' the problem of discrimination in hiring?: social, technical and legal perspectives from the UK on automated hiring systems",https://dl.acm.org/doi/10.1145/3351095.3372849,"['Javier S√°nchez-Monedero', 'Lina Dencik', 'Lilian Edwards']",113,"Discriminatory practices in recruitment and hiring are an ongoing issue that is a concern not just for workplace relations, but also for wider understandings of economic justice and inequality. The ability to get and keep a job is a key aspect of participating in society and sustaining livelihoods. Yet the way decisions are made on who is eligible for jobs, and why, are rapidly changing with the advent and growth in uptake of automated hiring systems (AHSs) powered by data-driven tools. Evidence of the extent of this uptake around the globe is scarce, but a recent report estimated that 98% of Fortune 500 companies use Applicant Tracking Systems of some kind in their hiring process, a trend driven by perceived efficiency measures and cost-savings. Key concerns about such AHSs include the lack of transparency and potential limitation of access to jobs for specific profiles. In relation to the latter, however, several of these AHSs claim to detect and mitigate discriminatory practices against protected groups and promote diversity and inclusion at work. Yet whilst these tools have a growing user-base around the world, such claims of 'bias mitigation' are rarely scrutinised and evaluated, and when done so, have almost exclusively been from a US socio-legal perspective. In this paper, we introduce a perspective outside the US by critically examining how three prominent automated hiring systems (AHSs) in regular use in the UK, HireVue, Pymetrics and Applied, understand and attempt to mitigate bias and discrimination. These systems have been chosen as they explicitly claim to address issues of discrimination in hiring and, unlike many of their competitors, provide some information about how their systems work that can inform an analysis. Using publicly available documents, we describe how their tools are designed, validated and audited for bias, highlighting assumptions and limitations, before situating these in the socio-legal context of the UK. The UK has a very different legal background to the US in terms not only of hiring and equality law, but also in terms of data protection (DP) law. We argue that this might be important for addressing concerns about transparency and could mean a challenge to building bias mitigation into AHSs definitively capable of meeting EU legal standards. This is significant as these AHSs, especially those developed in the US, may obscure rather than improve systemic discrimination in the workplace.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3351095.3372849,
author = {S\'{a}nchez-Monedero, Javier and Dencik, Lina and Edwards, Lilian},
title = {What Does It Mean to 'solve' the Problem of Discrimination in Hiring? Social, Technical and Legal Perspectives from the UK on Automated Hiring Systems},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372849},
doi = {10.1145/3351095.3372849},
abstract = {Discriminatory practices in recruitment and hiring are an ongoing issue that is a concern not just for workplace relations, but also for wider understandings of economic justice and inequality. The ability to get and keep a job is a key aspect of participating in society and sustaining livelihoods. Yet the way decisions are made on who is eligible for jobs, and why, are rapidly changing with the advent and growth in uptake of automated hiring systems (AHSs) powered by data-driven tools. Evidence of the extent of this uptake around the globe is scarce, but a recent report estimated that 98% of Fortune 500 companies use Applicant Tracking Systems of some kind in their hiring process, a trend driven by perceived efficiency measures and cost-savings. Key concerns about such AHSs include the lack of transparency and potential limitation of access to jobs for specific profiles. In relation to the latter, however, several of these AHSs claim to detect and mitigate discriminatory practices against protected groups and promote diversity and inclusion at work. Yet whilst these tools have a growing user-base around the world, such claims of 'bias mitigation' are rarely scrutinised and evaluated, and when done so, have almost exclusively been from a US socio-legal perspective.In this paper, we introduce a perspective outside the US by critically examining how three prominent automated hiring systems (AHSs) in regular use in the UK, HireVue, Pymetrics and Applied, understand and attempt to mitigate bias and discrimination. These systems have been chosen as they explicitly claim to address issues of discrimination in hiring and, unlike many of their competitors, provide some information about how their systems work that can inform an analysis. Using publicly available documents, we describe how their tools are designed, validated and audited for bias, highlighting assumptions and limitations, before situating these in the socio-legal context of the UK. The UK has a very different legal background to the US in terms not only of hiring and equality law, but also in terms of data protection (DP) law. We argue that this might be important for addressing concerns about transparency and could mean a challenge to building bias mitigation into AHSs definitively capable of meeting EU legal standards. This is significant as these AHSs, especially those developed in the US, may obscure rather than improve systemic discrimination in the workplace.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {458–468},
numpages = {11},
keywords = {social justice, fairness, automated hiring, discrimination, GDPR, algorithmic decision-making, socio-technical systems},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
FACCT,2020,Mitigating bias in algorithmic hiring: evaluating claims and practices,https://dl.acm.org/doi/10.1145/3351095.3372828,"['Manish Raghavan', 'Solon Barocas', 'Jon Kleinberg', 'Karen Levy']",320,"There has been rapidly growing interest in the use of algorithms in hiring, especially as a means to address or mitigate bias. Yet, to date, little is known about how these methods are used in practice. How are algorithmic assessments built, validated, and examined for bias? In this work, we document and analyze the claims and practices of companies offering algorithms for employment assessment. In particular, we identify vendors of algorithmic pre-employment assessments (i.e., algorithms to screen candidates), document what they have disclosed about their development and validation procedures, and evaluate their practices, focusing particularly on efforts to detect and mitigate bias. Our analysis considers both technical and legal perspectives. Technically, we consider the various choices vendors make regarding data collection and prediction targets, and explore the risks and trade-offs that these choices pose. We also discuss how algorithmic de-biasing techniques interface with, and create challenges for, antidiscrimination law.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3351095.3372828,
author = {Raghavan, Manish and Barocas, Solon and Kleinberg, Jon and Levy, Karen},
title = {Mitigating Bias in Algorithmic Hiring: Evaluating Claims and Practices},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372828},
doi = {10.1145/3351095.3372828},
abstract = {There has been rapidly growing interest in the use of algorithms in hiring, especially as a means to address or mitigate bias. Yet, to date, little is known about how these methods are used in practice. How are algorithmic assessments built, validated, and examined for bias? In this work, we document and analyze the claims and practices of companies offering algorithms for employment assessment. In particular, we identify vendors of algorithmic pre-employment assessments (i.e., algorithms to screen candidates), document what they have disclosed about their development and validation procedures, and evaluate their practices, focusing particularly on efforts to detect and mitigate bias. Our analysis considers both technical and legal perspectives. Technically, we consider the various choices vendors make regarding data collection and prediction targets, and explore the risks and trade-offs that these choices pose. We also discuss how algorithmic de-biasing techniques interface with, and create challenges for, antidiscrimination law.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {469–481},
numpages = {13},
keywords = {algorithmic bias, algorithmic hiring, discrimination law},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
FACCT,2020,The impact of overbooking on a pre-trial risk assessment tool,https://dl.acm.org/doi/10.1145/3351095.3372846,"['Kristian Lum', 'Chesa Boudin', 'Megan Price']",164,"Pre-trial risk assessment tools are used to make recommendations to judges about appropriate conditions of pre-trial supervision for people who have been arrested. Increasingly, there is concern about whether these models are operating fairly, including concerns about whether the models' input factors are fair measures of one's criminal activity. In this paper, we assess the impact of booking charges that do not result in a conviction on a popular risk assessment tool, the Arnold Public Safety Assessment. Using data from a pilot run of the tool in San Francisco, CA, we find that booking charges that do not result in a conviction (i.e. charges that are dropped or end in an acquittal) increased the recommended level of pre-trial supervision in around 27% of cases evaluated by the tool.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3351095.3372846,
author = {Lum, Kristian and Boudin, Chesa and Price, Megan},
title = {The Impact of Overbooking on a Pre-Trial Risk Assessment Tool},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372846},
doi = {10.1145/3351095.3372846},
abstract = {Pre-trial risk assessment tools are used to make recommendations to judges about appropriate conditions of pre-trial supervision for people who have been arrested. Increasingly, there is concern about whether these models are operating fairly, including concerns about whether the models' input factors are fair measures of one's criminal activity. In this paper, we assess the impact of booking charges that do not result in a conviction on a popular risk assessment tool, the Arnold Public Safety Assessment. Using data from a pilot run of the tool in San Francisco, CA, we find that booking charges that do not result in a conviction (i.e. charges that are dropped or end in an acquittal) increased the recommended level of pre-trial supervision in around 27% of cases evaluated by the tool.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {482–491},
numpages = {10},
keywords = {risk assessment, police accountability, overbooking, fairness},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
FACCT,2020,Awareness in practice: tensions in access to sensitive attribute data for antidiscrimination,https://dl.acm.org/doi/10.1145/3351095.3372877,"['Miranda Bogen', 'Aaron Rieke', 'Shazeda Ahmed']",34,"Organizations cannot address demographic disparities that they cannot see. Recent research on machine learning and fairness has emphasized that awareness of sensitive attributes, such as race and sex, is critical to the development of interventions. However, on the ground, the existence of these data cannot be taken for granted. This paper uses the domains of employment, credit, and healthcare in the United States to surface conditions that have shaped the availability of sensitive attribute data. For each domain, we describe how and when private companies collect or infer sensitive attribute data for antidiscrimination purposes. An inconsistent story emerges: Some companies are required by law to collect sensitive attribute data, while others are prohibited from doing so. Still others, in the absence of legal mandates, have determined that collection and imputation of these data are appropriate to address disparities. This story has important implications for fairness research and its future applications. If companies that mediate access to life opportunities are unable or hesitant to collect or infer sensitive attribute data, then proposed techniques to detect and mitigate bias in machine learning models might never be implemented outside the lab. We conclude that today's legal requirements and corporate practices, while highly inconsistent across domains, offer lessons for how to approach the collection and inference of sensitive data in appropriate circumstances. We urge stakeholders, including machine learning practitioners, to actively help chart a path forward that takes both policy goals and technical needs into account.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3351095.3372877,
author = {Bogen, Miranda and Rieke, Aaron and Ahmed, Shazeda},
title = {Awareness in Practice: Tensions in Access to Sensitive Attribute Data for Antidiscrimination},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372877},
doi = {10.1145/3351095.3372877},
abstract = {Organizations cannot address demographic disparities that they cannot see. Recent research on machine learning and fairness has emphasized that awareness of sensitive attributes, such as race and sex, is critical to the development of interventions. However, on the ground, the existence of these data cannot be taken for granted.This paper uses the domains of employment, credit, and healthcare in the United States to surface conditions that have shaped the availability of sensitive attribute data. For each domain, we describe how and when private companies collect or infer sensitive attribute data for antidiscrimination purposes. An inconsistent story emerges: Some companies are required by law to collect sensitive attribute data, while others are prohibited from doing so. Still others, in the absence of legal mandates, have determined that collection and imputation of these data are appropriate to address disparities.This story has important implications for fairness research and its future applications. If companies that mediate access to life opportunities are unable or hesitant to collect or infer sensitive attribute data, then proposed techniques to detect and mitigate bias in machine learning models might never be implemented outside the lab. We conclude that today's legal requirements and corporate practices, while highly inconsistent across domains, offer lessons for how to approach the collection and inference of sensitive data in appropriate circumstances. We urge stakeholders, including machine learning practitioners, to actively help chart a path forward that takes both policy goals and technical needs into account.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {492–500},
numpages = {9},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
FACCT,2020,Towards a critical race methodology in algorithmic fairness,https://dl.acm.org/doi/10.1145/3351095.3372826,"['Alex Hanna', 'Emily Denton', 'Andrew Smart', 'Jamila Smith-Loud']",185,"We examine the way race and racial categories are adopted in algorithmic fairness frameworks. Current methodologies fail to adequately account for the socially constructed nature of race, instead adopting a conceptualization of race as a fixed attribute. Treating race as an attribute, rather than a structural, institutional, and relational phenomenon, can serve to minimize the structural aspects of algorithmic unfairness. In this work, we focus on the history of racial categories and turn to critical race theory and sociological work on race and ethnicity to ground conceptualizations of race for fairness research, drawing on lessons from public health, biomedical research, and social survey research. We argue that algorithmic fairness researchers need to take into account the multidimensionality of race, take seriously the processes of conceptualizing and operationalizing race, focus on social processes which produce racial inequality, and consider perspectives of those most affected by sociotechnical systems.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3351095.3372826,
author = {Hanna, Alex and Denton, Emily and Smart, Andrew and Smith-Loud, Jamila},
title = {Towards a Critical Race Methodology in Algorithmic Fairness},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372826},
doi = {10.1145/3351095.3372826},
abstract = {We examine the way race and racial categories are adopted in algorithmic fairness frameworks. Current methodologies fail to adequately account for the socially constructed nature of race, instead adopting a conceptualization of race as a fixed attribute. Treating race as an attribute, rather than a structural, institutional, and relational phenomenon, can serve to minimize the structural aspects of algorithmic unfairness. In this work, we focus on the history of racial categories and turn to critical race theory and sociological work on race and ethnicity to ground conceptualizations of race for fairness research, drawing on lessons from public health, biomedical research, and social survey research. We argue that algorithmic fairness researchers need to take into account the multidimensionality of race, take seriously the processes of conceptualizing and operationalizing race, focus on social processes which produce racial inequality, and consider perspectives of those most affected by sociotechnical systems.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {501–512},
numpages = {12},
keywords = {algorithmic fairness, race and ethnicity, critical race theory},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
FACCT,2020,What's sex got to do with machine learning?,https://dl.acm.org/doi/10.1145/3351095.3375674,"['Lily Hu', 'Issa Kohler-Hausmann']",58,"The debate about fairness in machine learning has largely centered around competing substantive definitions of what fairness or nondiscrimination between groups requires. However, very little attention has been paid to what precisely a group is. Many recent approaches have abandoned observational, or purely statistical, definitions of fairness in favor of definitions that require one to specify a causal model of the data generating process. The implicit ontological assumption of these exercises is that a racial or sex group is a collection of individuals who share a trait or attribute, for example: the group ""female"" simply consists in grouping individuals who share female-coded sex features. We show this by exploring the formal assumption of modularity in causal models using directed acyclic graphs (DAGs), which hold that the dependencies captured by one causal pathway are invariant to interventions on any other causal pathways. Modeling sex, for example, as a node in a causal model aimed at elucidating fairness questions proposes two substantive claims: 1) There exists a feature, sex-on-its-own, that is an inherent trait of an individual that then (causally) brings about social phenomena external to it in the world; and 2) the relations between sex and its downstream effects can be modified in whichever ways and the former node would still retain the meaning that sex has in our world. Together, these claims suggest sex to be a category that could be different in its (causal) relations with other features of our social world via hypothetical interventions yet still mean what it means in our world. This fundamental stability of categories and causes (unless explicitly intervened on) is essential in the methodology of causal inference, because without it, causal operations can alter the meaning of a category, fundamentally change how it is situated within a causal diagram, and undermine the validity of any inferences drawn on the diagram as corresponding to any real phenomena in the world. We argue that these methods' ontological assumptions about social groups such as sex are conceptual errors. Many of the ""effects"" that sex purportedly ""causes"" are in fact constitutive features of sex as a social status. They constitute what it means to be sexed. In other words, together, they give the social meaning of sex features. These social meanings are precisely, we argue, what makes sex discrimination a distinctively morally problematic type of act that differs from mere irrationality or meanness on the basis of a physical feature. Correcting this conceptual error has a number of important implications for how analytical models can be used to detect discrimination. If what makes something discrimination on the basis of a particular social grouping is that the practice acts on what it means to be in that group in a way that we deem wrongful, then what we need from analytical diagrams is a model of what constitutes the social grouping. Such a model would allow us to explain the special moral (and legal) reasons we have to be concerned with the treatment of this category by reference to the empirical social relations and meanings that establish the category as what it is. Only then can we have the normative debate about what is fair or nondiscriminatory vis-√†-vis that group. We suggest that formal diagrams of constitutive relations would present an entirely different path toward reasoning about discrimination (and relatedly, counterfactuals) because they proffer a model of how the meaning of a social group emerges from its constitutive features. Whereas the value of causal diagrams is to guide the construction and testing of sophisticated modular counterfactuals, the value of constitutive diagrams would be to identify a different kind of counterfactual as central to our inquiry into discrimination: one that asks how the social meaning of a group would be changed if its non-modular features were altered.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3351095.3375674,
author = {Hu, Lily and Kohler-Hausmann, Issa},
title = {What's Sex Got to Do with Machine Learning?},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375674},
doi = {10.1145/3351095.3375674},
abstract = {The debate about fairness in machine learning has largely centered around competing substantive definitions of what fairness or nondiscrimination between groups requires. However, very little attention has been paid to what precisely a group is. Many recent approaches have abandoned observational, or purely statistical, definitions of fairness in favor of definitions that require one to specify a causal model of the data generating process. The implicit ontological assumption of these exercises is that a racial or sex group is a collection of individuals who share a trait or attribute, for example: the group ""female"" simply consists in grouping individuals who share female-coded sex features. We show this by exploring the formal assumption of modularity in causal models using directed acyclic graphs (DAGs), which hold that the dependencies captured by one causal pathway are invariant to interventions on any other causal pathways. Modeling sex, for example, as a node in a causal model aimed at elucidating fairness questions proposes two substantive claims: 1) There exists a feature, sex-on-its-own, that is an inherent trait of an individual that then (causally) brings about social phenomena external to it in the world; and 2) the relations between sex and its downstream effects can be modified in whichever ways and the former node would still retain the meaning that sex has in our world. Together, these claims suggest sex to be a category that could be different in its (causal) relations with other features of our social world via hypothetical interventions yet still mean what it means in our world. This fundamental stability of categories and causes (unless explicitly intervened on) is essential in the methodology of causal inference, because without it, causal operations can alter the meaning of a category, fundamentally change how it is situated within a causal diagram, and undermine the validity of any inferences drawn on the diagram as corresponding to any real phenomena in the world.We argue that these methods' ontological assumptions about social groups such as sex are conceptual errors. Many of the ""effects"" that sex purportedly ""causes"" are in fact constitutive features of sex as a social status. They constitute what it means to be sexed. In other words, together, they give the social meaning of sex features. These social meanings are precisely, we argue, what makes sex discrimination a distinctively morally problematic type of act that differs from mere irrationality or meanness on the basis of a physical feature.Correcting this conceptual error has a number of important implications for how analytical models can be used to detect discrimination. If what makes something discrimination on the basis of a particular social grouping is that the practice acts on what it means to be in that group in a way that we deem wrongful, then what we need from analytical diagrams is a model of what constitutes the social grouping. Such a model would allow us to explain the special moral (and legal) reasons we have to be concerned with the treatment of this category by reference to the empirical social relations and meanings that establish the category as what it is. Only then can we have the normative debate about what is fair or nondiscriminatory vis-\`{a}-vis that group. We suggest that formal diagrams of constitutive relations would present an entirely different path toward reasoning about discrimination (and relatedly, counterfactuals) because they proffer a model of how the meaning of a social group emerges from its constitutive features. Whereas the value of causal diagrams is to guide the construction and testing of sophisticated modular counterfactuals, the value of constitutive diagrams would be to identify a different kind of counterfactual as central to our inquiry into discrimination: one that asks how the social meaning of a group would be changed if its non-modular features were altered.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {513},
numpages = {1},
keywords = {law, social philosophy, algorithmic fairness, discrimination, machine learning, causal inference},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
FACCT,2020,On the apparent conflict between individual and group fairness,https://dl.acm.org/doi/10.1145/3351095.3372864,['Reuben Binns'],172,"A distinction has been drawn in fair machine learning research between 'group' and 'individual' fairness measures. Many technical research papers assume that both are important, but conflicting, and propose ways to minimise the trade-offs between these measures. This paper argues that this apparent conflict is based on a misconception. It draws on discussions from within the fair machine learning research, and from political and legal philosophy, to argue that individual and group fairness are not fundamentally in conflict. First, it outlines accounts of egalitarian fairness which encompass plausible motivations for both group and individual fairness, thereby suggesting that there need be no conflict in principle. Second, it considers the concept of individual justice, from legal philosophy and jurisprudence, which seems similar but actually contradicts the notion of individual fairness as proposed in the fair machine learning literature. The conclusion is that the apparent conflict between individual and group fairness is more of an artefact of the blunt application of fairness measures, rather than a matter of conflicting principles. In practice, this conflict may be resolved by a nuanced consideration of the sources of 'unfairness' in a particular deployment context, and the carefully justified application of measures to mitigate it.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3351095.3372864,
author = {Binns, Reuben},
title = {On the Apparent Conflict between Individual and Group Fairness},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372864},
doi = {10.1145/3351095.3372864},
abstract = {A distinction has been drawn in fair machine learning research between 'group' and 'individual' fairness measures. Many technical research papers assume that both are important, but conflicting, and propose ways to minimise the trade-offs between these measures. This paper argues that this apparent conflict is based on a misconception. It draws on discussions from within the fair machine learning research, and from political and legal philosophy, to argue that individual and group fairness are not fundamentally in conflict. First, it outlines accounts of egalitarian fairness which encompass plausible motivations for both group and individual fairness, thereby suggesting that there need be no conflict in principle. Second, it considers the concept of individual justice, from legal philosophy and jurisprudence, which seems similar but actually contradicts the notion of individual fairness as proposed in the fair machine learning literature. The conclusion is that the apparent conflict between individual and group fairness is more of an artefact of the blunt application of fairness measures, rather than a matter of conflicting principles. In practice, this conflict may be resolved by a nuanced consideration of the sources of 'unfairness' in a particular deployment context, and the carefully justified application of measures to mitigate it.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {514–524},
numpages = {11},
keywords = {individual fairness, discrimination, machine learning, fairness, justice, statistical parity},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
FACCT,2020,Fairness is not static: deeper understanding of long term fairness via simulation studies,https://dl.acm.org/doi/10.1145/3351095.3372878,"[""Alexander D'Amour"", 'Hansa Srinivasan', 'James Atwood', 'Pallavi Baljekar', 'D. Sculley', 'Yoni Halpern']",131,"As machine learning becomes increasingly incorporated within high impact decision ecosystems, there is a growing need to understand the long-term behaviors of deployed ML-based decision systems and their potential consequences. Most approaches to understanding or improving the fairness of these systems have focused on static settings without considering long-term dynamics. This is understandable; long term dynamics are hard to assess, particularly because they do not align with the traditional supervised ML research framework that uses fixed data sets. To address this structural difficulty in the field, we advocate for the use of simulation as a key tool in studying the fairness of algorithms. We explore three toy examples of dynamical systems that have been previously studied in the context of fair decision making for bank loans, college admissions, and allocation of attention. By analyzing how learning agents interact with these systems in simulation, we are able to extend previous work, showing that static or single-step analyses do not give a complete picture of the long-term consequences of an ML-based decision system. We provide an extensible open-source software framework for implementing fairness-focused simulation studies and further reproducible research, available at https://github.com/google/ml-fairness-gym.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3351095.3372878,
author = {D'Amour, Alexander and Srinivasan, Hansa and Atwood, James and Baljekar, Pallavi and Sculley, D. and Halpern, Yoni},
title = {Fairness is Not Static: Deeper Understanding of Long Term Fairness via Simulation Studies},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372878},
doi = {10.1145/3351095.3372878},
abstract = {As machine learning becomes increasingly incorporated within high impact decision ecosystems, there is a growing need to understand the long-term behaviors of deployed ML-based decision systems and their potential consequences. Most approaches to understanding or improving the fairness of these systems have focused on static settings without considering long-term dynamics. This is understandable; long term dynamics are hard to assess, particularly because they do not align with the traditional supervised ML research framework that uses fixed data sets. To address this structural difficulty in the field, we advocate for the use of simulation as a key tool in studying the fairness of algorithms. We explore three toy examples of dynamical systems that have been previously studied in the context of fair decision making for bank loans, college admissions, and allocation of attention. By analyzing how learning agents interact with these systems in simulation, we are able to extend previous work, showing that static or single-step analyses do not give a complete picture of the long-term consequences of an ML-based decision system. We provide an extensible open-source software framework for implementing fairness-focused simulation studies and further reproducible research, available at https://github.com/google/ml-fairness-gym.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {525–534},
numpages = {10},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
FACCT,2020,Fair classification and social welfare,https://dl.acm.org/doi/10.1145/3351095.3372857,"['Lily Hu', 'Yiling Chen']",76,"Now that machine learning algorithms lie at the center of many important resource allocation pipelines, computer scientists have been unwittingly cast as partial social planners. Given this state of affairs, important questions follow. How do leading notions of fairness as defined by computer scientists map onto longer-standing notions of social welfare? In this paper, we present a welfare-based analysis of fair classification regimes. Our main findings assess the welfare impact of fairness-constrained empirical risk minimization programs on the individuals and groups who are subject to their outputs. We fully characterize the ranges of Œî'e perturbations to a fairness parameter 'e in a fair Soft Margin SVM problem that yield better, worse, and neutral outcomes in utility for individuals and by extension, groups. Our method of analysis allows for fast and efficient computation of ""fairness-to-welfare"" solution paths, thereby allowing practitioners to easily assess whether and which fair learning procedures result in classification outcomes that make groups better-off. Our analyses show that applying stricter fairness criteria codified as parity constraints can worsen welfare outcomes for both groups. More generally, always preferring ""more fair"" classifiers does not abide by the Pareto Principle---a fundamental axiom of social choice theory and welfare economics. Recent work in machine learning has rallied around these notions of fairness as critical to ensuring that algorithmic systems do not have disparate negative impact on disadvantaged social groups. By showing that these constraints often fail to translate into improved outcomes for these groups, we cast doubt on their effectiveness as a means to ensure fairness and justice.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3351095.3372857,
author = {Hu, Lily and Chen, Yiling},
title = {Fair Classification and Social Welfare},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372857},
doi = {10.1145/3351095.3372857},
abstract = {Now that machine learning algorithms lie at the center of many important resource allocation pipelines, computer scientists have been unwittingly cast as partial social planners. Given this state of affairs, important questions follow. How do leading notions of fairness as defined by computer scientists map onto longer-standing notions of social welfare? In this paper, we present a welfare-based analysis of fair classification regimes. Our main findings assess the welfare impact of fairness-constrained empirical risk minimization programs on the individuals and groups who are subject to their outputs. We fully characterize the ranges of Δ'e perturbations to a fairness parameter 'e in a fair Soft Margin SVM problem that yield better, worse, and neutral outcomes in utility for individuals and by extension, groups. Our method of analysis allows for fast and efficient computation of ""fairness-to-welfare"" solution paths, thereby allowing practitioners to easily assess whether and which fair learning procedures result in classification outcomes that make groups better-off. Our analyses show that applying stricter fairness criteria codified as parity constraints can worsen welfare outcomes for both groups. More generally, always preferring ""more fair"" classifiers does not abide by the Pareto Principle---a fundamental axiom of social choice theory and welfare economics. Recent work in machine learning has rallied around these notions of fairness as critical to ensuring that algorithmic systems do not have disparate negative impact on disadvantaged social groups. By showing that these constraints often fail to translate into improved outcomes for these groups, we cast doubt on their effectiveness as a means to ensure fairness and justice.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {535–545},
numpages = {11},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
FACCT,2020,Preference-informed fairness,https://dl.acm.org/doi/10.1145/3351095.3373155,"['Michael P. Kim', 'Aleksandra Korolova', 'Guy N. Rothblum', 'Gal Yona']",25,"In this work, we study notions of fairness in decision-making systems when individuals have diverse preferences over the possible outcomes of the decisions. Our starting point is the seminal work of Dwork et al. [ITCS 2012] which introduced a notion of individual fairness (IF): given a task-specific similarity metric, every pair of individuals who are similarly qualified according to the metric should receive similar outcomes. We show that when individuals have diverse preferences over outcomes, requiring IF may unintentionally lead to less-preferred outcomes for the very individuals that IF aims to protect (e.g. a protected minority group). A natural alternative to IF is the classic notion of fair division, envy-freeness (EF): no individual should prefer another individual's outcome over their own. Although EF allows for solutions where all individuals receive a highly-preferred outcome, EF may also be overly-restrictive for the decision-maker. For instance, if many individuals agree on the best outcome, then if any individual receives this outcome, they all must receive it, regardless of each individual's underlying qualifications for the outcome. We introduce and study a new notion of preference-informed individual fairness (PIIF) that is a relaxation of both individual fairness and envy-freeness. At a high-level, PIIF requires that outcomes satisfy IF-style constraints, but allows for deviations provided they are in line with individuals' preferences. We show that PIIF can permit outcomes that are more favorable to individuals than any IF solution, while providing considerably more flexibility to the decision-maker than EF. In addition, we show how to efficiently optimize any convex objective over the outcomes subject to PIIF for a rich class of individual preferences. Finally, we demonstrate the broad applicability of the PIIF framework by extending our definitions and algorithms to the multiple-task targeted advertising setting introduced by Dwork and Ilvento [ITCS 2019].",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3351095.3373155,
author = {Kim, Michael P. and Korolova, Aleksandra and Rothblum, Guy N. and Yona, Gal},
title = {Preference-Informed Fairness},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3373155},
doi = {10.1145/3351095.3373155},
abstract = {In this work, we study notions of fairness in decision-making systems when individuals have diverse preferences over the possible outcomes of the decisions. Our starting point is the seminal work of Dwork et al. [ITCS 2012] which introduced a notion of individual fairness (IF): given a task-specific similarity metric, every pair of individuals who are similarly qualified according to the metric should receive similar outcomes. We show that when individuals have diverse preferences over outcomes, requiring IF may unintentionally lead to less-preferred outcomes for the very individuals that IF aims to protect (e.g. a protected minority group). A natural alternative to IF is the classic notion of fair division, envy-freeness (EF): no individual should prefer another individual's outcome over their own. Although EF allows for solutions where all individuals receive a highly-preferred outcome, EF may also be overly-restrictive for the decision-maker. For instance, if many individuals agree on the best outcome, then if any individual receives this outcome, they all must receive it, regardless of each individual's underlying qualifications for the outcome.We introduce and study a new notion of preference-informed individual fairness (PIIF) that is a relaxation of both individual fairness and envy-freeness. At a high-level, PIIF requires that outcomes satisfy IF-style constraints, but allows for deviations provided they are in line with individuals' preferences. We show that PIIF can permit outcomes that are more favorable to individuals than any IF solution, while providing considerably more flexibility to the decision-maker than EF. In addition, we show how to efficiently optimize any convex objective over the outcomes subject to PIIF for a rich class of individual preferences. Finally, we demonstrate the broad applicability of the PIIF framework by extending our definitions and algorithms to the multiple-task targeted advertising setting introduced by Dwork and Ilvento [ITCS 2019].},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {546},
numpages = {1},
keywords = {algorithmic fairness},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
FACCT,2020,Towards fairer datasets: filtering and balancing the distribution of the people subtree in the ImageNet hierarchy,https://dl.acm.org/doi/10.1145/3351095.3375709,"['Kaiyu Yang', 'Klint Qinami', 'Li Fei-Fei', 'Jia Deng', 'Olga Russakovsky']",207,"Computer vision technology is being used by many but remains representative of only a few. People have reported misbehavior of computer vision models, including offensive prediction results and lower performance for underrepresented groups. Current computer vision models are typically developed using datasets consisting of manually annotated images or videos; the data and label distributions in these datasets are critical to the models' behavior. In this paper, we examine ImageNet, a large-scale ontology of images that has spurred the development of many modern computer vision methods. We consider three key factors within the person subtree of ImageNet that may lead to problematic behavior in downstream computer vision technology: (1) the stagnant concept vocabulary of WordNet, (2) the attempt at exhaustive illustration of all categories with images, and (3) the inequality of representation in the images within concepts. We seek to illuminate the root causes of these concerns and take the first steps to mitigate them constructively.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3351095.3375709,
author = {Yang, Kaiyu and Qinami, Klint and Fei-Fei, Li and Deng, Jia and Russakovsky, Olga},
title = {Towards Fairer Datasets: Filtering and Balancing the Distribution of the People Subtree in the ImageNet Hierarchy},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375709},
doi = {10.1145/3351095.3375709},
abstract = {Computer vision technology is being used by many but remains representative of only a few. People have reported misbehavior of computer vision models, including offensive prediction results and lower performance for underrepresented groups. Current computer vision models are typically developed using datasets consisting of manually annotated images or videos; the data and label distributions in these datasets are critical to the models' behavior. In this paper, we examine ImageNet, a large-scale ontology of images that has spurred the development of many modern computer vision methods. We consider three key factors within the person subtree of ImageNet that may lead to problematic behavior in downstream computer vision technology: (1) the stagnant concept vocabulary of WordNet, (2) the attempt at exhaustive illustration of all categories with images, and (3) the inequality of representation in the images within concepts. We seek to illuminate the root causes of these concerns and take the first steps to mitigate them constructively.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {547–558},
numpages = {12},
keywords = {representative datasets, fairness, computer vision, dataset construction},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
FACCT,2020,The case for voter-centered audits of search engines during political elections,https://dl.acm.org/doi/10.1145/3351095.3372835,"['Eni Mustafaraj', 'Emma Lurie', 'Claire Devine']",23,"Search engines, by ranking a few links ahead of million others based on opaque rules, open themselves up to criticism of bias. Previous research has focused on measuring political bias of search engine algorithms to detect possible search engine manipulation effects on voters or unbalanced ideological representation in search results. Insofar that these concerns are related to the principle of fairness, this notion of fairness can be seen as explicitly oriented toward election candidates or political processes and only implicitly oriented toward the public at large. Thus, we ask the following research question: how should an auditing framework that is explicitly centered on the principle of ensuring and maximizing fairness for the public (i.e., voters) operate? To answer this question, we qualitatively explore four datasets about elections and politics in the United States: 1) a survey of eligible U.S. voters about their information needs ahead of the 2018 U.S. elections, 2) a dataset of biased political phrases used in a large-scale Google audit ahead of the 2018 U.S. election, 3) Google's ""related searches"" phrases for two groups of political candidates in the 2018 U.S. election (one group is composed entirely of women), and 4) autocomplete suggestions and result pages for a set of searches on the day of a statewide election in the U.S. state of Virginia in 2019. We find that voters have much broader information needs than the search engine audit literature has accounted for in the past, and that relying on political science theories of voter modeling provides a good starting point for informing the design of voter-centered audits.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3351095.3372835,
author = {Mustafaraj, Eni and Lurie, Emma and Devine, Claire},
title = {The Case for Voter-Centered Audits of Search Engines during Political Elections},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372835},
doi = {10.1145/3351095.3372835},
abstract = {Search engines, by ranking a few links ahead of million others based on opaque rules, open themselves up to criticism of bias. Previous research has focused on measuring political bias of search engine algorithms to detect possible search engine manipulation effects on voters or unbalanced ideological representation in search results. Insofar that these concerns are related to the principle of fairness, this notion of fairness can be seen as explicitly oriented toward election candidates or political processes and only implicitly oriented toward the public at large. Thus, we ask the following research question: how should an auditing framework that is explicitly centered on the principle of ensuring and maximizing fairness for the public (i.e., voters) operate? To answer this question, we qualitatively explore four datasets about elections and politics in the United States: 1) a survey of eligible U.S. voters about their information needs ahead of the 2018 U.S. elections, 2) a dataset of biased political phrases used in a large-scale Google audit ahead of the 2018 U.S. election, 3) Google's ""related searches"" phrases for two groups of political candidates in the 2018 U.S. election (one group is composed entirely of women), and 4) autocomplete suggestions and result pages for a set of searches on the day of a statewide election in the U.S. state of Virginia in 2019. We find that voters have much broader information needs than the search engine audit literature has accounted for in the past, and that relying on political science theories of voter modeling provides a good starting point for informing the design of voter-centered audits.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {559–569},
numpages = {11},
keywords = {voters, search engines, Google, bias, algorithm audits, elections},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
FACCT,2020,"Counterfactual risk assessments, evaluation, and fairness",https://dl.acm.org/doi/10.1145/3351095.3372851,"['Amanda Coston', 'Alan Mishler', 'Edward H. Kennedy', 'Alexandra Chouldechova']",60,"Algorithmic risk assessments are increasingly used to help humans make decisions in high-stakes settings, such as medicine, criminal justice and education. In each of these cases, the purpose of the risk assessment tool is to inform actions, such as medical treatments or release conditions, often with the aim of reducing the likelihood of an adverse event such as hospital readmission or recidivism. Problematically, most tools are trained and evaluated on historical data in which the outcomes observed depend on the historical decision-making policy. These tools thus reflect risk under the historical policy, rather than under the different decision options that the tool is intended to inform. Even when tools are constructed to predict risk under a specific decision, they are often improperly evaluated as predictors of the target outcome. Focusing on the evaluation task, in this paper we define counterfactual analogues of common predictive performance and algorithmic fairness metrics that we argue are better suited for the decision-making context. We introduce a new method for estimating the proposed metrics using doubly robust estimation. We provide theoretical results that show that only under strong conditions can fairness according to the standard metric and the counterfactual metric simultaneously hold. Consequently, fairness-promoting methods that target parity in a standard fairness metric may---and as we show empirically, do---induce greater imbalance in the counterfactual analogue. We provide empirical comparisons on both synthetic data and a real world child welfare dataset to demonstrate how the proposed method improves upon standard practice.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3351095.3372851,
author = {Coston, Amanda and Mishler, Alan and Kennedy, Edward H. and Chouldechova, Alexandra},
title = {Counterfactual Risk Assessments, Evaluation, and Fairness},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372851},
doi = {10.1145/3351095.3372851},
abstract = {Algorithmic risk assessments are increasingly used to help humans make decisions in high-stakes settings, such as medicine, criminal justice and education. In each of these cases, the purpose of the risk assessment tool is to inform actions, such as medical treatments or release conditions, often with the aim of reducing the likelihood of an adverse event such as hospital readmission or recidivism. Problematically, most tools are trained and evaluated on historical data in which the outcomes observed depend on the historical decision-making policy. These tools thus reflect risk under the historical policy, rather than under the different decision options that the tool is intended to inform. Even when tools are constructed to predict risk under a specific decision, they are often improperly evaluated as predictors of the target outcome.Focusing on the evaluation task, in this paper we define counterfactual analogues of common predictive performance and algorithmic fairness metrics that we argue are better suited for the decision-making context. We introduce a new method for estimating the proposed metrics using doubly robust estimation. We provide theoretical results that show that only under strong conditions can fairness according to the standard metric and the counterfactual metric simultaneously hold. Consequently, fairness-promoting methods that target parity in a standard fairness metric may---and as we show empirically, do---induce greater imbalance in the counterfactual analogue. We provide empirical comparisons on both synthetic data and a real world child welfare dataset to demonstrate how the proposed method improves upon standard practice.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {582–593},
numpages = {12},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
FACCT,2020,The false promise of risk assessments: epistemic reform and the limits of fairness,https://dl.acm.org/doi/10.1145/3351095.3372869,['Ben Green'],89,"Risk assessments have proliferated in the United States criminal justice system. The theory of change motivating their adoption involves two key assumptions: first, that risk assessments will reduce human biases by making objective decisions, and second, that risk assessments will promote criminal justice reform. In this paper I interrogate both of these assumptions, concluding that risk assessments are an ill-advised tool for challenging the centrality and legitimacy of incarceration within the criminal justice system. First, risk assessments fail to provide objectivity, as their use creates numerous sites of discretion. Second, risk assessments provide no guarantee of reducing incarceration; instead, they risk legitimizing the criminal justice system's structural racism. I then consider, via an ""epistemic reform,"" the path forward for criminal justice reform. I reinterpret recent results regarding the ""impossibility of fairness"" as not simply a tension between mathematical metrics but as evidence of a deeper tension between notions of equality. This expanded frame challenges the formalist, colorblind proceduralism at the heart of the criminal justice system and suggests a more structural approach to reform. Together, this analysis highlights how algorithmic fairness narrows the scope of judgments about justice and how ""fair"" algorithms can reinforce discrimination.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3351095.3372869,
author = {Green, Ben},
title = {The False Promise of Risk Assessments: Epistemic Reform and the Limits of Fairness},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372869},
doi = {10.1145/3351095.3372869},
abstract = {Risk assessments have proliferated in the United States criminal justice system. The theory of change motivating their adoption involves two key assumptions: first, that risk assessments will reduce human biases by making objective decisions, and second, that risk assessments will promote criminal justice reform. In this paper I interrogate both of these assumptions, concluding that risk assessments are an ill-advised tool for challenging the centrality and legitimacy of incarceration within the criminal justice system. First, risk assessments fail to provide objectivity, as their use creates numerous sites of discretion. Second, risk assessments provide no guarantee of reducing incarceration; instead, they risk legitimizing the criminal justice system's structural racism. I then consider, via an ""epistemic reform,"" the path forward for criminal justice reform. I reinterpret recent results regarding the ""impossibility of fairness"" as not simply a tension between mathematical metrics but as evidence of a deeper tension between notions of equality. This expanded frame challenges the formalist, colorblind proceduralism at the heart of the criminal justice system and suggests a more structural approach to reform. Together, this analysis highlights how algorithmic fairness narrows the scope of judgments about justice and how ""fair"" algorithms can reinforce discrimination.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {594–606},
numpages = {13},
keywords = {criminal justice system, risk assessment, fairness, social justice},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
FACCT,2020,Doctor XAI: an ontology-based approach to black-box sequential data classification explanations,https://dl.acm.org/doi/10.1145/3351095.3372855,"['Cecilia Panigutti', 'Alan Perotti', 'Dino Pedreschi']",109,"Several recent advancements in Machine Learning involve blackbox models: algorithms that do not provide human-understandable explanations in support of their decisions. This limitation hampers the fairness, accountability and transparency of these models; the field of eXplainable Artificial Intelligence (XAI) tries to solve this problem providing human-understandable explanations for black-box models. However, healthcare datasets (and the related learning tasks) often present peculiar features, such as sequential data, multi-label predictions, and links to structured background knowledge. In this paper, we introduce Doctor XAI, a model-agnostic explainability technique able to deal with multi-labeled, sequential, ontology-linked data. We focus on explaining Doctor AI, a multilabel classifier which takes as input the clinical history of a patient in order to predict the next visit. Furthermore, we show how exploiting the temporal dimension in the data and the domain knowledge encoded in the medical ontology improves the quality of the mined explanations.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3351095.3372855,
author = {Panigutti, Cecilia and Perotti, Alan and Pedreschi, Dino},
title = {Doctor XAI: An Ontology-Based Approach to Black-Box Sequential Data Classification Explanations},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372855},
doi = {10.1145/3351095.3372855},
abstract = {Several recent advancements in Machine Learning involve blackbox models: algorithms that do not provide human-understandable explanations in support of their decisions. This limitation hampers the fairness, accountability and transparency of these models; the field of eXplainable Artificial Intelligence (XAI) tries to solve this problem providing human-understandable explanations for black-box models. However, healthcare datasets (and the related learning tasks) often present peculiar features, such as sequential data, multi-label predictions, and links to structured background knowledge. In this paper, we introduce Doctor XAI, a model-agnostic explainability technique able to deal with multi-labeled, sequential, ontology-linked data. We focus on explaining Doctor AI, a multilabel classifier which takes as input the clinical history of a patient in order to predict the next visit. Furthermore, we show how exploiting the temporal dimension in the data and the domain knowledge encoded in the medical ontology improves the quality of the mined explanations.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {629–639},
numpages = {11},
keywords = {explainable artificial intelligence, healthcare data, machine learning},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
FACCT,2020,Fairness and utilization in allocating resources with uncertain demand,https://dl.acm.org/doi/10.1145/3351095.3372847,"['Kate Donahue', 'Jon Kleinberg']",30,"Resource allocation problems are a fundamental domain in which to evaluate the fairness properties of algorithms. The trade-offs between fairness and utilization have a long history in this domain. A recent line of work has considered fairness questions for resource allocation when the demands for the resource are distributed across multiple groups and drawn from probability distributions. In such cases, a natural fairness requirement is that individuals from different groups should have (approximately) equal probabilities of receiving the resource. A largely open question in this area has been to bound the gap between the maximum possible utilization of the resource and the maximum possible utilization subject to this fairness condition. Here, we obtain some of the first provable upper bounds on this gap. We obtain an upper bound for arbitrary distributions, as well as much stronger upper bounds for specific families of distributions that are typically used to model levels of demand. In particular, we find --- somewhat surprisingly --- that there are natural families of distributions (including Exponential and Weibull) for which the gap is non-existent: it is possible to simultaneously achieve maximum utilization and the given notion of fairness. Finally, we show that for power-law distributions, there is a non-trivial gap between the solutions, but this gap can be bounded by a constant factor independent of the parameters of the distribution.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3351095.3372847,
author = {Donahue, Kate and Kleinberg, Jon},
title = {Fairness and Utilization in Allocating Resources with Uncertain Demand},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372847},
doi = {10.1145/3351095.3372847},
abstract = {Resource allocation problems are a fundamental domain in which to evaluate the fairness properties of algorithms. The trade-offs between fairness and utilization have a long history in this domain. A recent line of work has considered fairness questions for resource allocation when the demands for the resource are distributed across multiple groups and drawn from probability distributions. In such cases, a natural fairness requirement is that individuals from different groups should have (approximately) equal probabilities of receiving the resource. A largely open question in this area has been to bound the gap between the maximum possible utilization of the resource and the maximum possible utilization subject to this fairness condition.Here, we obtain some of the first provable upper bounds on this gap. We obtain an upper bound for arbitrary distributions, as well as much stronger upper bounds for specific families of distributions that are typically used to model levels of demand. In particular, we find --- somewhat surprisingly --- that there are natural families of distributions (including Exponential and Weibull) for which the gap is non-existent: it is possible to simultaneously achieve maximum utilization and the given notion of fairness. Finally, we show that for power-law distributions, there is a non-trivial gap between the solutions, but this gap can be bounded by a constant factor independent of the parameters of the distribution.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {658–668},
numpages = {11},
keywords = {algorithmic fairness, power law distribution, resource allocation, uncertainty, weibull distribution},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
FACCT,2020,The effects of competition and regulation on error inequality in data-driven markets,https://dl.acm.org/doi/10.1145/3351095.3372842,"['Hadi Elzayn', 'Benjamin Fish']",7,"Recent work has documented instances of unfairness in deployed machine learning models, and significant researcher effort has been dedicated to creating algorithms that intrinsically consider fairness. In this work, we highlight another source of unfairness: market forces that drive differential investment in the data pipeline for differing groups. We develop a high-level model to study this question. First, we show that our model predicts unfairness in a monopoly setting. Then, we show that under all but the most extreme models, competition does not eliminate this tendency, and may even exacerbate it. Finally, we consider two avenues for regulating a machine-learning driven monopolist - relative error inequality and absolute error-bounds - and quantify the price of fairness (and who pays it). These models imply that mitigating fairness concerns may require policy-driven solutions, not only technological ones.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3351095.3372842,
author = {Elzayn, Hadi and Fish, Benjamin},
title = {The Effects of Competition and Regulation on Error Inequality in Data-Driven Markets},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372842},
doi = {10.1145/3351095.3372842},
abstract = {Recent work has documented instances of unfairness in deployed machine learning models, and significant researcher effort has been dedicated to creating algorithms that intrinsically consider fairness. In this work, we highlight another source of unfairness: market forces that drive differential investment in the data pipeline for differing groups. We develop a high-level model to study this question. First, we show that our model predicts unfairness in a monopoly setting. Then, we show that under all but the most extreme models, competition does not eliminate this tendency, and may even exacerbate it. Finally, we consider two avenues for regulating a machine-learning driven monopolist - relative error inequality and absolute error-bounds - and quantify the price of fairness (and who pays it). These models imply that mitigating fairness concerns may require policy-driven solutions, not only technological ones.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {669–679},
numpages = {11},
keywords = {algorithmic fairness, industrial organization, game theory, learning theory, economics, data markets},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
FACCT,2020,Measuring justice in machine learning,https://dl.acm.org/doi/10.1145/3351095.3372838,['Alan Lundgard'],6,"How can we build more just machine learning systems? To answer this question, we need to know both what justice is and how to tell whether one system is more or less just than another. That is, we need both a definition and a measure of justice. Theories of distributive justice hold that justice can be measured (in part) in terms of the fair distribution of benefits and burdens across people in society. Recently, the field known as fair machine learning has turned to John Rawls's theory of distributive justice for inspiration and operationalization. However, philosophers known as capability theorists have long argued that Rawls's theory uses the wrong measure of justice, thereby encoding biases against people with disabilities. If these theorists are right, is it possible to operationalize Rawls's theory in machine learning systems without also encoding its biases? In this paper, I draw on examples from fair machine learning to suggest that the answer to this question is no: the capability theorists' arguments against Rawls's theory carry over into machine learning systems. But capability theorists don't only argue that Rawls's theory uses the wrong measure, they also offer an alternative measure. Which measure of justice is right? And has fair machine learning been using the wrong one?",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3351095.3372838,
author = {Lundgard, Alan},
title = {Measuring Justice in Machine Learning},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372838},
doi = {10.1145/3351095.3372838},
abstract = {How can we build more just machine learning systems? To answer this question, we need to know both what justice is and how to tell whether one system is more or less just than another. That is, we need both a definition and a measure of justice. Theories of distributive justice hold that justice can be measured (in part) in terms of the fair distribution of benefits and burdens across people in society. Recently, the field known as fair machine learning has turned to John Rawls's theory of distributive justice for inspiration and operationalization. However, philosophers known as capability theorists have long argued that Rawls's theory uses the wrong measure of justice, thereby encoding biases against people with disabilities. If these theorists are right, is it possible to operationalize Rawls's theory in machine learning systems without also encoding its biases? In this paper, I draw on examples from fair machine learning to suggest that the answer to this question is no: the capability theorists' arguments against Rawls's theory carry over into machine learning systems. But capability theorists don't only argue that Rawls's theory uses the wrong measure, they also offer an alternative measure. Which measure of justice is right? And has fair machine learning been using the wrong one?},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {680},
numpages = {1},
keywords = {measure, operationalization, disability, blind, accessibility, fairness, distributive justice, philosophy, machine learning, theory, capability, bias, non-visual access, justice, discrimination, web accessibility},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
FACCT,2020,Bridging the gap from AI ethics research to practice,https://dl.acm.org/doi/10.1145/3351095.3375680,"['Kathy Baxter', 'Yoav Schlesinger', 'Sarah Aerni', 'Lewis Baker', 'Julie Dawson', 'Krishnaram Kenthapadi', 'Isabel Kloumann', 'Hanna Wallach']",2,"The study of fairness in machine learning applications has seen significant academic inquiry, research and publication in recent years. Concurrently, technology companies have begun to instantiate nascent program in AI ethics and product ethics more broadly. As a result of these efforts, AI ethics practitioners have piloted new processes to evaluate and ensure fairness in their machine learning applications. In this session, six industry practitioners, hailing from LinkedIn, Yoti, Microsoft, Pymetrics, Facebook, and Salesforce share insights from the work they have undertaken in the area of fairness, what has worked and what has not, lessons learned and best practices instituted as a result. ‚Ä¢ Krishnaram Kenthapadi presents LinkedIn's fairness-aware reranking for talent search. ‚Ä¢ Julie Dawson shares how Yoti applies ML fairness research to age estimation in their digital identity platform. ‚Ä¢ Hanna Wallach contributes how Microsoft is applying fairness principles in practice. ‚Ä¢ Lewis Baker presents Pymetric's fairness mechanisms in their hiring algorithm. ‚Ä¢ Isabel Kloumann presents Facebook's fairness assessment framework through a case study of fairness in a content moderation system. ‚Ä¢ Sarah Aerni contributes how Salesforce is building fairness features into the Einstein AI platform. Building on those insights, we discuss insights and brainstorm modalities through which to build upon the practitioners' work. Opportunities for further research or collaboration are identified, with the goal of developing a shared understanding of experiences and needs of AI ethics practitioners. Ultimately, the aim is to develop a playbook for more ethical and fair AI product development and deployment.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3351095.3375680,
author = {Baxter, Kathy and Schlesinger, Yoav and Aerni, Sarah and Baker, Lewis and Dawson, Julie and Kenthapadi, Krishnaram and Kloumann, Isabel and Wallach, Hanna},
title = {Bridging the Gap from AI Ethics Research to Practice},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375680},
doi = {10.1145/3351095.3375680},
abstract = {The study of fairness in machine learning applications has seen significant academic inquiry, research and publication in recent years. Concurrently, technology companies have begun to instantiate nascent program in AI ethics and product ethics more broadly. As a result of these efforts, AI ethics practitioners have piloted new processes to evaluate and ensure fairness in their machine learning applications. In this session, six industry practitioners, hailing from LinkedIn, Yoti, Microsoft, Pymetrics, Facebook, and Salesforce share insights from the work they have undertaken in the area of fairness, what has worked and what has not, lessons learned and best practices instituted as a result.• Krishnaram Kenthapadi presents LinkedIn's fairness-aware reranking for talent search.• Julie Dawson shares how Yoti applies ML fairness research to age estimation in their digital identity platform.• Hanna Wallach contributes how Microsoft is applying fairness principles in practice.• Lewis Baker presents Pymetric's fairness mechanisms in their hiring algorithm.• Isabel Kloumann presents Facebook's fairness assessment framework through a case study of fairness in a content moderation system.• Sarah Aerni contributes how Salesforce is building fairness features into the Einstein AI platform.Building on those insights, we discuss insights and brainstorm modalities through which to build upon the practitioners' work. Opportunities for further research or collaboration are identified, with the goal of developing a shared understanding of experiences and needs of AI ethics practitioners. Ultimately, the aim is to develop a playbook for more ethical and fair AI product development and deployment.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {682},
numpages = {1},
keywords = {algorithmic decision-making, ethics, artificial intelligence, fairness, ML fairness, practitioners, data},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
FACCT,2020,"Burn, dream and reboot!: speculating backwards for the missing archive on non-coercive computing",https://dl.acm.org/doi/10.1145/3351095.3375697,"['Helen Pritchard', 'Eric Snodgrass', 'Romi Ron Morrison', 'Loren Britton', 'Joana Moll']",0,"Whether one is speaking of barbed wire, the assembly line or computer operating systems, the history of coercive technologies for the automation of tasks has focused on optimization, determinate outcomes and an ongoing disciplining of components and bodies. Automated technologies of the present emerge and are marked by this lineage of coercive modes of implementation, whose scarred history of techniques of discrimination, exploitation and extraction point to an archive of automated injustices in computing, a history that continues to charge present paradigms and practices of computing. This workshop addresses the history of coercive technologies through attuning to how we perform speculation within practices of computing through a renewed attention to this history. We go backwards into the archive, rather than racing forward and proposing ever new speculative futures of automation. This is because speculative creative approaches are often conceived and positioned as methodological toolkits for addressing computing practices by imagining for/with others for a ""future otherwise"". We argue that ""speculation"" as the easy-go-to of designers and artists trying to address automated injustices needs some undoing, as without work it will always be confined within ongoing legacies of coercive modes of computing practice. Instead of creating more just-worlds, the generation of ever-new futures by creative speculation often merely reinforces the project of coercive computing. For this workshop, drawing on queer approaches to resisting futures and informed by activist feminist engagements with archives, we invite participants to temporarily resist imagining futures and instead to speculate backwards. We speculate backwards to various moments, artefacts and practices within computing history. What does it mean to understand techniques of computing and automation as coercive infrastructures? How did so many of the dreams and seeming promises of computing turn into the coercive practices that we see today? Has computing as a practice become so imbued with coercive techniques that we find it hard to imagine otherwise? Together, we will build a speculative understanding and possible archive of non-coercive computing. In the words of Alexis Pauline Gumbs, the emerging archive proposes ""how did their dreams make rooms to dream in""... or not, in the case of coercive practices of computing. And ""what if she changes her dream?"" What if we reboot this dream?1",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3351095.3375697,
author = {Pritchard, Helen and Snodgrass, Eric and Morrison, Romi Ron and Britton, Loren and Moll, Joana},
title = {Burn, Dream and Reboot! Speculating Backwards for the Missing Archive on Non-Coercive Computing},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375697},
doi = {10.1145/3351095.3375697},
abstract = {Whether one is speaking of barbed wire, the assembly line or computer operating systems, the history of coercive technologies for the automation of tasks has focused on optimization, determinate outcomes and an ongoing disciplining of components and bodies. Automated technologies of the present emerge and are marked by this lineage of coercive modes of implementation, whose scarred history of techniques of discrimination, exploitation and extraction point to an archive of automated injustices in computing, a history that continues to charge present paradigms and practices of computing.This workshop addresses the history of coercive technologies through attuning to how we perform speculation within practices of computing through a renewed attention to this history. We go backwards into the archive, rather than racing forward and proposing ever new speculative futures of automation. This is because speculative creative approaches are often conceived and positioned as methodological toolkits for addressing computing practices by imagining for/with others for a ""future otherwise"". We argue that ""speculation"" as the easy-go-to of designers and artists trying to address automated injustices needs some undoing, as without work it will always be confined within ongoing legacies of coercive modes of computing practice. Instead of creating more just-worlds, the generation of ever-new futures by creative speculation often merely reinforces the project of coercive computing.For this workshop, drawing on queer approaches to resisting futures and informed by activist feminist engagements with archives, we invite participants to temporarily resist imagining futures and instead to speculate backwards. We speculate backwards to various moments, artefacts and practices within computing history. What does it mean to understand techniques of computing and automation as coercive infrastructures? How did so many of the dreams and seeming promises of computing turn into the coercive practices that we see today? Has computing as a practice become so imbued with coercive techniques that we find it hard to imagine otherwise? Together, we will build a speculative understanding and possible archive of non-coercive computing. In the words of Alexis Pauline Gumbs, the emerging archive proposes ""how did their dreams make rooms to dream in""... or not, in the case of coercive practices of computing. And ""what if she changes her dream?"" What if we reboot this dream?1},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {683},
numpages = {1},
keywords = {optimization, social justice, automation, critical computing, speculative design, trans*feminist technoscience},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
FACCT,2020,"Centering disability perspectives in algorithmic fairness, accountability, & transparency",https://dl.acm.org/doi/10.1145/3351095.3375686,"['Alexandra Reeve Givens', 'Meredith Ringel Morris']",8,"It is vital to consider the unique risks and impacts of algorithmic decision-making for people with disabilities. The diverse nature of potential disabilities poses unique challenges for approaches to fairness, accountability, and transparency. Many disabled people choose not to disclose their disabilities, making auditing and accountability tools particularly hard to design and operate. Further, the variety inherent in disability poses challenges for collecting representative training data in any quantity sufficient to better train more inclusive and accountable algorithms. This panel highlights areas of concern, present emerging research efforts, and enlist more researchers and advocates to study the potential impacts of algorithmic decision-making on people with disabilities. A key objective is to surface new research projects and collaborations, including by integrating a critical disability perspective into existing research and advocacy efforts focused on identifying sources of bias and advancing equity. In the technology space, discussion topics will include methods to assess the fairness of current AI systems, and strategies to develop new systems and bias mitigation approaches that ensure fairness for people with disabilities. For example, how do today's currently-deployed AI systems impact people with disabilities? If developing inclusive datasets is part of the solution, how can researchers ethically gather such data, and what risks might centralizing data about disability pose? What new privacy solutions must developers create to reduce the risk of deductive disclosure of identities of people with disabilities in ""anonymized"" datasets? How can AI models and bias mitigation techniques be developed that handle the unique challenges of disability, i.e., the ""long tail"" and low incidence of many types of disability - for instance, how do we ensure that data about disability are not treated as outliers? What are the pros and cons of developing custom/personalized AI models for people with disabilities versus ensuring that general models are inclusive? In the law and policy space, the framework for people with disabilities requires specific study. For example, the Americans with Disabilities Act (ADA) requires employers to adopt ""reasonable accommodations"" for qualified individuals with a disability. But what is a ""reasonable accommodation"" in the context of machine learning and AI? How will the ADA's unique standards interact with case law and scholarship about algorithmic bias against other protected groups? When the ADA governs what questions employers can ask about a candidate's disability, and HIPAA and the Genetic Information Privacy Act regulate the sharing of health information, how should we think about inferences from data that approximate such questions? Panelists will bring varied perspectives to this conversation, including backgrounds in computer science, disability studies, legal studies, and activism. In addition to their scholarly expertise, several panelists have direct lived experience with disability. The session format will consist of brief position statements from each panelist, followed by questions from the moderator, and then open questions from and discussion with the audience.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3351095.3375686,
author = {Givens, Alexandra Reeve and Morris, Meredith Ringel},
title = {Centering Disability Perspectives in Algorithmic Fairness, Accountability, & Transparency},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375686},
doi = {10.1145/3351095.3375686},
abstract = {It is vital to consider the unique risks and impacts of algorithmic decision-making for people with disabilities. The diverse nature of potential disabilities poses unique challenges for approaches to fairness, accountability, and transparency. Many disabled people choose not to disclose their disabilities, making auditing and accountability tools particularly hard to design and operate. Further, the variety inherent in disability poses challenges for collecting representative training data in any quantity sufficient to better train more inclusive and accountable algorithms.This panel highlights areas of concern, present emerging research efforts, and enlist more researchers and advocates to study the potential impacts of algorithmic decision-making on people with disabilities. A key objective is to surface new research projects and collaborations, including by integrating a critical disability perspective into existing research and advocacy efforts focused on identifying sources of bias and advancing equity.In the technology space, discussion topics will include methods to assess the fairness of current AI systems, and strategies to develop new systems and bias mitigation approaches that ensure fairness for people with disabilities. For example, how do today's currently-deployed AI systems impact people with disabilities? If developing inclusive datasets is part of the solution, how can researchers ethically gather such data, and what risks might centralizing data about disability pose? What new privacy solutions must developers create to reduce the risk of deductive disclosure of identities of people with disabilities in ""anonymized"" datasets? How can AI models and bias mitigation techniques be developed that handle the unique challenges of disability, i.e., the ""long tail"" and low incidence of many types of disability - for instance, how do we ensure that data about disability are not treated as outliers? What are the pros and cons of developing custom/personalized AI models for people with disabilities versus ensuring that general models are inclusive?In the law and policy space, the framework for people with disabilities requires specific study. For example, the Americans with Disabilities Act (ADA) requires employers to adopt ""reasonable accommodations"" for qualified individuals with a disability. But what is a ""reasonable accommodation"" in the context of machine learning and AI? How will the ADA's unique standards interact with case law and scholarship about algorithmic bias against other protected groups? When the ADA governs what questions employers can ask about a candidate's disability, and HIPAA and the Genetic Information Privacy Act regulate the sharing of health information, how should we think about inferences from data that approximate such questions?Panelists will bring varied perspectives to this conversation, including backgrounds in computer science, disability studies, legal studies, and activism. In addition to their scholarly expertise, several panelists have direct lived experience with disability. The session format will consist of brief position statements from each panelist, followed by questions from the moderator, and then open questions from and discussion with the audience.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {684},
numpages = {1},
keywords = {accessibility, disability studies, algorithmic bias, AI FATE},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
FACCT,2020,"Creating community-based tech policy: case studies, lessons learned, and what technologists and communities can do together",https://dl.acm.org/doi/10.1145/3351095.3375689,"['Hannah Sassaman', 'Jennifer Lee', 'Jenessa Irvine', 'Shankar Narayan']",1,"What are the core ways the field of data science can center community voice and power throughout all the processes involved in conceptualizing, creating, and disseminating technology?? What are the most possible and most urgent ways communities can shape the field of algorithmic decision-making to center community power in the next few years? This interactive workshop will highlight some of the following lessons learned through our combined experience engaging with communities challenging technology in Seattle and Philadelphia, cities in the United States. We will discuss the historical context of disproportionate impacts of technology on marginalized and vulnerable communities; case studies including criminal justice risk assessments, face surveillance technologies, and surveillance regulations; and work in small-group and break-out sessions to engage questions about when and where technologists hold power, serve as gatekeepers, and can work in accountable partnership with impacted communities. By the end of the session, we hope that participants will learn how to actively center diverse communities in creating technology by examining successes, challenges, and ongoing work in Seattle and Philadelphia, through the following lessons we have learned: ‚Ä¢ that communities, policy-makers, and technologists need to work intimately together to lift up each other's' goals ‚Ä¢ that communities need to gain data justice and data literacy to understand and independently audit how a system is impacting them ‚Ä¢ that scientific analyses of algorithmic bias are powerful but heard most clearly when lifted up by local community members and stakeholders in decisions where algorithms might be deployed ‚Ä¢ that anecdotal stories of harm are most impactful on decisionmakers when tied to rigorous scientific analysis and examples from other communities that amplify and ground those stories ‚Ä¢ that communities and community goals and standards are often not heard in conversations between data scientists and people who deploy algorithms, as well as in decision-makers' conversations about what policy should look like ‚Ä¢ and that we need to begin to craft what it means for those with the least power in conversations about algorithmic fairness - those judged by those tools - to have far more, or even the most power in the future of their design or implementation.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3351095.3375689,
author = {Sassaman, Hannah and Lee, Jennifer and Irvine, Jenessa and Narayan, Shankar},
title = {Creating Community-Based Tech Policy: Case Studies, Lessons Learned, and What Technologists and Communities Can Do Together},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375689},
doi = {10.1145/3351095.3375689},
abstract = {What are the core ways the field of data science can center community voice and power throughout all the processes involved in conceptualizing, creating, and disseminating technology?? What are the most possible and most urgent ways communities can shape the field of algorithmic decision-making to center community power in the next few years? This interactive workshop will highlight some of the following lessons learned through our combined experience engaging with communities challenging technology in Seattle and Philadelphia, cities in the United States. We will discuss the historical context of disproportionate impacts of technology on marginalized and vulnerable communities; case studies including criminal justice risk assessments, face surveillance technologies, and surveillance regulations; and work in small-group and break-out sessions to engage questions about when and where technologists hold power, serve as gatekeepers, and can work in accountable partnership with impacted communities.By the end of the session, we hope that participants will learn how to actively center diverse communities in creating technology by examining successes, challenges, and ongoing work in Seattle and Philadelphia, through the following lessons we have learned:• that communities, policy-makers, and technologists need to work intimately together to lift up each other's' goals• that communities need to gain data justice and data literacy to understand and independently audit how a system is impacting them• that scientific analyses of algorithmic bias are powerful but heard most clearly when lifted up by local community members and stakeholders in decisions where algorithms might be deployed• that anecdotal stories of harm are most impactful on decisionmakers when tied to rigorous scientific analysis and examples from other communities that amplify and ground those stories• that communities and community goals and standards are often not heard in conversations between data scientists and people who deploy algorithms, as well as in decision-makers' conversations about what policy should look like• and that we need to begin to craft what it means for those with the least power in conversations about algorithmic fairness - those judged by those tools - to have far more, or even the most power in the future of their design or implementation.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {685},
numpages = {1},
keywords = {disproportionate impact, surveillance, community-centered, algorithms, criminal justice, bias, case studies},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
FACCT,2020,CtrlZ.AI zine fair: critical perspectives,https://dl.acm.org/doi/10.1145/3351095.3375692,"['Alex Hanna', 'Emily Denton']",4,"The FAT* conference has begun the necessary conversation on the normative implications and ethical ramifications of sociotechnical systems. However, many scholars have pointed to the limitations in methodologies and scope of analysis (e.g. [8, 11]). In addition to these critiques, we add in the fact that those who are most affected by this technology do not have the skills, training, or technical aptitude to participate in these conversations. With the exception of the 2018 FAT* tutorial which featured Terrance Wilkerson (who had been labeled as likely to highly recidivate by COMPAS) and his partner, there has been silence from those most impacted by algorithmic unfairness at FAT*. This silence has been deafening, as FAT* conversations - with a few notable exceptions (e.g. [1, 4]) - have failed to discuss anti-racist politics, prison abolition, and social justice.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3351095.3375692,
author = {Hanna, Alex and Denton, Emily},
title = {CtrlZ.AI Zine Fair: Critical Perspectives},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375692},
doi = {10.1145/3351095.3375692},
abstract = {The FAT* conference has begun the necessary conversation on the normative implications and ethical ramifications of sociotechnical systems. However, many scholars have pointed to the limitations in methodologies and scope of analysis (e.g. [8, 11]). In addition to these critiques, we add in the fact that those who are most affected by this technology do not have the skills, training, or technical aptitude to participate in these conversations. With the exception of the 2018 FAT* tutorial which featured Terrance Wilkerson (who had been labeled as likely to highly recidivate by COMPAS) and his partner, there has been silence from those most impacted by algorithmic unfairness at FAT*. This silence has been deafening, as FAT* conversations - with a few notable exceptions (e.g. [1, 4]) - have failed to discuss anti-racist politics, prison abolition, and social justice.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {686},
numpages = {1},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
FACCT,2020,"Deconstructing FAT: using memories to collectively explore implicit assumptions, values and context in practices of debiasing and discrimination-awareness",https://dl.acm.org/doi/10.1145/3351095.3375688,"['Doris Allhutter', 'Bettina Berendt']",81,"Research in fairness, accountability, and transparency (FAT) in socio-technical systems needs to take into account how practices of computing are entrenched with power relations in complex and multi-layered ways. Trying to disentangle the way in which structural discrimination and normative computational concepts and methods are intertwined, this frequently raises the question of WHO are the actors that shape technologies and research agendas---who gets to speak and to define bias, (un)fairness, and discrimination? ""Deconstructing FAT"" is a CRAFT workshop that aims at complicating this question by asking how ""we"" as researchers in FAT (often unknowingly) mobilize implicit assumptions, values and beliefs that reflect our own embeddedness in power relations, our disciplinary ways of thinking, and our historically, locally, and culturally-informed ways of solving computational problems or approaching our research. This is a vantage point to make visible and analyze the normativity of technical approaches, concepts and methods that are part of the repertoire of FAT research. Inspired by a previous international workshop [1], this CRAFT workshop engages an interdisciplinary panel of FAT researchers in a deconstruction exercise that traces the following issues: (1) FAT research frequently speaks of social bias that is amplified by algorithmic systems, of the problem of discriminatory consequences that is to be solved, and of underprivileged or vulnerable groups that need to be protected. What does this perspectivity imply in terms of the approaches, methods and metrics that are being applied? How do methods of debiasing and discrimination-awareness enact the epistemic power of a perspective of privilege as their norm? (2) FAT research has emphasized the need for multi- or interdisciplinary approaches to get a grip on the complex intertwining of social power relations and the normativity of computational methods, norms and practices. Clearly, multi- and interdisciplinary research includes different normative frameworks and ways of thinking that need to be negotiated. This is complicated by the fact that these frameworks are not fully transparent and ready for reflection. What are the normative implications of interdisciplinary collaboration in FAT research? (3) While many problems of discrimination, marginalization and exploitation can be similar across places, they can also have specific local shapes. How can FAT research e.g. consider historically grown specifics such as the effects of different colonial histories? If these specifics make patterns of discrimination have different and more nuanced dimensions than clear-cut 'redlining', what does this imply? To explore these questions, we use the method of 'mind scripting' which is based in theories of discourse, ideology, memory and affect and aims at investigating hidden patterns of meaning making in written memories of the panelists [2]. The workshop strives to challenge some of the implicit norms and tensions in FAT research and to trigger future directions.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3351095.3375688,
author = {Allhutter, Doris and Berendt, Bettina},
title = {Deconstructing FAT: Using Memories to Collectively Explore Implicit Assumptions, Values and Context in Practices of Debiasing and Discrimination-Awareness},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375688},
doi = {10.1145/3351095.3375688},
abstract = {Research in fairness, accountability, and transparency (FAT) in socio-technical systems needs to take into account how practices of computing are entrenched with power relations in complex and multi-layered ways. Trying to disentangle the way in which structural discrimination and normative computational concepts and methods are intertwined, this frequently raises the question of WHO are the actors that shape technologies and research agendas---who gets to speak and to define bias, (un)fairness, and discrimination? ""Deconstructing FAT"" is a CRAFT workshop that aims at complicating this question by asking how ""we"" as researchers in FAT (often unknowingly) mobilize implicit assumptions, values and beliefs that reflect our own embeddedness in power relations, our disciplinary ways of thinking, and our historically, locally, and culturally-informed ways of solving computational problems or approaching our research. This is a vantage point to make visible and analyze the normativity of technical approaches, concepts and methods that are part of the repertoire of FAT research. Inspired by a previous international workshop [1], this CRAFT workshop engages an interdisciplinary panel of FAT researchers in a deconstruction exercise that traces the following issues:(1) FAT research frequently speaks of social bias that is amplified by algorithmic systems, of the problem of discriminatory consequences that is to be solved, and of underprivileged or vulnerable groups that need to be protected. What does this perspectivity imply in terms of the approaches, methods and metrics that are being applied? How do methods of debiasing and discrimination-awareness enact the epistemic power of a perspective of privilege as their norm?(2) FAT research has emphasized the need for multi- or interdisciplinary approaches to get a grip on the complex intertwining of social power relations and the normativity of computational methods, norms and practices. Clearly, multi- and interdisciplinary research includes different normative frameworks and ways of thinking that need to be negotiated. This is complicated by the fact that these frameworks are not fully transparent and ready for reflection. What are the normative implications of interdisciplinary collaboration in FAT research? (3) While many problems of discrimination, marginalization and exploitation can be similar across places, they can also have specific local shapes. How can FAT research e.g. consider historically grown specifics such as the effects of different colonial histories? If these specifics make patterns of discrimination have different and more nuanced dimensions than clear-cut 'redlining', what does this imply?To explore these questions, we use the method of 'mind scripting' which is based in theories of discourse, ideology, memory and affect and aims at investigating hidden patterns of meaning making in written memories of the panelists [2]. The workshop strives to challenge some of the implicit norms and tensions in FAT research and to trigger future directions.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {687},
numpages = {1},
keywords = {deconstruction, discrimination-awareness in machine learning},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
FACCT,2020,"Fairness, accountability, transparency in AI at scale: lessons from national programs",https://dl.acm.org/doi/10.1145/3351095.3375690,"['Muhammad Aurangzeb Ahmad', 'Ankur Teredesai', 'Carly Eckert']",15,"The panel aims to elucidate how different national govenmental programs are implementing accountability of machine learning systems in healthcare and how accountability is operationlized in different cultural settings in legislation, policy and deployment. We have representatives from three different govenments, UAE, Singapore and Maldives who will discuss what accountability of AI and machine learning means in their contexts and use cases. We hope to have a fruitful conversation around FAT ML as it is operationalized ccross cultures, national boundries and legislative constraints.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3351095.3375690,
author = {Ahmad, Muhammad Aurangzeb and Teredesai, Ankur and Eckert, Carly},
title = {Fairness, Accountability, Transparency in AI at Scale: Lessons from National Programs},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375690},
doi = {10.1145/3351095.3375690},
abstract = {The panel aims to elucidate how different national govenmental programs are implementing accountability of machine learning systems in healthcare and how accountability is operationlized in different cultural settings in legislation, policy and deployment. We have representatives from three different govenments, UAE, Singapore and Maldives who will discuss what accountability of AI and machine learning means in their contexts and use cases. We hope to have a fruitful conversation around FAT ML as it is operationalized ccross cultures, national boundries and legislative constraints.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {690},
numpages = {1},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
FACCT,2020,Lost in translation: an interactive workshop mapping interdisciplinary translations for epistemic justice,https://dl.acm.org/doi/10.1145/3351095.3375685,"['Evelyn Wan', 'Aviva de Groot', 'Shazade Jameson', 'Mara PƒÉun', 'Phillip L√ºcking', 'Goda Klumbyte', 'Danny L√§mmerhirt']",2,"There are gaps in understanding in and between those who design systems of AI/ ML, those who critique them, and those positioned between these discourses. This gap can be defined in multiple ways - e.g. methodological, epistemological, linguistic, or cultural. To bridge this gap requires a set of translations: the generation of a collaborative space and a new set of shared sensibilities that traverse disciplinary boundaries. This workshop aims to explore translations across multiple fields, and translations between theory and practice, as well as how interdisciplinary work could generate new operationalizable approaches. We define 'knowledge' as a social product (L. Code) which requires fair and broad epistemic cooperation in its generation, development, and dissemination. As a ""marker for truth"" (B. Williams) and therefore a basis for action, knowledge circulation sustains the systems of power which produce it in the first place (M. Foucault). Enabled by epistemic credence, authority or knowledge, epistemic power can be an important driver of, but also result from, other (e.g. economic, political) powers. To produce reliable output, our standards and methods should serve us all and exclude no-one. Critical theorists have long revealed failings of epistemic practices, resulting in the marginalization and exclusion of some types of knowledge. How can we cultivate more reflexive epistemic practices in the interdisciplinary research setting of FAT*? We frame this ideal as 'epistemic justice' (M. Geuskens), the positive of 'epistemic injustice', defined by M. Fricker as injustice that exists when people are wronged as a knower or as an epistemic subject. Epistemic justice is the proper use and allocation of epistemic power; the inclusion and balancing of all epistemic sources. As S. Jasanoff reminds us, any authoritative way of seeing must be legitimized in discourse and practice, showing that practices can be developed to value and engage with other viewpoints and possibly reshape our ways of knowing. Our workshop aims to address the following questions: how could critical theory or higher level critiques be translated into and anchored in ML/AI design practices - and vice versa? What kind of cartographies and methodologies are needed in order to identify issues that can act as the basis of collaborative research and design? How can we (un)learn our established ways of thinking for such collaborative work to take place? During the workshop, participants will create, share and explode prototypical workflows of designing, researching and critiquing algorithmic systems. We will identify moments in which translations and interdisciplinary interventions could or should happen in order to build actionable steps and methodological frameworks that advance epistemic justice and are conducive to future interdisciplinary collaboration.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3351095.3375685,
author = {Wan, Evelyn and de Groot, Aviva and Jameson, Shazade and P\u{a}un, Mara and L\""{u}cking, Phillip and Klumbyte, Goda and L\""{a}mmerhirt, Danny},
title = {Lost in Translation: An Interactive Workshop Mapping Interdisciplinary Translations for Epistemic Justice},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375685},
doi = {10.1145/3351095.3375685},
abstract = {There are gaps in understanding in and between those who design systems of AI/ ML, those who critique them, and those positioned between these discourses. This gap can be defined in multiple ways - e.g. methodological, epistemological, linguistic, or cultural. To bridge this gap requires a set of translations: the generation of a collaborative space and a new set of shared sensibilities that traverse disciplinary boundaries. This workshop aims to explore translations across multiple fields, and translations between theory and practice, as well as how interdisciplinary work could generate new operationalizable approaches.We define 'knowledge' as a social product (L. Code) which requires fair and broad epistemic cooperation in its generation, development, and dissemination. As a ""marker for truth"" (B. Williams) and therefore a basis for action, knowledge circulation sustains the systems of power which produce it in the first place (M. Foucault). Enabled by epistemic credence, authority or knowledge, epistemic power can be an important driver of, but also result from, other (e.g. economic, political) powers.To produce reliable output, our standards and methods should serve us all and exclude no-one. Critical theorists have long revealed failings of epistemic practices, resulting in the marginalization and exclusion of some types of knowledge. How can we cultivate more reflexive epistemic practices in the interdisciplinary research setting of FAT*?We frame this ideal as 'epistemic justice' (M. Geuskens), the positive of 'epistemic injustice', defined by M. Fricker as injustice that exists when people are wronged as a knower or as an epistemic subject. Epistemic justice is the proper use and allocation of epistemic power; the inclusion and balancing of all epistemic sources.As S. Jasanoff reminds us, any authoritative way of seeing must be legitimized in discourse and practice, showing that practices can be developed to value and engage with other viewpoints and possibly reshape our ways of knowing.Our workshop aims to address the following questions: how could critical theory or higher level critiques be translated into and anchored in ML/AI design practices - and vice versa? What kind of cartographies and methodologies are needed in order to identify issues that can act as the basis of collaborative research and design? How can we (un)learn our established ways of thinking for such collaborative work to take place? During the workshop, participants will create, share and explode prototypical workflows of designing, researching and critiquing algorithmic systems. We will identify moments in which translations and interdisciplinary interventions could or should happen in order to build actionable steps and methodological frameworks that advance epistemic justice and are conducive to future interdisciplinary collaboration.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {692},
numpages = {1},
keywords = {workflow, algorithm development, epistemic justice, critical theory, methodologies, interdisciplinary collaboration},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
FACCT,2020,Manifesting the sociotechnical: experimenting with methods for social context and social justice,https://dl.acm.org/doi/10.1145/3351095.3375682,"['Ezra Goss', 'Lily Hu', 'Manuel Sabin', 'Stephanie Teeple']",2,"Critiques of 'algorithmic fairness' have counseled against a purely technical approach. Recent work from the FAT* conference has warned specifically about abstracting away the social context that these automated systems are operating within and has suggested that ""[fairness work] require[s] technical researchers to learn new skills or partner with social scientists"" [Fairness and abstraction in sociotechnical systems, Selbst et al. 2019, FAT* '19]. That ""social context"" includes groups outside the academy organizing for data and/or tech justice (e.g., Allied Media Projects, Stop LAPD Spying Coalition, data4blacklives, etc). These struggles have deep historical roots but have become prominent in the past several years alongside broader citizen-science efforts. In this CRAFT session we as STEM researchers hope to initiate conversation about methods used by community organizers to analyze power relations present in that social context. We will take this time to learn together and discuss if/how these and other methods, collaborations and efforts can be used to actualize oft-mentioned critiques of algorithmic fairness and move toward a data justice-oriented approach. Many scholars and activists have spoken on how to approach social context when discussing algorithmic fairness interventions. Community organizing and attendant methods for power analysis present one such approach: documenting all stakeholders and entities relevant to an issue and the nature of the power differentials between them. The facilitators for this session are not experts in community organizing theory or practice. Instead, we will share what we have learned from our readings of decades of rich work and writings from community organizers. This session is a collective, interdisciplinary learning experience, open to all who see their interests as relevant to the conversation. We will open with a discussion of community organizing practice: What is community organizing, what are its goals, methods, past and ongoing examples? What disciplines and intellectual lineages does it draw from? We will incorporate key sources we have found helpful for synthesizing this knowledge so that participants can continue exposing themselves to the field after the conference. We will also consider the concept of social power, including power that the algorithmic fairness community holds. Noting that there are many ways to theorize and understand power, we will share the framings that have been most useful to us. We plan to present different tools, models and procedures for doing power analysis in various organizing settings. We will propose to our group that we conduct a power analysis of our own. We have prepared a hypothetical but realistic scenario involving risk assessment in a hospital setting as an example. However, we encourage participants to bring their own experiences to the table, especially if they pertain in any way to data injustice. We also invite participants to bring examples of ongoing organizing efforts with which algorithmic fairness researchers could act in solidarity. Participants will walk away from this session with 1) an understanding of the key terms and sources necessary to gain further exposure to these topics and 2) preliminary experience analyzing power in realistic, grounded scenarios.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3351095.3375682,
author = {Goss, Ezra and Hu, Lily and Sabin, Manuel and Teeple, Stephanie},
title = {Manifesting the Sociotechnical: Experimenting with Methods for Social Context and Social Justice},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375682},
doi = {10.1145/3351095.3375682},
abstract = {Critiques of 'algorithmic fairness' have counseled against a purely technical approach. Recent work from the FAT* conference has warned specifically about abstracting away the social context that these automated systems are operating within and has suggested that ""[fairness work] require[s] technical researchers to learn new skills or partner with social scientists"" [Fairness and abstraction in sociotechnical systems, Selbst et al. 2019, FAT* '19]. That ""social context"" includes groups outside the academy organizing for data and/or tech justice (e.g., Allied Media Projects, Stop LAPD Spying Coalition, data4blacklives, etc). These struggles have deep historical roots but have become prominent in the past several years alongside broader citizen-science efforts. In this CRAFT session we as STEM researchers hope to initiate conversation about methods used by community organizers to analyze power relations present in that social context. We will take this time to learn together and discuss if/how these and other methods, collaborations and efforts can be used to actualize oft-mentioned critiques of algorithmic fairness and move toward a data justice-oriented approach.Many scholars and activists have spoken on how to approach social context when discussing algorithmic fairness interventions. Community organizing and attendant methods for power analysis present one such approach: documenting all stakeholders and entities relevant to an issue and the nature of the power differentials between them. The facilitators for this session are not experts in community organizing theory or practice. Instead, we will share what we have learned from our readings of decades of rich work and writings from community organizers. This session is a collective, interdisciplinary learning experience, open to all who see their interests as relevant to the conversation.We will open with a discussion of community organizing practice: What is community organizing, what are its goals, methods, past and ongoing examples? What disciplines and intellectual lineages does it draw from? We will incorporate key sources we have found helpful for synthesizing this knowledge so that participants can continue exposing themselves to the field after the conference. We will also consider the concept of social power, including power that the algorithmic fairness community holds. Noting that there are many ways to theorize and understand power, we will share the framings that have been most useful to us. We plan to present different tools, models and procedures for doing power analysis in various organizing settings.We will propose to our group that we conduct a power analysis of our own. We have prepared a hypothetical but realistic scenario involving risk assessment in a hospital setting as an example. However, we encourage participants to bring their own experiences to the table, especially if they pertain in any way to data injustice. We also invite participants to bring examples of ongoing organizing efforts with which algorithmic fairness researchers could act in solidarity. Participants will walk away from this session with 1) an understanding of the key terms and sources necessary to gain further exposure to these topics and 2) preliminary experience analyzing power in realistic, grounded scenarios.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {693},
numpages = {1},
keywords = {community organizing, power analysis, data justice, fairness},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
FACCT,2020,Assessing the intersection of organizational structure and FAT* efforts within industry: implications tutorial,https://dl.acm.org/doi/10.1145/3351095.3375672,"['Bogdana Rakova', 'Rumman Chowdhury', 'Jingying Yang']",4,"The work within the Fairness, Accountability, and Transparency of ML (fair-ML) community will positively benefit from appreciating the role of organizational culture and structure in the effective practice of fair-ML efforts of individuals, teams, and initiatives within industry. In this tutorial session we will explore various organizational structures and possible leverage points to effectively intervene in the process of design, development, and deployment of AI systems, towards contributing to positive fair-ML outcomes. We will begin by presenting the results of interviews conducted during an ethnographic study among practitioners working in industry, including themes related to: origination and evolution, common challenges, ethical tensions, and effective enablers. The study was designed through the lens of Industrial Organizational Psychology and aims to create a mapping of the current state of the fair-ML organizational structures inside major AI companies. We also look at the most-desired future state to enable effective work to increase algorithmic accountability, as well as the key elements in the transition from the current to that future state. We investigate drivers for change as well as the tensions between creating an 'ethical' system vs one that is 'ethical' enough. After presenting our preliminary findings, the rest of the tutorial will be highly interactive. Starting with a facilitated activity in break out groups, we will discuss the already identified challenges, best practices, and mitigation strategies. Finally, we hope to create space for productive discussion among AI practitioners in industry, academic researchers within various fields working directly on algorithmic accountability and transparency, advocates for various communities most impacted by technology, and others. Based on the interactive component of the tutorial, facilitators and interested participants will collaborate on further developing the discussed challenges into scenarios and guidelines that will be published as a follow up report.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3351095.3375672,
author = {Rakova, Bogdana and Chowdhury, Rumman and Yang, Jingying},
title = {Assessing the Intersection of Organizational Structure and FAT* Efforts within Industry: Implications Tutorial},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375672},
doi = {10.1145/3351095.3375672},
abstract = {The work within the Fairness, Accountability, and Transparency of ML (fair-ML) community will positively benefit from appreciating the role of organizational culture and structure in the effective practice of fair-ML efforts of individuals, teams, and initiatives within industry. In this tutorial session we will explore various organizational structures and possible leverage points to effectively intervene in the process of design, development, and deployment of AI systems, towards contributing to positive fair-ML outcomes. We will begin by presenting the results of interviews conducted during an ethnographic study among practitioners working in industry, including themes related to: origination and evolution, common challenges, ethical tensions, and effective enablers. The study was designed through the lens of Industrial Organizational Psychology and aims to create a mapping of the current state of the fair-ML organizational structures inside major AI companies. We also look at the most-desired future state to enable effective work to increase algorithmic accountability, as well as the key elements in the transition from the current to that future state. We investigate drivers for change as well as the tensions between creating an 'ethical' system vs one that is 'ethical' enough. After presenting our preliminary findings, the rest of the tutorial will be highly interactive. Starting with a facilitated activity in break out groups, we will discuss the already identified challenges, best practices, and mitigation strategies. Finally, we hope to create space for productive discussion among AI practitioners in industry, academic researchers within various fields working directly on algorithmic accountability and transparency, advocates for various communities most impacted by technology, and others. Based on the interactive component of the tutorial, facilitators and interested participants will collaborate on further developing the discussed challenges into scenarios and guidelines that will be published as a follow up report.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {697},
numpages = {1},
keywords = {organizational structure, fair machine learning, need-finding, empirical study, I/O psychology},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
FACCT,2020,Can an algorithmic system be a 'friend' to a police officer's discretion?: ACM FAT 2020 translation tutorial,https://dl.acm.org/doi/10.1145/3351095.3375673,"['Marion Oswald', 'David Powell']",0,"This tutorial aims to increase understanding of the importance of discretion in police decision-making. It will guide computer scientists, policy-makers, lawyers and others in considering practical and technical issues crucial to avoiding the prejudicial and instead develop algorithms that are supportive - a 'friend'- to legitimate discretionary decision-making. It combines explanation of the relevant law and related literature with discussion based upon deep operational experience in the area of preventative and protective policing work. Autonomy and discretion are fundamental to police work, not only in relation to strategy and policy but for day-to-day operational decisions taken by front line officers. Such discretion 'recognizes the fallibility of interfacing rules with their field of application.' (Hildebrandt 2016). This discretion is not unbounded however and English common law expects discretion to be exercised reasonably and fairly. Conversely, discretion must not be fettered unlawfully, by failing to take a relevant factor into account when making a decision, or by abdicating responsibility to another person, body or 'thing'. Algorithmic systems have the potential to contribute to factors relevant to the decision in question at the point of interaction between their outputs and the real-world outcome for the victim, offender and/or community. Algorithmic decision tools present a number of challenges to legitimate discretionary police decision-making. Unnuanced outputs could be highly influential on the human decision-maker (Cooke and Michie 2012) and may undermine discretionary power to deal with atypical cases and 'un-thought of' factors that rely upon uncodified knowledge (Oswald 2018). Practical and technical considerations will be crucial to developing MLA that are supportive to discretionary decision-making. These include the methodological approach, design of the humancomputer interface having regard the decision-maker's responsibility to give reasons for their decision, the avoidance of unnuanced or over-confident framing of results, understanding of the policing context in which the MLA will operate, and consideration of the implications of organisational culture and processes to the MLA's influence.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3351095.3375673,
author = {Oswald, Marion and Powell, David},
title = {Can an Algorithmic System Be a 'friend' to a Police Officer's Discretion? ACM FAT 2020 Translation Tutorial},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375673},
doi = {10.1145/3351095.3375673},
abstract = {This tutorial aims to increase understanding of the importance of discretion in police decision-making. It will guide computer scientists, policy-makers, lawyers and others in considering practical and technical issues crucial to avoiding the prejudicial and instead develop algorithms that are supportive - a 'friend'- to legitimate discretionary decision-making. It combines explanation of the relevant law and related literature with discussion based upon deep operational experience in the area of preventative and protective policing work.Autonomy and discretion are fundamental to police work, not only in relation to strategy and policy but for day-to-day operational decisions taken by front line officers. Such discretion 'recognizes the fallibility of interfacing rules with their field of application.' (Hildebrandt 2016). This discretion is not unbounded however and English common law expects discretion to be exercised reasonably and fairly. Conversely, discretion must not be fettered unlawfully, by failing to take a relevant factor into account when making a decision, or by abdicating responsibility to another person, body or 'thing'. Algorithmic systems have the potential to contribute to factors relevant to the decision in question at the point of interaction between their outputs and the real-world outcome for the victim, offender and/or community.Algorithmic decision tools present a number of challenges to legitimate discretionary police decision-making. Unnuanced outputs could be highly influential on the human decision-maker (Cooke and Michie 2012) and may undermine discretionary power to deal with atypical cases and 'un-thought of' factors that rely upon uncodified knowledge (Oswald 2018).Practical and technical considerations will be crucial to developing MLA that are supportive to discretionary decision-making. These include the methodological approach, design of the humancomputer interface having regard the decision-maker's responsibility to give reasons for their decision, the avoidance of unnuanced or over-confident framing of results, understanding of the policing context in which the MLA will operate, and consideration of the implications of organisational culture and processes to the MLA's influence.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {698},
numpages = {1},
keywords = {algorithms, discretion, machine learning, police},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
FACCT,2020,Explainable AI in industry: practical challenges and lessons learned: implications tutorial,https://dl.acm.org/doi/10.1145/3351095.3375664,"['Krishna Gade', 'Sahin Cem Geyik', 'Krishnaram Kenthapadi', 'Varun Mithal', 'Ankur Taly']",6,"Artificial Intelligence is increasingly playing an integral role in determining our day-to-day experiences. Moreover, with the proliferation of AI based solutions in areas such as hiring, lending, criminal justice, healthcare, and education, the resulting personal and professional implications of AI have become far-reaching. The dominant role played by AI models in these domains has led to a growing concern regarding potential bias in these models, and a demand for model transparency and interpretability [2, 4]. Model explainability is considered a prerequisite for building trust and adoption of AI systems in high stakes domains such as lending and healthcare [1] requiring reliability, safety, and fairness. It is also critical to automated transportation, and other industrial applications with significant socio-economic implications such as predictive maintenance, exploration of natural resources, and climate change modeling. As a consequence, AI researchers and practitioners have focused their attention on explainable AI to help them better trust and understand models at scale [5, 6, 8]. In fact, the field of explainability in AI/ML is at an inflexion point. There is a tremendous need from the societal, regulatory, commercial, end-user, and model developer perspectives. Consequently, practical and scalable explainability approaches are rapidly becoming available. The challenges for the research community include: (i) achieving consensus on the right notion of model explainability, (ii) identifying and formalizing explainability tasks from the perspectives of various stakeholders, and (iii) designing measures for evaluating explainability techniques. In this tutorial, we will first motivate the need for model interpretability and explainability in AI [3] from various perspectives. We will then provide a brief overview of several explainability techniques and tools. The rest of the tutorial will focus on the real-world application of explainability techniques in industry. We will present case studies spanning several domains such as: ‚Ä¢ Search and Recommendation systems: Understanding of search and recommendations systems, as well as how retrieval and ranking decisions happen in real-time [7]. Example applications include explanation of decisions made by an AI system towards job recommendations, ranking of potential candidates for job posters, and content recommendations. ‚Ä¢ Sales: Understanding of sales predictions in terms of customer up-sell/churn. ‚Ä¢ Fraud Detection: Examining and explaining AI systems that determine whether a content or event is fraudulent. ‚Ä¢ Lending: How to understand/interpret lending decisions made by an AI system. We will focus on the sociotechnical dimensions, practical challenges, and lessons learned during development and deployment of these systems, which would be beneficial for researchers and practitioners interested in explainable AI. Finally, we will discuss open challenges and research directions for the community.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3351095.3375664,
author = {Gade, Krishna and Geyik, Sahin Cem and Kenthapadi, Krishnaram and Mithal, Varun and Taly, Ankur},
title = {Explainable AI in Industry: Practical Challenges and Lessons Learned: Implications Tutorial},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375664},
doi = {10.1145/3351095.3375664},
abstract = {Artificial Intelligence is increasingly playing an integral role in determining our day-to-day experiences. Moreover, with the proliferation of AI based solutions in areas such as hiring, lending, criminal justice, healthcare, and education, the resulting personal and professional implications of AI have become far-reaching. The dominant role played by AI models in these domains has led to a growing concern regarding potential bias in these models, and a demand for model transparency and interpretability [2, 4]. Model explainability is considered a prerequisite for building trust and adoption of AI systems in high stakes domains such as lending and healthcare [1] requiring reliability, safety, and fairness. It is also critical to automated transportation, and other industrial applications with significant socio-economic implications such as predictive maintenance, exploration of natural resources, and climate change modeling.As a consequence, AI researchers and practitioners have focused their attention on explainable AI to help them better trust and understand models at scale [5, 6, 8]. In fact, the field of explainability in AI/ML is at an inflexion point. There is a tremendous need from the societal, regulatory, commercial, end-user, and model developer perspectives. Consequently, practical and scalable explainability approaches are rapidly becoming available. The challenges for the research community include: (i) achieving consensus on the right notion of model explainability, (ii) identifying and formalizing explainability tasks from the perspectives of various stakeholders, and (iii) designing measures for evaluating explainability techniques.In this tutorial, we will first motivate the need for model interpretability and explainability in AI [3] from various perspectives. We will then provide a brief overview of several explainability techniques and tools. The rest of the tutorial will focus on the real-world application of explainability techniques in industry. We will present case studies spanning several domains such as:• Search and Recommendation systems: Understanding of search and recommendations systems, as well as how retrieval and ranking decisions happen in real-time [7]. Example applications include explanation of decisions made by an AI system towards job recommendations, ranking of potential candidates for job posters, and content recommendations.• Sales: Understanding of sales predictions in terms of customer up-sell/churn.• Fraud Detection: Examining and explaining AI systems that determine whether a content or event is fraudulent.• Lending: How to understand/interpret lending decisions made by an AI system.We will focus on the sociotechnical dimensions, practical challenges, and lessons learned during development and deployment of these systems, which would be beneficial for researchers and practitioners interested in explainable AI. Finally, we will discuss open challenges and research directions for the community.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {699},
numpages = {1},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
FACCT,2020,Experimentation with fairness-aware recommendation using librec-auto: hands-on tutorial,https://dl.acm.org/doi/10.1145/3351095.3375670,"['Robin Douglas Burke', 'Masoud Mansoury', 'Nasim Sonboli']",5,"The field of machine learning fairness has developed metrics, methodologies, and data sets for experimenting with classification algorithms. However, equivalent research is lacking in the area of personalized recommender systems. This 180-minute hands-on tutorial will introduce participants to concepts in fairness-aware recommendation, and metrics and methodologies in evaluating recommendation fairness. Participants will also gain hands-on experience with conducting fairness-aware recommendation experiments with the LibRec recommendation system using the \libauto{} scripting platform, and learn the steps required to configure their own experiments, incorporate their own data sets, and design their own algorithms and metrics.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3351095.3375670,
author = {Burke, Robin Douglas and Mansoury, Masoud and Sonboli, Nasim},
title = {Experimentation with Fairness-Aware Recommendation Using Librec-Auto: Hands-on Tutorial},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375670},
doi = {10.1145/3351095.3375670},
abstract = {The field of machine learning fairness has developed metrics, methodologies, and data sets for experimenting with classification algorithms. However, equivalent research is lacking in the area of personalized recommender systems. This 180-minute hands-on tutorial will introduce participants to concepts in fairness-aware recommendation, and metrics and methodologies in evaluating recommendation fairness. Participants will also gain hands-on experience with conducting fairness-aware recommendation experiments with the LibRec recommendation system using the libauto{} scripting platform, and learn the steps required to configure their own experiments, incorporate their own data sets, and design their own algorithms and metrics.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {700},
numpages = {1},
keywords = {fairness, recommender systems, evaluation, software},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
FACCT,2020,Leap of FATE: human rights as a complementary framework for AI policy and practice,https://dl.acm.org/doi/10.1145/3351095.3375665,"['Corinne Cath', 'Mark Latonero', 'Vidushi Marda', 'Roya Pakzad']",5,"The premise of this translation tutorial is that human rights serves as a complementary framework - in addition to Fairness, Accountability, Transparency, and Ethics - for guiding and governing artificial intelligence (AI) and machine learning research and development. Attendees will participate in a case study, which will demonstrate show how a human rights framework, grounded in international law, fundamental values, and global systems of accountability, can offer the technical community a practical approach to addressing global AI risks and harms. This tutorial discusses how human rights frameworks can inform, guide and govern AI policy and practice in a manner that is complementary to Fairness, Accountability, Transparency, and Ethics (FATE) frameworks. Using the case study of researchers developing a facial recognition API at a tech company and its use by a law enforcement client, we will engage the audience to think through the benefits and challenges of applying human rights frameworks to AI system design and deployment. We will do so by providing a brief overview of the international human rights law, and various non-binding human rights frameworks in relation to our current discussions around FATE and then apply them to contemporary debates and case studies",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3351095.3375665,
author = {Cath, Corinne and Latonero, Mark and Marda, Vidushi and Pakzad, Roya},
title = {Leap of FATE: Human Rights as a Complementary Framework for AI Policy and Practice},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375665},
doi = {10.1145/3351095.3375665},
abstract = {The premise of this translation tutorial is that human rights serves as a complementary framework - in addition to Fairness, Accountability, Transparency, and Ethics - for guiding and governing artificial intelligence (AI) and machine learning research and development. Attendees will participate in a case study, which will demonstrate show how a human rights framework, grounded in international law, fundamental values, and global systems of accountability, can offer the technical community a practical approach to addressing global AI risks and harms. This tutorial discusses how human rights frameworks can inform, guide and govern AI policy and practice in a manner that is complementary to Fairness, Accountability, Transparency, and Ethics (FATE) frameworks. Using the case study of researchers developing a facial recognition API at a tech company and its use by a law enforcement client, we will engage the audience to think through the benefits and challenges of applying human rights frameworks to AI system design and deployment. We will do so by providing a brief overview of the international human rights law, and various non-binding human rights frameworks in relation to our current discussions around FATE and then apply them to contemporary debates and case studies},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {702},
numpages = {1},
keywords = {human rights, practice, policy, governance, ethics, law, AI, FAT},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
FACCT,2020,Probing ML models for fairness with the what-if tool and SHAP: hands-on tutorial,https://dl.acm.org/doi/10.1145/3351095.3375662,"['James Wexler', 'Mahima Pushkarna', 'Sara Robinson', 'Tolga Bolukbasi', 'Andrew Zaldivar']",1,"As more and more industries use machine learning, it's important to understand how these models make predictions, and where bias can be introduced in the process. In this tutorial we'll walk through two open source frameworks for analyzing your models from a fairness perspective. We'll start with the What-If Tool, a visualization tool that you can run inside a Python notebook to analyze an ML model. With the What-If Tool, you can identify dataset imbalances, see how individual features impact your model's prediction through partial dependence plots, and analyze human-centered ML models from a fairness perspective using various optimization strategies. Then we'll look at SHAP, a tool for interpreting the output of any machine learning model, and seeing how a model arrived at predictions for individual datapoints. We will then show how to use SHAP and the What-If Tool together. After the tutorial you'll have the skills to get started with both of these tools on your own datasets, and be better equipped to analyze your models from a fairness perspective.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3351095.3375662,
author = {Wexler, James and Pushkarna, Mahima and Robinson, Sara and Bolukbasi, Tolga and Zaldivar, Andrew},
title = {Probing ML Models for Fairness with the What-If Tool and SHAP: Hands-on Tutorial},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375662},
doi = {10.1145/3351095.3375662},
abstract = {As more and more industries use machine learning, it's important to understand how these models make predictions, and where bias can be introduced in the process. In this tutorial we'll walk through two open source frameworks for analyzing your models from a fairness perspective. We'll start with the What-If Tool, a visualization tool that you can run inside a Python notebook to analyze an ML model. With the What-If Tool, you can identify dataset imbalances, see how individual features impact your model's prediction through partial dependence plots, and analyze human-centered ML models from a fairness perspective using various optimization strategies.Then we'll look at SHAP, a tool for interpreting the output of any machine learning model, and seeing how a model arrived at predictions for individual datapoints. We will then show how to use SHAP and the What-If Tool together. After the tutorial you'll have the skills to get started with both of these tools on your own datasets, and be better equipped to analyze your models from a fairness perspective.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {705},
numpages = {1},
keywords = {fairness, machine learning, data visualization},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
FACCT,2020,The meaning and measurement of bias: lessons from natural language processing,https://dl.acm.org/doi/10.1145/3351095.3375671,"['Abigail Z. Jacobs', 'Su Lin Blodgett', 'Solon Barocas', 'Hal Daum√©', 'Hanna Wallach']",15,"The recent interest in identifying and mitigating bias in computational systems has introduced a wide range of different---and occasionally incomparable---proposals for what constitutes bias in such systems. This tutorial introduces the language of measurement modeling from the quantitative social sciences as a framework for examining how social, organizational, and political values enter computational systems and unpacking the varied normative concerns operationalized in different techniques for measuring ""bias."" We show that this framework helps to clarify the way unobservable theoretical constructs---such as ""creditworthiness,"" ""risk to society,"" or ""tweet toxicity""---are turned into measurable quantities and how this process may introduce fairness-related harms. In particular, we demonstrate how to systematically assess the construct validity and reliability of these measurements to detect and characterize specific types of harms, which arise from mismatches between constructs and their operationalizations. We then take a critical look at existing approaches to examining ""bias"" in NLP models, ranging from work on embedding spaces to machine translation and hate speech detection. We show that measurement modeling can help uncover the implicit constructs that such work aims to capture when measuring ""bias."" In so doing, we illustrate the limits of current ""debiasing"" techniques, which have obscured the specific harms whose measurements they implicitly aim to reduce. By introducing the language of measurement modeling, we provide the FAT* community with a framework for making explicit and testing assumptions about unobservable theoretical constructs embedded in computational systems, thereby clarifying and uniting our understandings of fairness-related harms.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3351095.3375671,
author = {Jacobs, Abigail Z. and Blodgett, Su Lin and Barocas, Solon and Daum\'{e}, Hal and Wallach, Hanna},
title = {The Meaning and Measurement of Bias: Lessons from Natural Language Processing},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375671},
doi = {10.1145/3351095.3375671},
abstract = {The recent interest in identifying and mitigating bias in computational systems has introduced a wide range of different---and occasionally incomparable---proposals for what constitutes bias in such systems. This tutorial introduces the language of measurement modeling from the quantitative social sciences as a framework for examining how social, organizational, and political values enter computational systems and unpacking the varied normative concerns operationalized in different techniques for measuring ""bias."" We show that this framework helps to clarify the way unobservable theoretical constructs---such as ""creditworthiness,"" ""risk to society,"" or ""tweet toxicity""---are turned into measurable quantities and how this process may introduce fairness-related harms. In particular, we demonstrate how to systematically assess the construct validity and reliability of these measurements to detect and characterize specific types of harms, which arise from mismatches between constructs and their operationalizations. We then take a critical look at existing approaches to examining ""bias"" in NLP models, ranging from work on embedding spaces to machine translation and hate speech detection. We show that measurement modeling can help uncover the implicit constructs that such work aims to capture when measuring ""bias."" In so doing, we illustrate the limits of current ""debiasing"" techniques, which have obscured the specific harms whose measurements they implicitly aim to reduce. By introducing the language of measurement modeling, we provide the FAT* community with a framework for making explicit and testing assumptions about unobservable theoretical constructs embedded in computational systems, thereby clarifying and uniting our understandings of fairness-related harms.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {706},
numpages = {1},
keywords = {word embeddings, fairness, measurement, bias, construct validity},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
FACCT,2020,Two computer scientists and a cultural scientist get hit by a driver-less car: a method for situating knowledge in the cross-disciplinary study of F-A-T in machine learning: translation tutorial,https://dl.acm.org/doi/10.1145/3351095.3375663,"['Maya Indira Ganesh', 'Francien Dechesne', 'Zeerak Waseem']",0,"In a workshop organized in December 2017 in Leiden, the Netherlands, a group of lawyers, computer scientists, artists, activists and social and cultural scientists collectively read a computer science paper about 'improving fairness'. This session was perceived by many participants as eye-opening on how different epistemologies shape approaches to the problem, method and solutions, thus enabling further cross-disciplinary discussions during the rest of the workshop. For many participants it was both refreshing and challenging, in equal measure, to understand how another discipline approached the problem of fairness. Now, as a follow-up we propose a translation tutorial that will engage participants at the FAT* conference in a similar exercise. We will invite participants to work in small groups reading excerpts of academic papers from different disciplinary perspectives on the same theme. We argue that most of us do not read outside our disciplines and thus are not familiar with how the same issues might be framed and addressed by our peers. Thus the purpose will be to have participants reflect on the different genealogies of knowledge in research, and how they erect walls, or generate opportunities for more productive inter-disciplinary work. We argue that addressing, through technical measures or otherwise, matters of ethics, bias and discrimination in AI/ML technologies in society is complicated by the different constructions of knowledge about what ethics (or bias or discrimination) means to different groups of practitioners. In the current academic structure, there are scarce resources to test, build on-or even discard-methods to talk across disciplinary lines. This tutorial is thus proposed to see if this particular method might work.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3351095.3375663,
author = {Ganesh, Maya Indira and Dechesne, Francien and Waseem, Zeerak},
title = {Two Computer Scientists and a Cultural Scientist Get Hit by a Driver-Less Car: A Method for Situating Knowledge in the Cross-Disciplinary Study of F-A-T in Machine Learning: Translation Tutorial},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375663},
doi = {10.1145/3351095.3375663},
abstract = {In a workshop organized in December 2017 in Leiden, the Netherlands, a group of lawyers, computer scientists, artists, activists and social and cultural scientists collectively read a computer science paper about 'improving fairness'. This session was perceived by many participants as eye-opening on how different epistemologies shape approaches to the problem, method and solutions, thus enabling further cross-disciplinary discussions during the rest of the workshop. For many participants it was both refreshing and challenging, in equal measure, to understand how another discipline approached the problem of fairness. Now, as a follow-up we propose a translation tutorial that will engage participants at the FAT* conference in a similar exercise. We will invite participants to work in small groups reading excerpts of academic papers from different disciplinary perspectives on the same theme. We argue that most of us do not read outside our disciplines and thus are not familiar with how the same issues might be framed and addressed by our peers. Thus the purpose will be to have participants reflect on the different genealogies of knowledge in research, and how they erect walls, or generate opportunities for more productive inter-disciplinary work. We argue that addressing, through technical measures or otherwise, matters of ethics, bias and discrimination in AI/ML technologies in society is complicated by the different constructions of knowledge about what ethics (or bias or discrimination) means to different groups of practitioners. In the current academic structure, there are scarce resources to test, build on-or even discard-methods to talk across disciplinary lines. This tutorial is thus proposed to see if this particular method might work.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {707},
numpages = {1},
keywords = {discrimination, epistemology, social sciences, situated knowledge, natural language processing, bias, humanities, cross-disciplinary, ethics, science, methodology},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
FACCT,2021,Black Feminist Musings on Algorithmic Oppression,https://dl.acm.org/doi/10.1145/3442188.3445929,['Lelia Marie Hampton'],33,"This paper uses a theory of oppression to ground and extend algorithmic oppression. Algorithmic oppression is then situated through a Black feminist lens part of which entails highlighting the double bind of technology. To reconcile algorithmic oppression with respect to the fairness, accountability, and transparency community, I critique the language of the community. Lastly, I place algorithmic oppression in a broader conversation of feminist science, technology, and society studies to ground the discussion of ways forward through abolition and empowering marginalized communities.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3442188.3445929,
author = {Hampton, Lelia Marie},
title = {Black Feminist Musings on Algorithmic Oppression},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445929},
doi = {10.1145/3442188.3445929},
abstract = {This paper uses a theory of oppression to ground and extend algorithmic oppression. Algorithmic oppression is then situated through a Black feminist lens part of which entails highlighting the double bind of technology. To reconcile algorithmic oppression with respect to the fairness, accountability, and transparency community, I critique the language of the community. Lastly, I place algorithmic oppression in a broader conversation of feminist science, technology, and society studies to ground the discussion of ways forward through abolition and empowering marginalized communities.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1},
numpages = {1},
keywords = {abolition, Black feminism, Algorithmic oppression},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
FACCT,2021,Price Discrimination with Fairness Constraints,https://dl.acm.org/doi/10.1145/3442188.3445864,"['Maxime C. Cohen', 'Adam N. Elmachtoub', 'Xiao Lei']",32,"Price discrimination - offering different prices to different customers - has become common practice. While it allows sellers to increase their profits, it also raises several concerns in terms of fairness. This topic has received extensive attention from media, industry, and regulatory agencies. In this paper, we consider the problem of setting prices for different groups under fairness constraints. In this paper, we propose a formal framework for pricing with fairness, including several definitions of fairness and their potential impact on consumers, sellers, and society at large. In a first step towards the ambitious agenda of designing pricing strategies that are fair, we consider the simplest scenario of a single-product seller facing consumers who can be partitioned into two groups based on a single, binary feature observable to the seller. For each group, we assume that the seller knows the valuation distribution and the population size. The seller's goal is to maximize profit by optimally selecting a price for each group, subject to a fairness constraint which may be self-imposed or explicitly enforced by laws and regulations. We first propose four definitions: fairness in price, demand, consumer surplus, and no-purchase valuation. With our model and definitions in place, we first show that satisfying all four fairness goals simultaneously is impossible unless the mean valuations are the same for both groups. In fact, even achieving two fairness measures simultaneously cannot be done in basic settings. We then consider the impact of imposing each fairness criterion separately, and identify conditions under which the consumer surplus and the social welfare increase or decrease. Under linear or exponential demand, we show that imposing a small amount of fairness in price or no-purchase valuation increases social welfare, whereas fairness in demand or surplus reduces social welfare. We fully characterize the impact of imposing different types of fairness for linear demand. We discover that imposing too much price fairness may result in a lower social welfare relative to imposing no price fairness. Imposing demand and surplus fairness always decreases social welfare. However, imposing no-purchase valuation fairness always increases social welfare. We also extend our results to the cases when there are multiple groups or there is an unprotected feature. Finally, we computationally show that most of our findings continue to hold for three common nonlinear demand models. Our results and insights provide a first step in understanding the impact of imposing fairness in the context of pricing.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3442188.3445864,
author = {Cohen, Maxime C. and Elmachtoub, Adam N. and Lei, Xiao},
title = {Price Discrimination with Fairness Constraints},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445864},
doi = {10.1145/3442188.3445864},
abstract = {Price discrimination - offering different prices to different customers - has become common practice. While it allows sellers to increase their profits, it also raises several concerns in terms of fairness. This topic has received extensive attention from media, industry, and regulatory agencies. In this paper, we consider the problem of setting prices for different groups under fairness constraints.In this paper, we propose a formal framework for pricing with fairness, including several definitions of fairness and their potential impact on consumers, sellers, and society at large. In a first step towards the ambitious agenda of designing pricing strategies that are fair, we consider the simplest scenario of a single-product seller facing consumers who can be partitioned into two groups based on a single, binary feature observable to the seller. For each group, we assume that the seller knows the valuation distribution and the population size. The seller's goal is to maximize profit by optimally selecting a price for each group, subject to a fairness constraint which may be self-imposed or explicitly enforced by laws and regulations.We first propose four definitions: fairness in price, demand, consumer surplus, and no-purchase valuation. With our model and definitions in place, we first show that satisfying all four fairness goals simultaneously is impossible unless the mean valuations are the same for both groups. In fact, even achieving two fairness measures simultaneously cannot be done in basic settings. We then consider the impact of imposing each fairness criterion separately, and identify conditions under which the consumer surplus and the social welfare increase or decrease. Under linear or exponential demand, we show that imposing a small amount of fairness in price or no-purchase valuation increases social welfare, whereas fairness in demand or surplus reduces social welfare. We fully characterize the impact of imposing different types of fairness for linear demand. We discover that imposing too much price fairness may result in a lower social welfare relative to imposing no price fairness. Imposing demand and surplus fairness always decreases social welfare. However, imposing no-purchase valuation fairness always increases social welfare. We also extend our results to the cases when there are multiple groups or there is an unprotected feature.Finally, we computationally show that most of our findings continue to hold for three common nonlinear demand models. Our results and insights provide a first step in understanding the impact of imposing fairness in the context of pricing.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {2},
numpages = {1},
keywords = {price discrimination, social welfare, fairness, personalization},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
FACCT,2021,Fairness Violations and Mitigation under Covariate Shift,https://dl.acm.org/doi/10.1145/3442188.3445865,"['Harvineet Singh', 'Rina Singh', 'Vishwali Mhasawade', 'Rumi Chunara']",35,"We study the problem of learning fair prediction models for unseen test sets distributed differently from the train set. Stability against changes in data distribution is an important mandate for responsible deployment of models. The domain adaptation literature addresses this concern, albeit with the notion of stability limited to that of prediction accuracy. We identify sufficient conditions under which stable models, both in terms of prediction accuracy and fairness, can be learned. Using the causal graph describing the data and the anticipated shifts, we specify an approach based on feature selection that exploits conditional independencies in the data to estimate accuracy and fairness metrics for the test set. We show that for specific fairness definitions, the resulting model satisfies a form of worst-case optimality. In context of a healthcare task, we illustrate the advantages of the approach in making more equitable decisions.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3442188.3445865,
author = {Singh, Harvineet and Singh, Rina and Mhasawade, Vishwali and Chunara, Rumi},
title = {Fairness Violations and Mitigation under Covariate Shift},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445865},
doi = {10.1145/3442188.3445865},
abstract = {We study the problem of learning fair prediction models for unseen test sets distributed differently from the train set. Stability against changes in data distribution is an important mandate for responsible deployment of models. The domain adaptation literature addresses this concern, albeit with the notion of stability limited to that of prediction accuracy. We identify sufficient conditions under which stable models, both in terms of prediction accuracy and fairness, can be learned. Using the causal graph describing the data and the anticipated shifts, we specify an approach based on feature selection that exploits conditional independencies in the data to estimate accuracy and fairness metrics for the test set. We show that for specific fairness definitions, the resulting model satisfies a form of worst-case optimality. In context of a healthcare task, we illustrate the advantages of the approach in making more equitable decisions.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {3–13},
numpages = {11},
keywords = {domain adaptation, causal inference, algorithmic fairness, covariate shift},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
FACCT,2021,Corporate Social Responsibility via Multi-Armed Bandits,https://dl.acm.org/doi/10.1145/3442188.3445868,"['Tom Ron', 'Omer Ben-Porat', 'Uri Shalit']",6,"We propose a multi-armed bandit setting where each arm corresponds to a subpopulation, and pulling an arm is equivalent to granting an opportunity to this subpopulation. In this setting the decision-maker's fairness policy governs the number of opportunities each subpopulation should receive, which typically depends on the (unknown) reward from granting an opportunity to this subpopulation. The decision-maker can decide whether to provide these opportunities, or pay a pre-defined monetary value for every withheld opportunity. The decision-maker's objective is to maximize her utility, which is the sum of rewards minus the cost paid for withheld opportunities. We provide a no-regret algorithm that maximizes the decision-maker's utility and complement our analysis with an almost-tight lower bound. Finally, we discuss the fairness policy and demonstrate its downstream implications on the utility and opportunities via simulations.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3442188.3445868,
author = {Ron, Tom and Ben-Porat, Omer and Shalit, Uri},
title = {Corporate Social Responsibility via Multi-Armed Bandits},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445868},
doi = {10.1145/3442188.3445868},
abstract = {We propose a multi-armed bandit setting where each arm corresponds to a subpopulation, and pulling an arm is equivalent to granting an opportunity to this subpopulation. In this setting the decision-maker's fairness policy governs the number of opportunities each subpopulation should receive, which typically depends on the (unknown) reward from granting an opportunity to this subpopulation. The decision-maker can decide whether to provide these opportunities, or pay a pre-defined monetary value for every withheld opportunity. The decision-maker's objective is to maximize her utility, which is the sum of rewards minus the cost paid for withheld opportunities. We provide a no-regret algorithm that maximizes the decision-maker's utility and complement our analysis with an almost-tight lower bound. Finally, we discuss the fairness policy and demonstrate its downstream implications on the utility and opportunities via simulations.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {26–40},
numpages = {15},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
FACCT,2021,Designing an Online Infrastructure for Collecting AI Data From People With Disabilities,https://dl.acm.org/doi/10.1145/3442188.3445870,"['Joon Sung Park', 'Danielle Bragg', 'Ece Kamar', 'Meredith Ringel Morris']",15,"AI technology offers opportunities to expand virtual and physical access for people with disabilities. However, an important part of bringing these opportunities to fruition is ensuring that upcoming AI technology works well for people with a wide range of abilities. In this paper, we identify the lack of data from disabled populations as one of the challenges to training and benchmarking fair and inclusive AI systems. As a potential solution, we envision an online infrastructure that can enable large-scale, remote data contributions from disability communities. We investigate the motivations, concerns, and challenges that people with disabilities might experience when asked to collect and upload various forms of AI-relevant data through a semi-structured interview and an online survey that simulated a data contribution process by collecting example data files through an online portal. Based on our findings, we outline design guidelines for developers creating online infrastructures for gathering data from people with disabilities.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3442188.3445870,
author = {Park, Joon Sung and Bragg, Danielle and Kamar, Ece and Morris, Meredith Ringel},
title = {Designing an Online Infrastructure for Collecting AI Data From People With Disabilities},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445870},
doi = {10.1145/3442188.3445870},
abstract = {AI technology offers opportunities to expand virtual and physical access for people with disabilities. However, an important part of bringing these opportunities to fruition is ensuring that upcoming AI technology works well for people with a wide range of abilities. In this paper, we identify the lack of data from disabled populations as one of the challenges to training and benchmarking fair and inclusive AI systems. As a potential solution, we envision an online infrastructure that can enable large-scale, remote data contributions from disability communities. We investigate the motivations, concerns, and challenges that people with disabilities might experience when asked to collect and upload various forms of AI-relevant data through a semi-structured interview and an online survey that simulated a data contribution process by collecting example data files through an online portal. Based on our findings, we outline design guidelines for developers creating online infrastructures for gathering data from people with disabilities.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {52–63},
numpages = {12},
keywords = {disability, accessibility, AI FATE, inclusion, representation, datasets},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
FACCT,2021,"Representativeness in Statistics, Politics, and Machine Learning",https://dl.acm.org/doi/10.1145/3442188.3445872,"['Kyla Chasalow', 'Karen Levy']",20,"Representativeness is a foundational yet slippery concept. Though familiar at first blush, it lacks a single precise meaning. Instead, meanings range from typical or characteristic, to a proportionate match between sample and population, to a more general sense of accuracy, generalizability, coverage, or inclusiveness. Moreover, the concept has long been contested. In statistics, debates about the merits and methods of selecting a representative sample date back to the late 19th century; in politics, debates about the value of likeness as a logic of political representation are older still. Today, as the concept crops up in the study of fairness and accountability in machine learning, we need to carefully consider the term's meanings in order to communicate clearly and account for their normative implications. In this paper, we ask what representativeness means, how it is mobilized socially, and what values and ideals it communicates or confronts. We trace the concept's history in statistics and discuss normative tensions concerning its relationship to likeness, exclusion, authority, and aspiration. We draw on these analyses to think through how representativeness is used in FAccT debates, with emphasis on data, shift, participation, and power.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3442188.3445872,
author = {Chasalow, Kyla and Levy, Karen},
title = {Representativeness in Statistics, Politics, and Machine Learning},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445872},
doi = {10.1145/3442188.3445872},
abstract = {Representativeness is a foundational yet slippery concept. Though familiar at first blush, it lacks a single precise meaning. Instead, meanings range from typical or characteristic, to a proportionate match between sample and population, to a more general sense of accuracy, generalizability, coverage, or inclusiveness. Moreover, the concept has long been contested. In statistics, debates about the merits and methods of selecting a representative sample date back to the late 19th century; in politics, debates about the value of likeness as a logic of political representation are older still. Today, as the concept crops up in the study of fairness and accountability in machine learning, we need to carefully consider the term's meanings in order to communicate clearly and account for their normative implications. In this paper, we ask what representativeness means, how it is mobilized socially, and what values and ideals it communicates or confronts. We trace the concept's history in statistics and discuss normative tensions concerning its relationship to likeness, exclusion, authority, and aspiration. We draw on these analyses to think through how representativeness is used in FAccT debates, with emphasis on data, shift, participation, and power.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {77–89},
numpages = {13},
keywords = {sampling, fairness, representativeness, inclusion, participation, bias},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
FACCT,2021,Group Fairness: Independence Revisited,https://dl.acm.org/doi/10.1145/3442188.3445876,['Tim R√§z'],23,"This paper critically examines arguments against independence, a measure of group fairness also known as statistical parity and as demographic parity. In recent discussions of fairness in computer science, some have maintained that independence is not a suitable measure of group fairness. This position is at least partially based on two influential papers (Dwork et al., 2012, Hardt et al., 2016) that provide arguments against independence. We revisit these arguments, and we find that the case against independence is rather weak. We also give arguments in favor of independence, showing that it plays a distinctive role in considerations of fairness. Finally, we discuss how to balance different fairness considerations.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3442188.3445876,
author = {R\""{a}z, Tim},
title = {Group Fairness: Independence Revisited},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445876},
doi = {10.1145/3442188.3445876},
abstract = {This paper critically examines arguments against independence, a measure of group fairness also known as statistical parity and as demographic parity. In recent discussions of fairness in computer science, some have maintained that independence is not a suitable measure of group fairness. This position is at least partially based on two influential papers (Dwork et al., 2012, Hardt et al., 2016) that provide arguments against independence. We revisit these arguments, and we find that the case against independence is rather weak. We also give arguments in favor of independence, showing that it plays a distinctive role in considerations of fairness. Finally, we discuss how to balance different fairness considerations.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {129–137},
numpages = {9},
keywords = {separation, accuracy, affirmative action, demographic parity, fairness, statistical parity, sufficiency, independence},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
FACCT,2021,Towards Fair Deep Anomaly Detection,https://dl.acm.org/doi/10.1145/3442188.3445878,"['Hongjing Zhang', 'Ian Davidson']",20,"Anomaly detection aims to find instances that are considered unusual and is a fundamental problem of data science. Recently, deep anomaly detection methods were shown to achieve superior results particularly in complex data such as images. Our work focuses on deep one-class classification for anomaly detection which learns a mapping only from the normal samples. However, the non-linear transformation performed by deep learning can potentially find patterns associated with social bias. The challenge with adding fairness to deep anomaly detection is to ensure both making fair and correct anomaly predictions simultaneously. In this paper, we propose a new architecture for the fair anomaly detection approach (Deep Fair SVDD) and train it using an adversarial network to de-correlate the relationships between the sensitive attributes and the learned representations. This differs from how fairness is typically added namely as a regularizer or a constraint. Further, we propose two effective fairness measures and empirically demonstrate that existing deep anomaly detection methods are unfair. We show that our proposed approach can remove the unfairness largely with minimal loss on the anomaly detection performance. Lastly, we conduct an in-depth analysis to show the strength and limitations of our proposed model, including parameter analysis, feature visualization, and run-time analysis.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3442188.3445878,
author = {Zhang, Hongjing and Davidson, Ian},
title = {Towards Fair Deep Anomaly Detection},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445878},
doi = {10.1145/3442188.3445878},
abstract = {Anomaly detection aims to find instances that are considered unusual and is a fundamental problem of data science. Recently, deep anomaly detection methods were shown to achieve superior results particularly in complex data such as images. Our work focuses on deep one-class classification for anomaly detection which learns a mapping only from the normal samples. However, the non-linear transformation performed by deep learning can potentially find patterns associated with social bias. The challenge with adding fairness to deep anomaly detection is to ensure both making fair and correct anomaly predictions simultaneously. In this paper, we propose a new architecture for the fair anomaly detection approach (Deep Fair SVDD) and train it using an adversarial network to de-correlate the relationships between the sensitive attributes and the learned representations. This differs from how fairness is typically added namely as a regularizer or a constraint. Further, we propose two effective fairness measures and empirically demonstrate that existing deep anomaly detection methods are unfair. We show that our proposed approach can remove the unfairness largely with minimal loss on the anomaly detection performance. Lastly, we conduct an in-depth analysis to show the strength and limitations of our proposed model, including parameter analysis, feature visualization, and run-time analysis.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {138–148},
numpages = {11},
keywords = {anomaly detection, deep learning, algorithmic fairness, adversarial learning, machine learning},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
FACCT,2021,Can You Fake It Until You Make It?: Impacts of Differentially Private Synthetic Data on Downstream Classification Fairness,https://dl.acm.org/doi/10.1145/3442188.3445879,"['Victoria Cheng', 'Vinith M. Suriyakumar', 'Natalie Dullerud', 'Shalmali Joshi', 'Marzyeh Ghassemi']",25,"The recent adoption of machine learning models in high-risk settings such as medicine has increased demand for developments in privacy and fairness. Rebalancing skewed datasets using synthetic data created by generative adversarial networks (GANs) has shown potential to mitigate disparate impact on minoritized subgroups. However, such generative models are subject to privacy attacks that can expose sensitive data from the training dataset. Differential privacy (DP) is the current leading solution for privacy-preserving machine learning. Differentially private GANs (DP GANs) are often considered a potential solution for improving model fairness while maintaining privacy of sensitive training data. We investigate the impact of using synthetic images from DP GANs on downstream classification model utility and fairness. We demonstrate that existing DP GANs cannot simultaneously maintain model utility, privacy, and fairness. The images generated from GAN models trained with DP exhibit extreme decreases in image quality and utility which leads to poor downstream classification model performance. Our evaluation highlights the friction between privacy, fairness, and utility and how this directly translates into real loss of performance and representation in common machine learning settings. Our results show that additional work improving the utility and fairness of DP generative models is required before they can be utilized as a potential solution to privacy and fairness issues stemming from lack of diversity in the training dataset.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3442188.3445879,
author = {Cheng, Victoria and Suriyakumar, Vinith M. and Dullerud, Natalie and Joshi, Shalmali and Ghassemi, Marzyeh},
title = {Can You Fake It Until You Make It? Impacts of Differentially Private Synthetic Data on Downstream Classification Fairness},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445879},
doi = {10.1145/3442188.3445879},
abstract = {The recent adoption of machine learning models in high-risk settings such as medicine has increased demand for developments in privacy and fairness. Rebalancing skewed datasets using synthetic data created by generative adversarial networks (GANs) has shown potential to mitigate disparate impact on minoritized subgroups. However, such generative models are subject to privacy attacks that can expose sensitive data from the training dataset. Differential privacy (DP) is the current leading solution for privacy-preserving machine learning. Differentially private GANs (DP GANs) are often considered a potential solution for improving model fairness while maintaining privacy of sensitive training data. We investigate the impact of using synthetic images from DP GANs on downstream classification model utility and fairness. We demonstrate that existing DP GANs cannot simultaneously maintain model utility, privacy, and fairness. The images generated from GAN models trained with DP exhibit extreme decreases in image quality and utility which leads to poor downstream classification model performance. Our evaluation highlights the friction between privacy, fairness, and utility and how this directly translates into real loss of performance and representation in common machine learning settings. Our results show that additional work improving the utility and fairness of DP generative models is required before they can be utilized as a potential solution to privacy and fairness issues stemming from lack of diversity in the training dataset.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {149–160},
numpages = {12},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
FACCT,2021,Better Together?: How Externalities of Size Complicate Notions of Solidarity and Actuarial Fairness,https://dl.acm.org/doi/10.1145/3442188.3445882,"['Kate Donahue', 'Solon Barocas']",2,"Consider a cost-sharing game with players of different costs: an example might be an insurance company calculating premiums for a population of mixed-risk individuals. Two natural and competing notions of fairness might be to a) charge each individual the same or b) charge each individual according to the cost that they bring to the pool. In the insurance literature, these approaches are referred to as ""solidarity"" and ""actuarial fairness"" and are commonly viewed as opposites. However, in insurance (and many other natural settings), the cost-sharing game also exhibits externalities of size: all else being equal, larger groups have lower average cost. In the insurance case, we analyze model where costs strictly decreases with pooling due to a reduction in the variability of losses. In this paper, we explore how this complicates traditional understandings of fairness, drawing on literature in cooperative game theory. First, we explore solidarity: we show that it is possible for both groups (high risk and low risk) to strictly benefit by joining an insurance pool where costs are evenly split, as opposed to being in separate risk pools. We build on this by producing a pricing scheme that maximally subsidizes the high risk group, while maintaining an incentive for lower risk people to stay in the insurance pool. Next, we demonstrate that with this new model, the price charged to each individual has to depend on the risk of other participants, making naive actuarial fairness inefficient. Furthermore, we prove that stable pricing schemes must be ones where players have the antisocial incentive desiring riskier partners, contradicting motivations for using actuarial fairness. Finally, we describe how these results relate to debates about fairness in machine learning and potential avenues for future research.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3442188.3445882,
author = {Donahue, Kate and Barocas, Solon},
title = {Better Together? How Externalities of Size Complicate Notions of Solidarity and Actuarial Fairness},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445882},
doi = {10.1145/3442188.3445882},
abstract = {Consider a cost-sharing game with players of different costs: an example might be an insurance company calculating premiums for a population of mixed-risk individuals. Two natural and competing notions of fairness might be to a) charge each individual the same or b) charge each individual according to the cost that they bring to the pool. In the insurance literature, these approaches are referred to as ""solidarity"" and ""actuarial fairness"" and are commonly viewed as opposites. However, in insurance (and many other natural settings), the cost-sharing game also exhibits externalities of size: all else being equal, larger groups have lower average cost. In the insurance case, we analyze model where costs strictly decreases with pooling due to a reduction in the variability of losses. In this paper, we explore how this complicates traditional understandings of fairness, drawing on literature in cooperative game theory.First, we explore solidarity: we show that it is possible for both groups (high risk and low risk) to strictly benefit by joining an insurance pool where costs are evenly split, as opposed to being in separate risk pools. We build on this by producing a pricing scheme that maximally subsidizes the high risk group, while maintaining an incentive for lower risk people to stay in the insurance pool. Next, we demonstrate that with this new model, the price charged to each individual has to depend on the risk of other participants, making naive actuarial fairness inefficient. Furthermore, we prove that stable pricing schemes must be ones where players have the antisocial incentive desiring riskier partners, contradicting motivations for using actuarial fairness. Finally, we describe how these results relate to debates about fairness in machine learning and potential avenues for future research.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {185–195},
numpages = {11},
keywords = {actuarial fairness, cooperative game theory, insurance, submodular cost function, fair cost sharing, solidarity},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
FACCT,2021,Evaluating Fairness of Machine Learning Models Under Uncertain and Incomplete Information,https://dl.acm.org/doi/10.1145/3442188.3445884,"['Pranjal Awasthi', 'Alex Beutel', 'Matth√§us Kleindessner', 'Jamie Morgenstern', 'Xuezhi Wang']",32,"Training and evaluation of fair classifiers is a challenging problem. This is partly due to the fact that most fairness metrics of interest depend on both the sensitive attribute information and label information of the data points. In many scenarios it is not possible to collect large datasets with such information. An alternate approach that is commonly used is to separately train an attribute classifier on data with sensitive attribute information, and then use it later in the ML pipeline to evaluate the bias of a given classifier. While such decoupling helps alleviate the problem of demographic scarcity, it raises several natural questions such as: how should the attribute classifier be trained?, and how should one use a given attribute classifier for accurate bias estimation? In this work we study this question from both theoretical and empirical perspectives. We first experimentally demonstrate that the test accuracy of the attribute classifier is not always correlated with its effectiveness in bias estimation for a downstream model. In order to further investigate this phenomenon, we analyze an idealized theoretical model and characterize the structure of the optimal classifier. Our analysis has surprising and counter-intuitive implications where in certain regimes one might want to distribute the error of the attribute classifier as unevenly as possible among the different subgroups. Based on our analysis we develop heuristics for both training and using attribute classifiers for bias estimation in the data scarce regime. We empirically demonstrate the effectiveness of our approach on real and simulated data.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3442188.3445884,
author = {Awasthi, Pranjal and Beutel, Alex and Kleindessner, Matth\""{a}us and Morgenstern, Jamie and Wang, Xuezhi},
title = {Evaluating Fairness of Machine Learning Models Under Uncertain and Incomplete Information},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445884},
doi = {10.1145/3442188.3445884},
abstract = {Training and evaluation of fair classifiers is a challenging problem. This is partly due to the fact that most fairness metrics of interest depend on both the sensitive attribute information and label information of the data points. In many scenarios it is not possible to collect large datasets with such information. An alternate approach that is commonly used is to separately train an attribute classifier on data with sensitive attribute information, and then use it later in the ML pipeline to evaluate the bias of a given classifier. While such decoupling helps alleviate the problem of demographic scarcity, it raises several natural questions such as: how should the attribute classifier be trained?, and how should one use a given attribute classifier for accurate bias estimation? In this work we study this question from both theoretical and empirical perspectives.We first experimentally demonstrate that the test accuracy of the attribute classifier is not always correlated with its effectiveness in bias estimation for a downstream model. In order to further investigate this phenomenon, we analyze an idealized theoretical model and characterize the structure of the optimal classifier. Our analysis has surprising and counter-intuitive implications where in certain regimes one might want to distribute the error of the attribute classifier as unevenly as possible among the different subgroups. Based on our analysis we develop heuristics for both training and using attribute classifiers for bias estimation in the data scarce regime. We empirically demonstrate the effectiveness of our approach on real and simulated data.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {206–214},
numpages = {9},
keywords = {Model Auditing, Active Learning, Bias Estimation},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
FACCT,2021,Data Leverage: A Framework for Empowering the Public in its Relationship with Technology Companies,https://dl.acm.org/doi/10.1145/3442188.3445885,"['Nicholas Vincent', 'Hanlin Li', 'Nicole Tilly', 'Stevie Chancellor', 'Brent Hecht']",28,"Many powerful computing technologies rely on implicit and explicit data contributions from the public. This dependency suggests a potential source of leverage for the public in its relationship with technology companies: by reducing, stopping, redirecting, or otherwise manipulating data contributions, the public can reduce the effectiveness of many lucrative technologies. In this paper, we synthesize emerging research that seeks to better understand and help people action this data leverage. Drawing on prior work in areas including machine learning, human-computer interaction, and fairness and accountability in computing, we present a framework for understanding data leverage that highlights new opportunities to change technology company behavior related to privacy, economic inequality, content moderation and other areas of societal concern. Our framework also points towards ways that policymakers can bolster data leverage as a means of changing the balance of power between the public and tech companies.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3442188.3445885,
author = {Vincent, Nicholas and Li, Hanlin and Tilly, Nicole and Chancellor, Stevie and Hecht, Brent},
title = {Data Leverage: A Framework for Empowering the Public in Its Relationship with Technology Companies},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445885},
doi = {10.1145/3442188.3445885},
abstract = {Many powerful computing technologies rely on implicit and explicit data contributions from the public. This dependency suggests a potential source of leverage for the public in its relationship with technology companies: by reducing, stopping, redirecting, or otherwise manipulating data contributions, the public can reduce the effectiveness of many lucrative technologies. In this paper, we synthesize emerging research that seeks to better understand and help people action this data leverage. Drawing on prior work in areas including machine learning, human-computer interaction, and fairness and accountability in computing, we present a framework for understanding data leverage that highlights new opportunities to change technology company behavior related to privacy, economic inequality, content moderation and other areas of societal concern. Our framework also points towards ways that policymakers can bolster data leverage as a means of changing the balance of power between the public and tech companies.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {215–227},
numpages = {13},
keywords = {data leverage, data poisoning, conscious data contribution, data strikes},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
FACCT+A71,2021,The Use and Misuse of Counterfactuals in Ethical Machine Learning,https://dl.acm.org/doi/10.1145/3442188.3445886,"['Atoosa Kasirzadeh', 'Andrew Smart']",46,"The use of counterfactuals for considerations of algorithmic fairness and explainability is gaining prominence within the machine learning community and industry. This paper argues for more caution with the use of counterfactuals when the facts to be considered are social categories such as race or gender. We review a broad body of papers from philosophy and social sciences on social ontology and the semantics of counterfactuals, and we conclude that the counterfactual approach in machine learning fairness and social explainability can require an incoherent theory of what social categories are. Our findings suggest that most often the social categories may not admit counterfactual manipulation, and hence may not appropriately satisfy the demands for evaluating the truth or falsity of counterfactuals. This is important because the widespread use of counterfactuals in machine learning can lead to misleading results when applied in high-stakes domains. Accordingly, we argue that even though counterfactuals play an essential part in some causal inferences, their use for questions of algorithmic fairness and social explanations can create more problems than they resolve. Our positive result is a set of tenets about using counterfactuals for fairness and explanations in machine learning.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3442188.3445886,
author = {Kasirzadeh, Atoosa and Smart, Andrew},
title = {The Use and Misuse of Counterfactuals in Ethical Machine Learning},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445886},
doi = {10.1145/3442188.3445886},
abstract = {The use of counterfactuals for considerations of algorithmic fairness and explainability is gaining prominence within the machine learning community and industry. This paper argues for more caution with the use of counterfactuals when the facts to be considered are social categories such as race or gender. We review a broad body of papers from philosophy and social sciences on social ontology and the semantics of counterfactuals, and we conclude that the counterfactual approach in machine learning fairness and social explainability can require an incoherent theory of what social categories are. Our findings suggest that most often the social categories may not admit counterfactual manipulation, and hence may not appropriately satisfy the demands for evaluating the truth or falsity of counterfactuals. This is important because the widespread use of counterfactuals in machine learning can lead to misleading results when applied in high-stakes domains. Accordingly, we argue that even though counterfactuals play an essential part in some causal inferences, their use for questions of algorithmic fairness and social explanations can create more problems than they resolve. Our positive result is a set of tenets about using counterfactuals for fairness and explanations in machine learning.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {228–236},
numpages = {9},
keywords = {Fairness, Counterfactuals, Social kind, Ethics of AI, Algorithmic Fairness, Philosophy of AI, Social category, Philosophy, Ethical AI, Social ontology, Explanation, Machine learning, Explainable AI},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
FACCT,2021,Mitigating Bias in Set Selection with Noisy Protected Attributes,https://dl.acm.org/doi/10.1145/3442188.3445887,"['Anay Mehrotra', 'L. Elisa Celis']",30,"Subset selection algorithms are ubiquitous in AI-driven applications, including, online recruiting portals and image search engines, so it is imperative that these tools are not discriminatory on the basis of protected attributes such as gender or race. Currently, fair subset selection algorithms assume that the protected attributes are known as part of the dataset. However, protected attributes may be noisy due to errors during data collection or if they are imputed (as is often the case in real-world settings). While a wide body of work addresses the effect of noise on the performance of machine learning algorithms, its effect on fairness remains largely unexamined. We find that in the presence of noisy protected attributes, in attempting to increase fairness without considering noise, one can, in fact, decrease the fairness of the result! Towards addressing this, we consider an existing noise model in which there is probabilistic information about the protected attributes (e.g., [19, 32, 44, 56]), and ask is fair selection possible under noisy conditions? We formulate a ""denoised"" selection problem which functions for a large class of fairness metrics; given the desired fairness goal, the solution to the denoised problem violates the goal by at most a small multiplicative amount with high probability. Although this denoised problem turns out to be NP-hard, we give a linear-programming based approximation algorithm for it. We evaluate this approach on both synthetic and real-world datasets. Our empirical results show that this approach can produce subsets which significantly improve the fairness metrics despite the presence of noisy protected attributes, and, compared to prior noise-oblivious approaches, has better Pareto-tradeoffs between utility and fairness.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3442188.3445887,
author = {Mehrotra, Anay and Celis, L. Elisa},
title = {Mitigating Bias in Set Selection with Noisy Protected Attributes},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445887},
doi = {10.1145/3442188.3445887},
abstract = {Subset selection algorithms are ubiquitous in AI-driven applications, including, online recruiting portals and image search engines, so it is imperative that these tools are not discriminatory on the basis of protected attributes such as gender or race. Currently, fair subset selection algorithms assume that the protected attributes are known as part of the dataset. However, protected attributes may be noisy due to errors during data collection or if they are imputed (as is often the case in real-world settings). While a wide body of work addresses the effect of noise on the performance of machine learning algorithms, its effect on fairness remains largely unexamined. We find that in the presence of noisy protected attributes, in attempting to increase fairness without considering noise, one can, in fact, decrease the fairness of the result!Towards addressing this, we consider an existing noise model in which there is probabilistic information about the protected attributes (e.g., [19, 32, 44, 56]), and ask is fair selection possible under noisy conditions? We formulate a ""denoised"" selection problem which functions for a large class of fairness metrics; given the desired fairness goal, the solution to the denoised problem violates the goal by at most a small multiplicative amount with high probability. Although this denoised problem turns out to be NP-hard, we give a linear-programming based approximation algorithm for it. We evaluate this approach on both synthetic and real-world datasets. Our empirical results show that this approach can produce subsets which significantly improve the fairness metrics despite the presence of noisy protected attributes, and, compared to prior noise-oblivious approaches, has better Pareto-tradeoffs between utility and fairness.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {237–248},
numpages = {12},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
FACCT,2021,"What We Can't Measure, We Can't Understand: Challenges to Demographic Data Procurement in the Pursuit of Fairness",https://dl.acm.org/doi/10.1145/3442188.3445888,"['McKane Andrus', 'Elena Spitzer', 'Jeffrey Brown', 'Alice Xiang']",63,"As calls for fair and unbiased algorithmic systems increase, so too does the number of individuals working on algorithmic fairness in industry. However, these practitioners often do not have access to the demographic data they feel they need to detect bias in practice. Even with the growing variety of toolkits and strategies for working towards algorithmic fairness, they almost invariably require access to demographic attributes or proxies. We investigated this dilemma through semi-structured interviews with 38 practitioners and professionals either working in or adjacent to algorithmic fairness. Participants painted a complex picture of what demographic data availability and use look like on the ground, ranging from not having access to personal data of any kind to being legally required to collect and use demographic data for discrimination assessments. In many domains, demographic data collection raises a host of difficult questions, including how to balance privacy and fairness, how to define relevant social categories, how to ensure meaningful consent, and whether it is appropriate for private companies to infer someone's demographics. Our research suggests challenges that must be considered by businesses, regulators, researchers, and community groups in order to enable practitioners to address algorithmic bias in practice. Critically, we do not propose that the overall goal of future work should be to simply lower the barriers to collecting demographic data. Rather, our study surfaces a swath of normative questions about how, when, and whether this data should be procured, and, in cases where it is not, what should still be done to mitigate bias.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3442188.3445888,
author = {Andrus, McKane and Spitzer, Elena and Brown, Jeffrey and Xiang, Alice},
title = {What We Can't Measure, We Can't Understand: Challenges to Demographic Data Procurement in the Pursuit of Fairness},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445888},
doi = {10.1145/3442188.3445888},
abstract = {As calls for fair and unbiased algorithmic systems increase, so too does the number of individuals working on algorithmic fairness in industry. However, these practitioners often do not have access to the demographic data they feel they need to detect bias in practice. Even with the growing variety of toolkits and strategies for working towards algorithmic fairness, they almost invariably require access to demographic attributes or proxies. We investigated this dilemma through semi-structured interviews with 38 practitioners and professionals either working in or adjacent to algorithmic fairness. Participants painted a complex picture of what demographic data availability and use look like on the ground, ranging from not having access to personal data of any kind to being legally required to collect and use demographic data for discrimination assessments. In many domains, demographic data collection raises a host of difficult questions, including how to balance privacy and fairness, how to define relevant social categories, how to ensure meaningful consent, and whether it is appropriate for private companies to infer someone's demographics. Our research suggests challenges that must be considered by businesses, regulators, researchers, and community groups in order to enable practitioners to address algorithmic bias in practice. Critically, we do not propose that the overall goal of future work should be to simply lower the barriers to collecting demographic data. Rather, our study surfaces a swath of normative questions about how, when, and whether this data should be procured, and, in cases where it is not, what should still be done to mitigate bias.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {249–260},
numpages = {12},
keywords = {fairness, demographic data, special category data, anti-discrimination, data privacy, sensitive data},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
FACCT,2021,Algorithmic Fairness in Predicting Opioid Use Disorder using Machine Learning,https://dl.acm.org/doi/10.1145/3442188.3445891,['Angela E. Kilby'],8,"There has been recent interest by payers, health care systems, and researchers in the development of machine learning and artificial intelligence models that predict an individual's probability of developing opioid use disorder. The scores generated by these algorithms can be used by physicians to tailor the prescribing of opioids for the treatment of pain, reducing or foregoing prescribing to individuals deemed to be at high risk, or increasing prescribing for patients deemed to be at low risk. This paper constructs a machine learning algorithm to predict opioid use disorder risk using commercially available claims data similar to those utilized in the development of proprietary opioid use disorder prediction algorithms. We study risk scores generated by the machine learning model in a setting with quasi-experimental variation in the likelihood that doctors prescribe opioids, generated by changes in the legal structure for monitoring physician prescribing. We find that machine-predicted risk scores do not appear to correlate at all with the individual-specific heterogeneous treatment effect of receiving opioids. The paper identifies a new source of algorithmic unfairness in machine learning applications for health care and precision medicine, arising from the researcher's choice of objective function. While precision medicine should guide physician treatment decisions based on the heterogeneous causal impact of a course of treatment for an individual, allocating treatments to individuals receiving the most benefit and recommending caution for those most likely to experience harmful side effects, ML models in health care are often trained on proxies like individual baseline risk, and are not necessarily informative in deciding who would most benefit, or be harmed, by a course of treatment.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3442188.3445891,
author = {Kilby, Angela E.},
title = {Algorithmic Fairness in Predicting Opioid Use Disorder Using Machine Learning},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445891},
doi = {10.1145/3442188.3445891},
abstract = {There has been recent interest by payers, health care systems, and researchers in the development of machine learning and artificial intelligence models that predict an individual's probability of developing opioid use disorder. The scores generated by these algorithms can be used by physicians to tailor the prescribing of opioids for the treatment of pain, reducing or foregoing prescribing to individuals deemed to be at high risk, or increasing prescribing for patients deemed to be at low risk. This paper constructs a machine learning algorithm to predict opioid use disorder risk using commercially available claims data similar to those utilized in the development of proprietary opioid use disorder prediction algorithms. We study risk scores generated by the machine learning model in a setting with quasi-experimental variation in the likelihood that doctors prescribe opioids, generated by changes in the legal structure for monitoring physician prescribing. We find that machine-predicted risk scores do not appear to correlate at all with the individual-specific heterogeneous treatment effect of receiving opioids. The paper identifies a new source of algorithmic unfairness in machine learning applications for health care and precision medicine, arising from the researcher's choice of objective function. While precision medicine should guide physician treatment decisions based on the heterogeneous causal impact of a course of treatment for an individual, allocating treatments to individuals receiving the most benefit and recommending caution for those most likely to experience harmful side effects, ML models in health care are often trained on proxies like individual baseline risk, and are not necessarily informative in deciding who would most benefit, or be harmed, by a course of treatment.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {272},
numpages = {1},
keywords = {evaluations, algorithm development, critical data/algorithm studies, fairness, auditing, ethics, disability studies, causality, algorithmic impacts on social phenomena, social and organizational processes},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
FACCT,2021,Avoiding Disparity Amplification under Different Worldviews,https://dl.acm.org/doi/10.1145/3442188.3445892,"['Samuel Yeom', 'Michael Carl Tschantz']",11,"We mathematically compare four competing definitions of group-level nondiscrimination: demographic parity, equalized odds, predictive parity, and calibration. Using the theoretical framework of Friedler et al., we study the properties of each definition under various worldviews, which are assumptions about how, if at all, the observed data is biased. We argue that different worldviews call for different definitions of fairness, and we specify the worldviews that, when combined with the desire to avoid a criterion for discrimination that we call disparity amplification, motivate demographic parity and equalized odds. We also argue that predictive parity and calibration are insufficient for avoiding disparity amplification because predictive parity allows an arbitrarily large inter-group disparity and calibration is not robust to post-processing. Finally, we define a worldview that is more realistic than the previously considered ones, and we introduce a new notion of fairness that corresponds to this worldview.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3442188.3445892,
author = {Yeom, Samuel and Tschantz, Michael Carl},
title = {Avoiding Disparity Amplification under Different Worldviews},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445892},
doi = {10.1145/3442188.3445892},
abstract = {We mathematically compare four competing definitions of group-level nondiscrimination: demographic parity, equalized odds, predictive parity, and calibration. Using the theoretical framework of Friedler et al., we study the properties of each definition under various worldviews, which are assumptions about how, if at all, the observed data is biased. We argue that different worldviews call for different definitions of fairness, and we specify the worldviews that, when combined with the desire to avoid a criterion for discrimination that we call disparity amplification, motivate demographic parity and equalized odds. We also argue that predictive parity and calibration are insufficient for avoiding disparity amplification because predictive parity allows an arbitrarily large inter-group disparity and calibration is not robust to post-processing. Finally, we define a worldview that is more realistic than the previously considered ones, and we introduce a new notion of fairness that corresponds to this worldview.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {273–283},
numpages = {11},
keywords = {equalized odds, worldview, predictive parity, disparity amplification, demographic parity, fairness, calibration},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
FACCT,2021,Leave-one-out Unfairness,https://dl.acm.org/doi/10.1145/3442188.3445894,"['Emily Black', 'Matt Fredrikson']",17,"We introduce leave-one-out unfairness, which characterizes how likely a model's prediction for an individual will change due to the inclusion or removal of a single other person in the model's training data. Leave-one-out unfairness appeals to the idea that fair decisions are not arbitrary: they should not be based on the chance event of any one person's inclusion in the training data. Leave-one-out unfairness is closely related to algorithmic stability, but it focuses on the consistency of an individual point's prediction outcome over unit changes to the training data, rather than the error of the model in aggregate. Beyond formalizing leave-one-out unfairness, we characterize the extent to which deep models behave leave-one-out unfairly on real data, including in cases where the generalization error is small. Further, we demonstrate that adversarial training and randomized smoothing techniques have opposite effects on leave-one-out fairness, which sheds light on the relationships between robustness, memorization, individual fairness, and leave-one-out fairness in deep models. Finally, we discuss salient practical applications that may be negatively affected by leave-one-out unfairness.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3442188.3445894,
author = {Black, Emily and Fredrikson, Matt},
title = {Leave-One-out Unfairness},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445894},
doi = {10.1145/3442188.3445894},
abstract = {We introduce leave-one-out unfairness, which characterizes how likely a model's prediction for an individual will change due to the inclusion or removal of a single other person in the model's training data. Leave-one-out unfairness appeals to the idea that fair decisions are not arbitrary: they should not be based on the chance event of any one person's inclusion in the training data. Leave-one-out unfairness is closely related to algorithmic stability, but it focuses on the consistency of an individual point's prediction outcome over unit changes to the training data, rather than the error of the model in aggregate. Beyond formalizing leave-one-out unfairness, we characterize the extent to which deep models behave leave-one-out unfairly on real data, including in cases where the generalization error is small. Further, we demonstrate that adversarial training and randomized smoothing techniques have opposite effects on leave-one-out fairness, which sheds light on the relationships between robustness, memorization, individual fairness, and leave-one-out fairness in deep models. Finally, we discuss salient practical applications that may be negatively affected by leave-one-out unfairness.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {285–295},
numpages = {11},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
FACCT,2021,"Fairness, Welfare, and Equity in Personalized Pricing",https://dl.acm.org/doi/10.1145/3442188.3445895,"['Nathan Kallus', 'Angela Zhou']",19,"We study the interplay of fairness, welfare, and equity considerations in personalized pricing based on customer features. Sellers are increasingly able to conduct price personalization based on predictive modeling of demand conditional on covariates: setting customized interest rates, targeted discounts of consumer goods, and personalized subsidies of scarce resources with positive externalities like vaccines and bed nets. These different application areas may lead to different concerns around fairness, welfare, and equity on different objectives: price burdens on consumers, price envy, firm revenue, access to a good, equal access, and distributional consequences when the good in question further impacts downstream outcomes of interest. We conduct a comprehensive literature review in order to disentangle these different normative considerations and propose a taxonomy of different objectives with mathematical definitions. We focus on observational metrics that do not assume access to an underlying valuation distribution which is either unobserved due to binary feedback or ill-defined due to overriding behavioral concerns regarding interpreting revealed preferences. In the setting of personalized pricing for the provision of goods with positive benefits, we discuss how price optimization may provide unambiguous benefit by achieving a ""triple bottom line"": personalized pricing enables expanding access, which in turn may lead to gains in welfare due to heterogeneous utility, and improve revenue or budget utilization. We empirically demonstrate the potential benefits of personalized pricing in two settings: pricing subsidies for an elective vaccine, and the effects of personalized interest rates on downstream outcomes in microcredit.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3442188.3445895,
author = {Kallus, Nathan and Zhou, Angela},
title = {Fairness, Welfare, and Equity in Personalized Pricing},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445895},
doi = {10.1145/3442188.3445895},
abstract = {We study the interplay of fairness, welfare, and equity considerations in personalized pricing based on customer features. Sellers are increasingly able to conduct price personalization based on predictive modeling of demand conditional on covariates: setting customized interest rates, targeted discounts of consumer goods, and personalized subsidies of scarce resources with positive externalities like vaccines and bed nets. These different application areas may lead to different concerns around fairness, welfare, and equity on different objectives: price burdens on consumers, price envy, firm revenue, access to a good, equal access, and distributional consequences when the good in question further impacts downstream outcomes of interest. We conduct a comprehensive literature review in order to disentangle these different normative considerations and propose a taxonomy of different objectives with mathematical definitions. We focus on observational metrics that do not assume access to an underlying valuation distribution which is either unobserved due to binary feedback or ill-defined due to overriding behavioral concerns regarding interpreting revealed preferences. In the setting of personalized pricing for the provision of goods with positive benefits, we discuss how price optimization may provide unambiguous benefit by achieving a ""triple bottom line"": personalized pricing enables expanding access, which in turn may lead to gains in welfare due to heterogeneous utility, and improve revenue or budget utilization. We empirically demonstrate the potential benefits of personalized pricing in two settings: pricing subsidies for an elective vaccine, and the effects of personalized interest rates on downstream outcomes in microcredit.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {296–314},
numpages = {19},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
FACCT,2021,Re-imagining Algorithmic Fairness in India and Beyond,https://dl.acm.org/doi/10.1145/3442188.3445896,"['Nithya Sambasivan', 'Erin Arnesen', 'Ben Hutchinson', 'Tulsee Doshi', 'Vinodkumar Prabhakaran']",71,"Conventional algorithmic fairness is West-centric, as seen in its subgroups, values, and methods. In this paper, we de-center algorithmic fairness and analyse AI power in India. Based on 36 qualitative interviews and a discourse analysis of algorithmic deployments in India, we find that several assumptions of algorithmic fairness are challenged. We find that in India, data is not always reliable due to socio-economic factors, ML makers appear to follow double standards, and AI evokes unquestioning aspiration. We contend that localising model fairness alone can be window dressing in India, where the distance between models and oppressed communities is large. Instead, we re-imagine algorithmic fairness in India and provide a roadmap to re-contextualise data and models, empower oppressed communities, and enable Fair-ML ecosystems.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3442188.3445896,
author = {Sambasivan, Nithya and Arnesen, Erin and Hutchinson, Ben and Doshi, Tulsee and Prabhakaran, Vinodkumar},
title = {Re-Imagining Algorithmic Fairness in India and Beyond},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445896},
doi = {10.1145/3442188.3445896},
abstract = {Conventional algorithmic fairness is West-centric, as seen in its subgroups, values, and methods. In this paper, we de-center algorithmic fairness and analyse AI power in India. Based on 36 qualitative interviews and a discourse analysis of algorithmic deployments in India, we find that several assumptions of algorithmic fairness are challenged. We find that in India, data is not always reliable due to socio-economic factors, ML makers appear to follow double standards, and AI evokes unquestioning aspiration. We contend that localising model fairness alone can be window dressing in India, where the distance between models and oppressed communities is large. Instead, we re-imagine algorithmic fairness in India and provide a roadmap to re-contextualise data and models, empower oppressed communities, and enable Fair-ML ecosystems.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {315–328},
numpages = {14},
keywords = {anti-caste politics, caste, gender, class, religion, critical algorithmic studies, India, decoloniality, feminism, ability, algorithmic fairness},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
FACCT,2021,A Semiotics-based epistemic tool to reason about ethical issues in digital technology design and development,https://dl.acm.org/doi/10.1145/3442188.3445900,"['Simone Diniz Junqueira Barbosa', 'Gabriel Diniz Junqueira Barbosa', 'Clarisse Sieckenius de Souza', 'Carla Faria Leit√£o']",9,"One of the important challenges regarding the development of morally responsible and ethically qualified digital technologies is how to support designers and developers in producing those technologies, especially when conceptualizing their vision of what the technology will be, how it will benefit users, and avoid doing harm. However, traditional software design and development life cycles do not explicitly support the reflection upon either ethical or moral issues. In this paper we look at how a number of ethical issues may be dealt with during digital technology design and development, to prevent damage and improve technological fairness, accountability, and transparency. Starting from mature work on semiotic theory and methods in human-computer interaction, we propose to extend the core artifact used in semiotic engineering of human-centered technology design, so as to directly address moral responsibility and ethical issues. The resulting extension is an epistemic tool, that is, an instrument to create and elaborate on this specific kind of knowledge. The paper describes the tool, illustrates how it is to be used, and discusses its promises and limitations against the background of related work. It also includes proposed empirical studies, accompanied by briefly described methodological challenges and considerations that deserve our attention.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3442188.3445900,
author = {Barbosa, Simone Diniz Junqueira and Barbosa, Gabriel Diniz Junqueira and Souza, Clarisse Sieckenius de and Leit\~{a}o, Carla Faria},
title = {A Semiotics-Based Epistemic Tool to Reason about Ethical Issues in Digital Technology Design and Development},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445900},
doi = {10.1145/3442188.3445900},
abstract = {One of the important challenges regarding the development of morally responsible and ethically qualified digital technologies is how to support designers and developers in producing those technologies, especially when conceptualizing their vision of what the technology will be, how it will benefit users, and avoid doing harm. However, traditional software design and development life cycles do not explicitly support the reflection upon either ethical or moral issues. In this paper we look at how a number of ethical issues may be dealt with during digital technology design and development, to prevent damage and improve technological fairness, accountability, and transparency. Starting from mature work on semiotic theory and methods in human-computer interaction, we propose to extend the core artifact used in semiotic engineering of human-centered technology design, so as to directly address moral responsibility and ethical issues. The resulting extension is an epistemic tool, that is, an instrument to create and elaborate on this specific kind of knowledge. The paper describes the tool, illustrates how it is to be used, and discusses its promises and limitations against the background of related work. It also includes proposed empirical studies, accompanied by briefly described methodological challenges and considerations that deserve our attention.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {363–374},
numpages = {12},
keywords = {semiotic engineering, epistemic tool, ethics},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
FACCT,2021,Measurement and Fairness,https://dl.acm.org/doi/10.1145/3442188.3445901,"['Abigail Z. Jacobs', 'Hanna Wallach']",190,"We propose measurement modeling from the quantitative social sciences as a framework for understanding fairness in computational systems. Computational systems often involve unobservable theoretical constructs, such as socioeconomic status, teacher effectiveness, and risk of recidivism. Such constructs cannot be measured directly and must instead be inferred from measurements of observable properties (and other unobservable theoretical constructs) thought to be related to them---i.e., operationalized via a measurement model. This process, which necessarily involves making assumptions, introduces the potential for mismatches between the theoretical understanding of the construct purported to be measured and its operationalization. We argue that many of the harms discussed in the literature on fairness in computational systems are direct results of such mismatches. We show how some of these harms could have been anticipated and, in some cases, mitigated if viewed through the lens of measurement modeling. To do this, we contribute fairness-oriented conceptualizations of construct reliability and construct validity that unite traditions from political science, education, and psychology and provide a set of tools for making explicit and testing assumptions about constructs and their operationalizations. We then turn to fairness itself, an essentially contested construct that has different theoretical understandings in different contexts. We argue that this contestedness underlies recent debates about fairness definitions: although these debates appear to be about different operationalizations, they are, in fact, debates about different theoretical understandings of fairness. We show how measurement modeling can provide a framework for getting to the core of these debates.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3442188.3445901,
author = {Jacobs, Abigail Z. and Wallach, Hanna},
title = {Measurement and Fairness},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445901},
doi = {10.1145/3442188.3445901},
abstract = {We propose measurement modeling from the quantitative social sciences as a framework for understanding fairness in computational systems. Computational systems often involve unobservable theoretical constructs, such as socioeconomic status, teacher effectiveness, and risk of recidivism. Such constructs cannot be measured directly and must instead be inferred from measurements of observable properties (and other unobservable theoretical constructs) thought to be related to them---i.e., operationalized via a measurement model. This process, which necessarily involves making assumptions, introduces the potential for mismatches between the theoretical understanding of the construct purported to be measured and its operationalization. We argue that many of the harms discussed in the literature on fairness in computational systems are direct results of such mismatches. We show how some of these harms could have been anticipated and, in some cases, mitigated if viewed through the lens of measurement modeling. To do this, we contribute fairness-oriented conceptualizations of construct reliability and construct validity that unite traditions from political science, education, and psychology and provide a set of tools for making explicit and testing assumptions about constructs and their operationalizations. We then turn to fairness itself, an essentially contested construct that has different theoretical understandings in different contexts. We argue that this contestedness underlies recent debates about fairness definitions: although these debates appear to be about different operationalizations, they are, in fact, debates about different theoretical understandings of fairness. We show how measurement modeling can provide a framework for getting to the core of these debates.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {375–385},
numpages = {11},
keywords = {fairness, construct validity, measurement, construct reliability},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
FACCT,2021,Fairness in Risk Assessment Instruments: Post-Processing to Achieve Counterfactual Equalized Odds,https://dl.acm.org/doi/10.1145/3442188.3445902,"['Alan Mishler', 'Edward H. Kennedy', 'Alexandra Chouldechova']",30,"In domains such as criminal justice, medicine, and social welfare, decision makers increasingly have access to algorithmic Risk Assessment Instruments (RAIs). RAIs estimate the risk of an adverse outcome such as recidivism or child neglect, potentially informing high-stakes decisions such as whether to release a defendant on bail or initiate a child welfare investigation. It is important to ensure that RAIs are fair, so that the benefits and harms of such decisions are equitably distributed. The most widely used algorithmic fairness criteria are formulated with respect to observable outcomes, such as whether a person actually recidivates, but these criteria are misleading when applied to RAIs. Since RAIs are intended to inform interventions that can reduce risk, the prediction itself affects the downstream outcome. Recent work has argued that fairness criteria for RAIs should instead utilize potential outcomes, i.e. the outcomes that would occur in the absence of an appropriate intervention [11]. However, no methods currently exist to satisfy such fairness criteria. In this paper, we target one such criterion, counterfactual equalized odds. We develop a post-processed predictor that is estimated via doubly robust estimators, extending and adapting previous postprocessing approaches [16] to the counterfactual setting. We also provide doubly robust estimators of the risk and fairness properties of arbitrary fixed post-processed predictors. Our predictor converges to an optimal fair predictor at fast rates. We illustrate properties of our method and show that it performs well on both simulated and real data.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3442188.3445902,
author = {Mishler, Alan and Kennedy, Edward H. and Chouldechova, Alexandra},
title = {Fairness in Risk Assessment Instruments: Post-Processing to Achieve Counterfactual Equalized Odds},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445902},
doi = {10.1145/3442188.3445902},
abstract = {In domains such as criminal justice, medicine, and social welfare, decision makers increasingly have access to algorithmic Risk Assessment Instruments (RAIs). RAIs estimate the risk of an adverse outcome such as recidivism or child neglect, potentially informing high-stakes decisions such as whether to release a defendant on bail or initiate a child welfare investigation. It is important to ensure that RAIs are fair, so that the benefits and harms of such decisions are equitably distributed.The most widely used algorithmic fairness criteria are formulated with respect to observable outcomes, such as whether a person actually recidivates, but these criteria are misleading when applied to RAIs. Since RAIs are intended to inform interventions that can reduce risk, the prediction itself affects the downstream outcome. Recent work has argued that fairness criteria for RAIs should instead utilize potential outcomes, i.e. the outcomes that would occur in the absence of an appropriate intervention [11]. However, no methods currently exist to satisfy such fairness criteria.In this paper, we target one such criterion, counterfactual equalized odds. We develop a post-processed predictor that is estimated via doubly robust estimators, extending and adapting previous postprocessing approaches [16] to the counterfactual setting. We also provide doubly robust estimators of the risk and fairness properties of arbitrary fixed post-processed predictors. Our predictor converges to an optimal fair predictor at fast rates. We illustrate properties of our method and show that it performs well on both simulated and real data.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {386–400},
numpages = {15},
keywords = {counterfactual, risk assessment, fairness, post-processing},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
FACCT,2021,Socially Fair k-Means Clustering,https://dl.acm.org/doi/10.1145/3442188.3445906,"['Mehrdad Ghadiri', 'Samira Samadi', 'Santosh Vempala']",51,"We show that the popular k-means clustering algorithm (Lloyd's heuristic), used for a variety of scientific data, can result in outcomes that are unfavorable to subgroups of data (e.g., demographic groups). Such biased clusterings can have deleterious implications for human-centric applications such as resource allocation. We present a fair k-means objective and algorithm to choose cluster centers that provide equitable costs for different groups. The algorithm, Fair-Lloyd, is a modification of Lloyd's heuristic for k-means, inheriting its simplicity, efficiency, and stability. In comparison with standard Lloyd's, we find that on benchmark datasets, Fair-Lloyd exhibits unbiased performance by ensuring that all groups have equal costs in the output k-clustering, while incurring a negligible increase in running time, thus making it a viable fair option wherever k-means is currently used.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3442188.3445906,
author = {Ghadiri, Mehrdad and Samadi, Samira and Vempala, Santosh},
title = {Socially Fair K-Means Clustering},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445906},
doi = {10.1145/3442188.3445906},
abstract = {We show that the popular k-means clustering algorithm (Lloyd's heuristic), used for a variety of scientific data, can result in outcomes that are unfavorable to subgroups of data (e.g., demographic groups). Such biased clusterings can have deleterious implications for human-centric applications such as resource allocation. We present a fair k-means objective and algorithm to choose cluster centers that provide equitable costs for different groups. The algorithm, Fair-Lloyd, is a modification of Lloyd's heuristic for k-means, inheriting its simplicity, efficiency, and stability. In comparison with standard Lloyd's, we find that on benchmark datasets, Fair-Lloyd exhibits unbiased performance by ensuring that all groups have equal costs in the output k-clustering, while incurring a negligible increase in running time, thus making it a viable fair option wherever k-means is currently used.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {438–448},
numpages = {11},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
FACCT,2021,Fairness Through Robustness: Investigating Robustness Disparity in Deep Learning,https://dl.acm.org/doi/10.1145/3442188.3445910,"['Vedant Nanda', 'Samuel Dooley', 'Sahil Singla', 'Soheil Feizi', 'John P. Dickerson']",45,"Deep neural networks (DNNs) are increasingly used in real-world applications (e.g. facial recognition). This has resulted in concerns about the fairness of decisions made by these models. Various notions and measures of fairness have been proposed to ensure that a decision-making system does not disproportionately harm (or benefit) particular subgroups of the population. In this paper, we argue that traditional notions of fairness that are only based on models' outputs are not sufficient when the model is vulnerable to adversarial attacks. We argue that in some cases, it may be easier for an attacker to target a particular subgroup, resulting in a form of robustness bias. We show that measuring robustness bias is a challenging task for DNNs and propose two methods to measure this form of bias. We then conduct an empirical study on state-of-the-art neural networks on commonly used real-world datasets such as CIFAR-10, CIFAR-100, Adience, and UTKFace and show that in almost all cases there are subgroups (in some cases based on sensitive attributes like race, gender, etc) which are less robust and are thus at a disadvantage. We argue that this kind of bias arises due to both the data distribution and the highly complex nature of the learned decision boundary in the case of DNNs, thus making mitigation of such biases a non-trivial task. Our results show that robustness bias is an important criterion to consider while auditing real-world systems that rely on DNNs for decision making. Code to reproduce all our results can be found here: https://github.com/nvedant07/Fairness-Through-Robustness",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3442188.3445910,
author = {Nanda, Vedant and Dooley, Samuel and Singla, Sahil and Feizi, Soheil and Dickerson, John P.},
title = {Fairness Through Robustness: Investigating Robustness Disparity in Deep Learning},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445910},
doi = {10.1145/3442188.3445910},
abstract = {Deep neural networks (DNNs) are increasingly used in real-world applications (e.g. facial recognition). This has resulted in concerns about the fairness of decisions made by these models. Various notions and measures of fairness have been proposed to ensure that a decision-making system does not disproportionately harm (or benefit) particular subgroups of the population. In this paper, we argue that traditional notions of fairness that are only based on models' outputs are not sufficient when the model is vulnerable to adversarial attacks. We argue that in some cases, it may be easier for an attacker to target a particular subgroup, resulting in a form of robustness bias. We show that measuring robustness bias is a challenging task for DNNs and propose two methods to measure this form of bias. We then conduct an empirical study on state-of-the-art neural networks on commonly used real-world datasets such as CIFAR-10, CIFAR-100, Adience, and UTKFace and show that in almost all cases there are subgroups (in some cases based on sensitive attributes like race, gender, etc) which are less robust and are thus at a disadvantage. We argue that this kind of bias arises due to both the data distribution and the highly complex nature of the learned decision boundary in the case of DNNs, thus making mitigation of such biases a non-trivial task. Our results show that robustness bias is an important criterion to consider while auditing real-world systems that rely on DNNs for decision making. Code to reproduce all our results can be found here: https://github.com/nvedant07/Fairness-Through-Robustness},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {466–477},
numpages = {12},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
FACCT,2021,Bridging Machine Learning and Mechanism Design towards Algorithmic Fairness,https://dl.acm.org/doi/10.1145/3442188.3445912,"['Jessie Finocchiaro', 'Roland Maio', 'Faidra Monachou', 'Gourab K Patro', 'Manish Raghavan', 'Ana-Andreea Stoica', 'Stratis Tsirtsis']",34,"Decision-making systems increasingly orchestrate our world: how to intervene on the algorithmic components to build fair and equitable systems is therefore a question of utmost importance; one that is substantially complicated by the context-dependent nature of fairness and discrimination. Modern decision-making systems that involve allocating resources or information to people (e.g., school choice, advertising) incorporate machine-learned predictions in their pipelines, raising concerns about potential strategic behavior or constrained allocation, concerns usually tackled in the context of mechanism design. Although both machine learning and mechanism design have developed frameworks for addressing issues of fairness and equity, in some complex decision-making systems, neither framework is individually sufficient. In this paper, we develop the position that building fair decision-making systems requires overcoming these limitations which, we argue, are inherent to each field. Our ultimate objective is to build an encompassing framework that cohesively bridges the individual frameworks of mechanism design and machine learning. We begin to lay the ground work towards this goal by comparing the perspective each discipline takes on fair decision-making, teasing out the lessons each field has taught and can teach the other, and highlighting application domains that require a strong collaboration between these disciplines.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3442188.3445912,
author = {Finocchiaro, Jessie and Maio, Roland and Monachou, Faidra and Patro, Gourab K and Raghavan, Manish and Stoica, Ana-Andreea and Tsirtsis, Stratis},
title = {Bridging Machine Learning and Mechanism Design towards Algorithmic Fairness},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445912},
doi = {10.1145/3442188.3445912},
abstract = {Decision-making systems increasingly orchestrate our world: how to intervene on the algorithmic components to build fair and equitable systems is therefore a question of utmost importance; one that is substantially complicated by the context-dependent nature of fairness and discrimination. Modern decision-making systems that involve allocating resources or information to people (e.g., school choice, advertising) incorporate machine-learned predictions in their pipelines, raising concerns about potential strategic behavior or constrained allocation, concerns usually tackled in the context of mechanism design. Although both machine learning and mechanism design have developed frameworks for addressing issues of fairness and equity, in some complex decision-making systems, neither framework is individually sufficient. In this paper, we develop the position that building fair decision-making systems requires overcoming these limitations which, we argue, are inherent to each field. Our ultimate objective is to build an encompassing framework that cohesively bridges the individual frameworks of mechanism design and machine learning. We begin to lay the ground work towards this goal by comparing the perspective each discipline takes on fair decision-making, teasing out the lessons each field has taught and can teach the other, and highlighting application domains that require a strong collaboration between these disciplines.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {489–503},
numpages = {15},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
FACCT,2021,Fair Clustering via Equitable Group Representations,https://dl.acm.org/doi/10.1145/3442188.3445913,"['Mohsen Abbasi', 'Aditya Bhaskara', 'Suresh Venkatasubramanian']",51,"What does it mean for a clustering to be fair? One popular approach seeks to ensure that each cluster contains groups in (roughly) the same proportion in which they exist in the population. The normative principle at play is balance: any cluster might act as a representative of the data, and thus should reflect its diversity. But clustering also captures a different form of representativeness. A core principle in most clustering problems is that a cluster center should be representative of the cluster it represents, by being ""close"" to the points associated with it. This is so that we can effectively replace the points by their cluster centers without significant loss in fidelity, and indeed is a common ""use case"" for clustering. For such a clustering to be fair, the centers should ""represent"" different groups equally well. We call such a clustering a group-representative clustering. In this paper, we study the structure and computation of group-representative clusterings. We show that this notion naturally parallels the development of fairness notions in classification, with direct analogs of ideas like demographic parity and equal opportunity. We demonstrate how these notions are distinct from and cannot be captured by balance-based notions of fairness. We present approximation algorithms for group representative k-median clustering and couple this with an empirical evaluation on various real-world data sets. We also extend this idea to facility location, motivated by the current problem of assigning polling locations for voting",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3442188.3445913,
author = {Abbasi, Mohsen and Bhaskara, Aditya and Venkatasubramanian, Suresh},
title = {Fair Clustering via Equitable Group Representations},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445913},
doi = {10.1145/3442188.3445913},
abstract = {What does it mean for a clustering to be fair? One popular approach seeks to ensure that each cluster contains groups in (roughly) the same proportion in which they exist in the population. The normative principle at play is balance: any cluster might act as a representative of the data, and thus should reflect its diversity.But clustering also captures a different form of representativeness. A core principle in most clustering problems is that a cluster center should be representative of the cluster it represents, by being ""close"" to the points associated with it. This is so that we can effectively replace the points by their cluster centers without significant loss in fidelity, and indeed is a common ""use case"" for clustering. For such a clustering to be fair, the centers should ""represent"" different groups equally well. We call such a clustering a group-representative clustering.In this paper, we study the structure and computation of group-representative clusterings. We show that this notion naturally parallels the development of fairness notions in classification, with direct analogs of ideas like demographic parity and equal opportunity. We demonstrate how these notions are distinct from and cannot be captured by balance-based notions of fairness. We present approximation algorithms for group representative k-median clustering and couple this with an empirical evaluation on various real-world data sets. We also extend this idea to facility location, motivated by the current problem of assigning polling locations for voting},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {504–514},
numpages = {11},
keywords = {clustering, algorithmic fairness, representation},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
FACCT,2021,Fair Classification with Group-Dependent Label Noise,https://dl.acm.org/doi/10.1145/3442188.3445915,"['Jialu Wang', 'Yang Liu', 'Caleb Levy']",48,"This work examines how to train fair classifiers in settings where training labels are corrupted with random noise, and where the error rates of corruption depend both on the label class and on the membership function for a protected subgroup. Heterogeneous label noise models systematic biases towards particular groups when generating annotations. We begin by presenting analytical results which show that naively imposing parity constraints on demographic disparity measures, without accounting for heterogeneous and group-dependent error rates, can decrease both the accuracy and the fairness of the resulting classifier. Our experiments demonstrate these issues arise in practice as well. We address these problems by performing empirical risk minimization with carefully defined surrogate loss functions and surrogate constraints that help avoid the pitfalls introduced by heterogeneous label noise. We provide both theoretical and empirical justifications for the efficacy of our methods. We view our results as an important example of how imposing fairness on biased data sets without proper care can do at least as much harm as it does good.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3442188.3445915,
author = {Wang, Jialu and Liu, Yang and Levy, Caleb},
title = {Fair Classification with Group-Dependent Label Noise},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445915},
doi = {10.1145/3442188.3445915},
abstract = {This work examines how to train fair classifiers in settings where training labels are corrupted with random noise, and where the error rates of corruption depend both on the label class and on the membership function for a protected subgroup. Heterogeneous label noise models systematic biases towards particular groups when generating annotations. We begin by presenting analytical results which show that naively imposing parity constraints on demographic disparity measures, without accounting for heterogeneous and group-dependent error rates, can decrease both the accuracy and the fairness of the resulting classifier. Our experiments demonstrate these issues arise in practice as well. We address these problems by performing empirical risk minimization with carefully defined surrogate loss functions and surrogate constraints that help avoid the pitfalls introduced by heterogeneous label noise. We provide both theoretical and empirical justifications for the efficacy of our methods. We view our results as an important example of how imposing fairness on biased data sets without proper care can do at least as much harm as it does good.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {526–536},
numpages = {11},
keywords = {algorithmic fairness, learning with noisy and biased labels, machine learning},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
FACCT,2021,Censorship of Online Encyclopedias: Implications for NLP Models,https://dl.acm.org/doi/10.1145/3442188.3445916,"['Eddie Yang', 'Margaret E. Roberts']",11,"While artificial intelligence provides the backbone for many tools people use around the world, recent work has brought to attention that the algorithms powering AI are not free of politics, stereotypes, and bias. While most work in this area has focused on the ways in which AI can exacerbate existing inequalities and discrimination, very little work has studied how governments actively shape training data. We describe how censorship has affected the development of Wikipedia corpuses, text data which are regularly used for pre-trained inputs into NLP algorithms. We show that word embeddings trained on Baidu Baike, an online Chinese encyclopedia, have very different associations between adjectives and a range of concepts about democracy, freedom, collective action, equality, and people and historical events in China than its regularly blocked but uncensored counterpart - Chinese language Wikipedia. We examine the implications of these discrepancies by studying their use in downstream AI applications. Our paper shows how government repression, censorship, and self-censorship may impact training data and the applications that draw from them.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3442188.3445916,
author = {Yang, Eddie and Roberts, Margaret E.},
title = {Censorship of Online Encyclopedias: Implications for NLP Models},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445916},
doi = {10.1145/3442188.3445916},
abstract = {While artificial intelligence provides the backbone for many tools people use around the world, recent work has brought to attention that the algorithms powering AI are not free of politics, stereotypes, and bias. While most work in this area has focused on the ways in which AI can exacerbate existing inequalities and discrimination, very little work has studied how governments actively shape training data. We describe how censorship has affected the development of Wikipedia corpuses, text data which are regularly used for pre-trained inputs into NLP algorithms. We show that word embeddings trained on Baidu Baike, an online Chinese encyclopedia, have very different associations between adjectives and a range of concepts about democracy, freedom, collective action, equality, and people and historical events in China than its regularly blocked but uncensored counterpart - Chinese language Wikipedia. We examine the implications of these discrepancies by studying their use in downstream AI applications. Our paper shows how government repression, censorship, and self-censorship may impact training data and the applications that draw from them.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {537–548},
numpages = {12},
keywords = {censorship, machine learning, word embeddings, training data},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
FACCT,2021,Impossible Explanations?: Beyond explainable AI in the GDPR from a COVID-19 use case scenario,https://dl.acm.org/doi/10.1145/3442188.3445917,"['Ronan Hamon', 'Henrik Junklewitz', 'Gianclaudio Malgieri', 'Paul De Hert', 'Laurent Beslay', 'Ignacio Sanchez']",24,"Can we achieve an adequate level of explanation for complex machine learning models in high-risk AI applications when applying the EU data protection framework? In this article, we address this question, analysing from a multidisciplinary point of view the connection between existing legal requirements for the explainability of AI systems and the current state of the art in the field of explainable AI. We present a case study of a real-life scenario designed to illustrate the application of an AI-based automated decision making process for the medical diagnosis of COVID-19 patients. The scenario exemplifies the trend in the usage of increasingly complex machine-learning algorithms with growing dimensionality of data and model parameters. Based on this setting, we analyse the challenges of providing human legible explanations in practice and we discuss their legal implications following the General Data Protection Regulation (GDPR). Although it might appear that there is just one single form of explanation in the GDPR, we conclude that the context in which the decision-making system operates requires that several forms of explanation are considered. Thus, we propose to design explanations in multiple forms, depending on: the moment of the disclosure of the explanation (either ex ante or ex post); the audience of the explanation (explanation for an expert or a data controller and explanation for the final data subject); the layer of granularity (such as general, group-based or individual explanations); the level of the risks of the automated decision regarding fundamental rights and freedoms. Consequently, explanations should embrace this multifaceted environment. Furthermore, we highlight how the current inability of complex, deep learning based machine learning models to make clear causal links between input data and final decisions represents a limitation for providing exact, human-legible reasons behind specific decisions. This makes the provision of satisfactorily, fair and transparent explanations a serious challenge. Therefore, there are cases where the quality of possible explanations might not be assessed as an adequate safeguard for automated decision-making processes under Article 22(3) GDPR. Accordingly, we suggest that further research should focus on alternative tools in the GDPR (such as algorithmic impact assessments from Article 35 GDPR or algorithmic lawfulness justifications) that might be considered to complement the explanations of automated decision-making.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3442188.3445917,
author = {Hamon, Ronan and Junklewitz, Henrik and Malgieri, Gianclaudio and Hert, Paul De and Beslay, Laurent and Sanchez, Ignacio},
title = {Impossible Explanations? Beyond Explainable AI in the GDPR from a COVID-19 Use Case Scenario},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445917},
doi = {10.1145/3442188.3445917},
abstract = {Can we achieve an adequate level of explanation for complex machine learning models in high-risk AI applications when applying the EU data protection framework? In this article, we address this question, analysing from a multidisciplinary point of view the connection between existing legal requirements for the explainability of AI systems and the current state of the art in the field of explainable AI.We present a case study of a real-life scenario designed to illustrate the application of an AI-based automated decision making process for the medical diagnosis of COVID-19 patients. The scenario exemplifies the trend in the usage of increasingly complex machine-learning algorithms with growing dimensionality of data and model parameters. Based on this setting, we analyse the challenges of providing human legible explanations in practice and we discuss their legal implications following the General Data Protection Regulation (GDPR).Although it might appear that there is just one single form of explanation in the GDPR, we conclude that the context in which the decision-making system operates requires that several forms of explanation are considered. Thus, we propose to design explanations in multiple forms, depending on: the moment of the disclosure of the explanation (either ex ante or ex post); the audience of the explanation (explanation for an expert or a data controller and explanation for the final data subject); the layer of granularity (such as general, group-based or individual explanations); the level of the risks of the automated decision regarding fundamental rights and freedoms. Consequently, explanations should embrace this multifaceted environment.Furthermore, we highlight how the current inability of complex, deep learning based machine learning models to make clear causal links between input data and final decisions represents a limitation for providing exact, human-legible reasons behind specific decisions. This makes the provision of satisfactorily, fair and transparent explanations a serious challenge. Therefore, there are cases where the quality of possible explanations might not be assessed as an adequate safeguard for automated decision-making processes under Article 22(3) GDPR. Accordingly, we suggest that further research should focus on alternative tools in the GDPR (such as algorithmic impact assessments from Article 35 GDPR or algorithmic lawfulness justifications) that might be considered to complement the explanations of automated decision-making.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {549–559},
numpages = {11},
keywords = {GDPR, Black-Box, Automated Decision-Making, Data Protection, Machine Learning, Explainability, AI},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
FACCT,2021,"Fairness, Equality, and Power in Algorithmic Decision-Making",https://dl.acm.org/doi/10.1145/3442188.3445919,"['Maximilian Kasy', 'Rediet Abebe']",108,"Much of the debate on the impact of algorithms is concerned with fairness, defined as the absence of discrimination for individuals with the same ""merit."" Drawing on the theory of justice, we argue that leading notions of fairness suffer from three key limitations: they legitimize inequalities justified by ""merit;"" they are narrowly bracketed, considering only differences of treatment within the algorithm; and they consider between-group and not within-group differences. We contrast this fairness-based perspective with two alternate perspectives: the first focuses on inequality and the causal impact of algorithms and the second on the distribution of power. We formalize these perspectives drawing on techniques from causal inference and empirical economics, and characterize when they give divergent evaluations. We present theoretical results and empirical examples which demonstrate this tension. We further use these insights to present a guide for algorithmic auditing and discuss the importance of inequality- and power-centered frameworks in algorithmic decision-making.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3442188.3445919,
author = {Kasy, Maximilian and Abebe, Rediet},
title = {Fairness, Equality, and Power in Algorithmic Decision-Making},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445919},
doi = {10.1145/3442188.3445919},
abstract = {Much of the debate on the impact of algorithms is concerned with fairness, defined as the absence of discrimination for individuals with the same ""merit."" Drawing on the theory of justice, we argue that leading notions of fairness suffer from three key limitations: they legitimize inequalities justified by ""merit;"" they are narrowly bracketed, considering only differences of treatment within the algorithm; and they consider between-group and not within-group differences. We contrast this fairness-based perspective with two alternate perspectives: the first focuses on inequality and the causal impact of algorithms and the second on the distribution of power. We formalize these perspectives drawing on techniques from causal inference and empirical economics, and characterize when they give divergent evaluations. We present theoretical results and empirical examples which demonstrate this tension. We further use these insights to present a guide for algorithmic auditing and discuss the importance of inequality- and power-centered frameworks in algorithmic decision-making.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {576–586},
numpages = {11},
keywords = {power, empirical economics, Algorithmic fairness, inequality, auditing},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
FACCT,2021,A Statistical Test for Probabilistic Fairness,https://dl.acm.org/doi/10.1145/3442188.3445927,"['Bahar Taskesen', 'Jose Blanchet', 'Daniel Kuhn', 'Viet Anh Nguyen']",16,"Algorithms are now routinely used to make consequential decisions that affect human lives. Examples include college admissions, medical interventions or law enforcement. While algorithms empower us to harness all information hidden in vast amounts of data, they may inadvertently amplify existing biases in the available datasets. This concern has sparked increasing interest in fair machine learning, which aims to quantify and mitigate algorithmic discrimination. Indeed, machine learning models should undergo intensive tests to detect algorithmic biases before being deployed at scale. In this paper, we use ideas from the theory of optimal transport to propose a statistical hypothesis test for detecting unfair classifiers. Leveraging the geometry of the feature space, the test statistic quantifies the distance of the empirical distribution supported on the test samples to the manifold of distributions that render a pre-trained classifier fair. We develop a rigorous hypothesis testing mechanism for assessing the probabilistic fairness of any pre-trained logistic classifier, and we show both theoretically as well as empirically that the proposed test is asymptotically correct. In addition, the proposed framework offers interpretability by identifying the most favorable perturbation of the data so that the given classifier becomes fair.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3442188.3445927,
author = {Taskesen, Bahar and Blanchet, Jose and Kuhn, Daniel and Nguyen, Viet Anh},
title = {A Statistical Test for Probabilistic Fairness},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445927},
doi = {10.1145/3442188.3445927},
abstract = {Algorithms are now routinely used to make consequential decisions that affect human lives. Examples include college admissions, medical interventions or law enforcement. While algorithms empower us to harness all information hidden in vast amounts of data, they may inadvertently amplify existing biases in the available datasets. This concern has sparked increasing interest in fair machine learning, which aims to quantify and mitigate algorithmic discrimination. Indeed, machine learning models should undergo intensive tests to detect algorithmic biases before being deployed at scale. In this paper, we use ideas from the theory of optimal transport to propose a statistical hypothesis test for detecting unfair classifiers. Leveraging the geometry of the feature space, the test statistic quantifies the distance of the empirical distribution supported on the test samples to the manifold of distributions that render a pre-trained classifier fair. We develop a rigorous hypothesis testing mechanism for assessing the probabilistic fairness of any pre-trained logistic classifier, and we show both theoretically as well as empirically that the proposed test is asymptotically correct. In addition, the proposed framework offers interpretability by identifying the most favorable perturbation of the data so that the given classifier becomes fair.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {648–665},
numpages = {18},
keywords = {Wasserstein distance, equalized odds, algorithmic bias, fairness, equal opportunity},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
FACCT,2021,Building and Auditing Fair Algorithms: A Case Study in Candidate Screening,https://dl.acm.org/doi/10.1145/3442188.3445928,"['Christo Wilson', 'Avijit Ghosh', 'Shan Jiang', 'Alan Mislove', 'Lewis Baker', 'Janelle Szary', 'Kelly Trindel', 'Frida Polli']",57,"Academics, activists, and regulators are increasingly urging companies to develop and deploy sociotechnical systems that are fair and unbiased. Achieving this goal, however, is complex: the developer must (1) deeply engage with social and legal facets of ""fairness"" in a given context, (2) develop software that concretizes these values, and (3) undergo an independent algorithm audit to ensure technical correctness and social accountability of their algorithms. To date, there are few examples of companies that have transparently undertaken all three steps. In this paper we outline a framework for algorithmic auditing by way of a case-study of pymetrics, a startup that uses machine learning to recommend job candidates to their clients. We discuss how pymetrics approaches the question of fairness given the constraints of ethical, regulatory, and client demands, and how pymetrics' software implements adverse impact testing. We also present the results of an independent audit of pymetrics' candidate screening tool. We conclude with recommendations on how to structure audits to be practical, independent, and constructive, so that companies have better incentive to participate in third party audits, and that watchdog groups can be better prepared to investigate companies.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3442188.3445928,
author = {Wilson, Christo and Ghosh, Avijit and Jiang, Shan and Mislove, Alan and Baker, Lewis and Szary, Janelle and Trindel, Kelly and Polli, Frida},
title = {Building and Auditing Fair Algorithms: A Case Study in Candidate Screening},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445928},
doi = {10.1145/3442188.3445928},
abstract = {Academics, activists, and regulators are increasingly urging companies to develop and deploy sociotechnical systems that are fair and unbiased. Achieving this goal, however, is complex: the developer must (1) deeply engage with social and legal facets of ""fairness"" in a given context, (2) develop software that concretizes these values, and (3) undergo an independent algorithm audit to ensure technical correctness and social accountability of their algorithms. To date, there are few examples of companies that have transparently undertaken all three steps.In this paper we outline a framework for algorithmic auditing by way of a case-study of pymetrics, a startup that uses machine learning to recommend job candidates to their clients. We discuss how pymetrics approaches the question of fairness given the constraints of ethical, regulatory, and client demands, and how pymetrics' software implements adverse impact testing. We also present the results of an independent audit of pymetrics' candidate screening tool.We conclude with recommendations on how to structure audits to be practical, independent, and constructive, so that companies have better incentive to participate in third party audits, and that watchdog groups can be better prepared to investigate companies.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {666–677},
numpages = {12},
keywords = {algorithm auditing, fairness, four-fifths rule, adverse impact testing},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
FACCT,2021,"I agree with the decision, but they didn't deserve this: Future Developers' Perception of Fairness in Algorithmic Decisions",https://dl.acm.org/doi/10.1145/3442188.3445931,"['Maria Kasinidou', 'Styliani Kleanthous', 'Pƒ±nar Barlas', 'Jahna Otterbacher']",23,"While professionals are increasingly relying on algorithmic systems for making a decision, on some occasions, algorithmic decisions may be perceived as biased or not just. Prior work has looked into the perception of algorithmic decision-making from the user's point of view. In this work, we investigate how students in fields adjacent to algorithm development perceive algorithmic decisionmaking. Participants (N=99) were asked to rate their agreement with statements regarding six constructs that are related to facets of fairness and justice in algorithmic decision-making in three separate scenarios. Two of the three scenarios were independent of each other, while the third scenario presented three different outcomes of the same algorithmic system, demonstrating perception changes triggered by different outputs. Quantitative analysis indicates that a) 'agreeing' with a decision does not mean the person 'deserves the outcome', b) perceiving the factors used in the decision-making as 'appropriate' does not make the decision of the system 'fair' and c) perceiving a system's decision as 'not fair' is affecting the participants' 'trust' in the system. In addition, participants found proportional distribution of benefits more fair than other approaches. Qualitative analysis provides further insights into that information the participants find essential to judge and understand an algorithmic decision-making system's fairness. Finally, the level of academic education has a role to play in the perception of fairness and justice in algorithmic decision-making.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3442188.3445931,
author = {Kasinidou, Maria and Kleanthous, Styliani and Barlas, P\i{}nar and Otterbacher, Jahna},
title = {I Agree with the Decision, but They Didn't Deserve This: Future Developers' Perception of Fairness in Algorithmic Decisions},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445931},
doi = {10.1145/3442188.3445931},
abstract = {While professionals are increasingly relying on algorithmic systems for making a decision, on some occasions, algorithmic decisions may be perceived as biased or not just. Prior work has looked into the perception of algorithmic decision-making from the user's point of view. In this work, we investigate how students in fields adjacent to algorithm development perceive algorithmic decisionmaking. Participants (N=99) were asked to rate their agreement with statements regarding six constructs that are related to facets of fairness and justice in algorithmic decision-making in three separate scenarios. Two of the three scenarios were independent of each other, while the third scenario presented three different outcomes of the same algorithmic system, demonstrating perception changes triggered by different outputs. Quantitative analysis indicates that a) 'agreeing' with a decision does not mean the person 'deserves the outcome', b) perceiving the factors used in the decision-making as 'appropriate' does not make the decision of the system 'fair' and c) perceiving a system's decision as 'not fair' is affecting the participants' 'trust' in the system. In addition, participants found proportional distribution of benefits more fair than other approaches. Qualitative analysis provides further insights into that information the participants find essential to judge and understand an algorithmic decision-making system's fairness. Finally, the level of academic education has a role to play in the perception of fairness and justice in algorithmic decision-making.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {690–700},
numpages = {11},
keywords = {algorithmic transparency, algorithmic decision-making, algorithmic accountability, algorithmic fairness},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
FACCT,2021,Chasing Your Long Tails: Differentially Private Prediction in Health Care Settings,https://dl.acm.org/doi/10.1145/3442188.3445934,"['Vinith M. Suriyakumar', 'Nicolas Papernot', 'Anna Goldenberg', 'Marzyeh Ghassemi']",36,"Machine learning models in health care are often deployed in settings where it is important to protect patient privacy. In such settings, methods for differentially private (DP) learning provide a general-purpose approach to learn models with privacy guarantees. Modern methods for DP learning ensure privacy through the addition of calibrated noise. The resulting privacy-preserving models are unable to learn too much information about the tails of a data distribution, resulting in a loss of accuracy that can disproportionately affect small groups. In this paper, we study the effects of DP learning in health care. We use state-of-the-art methods for DP learning to train privacy-preserving models in clinical prediction tasks, including x-ray classification of images and mortality prediction in time series data. We use these models to perform a comprehensive empirical investigation of the tradeoffs between privacy, utility, robustness to dataset shift and fairness. Our results highlight lesser-known limitations of methods for DP learning in health care, models that exhibit steep tradeoffs between privacy and utility, and models whose predictions are disproportionately influenced by large demographic groups in the training data. We discuss the costs and benefits of differentially private learning in health care with open directions for differential privacy, machine learning and health care.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3442188.3445934,
author = {Suriyakumar, Vinith M. and Papernot, Nicolas and Goldenberg, Anna and Ghassemi, Marzyeh},
title = {Chasing Your Long Tails: Differentially Private Prediction in Health Care Settings},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445934},
doi = {10.1145/3442188.3445934},
abstract = {Machine learning models in health care are often deployed in settings where it is important to protect patient privacy. In such settings, methods for differentially private (DP) learning provide a general-purpose approach to learn models with privacy guarantees. Modern methods for DP learning ensure privacy through the addition of calibrated noise. The resulting privacy-preserving models are unable to learn too much information about the tails of a data distribution, resulting in a loss of accuracy that can disproportionately affect small groups. In this paper, we study the effects of DP learning in health care. We use state-of-the-art methods for DP learning to train privacy-preserving models in clinical prediction tasks, including x-ray classification of images and mortality prediction in time series data. We use these models to perform a comprehensive empirical investigation of the tradeoffs between privacy, utility, robustness to dataset shift and fairness. Our results highlight lesser-known limitations of methods for DP learning in health care, models that exhibit steep tradeoffs between privacy and utility, and models whose predictions are disproportionately influenced by large demographic groups in the training data. We discuss the costs and benefits of differentially private learning in health care with open directions for differential privacy, machine learning and health care.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {723–734},
numpages = {12},
keywords = {privacy, robustness, fairness, health care, machine learning},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
FACCT,2021,On the Moral Justification of Statistical Parity,https://dl.acm.org/doi/10.1145/3442188.3445936,"['Corinna Hertweck', 'Christoph Heitz', 'Michele Loi']",30,"A crucial but often neglected aspect of algorithmic fairness is the question of how we justify enforcing a certain fairness metric from a moral perspective. When fairness metrics are proposed, they are typically argued for by highlighting their mathematical properties. Rarely are the moral assumptions beneath the metric explained. Our aim in this paper is to consider the moral aspects associated with the statistical fairness criterion of independence (statistical parity). To this end, we consider previous work, which discusses the two worldviews ""What You See Is What You Get"" (WYSIWYG) and ""We're All Equal"" (WAE) and by doing so provides some guidance for clarifying the possible assumptions in the design of algorithms. We present an extension of this work, which centers on morality. The most natural moral extension is that independence needs to be fulfilled if and only if differences in predictive features (e.g. high school grades and standardized test scores are predictive of performance at university) between socio-demographic groups are caused by unjust social disparities or measurement errors. Through two counterexamples, we demonstrate that this extension is not universally true. This means that the question of whether independence should be used or not cannot be satisfactorily answered by only considering the justness of differences in the predictive features.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3442188.3445936,
author = {Hertweck, Corinna and Heitz, Christoph and Loi, Michele},
title = {On the Moral Justification of Statistical Parity},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445936},
doi = {10.1145/3442188.3445936},
abstract = {A crucial but often neglected aspect of algorithmic fairness is the question of how we justify enforcing a certain fairness metric from a moral perspective. When fairness metrics are proposed, they are typically argued for by highlighting their mathematical properties. Rarely are the moral assumptions beneath the metric explained. Our aim in this paper is to consider the moral aspects associated with the statistical fairness criterion of independence (statistical parity). To this end, we consider previous work, which discusses the two worldviews ""What You See Is What You Get"" (WYSIWYG) and ""We're All Equal"" (WAE) and by doing so provides some guidance for clarifying the possible assumptions in the design of algorithms. We present an extension of this work, which centers on morality. The most natural moral extension is that independence needs to be fulfilled if and only if differences in predictive features (e.g. high school grades and standardized test scores are predictive of performance at university) between socio-demographic groups are caused by unjust social disparities or measurement errors. Through two counterexamples, we demonstrate that this extension is not universally true. This means that the question of whether independence should be used or not cannot be satisfactorily answered by only considering the justness of differences in the predictive features.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {747–757},
numpages = {11},
keywords = {statistical parity, distributive justice, bias, independence, fairness},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
FACCT,2021,An Action-Oriented AI Policy Toolkit for Technology Audits by Community Advocates and Activists,https://dl.acm.org/doi/10.1145/3442188.3445938,"['P. M. Krafft', 'Meg Young', 'Michael Katell', 'Jennifer E. Lee', 'Shankar Narayan', 'Micah Epstein', 'Dharma Dailey', 'Bernease Herman', 'Aaron Tam', 'Vivian Guetler', 'Corinne Bintz', 'Daniella Raz', 'Pa Ousman Jobe', 'Franziska Putz', 'Brian Robick', 'Bissan Barghouti']",24,"Motivated by the extensive documented disparate harms of artificial intelligence (AI), many recent practitioner-facing reflective tools have been created to promote responsible AI development. However, the use of such tools internally by technology development firms addresses responsible AI as an issue of closed-door compliance rather than a matter of public concern. Recent advocate and activist efforts intervene in AI as a public policy problem, inciting a growing number of cities to pass bans or other ordinances on AI and surveillance technologies. In support of this broader ecology of political actors, we present a set of reflective tools intended to increase public participation in technology advocacy for AI policy action. To this end, the Algorithmic Equity Toolkit (the AEKit) provides a practical policy-facing definition of AI, a flowchart for assessing technologies against that definition, a worksheet for decomposing AI systems into constituent parts, and a list of probing questions that can be posed to vendors, policy-makers, or government agencies. The AEKit carries an action-orientation towards political encounters between community groups in the public and their representatives, opening up the work of AI reflection and remediation to multiple points of intervention. Unlike current reflective tools available to practitioners, our toolkit carries with it a politics of community participation and activism.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3442188.3445938,
author = {Krafft, P. M. and Young, Meg and Katell, Michael and Lee, Jennifer E. and Narayan, Shankar and Epstein, Micah and Dailey, Dharma and Herman, Bernease and Tam, Aaron and Guetler, Vivian and Bintz, Corinne and Raz, Daniella and Jobe, Pa Ousman and Putz, Franziska and Robick, Brian and Barghouti, Bissan},
title = {An Action-Oriented AI Policy Toolkit for Technology Audits by Community Advocates and Activists},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445938},
doi = {10.1145/3442188.3445938},
abstract = {Motivated by the extensive documented disparate harms of artificial intelligence (AI), many recent practitioner-facing reflective tools have been created to promote responsible AI development. However, the use of such tools internally by technology development firms addresses responsible AI as an issue of closed-door compliance rather than a matter of public concern. Recent advocate and activist efforts intervene in AI as a public policy problem, inciting a growing number of cities to pass bans or other ordinances on AI and surveillance technologies. In support of this broader ecology of political actors, we present a set of reflective tools intended to increase public participation in technology advocacy for AI policy action. To this end, the Algorithmic Equity Toolkit (the AEKit) provides a practical policy-facing definition of AI, a flowchart for assessing technologies against that definition, a worksheet for decomposing AI systems into constituent parts, and a list of probing questions that can be posed to vendors, policy-makers, or government agencies. The AEKit carries an action-orientation towards political encounters between community groups in the public and their representatives, opening up the work of AI reflection and remediation to multiple points of intervention. Unlike current reflective tools available to practitioners, our toolkit carries with it a politics of community participation and activism.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {772–781},
numpages = {10},
keywords = {algorithmic justice, surveillance, algorithmic equity, regulation, accountability, participatory action research, Participatory design},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
FACCT,2021,Detecting discriminatory risk through data annotation based on Bayesian inferences,https://dl.acm.org/doi/10.1145/3442188.3445940,"['Elena Beretta', 'Antonio Vetr√≤', 'Bruno Lepri', 'Juan Carlos De Martin']",11,"Thanks to the increasing growth of computational power and data availability, the research in machine learning has advanced with tremendous rapidity. Nowadays, the majority of automatic decision making systems are based on data. However, it is well known that machine learning systems can present problematic results if they are built on partial or incomplete data. In fact, in recent years several studies have found a convergence of issues related to the ethics and transparency of these systems in the process of data collection and how they are recorded. Although the process of rigorous data collection and analysis is fundamental in the model design, this step is still largely overlooked by the machine learning community. For this reason, we propose a method of data annotation based on Bayesian statistical inference that aims to warn about the risk of discriminatory results of a given data set. In particular, our method aims to deepen knowledge and promote awareness about the sampling practices employed to create the training set, highlighting that the probability of success or failure conditioned to a minority membership is given by the structure of the data available. We empirically test our system on three datasets commonly accessed by the machine learning community and we investigate the risk of racial discrimination.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3442188.3445940,
author = {Beretta, Elena and Vetr\`{o}, Antonio and Lepri, Bruno and Martin, Juan Carlos De},
title = {Detecting Discriminatory Risk through Data Annotation Based on Bayesian Inferences},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445940},
doi = {10.1145/3442188.3445940},
abstract = {Thanks to the increasing growth of computational power and data availability, the research in machine learning has advanced with tremendous rapidity. Nowadays, the majority of automatic decision making systems are based on data. However, it is well known that machine learning systems can present problematic results if they are built on partial or incomplete data. In fact, in recent years several studies have found a convergence of issues related to the ethics and transparency of these systems in the process of data collection and how they are recorded. Although the process of rigorous data collection and analysis is fundamental in the model design, this step is still largely overlooked by the machine learning community. For this reason, we propose a method of data annotation based on Bayesian statistical inference that aims to warn about the risk of discriminatory results of a given data set. In particular, our method aims to deepen knowledge and promote awareness about the sampling practices employed to create the training set, highlighting that the probability of success or failure conditioned to a minority membership is given by the structure of the data available. We empirically test our system on three datasets commonly accessed by the machine learning community and we investigate the risk of racial discrimination.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {794–804},
numpages = {11},
keywords = {data ethics, sampling bias, data labeling, race discrimination, human annotation, machine learning},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
FACCT,2021,"The Algorithmic Leviathan: Arbitrariness, Fairness, and Opportunity in Algorithmic Decision Making Systems",https://dl.acm.org/doi/10.1145/3442188.3445942,"['Kathleen Creel', 'Deborah Hellman']",24,"Automated decision-making systems implemented in public life are typically standardized. One algorithmic decision-making system can replace thousands of human deciders. Each of the humans so replaced had her own decision-making criteria: some good, some bad, and some arbitrary. Is such arbitrariness of moral concern? We argue that an isolated arbitrary decision need not morally wrong the individual whom it misclassifies. However, if the same algorithms are applied across a public sphere, such as hiring or lending, a person could be excluded from a large number of opportunities. This harm persists even when the automated decision-making systems are ""fair"" on standard metrics of fairness. We argue that such arbitrariness at scale is morally problematic and propose technically informed solutions that can lessen the impact of algorithms at scale and so mitigate or avoid the moral harms we identify.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3442188.3445942,
author = {Creel, Kathleen and Hellman, Deborah},
title = {The Algorithmic Leviathan: Arbitrariness, Fairness, and Opportunity in Algorithmic Decision Making Systems},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445942},
doi = {10.1145/3442188.3445942},
abstract = {Automated decision-making systems implemented in public life are typically standardized. One algorithmic decision-making system can replace thousands of human deciders. Each of the humans so replaced had her own decision-making criteria: some good, some bad, and some arbitrary. Is such arbitrariness of moral concern?We argue that an isolated arbitrary decision need not morally wrong the individual whom it misclassifies. However, if the same algorithms are applied across a public sphere, such as hiring or lending, a person could be excluded from a large number of opportunities. This harm persists even when the automated decision-making systems are ""fair"" on standard metrics of fairness. We argue that such arbitrariness at scale is morally problematic and propose technically informed solutions that can lessen the impact of algorithms at scale and so mitigate or avoid the moral harms we identify.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {816},
numpages = {1},
keywords = {fairness, algorithmic decision making, opportunity, automated hiring, machine learning, arbitrariness},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
FACCT,2021,A Bayesian Model of Cash Bail Decisions,https://dl.acm.org/doi/10.1145/3442188.3445908,"['Joshua Williams', 'J. Zico Kolter']",0,"The use of cash bail as a mechanism for detaining defendants pretrial is an often-criticized system that many have argued violates the presumption of ""innocent until proven guilty."" Many studies have sought to understand both the long-term effects of cash bail's use and the disparate rate of cash bail assignments along demographic lines (race, gender, etc). However, such work is often susceptible to problems of infra-marginality - that the data we observe can only describe average outcomes, and not the outcomes associated with the marginal decision. In this work, we address this problem by creating a hierarchical Bayesian model of cash bail assignments. Specifically, our approach models cash bail decisions as a probabilistic process whereby judges balance the relative costs of assigning cash bail with the cost of defendants potentially skipping court dates, and where these skip probabilities are estimated based upon features of the individual case. We then use Monte Carlo inference to sample the distribution over these costs for different magistrates and across different races. We fit this model to a data set we have collected of over 50,000 court cases in the Allegheny and Philadelphia counties in Pennsylvania. Our analysis of 50 separate judges shows that they are uniformly more likely to assign cash bail to black defendants than to white defendants, even given identical likelihood of skipping a court appearance. This analysis raises further questions about the equity of the practice of cash bail, irrespective of its underlying legal justification.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3442188.3445908,
author = {Williams, Joshua and Kolter, J. Zico},
title = {A Bayesian Model of Cash Bail Decisions},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445908},
doi = {10.1145/3442188.3445908},
abstract = {The use of cash bail as a mechanism for detaining defendants pretrial is an often-criticized system that many have argued violates the presumption of ""innocent until proven guilty."" Many studies have sought to understand both the long-term effects of cash bail's use and the disparate rate of cash bail assignments along demographic lines (race, gender, etc). However, such work is often susceptible to problems of infra-marginality - that the data we observe can only describe average outcomes, and not the outcomes associated with the marginal decision. In this work, we address this problem by creating a hierarchical Bayesian model of cash bail assignments. Specifically, our approach models cash bail decisions as a probabilistic process whereby judges balance the relative costs of assigning cash bail with the cost of defendants potentially skipping court dates, and where these skip probabilities are estimated based upon features of the individual case. We then use Monte Carlo inference to sample the distribution over these costs for different magistrates and across different races. We fit this model to a data set we have collected of over 50,000 court cases in the Allegheny and Philadelphia counties in Pennsylvania. Our analysis of 50 separate judges shows that they are uniformly more likely to assign cash bail to black defendants than to white defendants, even given identical likelihood of skipping a court appearance. This analysis raises further questions about the equity of the practice of cash bail, irrespective of its underlying legal justification.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {827–837},
numpages = {11},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
FACCT,2021,The effect of differential victim crime reporting on predictive policing systems,https://dl.acm.org/doi/10.1145/3442188.3445877,"['Nil-Jana Akpinar', 'Maria De-Arteaga', 'Alexandra Chouldechova']",18,"Police departments around the world have been experimenting with forms of place-based data-driven proactive policing for over two decades. Modern incarnations of such systems are commonly known as hot spot predictive policing. These systems predict where future crime is likely to concentrate such that police can allocate patrols to these areas and deter crime before it occurs. Previous research on fairness in predictive policing has concentrated on the feedback loops which occur when models are trained on discovered crime data, but has limited implications for models trained on victim crime reporting data. We demonstrate how differential victim crime reporting rates across geographical areas can lead to outcome disparities in common crime hot spot prediction models. Our analysis is based on a simulation1 patterned after district-level victimization and crime reporting survey data for Bogot√°, Colombia. Our results suggest that differential crime reporting rates can lead to a displacement of predicted hotspots from high crime but low reporting areas to high or medium crime and high reporting areas. This may lead to misallocations both in the form of over-policing and under-policing.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3442188.3445877,
author = {Akpinar, Nil-Jana and De-Arteaga, Maria and Chouldechova, Alexandra},
title = {The Effect of Differential Victim Crime Reporting on Predictive Policing Systems},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445877},
doi = {10.1145/3442188.3445877},
abstract = {Police departments around the world have been experimenting with forms of place-based data-driven proactive policing for over two decades. Modern incarnations of such systems are commonly known as hot spot predictive policing. These systems predict where future crime is likely to concentrate such that police can allocate patrols to these areas and deter crime before it occurs. Previous research on fairness in predictive policing has concentrated on the feedback loops which occur when models are trained on discovered crime data, but has limited implications for models trained on victim crime reporting data. We demonstrate how differential victim crime reporting rates across geographical areas can lead to outcome disparities in common crime hot spot prediction models. Our analysis is based on a simulation1 patterned after district-level victimization and crime reporting survey data for Bogot\'{a}, Colombia. Our results suggest that differential crime reporting rates can lead to a displacement of predicted hotspots from high crime but low reporting areas to high or medium crime and high reporting areas. This may lead to misallocations both in the form of over-policing and under-policing.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {838–849},
numpages = {12},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
FACCT,2021,Value Cards: An Educational Toolkit for Teaching Social Impacts of Machine Learning through Deliberation,https://dl.acm.org/doi/10.1145/3442188.3445971,"['Hong Shen', 'Wesley H. Deng', 'Aditi Chattopadhyay', 'Zhiwei Steven Wu', 'Xu Wang', 'Haiyi Zhu']",26,"Recently, there have been increasing calls for computer science curricula to complement existing technical training with topics related to Fairness, Accountability, Transparency and Ethics (FATE). In this paper, we present Value Cards, an educational toolkit to inform students and practitioners the social impacts of different machine learning models via deliberation. This paper presents an early use of our approach in a college-level computer science course. Through an in-class activity, we report empirical data for the initial effectiveness of our approach. Our results suggest that the use of the Value Cards toolkit can improve students' understanding of both the technical definitions and trade-offs of performance metrics and apply them in real-world contexts, help them recognize the significance of considering diverse social values in the development and deployment of algorithmic systems, and enable them to communicate, negotiate and synthesize the perspectives of diverse stakeholders. Our study also demonstrates a number of caveats we need to consider when using the different variants of the Value Cards toolkit. Finally, we discuss the challenges as well as future applications of our approach.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3442188.3445971,
author = {Shen, Hong and Deng, Wesley H. and Chattopadhyay, Aditi and Wu, Zhiwei Steven and Wang, Xu and Zhu, Haiyi},
title = {Value Cards: An Educational Toolkit for Teaching Social Impacts of Machine Learning through Deliberation},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445971},
doi = {10.1145/3442188.3445971},
abstract = {Recently, there have been increasing calls for computer science curricula to complement existing technical training with topics related to Fairness, Accountability, Transparency and Ethics (FATE). In this paper, we present Value Cards, an educational toolkit to inform students and practitioners the social impacts of different machine learning models via deliberation. This paper presents an early use of our approach in a college-level computer science course. Through an in-class activity, we report empirical data for the initial effectiveness of our approach. Our results suggest that the use of the Value Cards toolkit can improve students' understanding of both the technical definitions and trade-offs of performance metrics and apply them in real-world contexts, help them recognize the significance of considering diverse social values in the development and deployment of algorithmic systems, and enable them to communicate, negotiate and synthesize the perspectives of diverse stakeholders. Our study also demonstrates a number of caveats we need to consider when using the different variants of the Value Cards toolkit. Finally, we discuss the challenges as well as future applications of our approach.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {850–861},
numpages = {12},
keywords = {Deliberation, CS Education, Value Cards, Machine Learning, Fairness},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
FACCT,2021,When the Umpire is also a Player: Bias in Private Label Product Recommendations on E-commerce Marketplaces,https://dl.acm.org/doi/10.1145/3442188.3445944,"['Abhisek Dash', 'Abhijnan Chakraborty', 'Saptarshi Ghosh', 'Animesh Mukherjee', 'Krishna P. Gummadi']",18,"Algorithmic recommendations mediate interactions between millions of customers and products (in turn, their producers and sellers) on large e-commerce marketplaces like Amazon. In recent years, the producers and sellers have raised concerns about the fairness of black-box recommendation algorithms deployed on these marketplaces. Many complaints are centered around marketplaces biasing the algorithms to preferentially favor their own 'private label' products over competitors. These concerns are exacerbated as marketplaces increasingly de-emphasize or replace 'organic' recommendations with ad-driven 'sponsored' recommendations, which include their own private labels. While these concerns have been covered in popular press and have spawned regulatory investigations, to our knowledge, there has not been any public audit of these marketplace algorithms. In this study, we bridge this gap by performing an end-to-end systematic audit of related item recommendations on Amazon. We propose a network-centric framework to quantify and compare the biases across organic and sponsored related item recommendations. Along a number of our proposed bias measures, we find that the sponsored recommendations are significantly more biased toward Amazon private label products compared to organic recommendations. While our findings are primarily interesting to producers and sellers on Amazon, our proposed bias measures are generally useful for measuring link formation bias in any social or content networks.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3442188.3445944,
author = {Dash, Abhisek and Chakraborty, Abhijnan and Ghosh, Saptarshi and Mukherjee, Animesh and Gummadi, Krishna P.},
title = {When the Umpire is Also a Player: Bias in Private Label Product Recommendations on E-Commerce Marketplaces},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445944},
doi = {10.1145/3442188.3445944},
abstract = {Algorithmic recommendations mediate interactions between millions of customers and products (in turn, their producers and sellers) on large e-commerce marketplaces like Amazon. In recent years, the producers and sellers have raised concerns about the fairness of black-box recommendation algorithms deployed on these marketplaces. Many complaints are centered around marketplaces biasing the algorithms to preferentially favor their own 'private label' products over competitors. These concerns are exacerbated as marketplaces increasingly de-emphasize or replace 'organic' recommendations with ad-driven 'sponsored' recommendations, which include their own private labels. While these concerns have been covered in popular press and have spawned regulatory investigations, to our knowledge, there has not been any public audit of these marketplace algorithms. In this study, we bridge this gap by performing an end-to-end systematic audit of related item recommendations on Amazon. We propose a network-centric framework to quantify and compare the biases across organic and sponsored related item recommendations. Along a number of our proposed bias measures, we find that the sponsored recommendations are significantly more biased toward Amazon private label products compared to organic recommendations. While our findings are primarily interesting to producers and sellers on Amazon, our proposed bias measures are generally useful for measuring link formation bias in any social or content networks.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {873–884},
numpages = {12},
keywords = {e-commerce marketplace, Recommendation, algorithmic auditing},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
FACCT,2022,Fairness Indicators for Systematic Assessments of Visual Feature Extractors,https://dl.acm.org/doi/10.1145/3531146.3533074,"['Priya Goyal', 'Adriana Romero Soriano', 'Caner Hazirbas', 'Levent Sagun', 'Nicolas Usunier']",2,"Does everyone equally benefit from computer vision systems? Answers to this question become more and more important as computer vision systems are deployed at large scale, and can spark major concerns when they exhibit vast performance discrepancies between people from various demographic and social backgrounds. Systematic diagnosis of fairness, harms, and biases of computer vision systems is an important step towards building socially responsible systems. To initiate an effort towards standardized fairness audits, we propose three fairness indicators, which aim at quantifying harms and biases of visual systems. Our indicators use existing publicly available datasets collected for fairness evaluations, and focus on three main types of harms and bias identified in the literature, namely harmful label associations, disparity in learned representations of social and demographic traits, and biased performance on geographically diverse images from across the world. We define precise experimental protocols applicable to a wide range of computer vision models. These indicators are part of an ever-evolving suite of fairness probes and are not intended to be a substitute for a thorough analysis of the broader impact of the new computer vision technologies. Yet, we believe it is a necessary first step towards (1) facilitating the widespread adoption and mandate of the fairness assessments in computer vision research, and (2) tracking progress towards building socially responsible models. To study the practical effectiveness and broad applicability of our proposed indicators to any visual system, we apply them to ‚Äúoff-the-shelf‚Äù models built using widely adopted model training paradigms which vary in their ability to whether they can predict labels on a given image or only produce the embeddings. We also systematically study the effect of data domain and model size. The results of our fairness indicators on these systems suggest that blatant disparities still exist, which highlight the importance on the relationship between the context of the task and contents of a datasets. The code will be released to encourage the use of indicators.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3533074,
author = {Goyal, Priya and Soriano, Adriana Romero and Hazirbas, Caner and Sagun, Levent and Usunier, Nicolas},
title = {Fairness Indicators for Systematic Assessments of Visual Feature Extractors},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533074},
doi = {10.1145/3531146.3533074},
abstract = {Does everyone equally benefit from computer vision systems? Answers to this question become more and more important as computer vision systems are deployed at large scale, and can spark major concerns when they exhibit vast performance discrepancies between people from various demographic and social backgrounds. Systematic diagnosis of fairness, harms, and biases of computer vision systems is an important step towards building socially responsible systems. To initiate an effort towards standardized fairness audits, we propose three fairness indicators, which aim at quantifying harms and biases of visual systems. Our indicators use existing publicly available datasets collected for fairness evaluations, and focus on three main types of harms and bias identified in the literature, namely harmful label associations, disparity in learned representations of social and demographic traits, and biased performance on geographically diverse images from across the world. We define precise experimental protocols applicable to a wide range of computer vision models. These indicators are part of an ever-evolving suite of fairness probes and are not intended to be a substitute for a thorough analysis of the broader impact of the new computer vision technologies. Yet, we believe it is a necessary first step towards (1) facilitating the widespread adoption and mandate of the fairness assessments in computer vision research, and (2) tracking progress towards building socially responsible models. To study the practical effectiveness and broad applicability of our proposed indicators to any visual system, we apply them to “off-the-shelf” models built using widely adopted model training paradigms which vary in their ability to whether they can predict labels on a given image or only produce the embeddings. We also systematically study the effect of data domain and model size. The results of our fairness indicators on these systems suggest that blatant disparities still exist, which highlight the importance on the relationship between the context of the task and contents of a datasets. The code will be released to encourage the use of indicators.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {70–88},
numpages = {19},
keywords = {Fairness, benchmarks, Computer Vision, metrics},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,FAccT-Check on AI regulation: Systematic Evaluation of AI Regulation on the Example of the Legislation on the Use of AI in the Public Sector in the German Federal State of Schleswig-Holstein,https://dl.acm.org/doi/10.1145/3531146.3533076,['Katharina Simbeck'],0,"In the framework of the current discussions about regulating Artificial Intelligence (AI) and machine learning (ML), the small Federal State of Schleswig-Holstein in Northern Germany hurries ahead and adopts legislation on the Use of AI in the public sector. The legislation aims on the one hand to enable the use of AI in the public sector by creating a legal framework and to limit its potential discriminatory effect on the other hand. Contrary to the European AI Act, which is valid for all companies and organizations in Europe, and contrary to the Chinese administrative rule on Internet information recommender systems, the Schleswig-Holstein ‚ÄúIT Deployment Law‚Äù (ITDL) would therefore only apply to public administrations and agencies in the federal state. The legislation addresses several AI risks, including fairness and transparency, and mitigates them with approaches quite different from the proposed European AI Act (AIA). In this paper, the legislation will be systematically reviewed and discussed with regards to its definition of AI, risk handling, fairness, accountability, and transparency.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3533076,
author = {Simbeck, Katharina},
title = {FAccT-Check on AI Regulation: Systematic Evaluation of AI Regulation on the Example of the Legislation on the Use of AI in the Public Sector in the German Federal State of Schleswig-Holstein},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533076},
doi = {10.1145/3531146.3533076},
abstract = {In the framework of the current discussions about regulating Artificial Intelligence (AI) and machine learning (ML), the small Federal State of Schleswig-Holstein in Northern Germany hurries ahead and adopts legislation on the Use of AI in the public sector. The legislation aims on the one hand to enable the use of AI in the public sector by creating a legal framework and to limit its potential discriminatory effect on the other hand. Contrary to the European AI Act, which is valid for all companies and organizations in Europe, and contrary to the Chinese administrative rule on Internet information recommender systems, the Schleswig-Holstein “IT Deployment Law” (ITDL) would therefore only apply to public administrations and agencies in the federal state. The legislation addresses several AI risks, including fairness and transparency, and mitigates them with approaches quite different from the proposed European AI Act (AIA). In this paper, the legislation will be systematically reviewed and discussed with regards to its definition of AI, risk handling, fairness, accountability, and transparency.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {89–96},
numpages = {8},
keywords = {AI regulation, AI transparency, AI fairness},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,Providing Item-side Individual Fairness for Deep Recommender Systems,https://dl.acm.org/doi/10.1145/3531146.3533079,"['Xiuling Wang', 'Wendy Hui Wang']",2,"Recent advent of deep learning techniques have reinforced the development of new recommender systems. Although these systems have been demonstrated as efficient and effective, the issue of item popularity bias in these recommender systems has raised serious concerns. While most of the existing works focus on group fairness at item side, individual fairness at item side is left largely unexplored. To address this issue, in this paper, first, we define a new notion of individual fairness from the perspective of items, namely (Œ±, Œ≤)-fairness, to deal with item popularity bias in recommendations. In particular, (Œ±, Œ≤)-fairness requires that similar items should receive similar coverage in the recommendations, where Œ± and Œ≤ control item similarity and coverage similarity respectively, and both item and coverage similarity metrics are defined as task specific for deep recommender systems. Next, we design two bias mitigation methods, namely embedding-based re-ranking (ER) and greedy substitution (GS), for deep recommender systems. ER is an in-processing mitigation method that equips (Œ±, Œ≤)-fairness as a constraint to the objective function of the recommendation algorithm, while GS is a post-processing approach that accepts the biased recommendations as the input, and substitutes high-coverage items with low-coverage ones in the recommendations to satisfy (Œ±, Œ≤)-fairness. We evaluate the performance of both mitigation algorithms on two real-world datasets and a set of state-of-the-art deep recommender systems. Our results demonstrate that both ER and GS outperform the existing minimum-coverage (MC) mitigation solutions [Koutsopoulos and Halkidi 2018; Patro et¬†al. 2020] in terms of both fairness and accuracy of recommendations. Furthermore, ER delivers the best trade-off between fairness and recommendation accuracy among a set of alternative mitigation methods, including GS, the hybrid of ER and GS, and the existing MC solutions.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3533079,
author = {Wang, Xiuling and Wang, Wendy Hui},
title = {Providing Item-Side Individual Fairness for Deep Recommender Systems},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533079},
doi = {10.1145/3531146.3533079},
abstract = {Recent advent of deep learning techniques have reinforced the development of new recommender systems. Although these systems have been demonstrated as efficient and effective, the issue of item popularity bias in these recommender systems has raised serious concerns. While most of the existing works focus on group fairness at item side, individual fairness at item side is left largely unexplored. To address this issue, in this paper, first, we define a new notion of individual fairness from the perspective of items, namely (α, β)-fairness, to deal with item popularity bias in recommendations. In particular, (α, β)-fairness requires that similar items should receive similar coverage in the recommendations, where α and β control item similarity and coverage similarity respectively, and both item and coverage similarity metrics are defined as task specific for deep recommender systems. Next, we design two bias mitigation methods, namely embedding-based re-ranking (ER) and greedy substitution (GS), for deep recommender systems. ER is an in-processing mitigation method that equips (α, β)-fairness as a constraint to the objective function of the recommendation algorithm, while GS is a post-processing approach that accepts the biased recommendations as the input, and substitutes high-coverage items with low-coverage ones in the recommendations to satisfy (α, β)-fairness. We evaluate the performance of both mitigation algorithms on two real-world datasets and a set of state-of-the-art deep recommender systems. Our results demonstrate that both ER and GS outperform the existing minimum-coverage (MC) mitigation solutions [Koutsopoulos and Halkidi 2018; Patro et al. 2020] in terms of both fairness and accuracy of recommendations. Furthermore, ER delivers the best trade-off between fairness and recommendation accuracy among a set of alternative mitigation methods, including GS, the hybrid of ER and GS, and the existing MC solutions.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {117–127},
numpages = {11},
keywords = {deep recommender systems, algorithmic fairness in machine learning, Individual fairness},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,What People Think AI Should Infer From Faces,https://dl.acm.org/doi/10.1145/3531146.3533080,"['Severin Engelmann', 'Chiara Ullstein', 'Orestis Papakyriakopoulos', 'Jens Grossklags']",3,"Faces play an indispensable role in human social life. At present, computer vision artificial intelligence (AI) captures and interprets human faces for a variety of digital applications and services. The ambiguity of facial information has recently led to a debate among scholars in different fields about the types of inferences AI should make about people based on their facial looks. AI research often justifies facial AI inference-making by referring to how people form impressions in first-encounter scenarios. Critics raise concerns about bias and discrimination and warn that facial analysis AI resembles an automated version of physiognomy. What has been missing from this debate, however, is an understanding of how ‚Äúnon-experts‚Äù in AI ethically evaluate facial AI inference-making. In a two-scenario vignette study with 24 treatment groups, we show that non-experts (N = 3745) reject facial AI inferences such as trustworthiness and likability from portrait images in a low-stake advertising and a high-stake hiring context. In contrast, non-experts agree with facial AI inferences such as skin color or gender in the advertising but not the hiring decision context. For each AI inference, we ask non-experts to justify their evaluation in a written response. Analyzing 29,760 written justifications, we find that non-experts are either ‚Äúevidentialists‚Äù or ‚Äúpragmatists‚Äù: they assess the ethical status of a facial AI inference based on whether they think faces warrant sufficient or insufficient evidence for an inference (evidentialist justification) or whether making the inference results in beneficial or detrimental outcomes (pragmatist justification). Non-experts‚Äô justifications underscore the normative complexity behind facial AI inference-making. AI inferences with insufficient evidence can be rationalized by considerations of relevance while irrelevant inferences can be justified by reference to sufficient evidence. We argue that participatory approaches contribute valuable insights for the development of ethical AI in an increasingly visual data culture.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3533080,
author = {Engelmann, Severin and Ullstein, Chiara and Papakyriakopoulos, Orestis and Grossklags, Jens},
title = {What People Think AI Should Infer From Faces},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533080},
doi = {10.1145/3531146.3533080},
abstract = {Faces play an indispensable role in human social life. At present, computer vision artificial intelligence (AI) captures and interprets human faces for a variety of digital applications and services. The ambiguity of facial information has recently led to a debate among scholars in different fields about the types of inferences AI should make about people based on their facial looks. AI research often justifies facial AI inference-making by referring to how people form impressions in first-encounter scenarios. Critics raise concerns about bias and discrimination and warn that facial analysis AI resembles an automated version of physiognomy. What has been missing from this debate, however, is an understanding of how “non-experts” in AI ethically evaluate facial AI inference-making. In a two-scenario vignette study with 24 treatment groups, we show that non-experts (N = 3745) reject facial AI inferences such as trustworthiness and likability from portrait images in a low-stake advertising and a high-stake hiring context. In contrast, non-experts agree with facial AI inferences such as skin color or gender in the advertising but not the hiring decision context. For each AI inference, we ask non-experts to justify their evaluation in a written response. Analyzing 29,760 written justifications, we find that non-experts are either “evidentialists” or “pragmatists”: they assess the ethical status of a facial AI inference based on whether they think faces warrant sufficient or insufficient evidence for an inference (evidentialist justification) or whether making the inference results in beneficial or detrimental outcomes (pragmatist justification). Non-experts’ justifications underscore the normative complexity behind facial AI inference-making. AI inferences with insufficient evidence can be rationalized by considerations of relevance while irrelevant inferences can be justified by reference to sufficient evidence. We argue that participatory approaches contribute valuable insights for the development of ethical AI in an increasingly visual data culture.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {128–141},
numpages = {14},
keywords = {computer vision, participatory AI ethics, artificial intelligence, human faces},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,Minimax Demographic Group Fairness in Federated Learning,https://dl.acm.org/doi/10.1145/3531146.3533081,"['Afroditi Papadaki', 'Natalia Martinez', 'Martin Bertran', 'Guillermo Sapiro', 'Miguel Rodrigues']",1,"Federated learning is an increasingly popular paradigm that enables a large number of entities to collaboratively learn better models. In this work, we study minimax group fairness in federated learning scenarios where different participating entities may only have access to a subset of the population groups during the training phase. We formally analyze how our proposed group fairness objective differs from existing federated learning fairness criteria that impose similar performance across participants instead of demographic groups. We provide an optimization algorithm ‚Äì FedMinMax ‚Äì for solving the proposed problem that provably enjoys the performance guarantees of centralized learning algorithms. We experimentally compare the proposed approach against other state-of-the-art methods in terms of group fairness in various federated learning setups, showing that our approach exhibits competitive or superior performance.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3533081,
author = {Papadaki, Afroditi and Martinez, Natalia and Bertran, Martin and Sapiro, Guillermo and Rodrigues, Miguel},
title = {Minimax Demographic Group Fairness in Federated Learning},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533081},
doi = {10.1145/3531146.3533081},
abstract = {Federated learning is an increasingly popular paradigm that enables a large number of entities to collaboratively learn better models. In this work, we study minimax group fairness in federated learning scenarios where different participating entities may only have access to a subset of the population groups during the training phase. We formally analyze how our proposed group fairness objective differs from existing federated learning fairness criteria that impose similar performance across participants instead of demographic groups. We provide an optimization algorithm – FedMinMax – for solving the proposed problem that provably enjoys the performance guarantees of centralized learning algorithms. We experimentally compare the proposed approach against other state-of-the-art methods in terms of group fairness in various federated learning setups, showing that our approach exhibits competitive or superior performance.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {142–159},
numpages = {18},
keywords = {Algorithmic Fairness, Minimax Group Fairness, Federated Learning},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,Taxonomy of Risks posed by Language Models,https://dl.acm.org/doi/10.1145/3531146.3533088,"['Laura Weidinger', 'Jonathan Uesato', 'Maribeth Rauh', 'Conor Griffin', 'Po-Sen Huang', 'John Mellor', 'Amelia Glaese', 'Myra Cheng', 'Borja Balle', 'Atoosa Kasirzadeh', 'Courtney Biles', 'Sasha Brown', 'Zac Kenton', 'Will Hawkins', 'Tom Stepleton', 'Abeba Birhane', 'Lisa Anne Hendricks', 'Laura Rimell', 'William Isaac', 'Julia Haas', 'Sean Legassick', 'Geoffrey Irving', 'Iason Gabriel']",19,"Responsible innovation on large-scale Language Models (LMs) requires foresight into and in-depth understanding of the risks these models may pose. This paper develops a comprehensive taxonomy of ethical and social risks associated with LMs. We identify twenty-one risks, drawing on expertise and literature from computer science, linguistics, and the social sciences. We situate these risks in our taxonomy of six risk areas: I. Discrimination, Hate speech and Exclusion, II. Information Hazards, III. Misinformation Harms, IV. Malicious Uses, V. Human-Computer Interaction Harms, and VI. Environmental and Socioeconomic harms. For risks that have already been observed in LMs, the causal mechanism leading to harm, evidence of the risk, and approaches to risk mitigation are discussed. We further describe and analyse risks that have not yet been observed but are anticipated based on assessments of other language technologies, and situate these in the same taxonomy. We underscore that it is the responsibility of organizations to engage with the mitigations we discuss throughout the paper. We close by highlighting challenges and directions for further research on risk evaluation and mitigation with the goal of ensuring that language models are developed responsibly.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3533088,
author = {Weidinger, Laura and Uesato, Jonathan and Rauh, Maribeth and Griffin, Conor and Huang, Po-Sen and Mellor, John and Glaese, Amelia and Cheng, Myra and Balle, Borja and Kasirzadeh, Atoosa and Biles, Courtney and Brown, Sasha and Kenton, Zac and Hawkins, Will and Stepleton, Tom and Birhane, Abeba and Hendricks, Lisa Anne and Rimell, Laura and Isaac, William and Haas, Julia and Legassick, Sean and Irving, Geoffrey and Gabriel, Iason},
title = {Taxonomy of Risks Posed by Language Models},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533088},
doi = {10.1145/3531146.3533088},
abstract = {Responsible innovation on large-scale Language Models (LMs) requires foresight into and in-depth understanding of the risks these models may pose. This paper develops a comprehensive taxonomy of ethical and social risks associated with LMs. We identify twenty-one risks, drawing on expertise and literature from computer science, linguistics, and the social sciences. We situate these risks in our taxonomy of six risk areas: I. Discrimination, Hate speech and Exclusion, II. Information Hazards, III. Misinformation Harms, IV. Malicious Uses, V. Human-Computer Interaction Harms, and VI. Environmental and Socioeconomic harms. For risks that have already been observed in LMs, the causal mechanism leading to harm, evidence of the risk, and approaches to risk mitigation are discussed. We further describe and analyse risks that have not yet been observed but are anticipated based on assessments of other language technologies, and situate these in the same taxonomy. We underscore that it is the responsibility of organizations to engage with the mitigations we discuss throughout the paper. We close by highlighting challenges and directions for further research on risk evaluation and mitigation with the goal of ensuring that language models are developed responsibly.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {214–229},
numpages = {16},
keywords = {responsible innovation, language models, risk assessment, technology risks, responsible AI},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,Equitable Public Bus Network Optimization for Social Good: A Case Study of Singapore,https://dl.acm.org/doi/10.1145/3531146.3533092,"['David Tedjopurnomo', 'Zhifeng Bao', 'Farhana Choudhury', 'Hui Luo', 'A. K. Qin']",0,"Public bus transport is a major backbone of many cities‚Äô socioeconomic activities. As such, the topic of public bus network optimization has received substantial attention in Geographic Information System (GIS) research. Unfortunately, most of the current literature are focused on improving only the efficiency of the bus network, neglecting the important equity factors. Optimizing only the efficiency of a bus network may cause these limited public transportation resources to be shifted away from areas with disadvantaged demographics, compounding the equity problem. In this work, we make the first attempt to explore the intricacies of the equitable public bus network optimization problem by performing a case study of Singapore‚Äôs public bus network. We describe the challenges in designing an equitable public bus network, tackle the fundamental problem of formulating efficiency and equity metrics, perform exploratory experiments to assess each metric‚Äôs real-life impact, and analyze the challenges of the equitable bus network optimization task. For our experiments, we have curated and combined Singapore‚Äôs bus network data, road network data, census area boundaries data, and demographics data into a unified dataset which we released publicly. Our objective is not only to explore this important yet relatively unexplored problem, but also to inspire more discussion and research.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3533092,
author = {Tedjopurnomo, David and Bao, Zhifeng and Choudhury, Farhana and Luo, Hui and Qin, A. K.},
title = {Equitable Public Bus Network Optimization for Social Good: A Case Study of Singapore},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533092},
doi = {10.1145/3531146.3533092},
abstract = {Public bus transport is a major backbone of many cities’ socioeconomic activities. As such, the topic of public bus network optimization has received substantial attention in Geographic Information System (GIS) research. Unfortunately, most of the current literature are focused on improving only the efficiency of the bus network, neglecting the important equity factors. Optimizing only the efficiency of a bus network may cause these limited public transportation resources to be shifted away from areas with disadvantaged demographics, compounding the equity problem. In this work, we make the first attempt to explore the intricacies of the equitable public bus network optimization problem by performing a case study of Singapore’s public bus network. We describe the challenges in designing an equitable public bus network, tackle the fundamental problem of formulating efficiency and equity metrics, perform exploratory experiments to assess each metric’s real-life impact, and analyze the challenges of the equitable bus network optimization task. For our experiments, we have curated and combined Singapore’s bus network data, road network data, census area boundaries data, and demographics data into a unified dataset which we released publicly. Our objective is not only to explore this important yet relatively unexplored problem, but also to inspire more discussion and research.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {278–288},
numpages = {11},
keywords = {datasets, bus network, equity, public transportation, metric},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,GetFair: Generalized Fairness Tuning of Classification Models,https://dl.acm.org/doi/10.1145/3531146.3533094,"['Sandipan Sikdar', 'Florian Lemmerich', 'Markus Strohmaier']",0,"We present GetFair, a novel framework for tuning fairness of classification models. The fair classification problem deals with training models for a given classification task where data points have sensitive attributes. The goal of fair classification models is to not only generate accurate classification results but also to prevent discrimination against subpopulations (i.e., individuals with a specific value for the sensitive attribute). Existing methods for enhancing fairness of classification models, however, are often specifically designed for a particular fairness metric or a classifier model. They may also not be suitable for scenarios with incomplete training data or where optimizing for multiple fairness metrics is important. GetFair represents a general solution to this problem. The GetFair approach works in the following way: First, a given classifier is trained on training data without any fairness objective. This is followed by a reinforcement learning inspired tuning procedure which updates the parameters of the learned model on a given fairness objective. This disentangles classifier training from fairness tuning, making our framework more general and allowing for the adoption of any parameterized classifier model. Because fairness metrics are designed as reward functions during tuning, GetFair generalizes across any fairness metric. We demonstrate the generalizability of GetFair via evaluation over a benchmark suite of datasets, classification models, and fairness metrics. In addition, GetFair can also be deployed in settings where the training data is incomplete or the classifier needs to be tuned on multiple fairness metrics. GetFair not only contributes a flexible method to the repertoire of tools available to improve the fairness of classification models, it also seamlessly adapts to settings where existing fair classification methods may not be suitable or applicable.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3533094,
author = {Sikdar, Sandipan and Lemmerich, Florian and Strohmaier, Markus},
title = {GetFair: Generalized Fairness Tuning of Classification Models},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533094},
doi = {10.1145/3531146.3533094},
abstract = {We present GetFair, a novel framework for tuning fairness of classification models. The fair classification problem deals with training models for a given classification task where data points have sensitive attributes. The goal of fair classification models is to not only generate accurate classification results but also to prevent discrimination against subpopulations (i.e., individuals with a specific value for the sensitive attribute). Existing methods for enhancing fairness of classification models, however, are often specifically designed for a particular fairness metric or a classifier model. They may also not be suitable for scenarios with incomplete training data or where optimizing for multiple fairness metrics is important. GetFair represents a general solution to this problem. The GetFair approach works in the following way: First, a given classifier is trained on training data without any fairness objective. This is followed by a reinforcement learning inspired tuning procedure which updates the parameters of the learned model on a given fairness objective. This disentangles classifier training from fairness tuning, making our framework more general and allowing for the adoption of any parameterized classifier model. Because fairness metrics are designed as reward functions during tuning, GetFair generalizes across any fairness metric. We demonstrate the generalizability of GetFair via evaluation over a benchmark suite of datasets, classification models, and fairness metrics. In addition, GetFair can also be deployed in settings where the training data is incomplete or the classifier needs to be tuned on multiple fairness metrics. GetFair not only contributes a flexible method to the repertoire of tools available to improve the fairness of classification models, it also seamlessly adapts to settings where existing fair classification methods may not be suitable or applicable.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {289–299},
numpages = {11},
keywords = {fairness metrics, sensitive attribute, Fair classification, classifier models},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,How Different Groups Prioritize Ethical Values for Responsible AI,https://dl.acm.org/doi/10.1145/3531146.3533097,"['Maurice Jakesch', 'Zana Bu√ßinca', 'Saleema Amershi', 'Alexandra Olteanu']",5,"Private companies, public sector organizations, and academic groups have outlined ethical values they consider important for responsible artificial intelligence technologies. While their recommendations converge on a set of central values, little is known about the values a more representative public would find important for the AI technologies they interact with and might be affected by. We conducted a survey examining how individuals perceive and prioritize responsible AI values across three groups: a representative sample of the US population (N=743), a sample of crowdworkers (N=755), and a sample of AI practitioners (N=175). Our results empirically confirm a common concern: AI practitioners‚Äô value priorities differ from those of the general public. Compared to the US-representative sample, AI practitioners appear to consider responsible AI values as less important and emphasize a different set of values. In contrast, self-identified women and black respondents found responsible AI values more important than other groups. Surprisingly, more liberal-leaning participants, rather than participants reporting experiences with discrimination, were more likely to prioritize fairness than other groups. Our findings highlight the importance of paying attention to who gets to define ‚Äúresponsible AI.‚Äù",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3533097,
author = {Jakesch, Maurice and Bu\c{c}inca, Zana and Amershi, Saleema and Olteanu, Alexandra},
title = {How Different Groups Prioritize Ethical Values for Responsible AI},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533097},
doi = {10.1145/3531146.3533097},
abstract = {Private companies, public sector organizations, and academic groups have outlined ethical values they consider important for responsible artificial intelligence technologies. While their recommendations converge on a set of central values, little is known about the values a more representative public would find important for the AI technologies they interact with and might be affected by. We conducted a survey examining how individuals perceive and prioritize responsible AI values across three groups: a representative sample of the US population (N=743), a sample of crowdworkers (N=755), and a sample of AI practitioners (N=175). Our results empirically confirm a common concern: AI practitioners’ value priorities differ from those of the general public. Compared to the US-representative sample, AI practitioners appear to consider responsible AI values as less important and emphasize a different set of values. In contrast, self-identified women and black respondents found responsible AI values more important than other groups. Surprisingly, more liberal-leaning participants, rather than participants reporting experiences with discrimination, were more likely to prioritize fairness than other groups. Our findings highlight the importance of paying attention to who gets to define “responsible AI.”},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {310–323},
numpages = {14},
keywords = {Responsible AI, empirical ethics, value-sensitive design},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,Measuring Representational Harms in Image Captioning,https://dl.acm.org/doi/10.1145/3531146.3533099,"['Angelina Wang', 'Solon Barocas', 'Kristen Laird', 'Hanna Wallach']",4,"Previous work has largely considered the fairness of image captioning systems through the underspecified lens of ‚Äúbias.‚Äù In contrast, we present a set of techniques for measuring five types of representational harms, as well as the resulting measurements obtained for two of the most popular image captioning datasets using a state-of-the-art image captioning system. Our goal was not to audit this image captioning system, but rather to develop normatively grounded measurement techniques, in turn providing an opportunity to reflect on the many challenges involved. We propose multiple measurement techniques for each type of harm. We argue that by doing so, we are better able to capture the multi-faceted nature of each type of harm, in turn improving the (collective) validity of the resulting measurements. Throughout, we discuss the assumptions underlying our measurement approach and point out when they do not hold.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3533099,
author = {Wang, Angelina and Barocas, Solon and Laird, Kristen and Wallach, Hanna},
title = {Measuring Representational Harms in Image Captioning},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533099},
doi = {10.1145/3531146.3533099},
abstract = {Previous work has largely considered the fairness of image captioning systems through the underspecified lens of “bias.” In contrast, we present a set of techniques for measuring five types of representational harms, as well as the resulting measurements obtained for two of the most popular image captioning datasets using a state-of-the-art image captioning system. Our goal was not to audit this image captioning system, but rather to develop normatively grounded measurement techniques, in turn providing an opportunity to reflect on the many challenges involved. We propose multiple measurement techniques for each type of harm. We argue that by doing so, we are better able to capture the multi-faceted nature of each type of harm, in turn improving the (collective) validity of the resulting measurements. Throughout, we discuss the assumptions underlying our measurement approach and point out when they do not hold.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {324–335},
numpages = {12},
keywords = {fairness measurement, harm propagation, image captioning},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,"Towards Intersectionality in Machine Learning: Including More Identities, Handling Underrepresentation, and Performing Evaluation",https://dl.acm.org/doi/10.1145/3531146.3533101,"['Angelina Wang', 'Vikram V Ramaswamy', 'Olga Russakovsky']",9,"Research in machine learning fairness has historically considered a single binary demographic attribute; however, the reality is of course far more complicated. In this work, we grapple with questions that arise along three stages of the machine learning pipeline when incorporating intersectionality as multiple demographic attributes: (1) which demographic attributes to include as dataset labels, (2) how to handle the progressively smaller size of subgroups during model training, and (3) how to move beyond existing evaluation metrics when benchmarking model fairness for more subgroups. For each question, we provide thorough empirical evaluation on tabular datasets derived from the US Census, and present constructive recommendations for the machine learning community. First, we advocate for supplementing domain knowledge with empirical validation when choosing which demographic attribute labels to train on, while always evaluating on the full set of demographic attributes. Second, we warn against using data imbalance techniques without considering their normative implications and suggest an alternative using the structure in the data. Third, we introduce new evaluation metrics which are more appropriate for the intersectional setting. Overall, we provide substantive suggestions on three necessary (albeit not sufficient!) considerations when incorporating intersectionality into machine learning.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3533101,
author = {Wang, Angelina and Ramaswamy, Vikram V and Russakovsky, Olga},
title = {Towards Intersectionality in Machine Learning: Including More Identities, Handling Underrepresentation, and Performing Evaluation},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533101},
doi = {10.1145/3531146.3533101},
abstract = {Research in machine learning fairness has historically considered a single binary demographic attribute; however, the reality is of course far more complicated. In this work, we grapple with questions that arise along three stages of the machine learning pipeline when incorporating intersectionality as multiple demographic attributes: (1) which demographic attributes to include as dataset labels, (2) how to handle the progressively smaller size of subgroups during model training, and (3) how to move beyond existing evaluation metrics when benchmarking model fairness for more subgroups. For each question, we provide thorough empirical evaluation on tabular datasets derived from the US Census, and present constructive recommendations for the machine learning community. First, we advocate for supplementing domain knowledge with empirical validation when choosing which demographic attribute labels to train on, while always evaluating on the full set of demographic attributes. Second, we warn against using data imbalance techniques without considering their normative implications and suggest an alternative using the structure in the data. Third, we introduce new evaluation metrics which are more appropriate for the intersectional setting. Overall, we provide substantive suggestions on three necessary (albeit not sufficient!) considerations when incorporating intersectionality into machine learning.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {336–349},
numpages = {14},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,An Outcome Test of Discrimination for Ranked Lists,https://dl.acm.org/doi/10.1145/3531146.3533102,"['Jonathan Roth', 'Guillaume Saint-Jacques', 'YinYin Yu']",0,"This paper extends Becker [3]‚Äôs outcome test of discrimination to settings where a (human or algorithmic) decision-maker produces a ranked list of candidates. Ranked lists are particularly relevant in the context of online platforms that produce search results or feeds, and also arise when human decisionmakers express ordinal preferences over a list of candidates. We show that non-discrimination implies a system of moment inequalities, which intuitively impose that one cannot permute the position of a lower-ranked candidate from one group with a higher-ranked candidate from a second group and systematically improve the objective. Moreover, we show that that these moment inequalities are the only testable implications of non-discrimination when the auditor observes only outcomes and group membership by rank. We show how to statistically test the implied inequalities, and validate our approach in an application using data from LinkedIn.1",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3533102,
author = {Roth, Jonathan and Saint-Jacques, Guillaume and Yu, YinYin},
title = {An Outcome Test of Discrimination for Ranked Lists},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533102},
doi = {10.1145/3531146.3533102},
abstract = {This paper extends Becker [3]’s outcome test of discrimination to settings where a (human or algorithmic) decision-maker produces a ranked list of candidates. Ranked lists are particularly relevant in the context of online platforms that produce search results or feeds, and also arise when human decisionmakers express ordinal preferences over a list of candidates. We show that non-discrimination implies a system of moment inequalities, which intuitively impose that one cannot permute the position of a lower-ranked candidate from one group with a higher-ranked candidate from a second group and systematically improve the objective. Moreover, we show that that these moment inequalities are the only testable implications of non-discrimination when the auditor observes only outcomes and group membership by rank. We show how to statistically test the implied inequalities, and validate our approach in an application using data from LinkedIn.1},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {350–356},
numpages = {7},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,De-biasing ‚Äúbias‚Äù measurement,https://dl.acm.org/doi/10.1145/3531146.3533105,"['Kristian Lum', 'Yunfeng Zhang', 'Amanda Bower']",5,"When a model‚Äôs performance differs across socially or culturally relevant groups‚Äìlike race, gender, or the intersections of many such groups‚Äìit is often called ‚Äùbiased.‚Äù While much of the work in algorithmic fairness over the last several years has focused on developing various definitions of model fairness (the absence of group-wise model performance disparities) and eliminating such ‚Äúbias,‚Äù much less work has gone into rigorously measuring it. In practice, it important to have high quality, human digestible measures of model performance disparities and associated uncertainty quantification about them that can serve as inputs into multi-faceted decision-making processes. In this paper, we show both mathematically and through simulation that many of the metrics used to measure group-wise model performance disparities are themselves statistically biased estimators of the underlying quantities they purport to represent. We argue that this can cause misleading conclusions about the relative group-wise model performance disparities along different dimensions, especially in cases where some sensitive variables consist of categories with few members. We propose the ‚Äúdouble-corrected‚Äù variance estimator, which provides unbiased estimates and uncertainty quantification of the variance of model performance across groups. It is conceptually simple and easily implementable without statistical software package or numerical optimization. We demonstrate the utility of this approach through simulation and show on a real dataset that while statistically biased estimators of model group-wise model performance disparities indicate statistically significant between-group model performance disparities, when accounting for statistical bias in the estimator, the estimated group-wise disparities in model performance are no longer statistically significant.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3533105,
author = {Lum, Kristian and Zhang, Yunfeng and Bower, Amanda},
title = {De-Biasing “Bias” Measurement},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533105},
doi = {10.1145/3531146.3533105},
abstract = {When a model’s performance differs across socially or culturally relevant groups–like race, gender, or the intersections of many such groups–it is often called ”biased.” While much of the work in algorithmic fairness over the last several years has focused on developing various definitions of model fairness (the absence of group-wise model performance disparities) and eliminating such “bias,” much less work has gone into rigorously measuring it. In practice, it important to have high quality, human digestible measures of model performance disparities and associated uncertainty quantification about them that can serve as inputs into multi-faceted decision-making processes. In this paper, we show both mathematically and through simulation that many of the metrics used to measure group-wise model performance disparities are themselves statistically biased estimators of the underlying quantities they purport to represent. We argue that this can cause misleading conclusions about the relative group-wise model performance disparities along different dimensions, especially in cases where some sensitive variables consist of categories with few members. We propose the “double-corrected” variance estimator, which provides unbiased estimates and uncertainty quantification of the variance of model performance across groups. It is conceptually simple and easily implementable without statistical software package or numerical optimization. We demonstrate the utility of this approach through simulation and show on a real dataset that while statistically biased estimators of model group-wise model performance disparities indicate statistically significant between-group model performance disparities, when accounting for statistical bias in the estimator, the estimated group-wise disparities in model performance are no longer statistically significant.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {379–389},
numpages = {11},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,"Four Years of FAccT: A Reflexive, Mixed-Methods Analysis of Research Contributions, Shortcomings, and Future Prospects",https://dl.acm.org/doi/10.1145/3531146.3533107,"['Benjamin Laufer', 'Sameer Jain', 'A. Feder Cooper', 'Jon Kleinberg', 'Hoda Heidari']",2,"Fairness, Accountability, and Transparency (FAccT) for socio-technical systems has been a thriving area of research in recent years. An ACM conference bearing the same name has been the central venue for scholars in this area to come together, provide peer feedback to one another, and publish their work. This reflexive study aims to shed light on FAccT‚Äôs activities to date and identify major gaps and opportunities for translating contributions into broader positive impact. To this end, we utilize a mixed-methods research design. On the qualitative front, we develop a protocol for reviewing and coding prior FAccT papers, tracing their distribution of topics, methods, datasets, and disciplinary roots. We also design and administer a questionnaire to reflect the voices of FAccT community members and affiliates on a wide range of topics. On the quantitative front, we use the full text and citation network associated with prior FAccT publications to provide further evidence about topics and values represented in FAccT. We organize the findings from our analysis into four main dimensions: the themes present in FAccT scholarship, the values that underpin the work, the impact of the contributions both within academic circles and beyond, and the practices and informal norms of the community that has formed around FAccT. Finally, our work identifies several suggestions on directions for change, as voiced by community members.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3533107,
author = {Laufer, Benjamin and Jain, Sameer and Cooper, A. Feder and Kleinberg, Jon and Heidari, Hoda},
title = {Four Years of FAccT: A Reflexive, Mixed-Methods Analysis of Research Contributions, Shortcomings, and Future Prospects},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533107},
doi = {10.1145/3531146.3533107},
abstract = {Fairness, Accountability, and Transparency (FAccT) for socio-technical systems has been a thriving area of research in recent years. An ACM conference bearing the same name has been the central venue for scholars in this area to come together, provide peer feedback to one another, and publish their work. This reflexive study aims to shed light on FAccT’s activities to date and identify major gaps and opportunities for translating contributions into broader positive impact. To this end, we utilize a mixed-methods research design. On the qualitative front, we develop a protocol for reviewing and coding prior FAccT papers, tracing their distribution of topics, methods, datasets, and disciplinary roots. We also design and administer a questionnaire to reflect the voices of FAccT community members and affiliates on a wide range of topics. On the quantitative front, we use the full text and citation network associated with prior FAccT publications to provide further evidence about topics and values represented in FAccT. We organize the findings from our analysis into four main dimensions: the themes present in FAccT scholarship, the values that underpin the work, the impact of the contributions both within academic circles and beyond, and the practices and informal norms of the community that has formed around FAccT. Finally, our work identifies several suggestions on directions for change, as voiced by community members.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {401–426},
numpages = {26},
keywords = {community perspectives, reflexivity, topics, impact, mixed methods, values, FAccT},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,Exploring How Machine Learning Practitioners (Try To) Use Fairness Toolkits,https://dl.acm.org/doi/10.1145/3531146.3533113,"['Wesley Hanwen Deng', 'Manish Nagireddy', 'Michelle Seng Ah Lee', 'Jatinder Singh', 'Zhiwei Steven Wu', 'Kenneth Holstein', 'Haiyi Zhu']",5,"Recent years have seen the development of many open-source ML fairness toolkits aimed at helping ML practitioners assess and address unfairness in their systems. However, there has been little research investigating how ML practitioners actually use these toolkits in practice. In this paper, we conducted the first in-depth empirical exploration of how industry practitioners (try to) work with existing fairness toolkits. In particular, we conducted think-aloud interviews to understand how participants learn about and use fairness toolkits, and explored the generality of our findings through an anonymous online survey. We identified several opportunities for fairness toolkits to better address practitioner needs and scaffold them in using toolkits effectively and responsibly. Based on these findings, we highlight implications for the design of future open-source fairness toolkits that can support practitioners in better contextualizing, communicating, and collaborating around ML fairness efforts.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3533113,
author = {Deng, Wesley Hanwen and Nagireddy, Manish and Lee, Michelle Seng Ah and Singh, Jatinder and Wu, Zhiwei Steven and Holstein, Kenneth and Zhu, Haiyi},
title = {Exploring How Machine Learning Practitioners (Try To) Use Fairness Toolkits},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533113},
doi = {10.1145/3531146.3533113},
abstract = {Recent years have seen the development of many open-source ML fairness toolkits aimed at helping ML practitioners assess and address unfairness in their systems. However, there has been little research investigating how ML practitioners actually use these toolkits in practice. In this paper, we conducted the first in-depth empirical exploration of how industry practitioners (try to) work with existing fairness toolkits. In particular, we conducted think-aloud interviews to understand how participants learn about and use fairness toolkits, and explored the generality of our findings through an anonymous online survey. We identified several opportunities for fairness toolkits to better address practitioner needs and scaffold them in using toolkits effectively and responsibly. Based on these findings, we highlight implications for the design of future open-source fairness toolkits that can support practitioners in better contextualizing, communicating, and collaborating around ML fairness efforts.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {473–484},
numpages = {12},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,Are ‚ÄúIntersectionally Fair‚Äù AI Algorithms Really Fair to Women of Color? A Philosophical Analysis,https://dl.acm.org/doi/10.1145/3531146.3533114,['Youjin Kong'],2,"A growing number of studies on fairness in artificial intelligence (AI) use the notion of intersectionality to measure AI fairness. Most of these studies take intersectional fairness to be a matter of statistical parity among intersectional subgroups: an AI algorithm is ‚Äúintersectionally fair‚Äù if the probability of the outcome is roughly the same across all subgroups defined by different combinations of the protected attributes. This paper identifies and examines three fundamental problems with this dominant interpretation of intersectional fairness in AI. First, the dominant approach is so preoccupied with the intersection of attributes/categories (e.g., race, gender) that it fails to address the intersection of oppression (e.g., racism, sexism), which is more central to intersectionality as a critical framework. Second, the dominant approach faces a dilemma between infinite regress and fairness gerrymandering: it either keeps splitting groups into smaller subgroups or arbitrarily selects protected groups. Lastly, the dominant view fails to capture what it really means for AI algorithms to be fair, in terms of both distributive and non-distributive fairness. I distinguish a strong sense of AI fairness from a weak sense that is prevalent in the literature, and conclude by envisioning paths towards strong intersectional fairness in AI.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3533114,
author = {Kong, Youjin},
title = {Are “Intersectionally Fair” AI Algorithms Really Fair to Women of Color? A Philosophical Analysis},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533114},
doi = {10.1145/3531146.3533114},
abstract = {A growing number of studies on fairness in artificial intelligence (AI) use the notion of intersectionality to measure AI fairness. Most of these studies take intersectional fairness to be a matter of statistical parity among intersectional subgroups: an AI algorithm is “intersectionally fair” if the probability of the outcome is roughly the same across all subgroups defined by different combinations of the protected attributes. This paper identifies and examines three fundamental problems with this dominant interpretation of intersectional fairness in AI. First, the dominant approach is so preoccupied with the intersection of attributes/categories (e.g., race, gender) that it fails to address the intersection of oppression (e.g., racism, sexism), which is more central to intersectionality as a critical framework. Second, the dominant approach faces a dilemma between infinite regress and fairness gerrymandering: it either keeps splitting groups into smaller subgroups or arbitrarily selects protected groups. Lastly, the dominant view fails to capture what it really means for AI algorithms to be fair, in terms of both distributive and non-distributive fairness. I distinguish a strong sense of AI fairness from a weak sense that is prevalent in the literature, and conclude by envisioning paths towards strong intersectional fairness in AI.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {485–494},
numpages = {10},
keywords = {Intersectionality, Philosophical Analysis of Fairness, Fairness and Bias in AI, Feminist and Critical Race Social Philosophy},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,Affirmative Algorithms: Relational Equality as Algorithmic Fairness,https://dl.acm.org/doi/10.1145/3531146.3533115,['Marilyn Zhang'],2,"Many statistical fairness notions have been proposed for algorithmic decision-making systems, and especially public safety pretrial risk assessment (PSPRA) algorithms such as COMPAS. Most fairness notions equalize something between groups, whether it is false positive rates or accuracy. In fact, I demonstrate that most prominent notions have their basis in equalizing some form of accuracy. However, statistical fairness metrics often do not capture the substantive point of equality. I argue that equal accuracy is not only difficult to measure but also unsatisfactory for ensuring equal justice. In response, I introduce philosopher Elizabeth Anderson‚Äôs theory of relational equality as a fruitful alternative framework: to relate as equals, people need access to certain basic capabilities. I show that relational equality requires Affirmative PSPRA algorithms that lower risk scores for Black defendants. This is because fairness based on relational equality means considering the impact of PSPRA algorithms‚Äô decisions on access to basic capabilities. This impact is racially asymmetric in an unjust society. I make three main contributions: (1) I illustrate the shortcomings of statistical fairness notions in their reliance on equalizing some form of accuracy; (2) I present the first comprehensive ethical defense of Affirmative PSPRA algorithms, based on fairness in terms of relational equality instead; and (3) I show that equalizing accuracy is neither sufficient nor necessary for fairness based on relational equality. Overall, this work serves narrowly as a reason to re-evaluate algorithmic fairness for PSPRA algorithms, and serves broadly as an example of how discussions of algorithmic fairness can benefit from egalitarian philosophical frameworks.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3533115,
author = {Zhang, Marilyn},
title = {Affirmative Algorithms: Relational Equality as Algorithmic Fairness},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533115},
doi = {10.1145/3531146.3533115},
abstract = {Many statistical fairness notions have been proposed for algorithmic decision-making systems, and especially public safety pretrial risk assessment (PSPRA) algorithms such as COMPAS. Most fairness notions equalize something between groups, whether it is false positive rates or accuracy. In fact, I demonstrate that most prominent notions have their basis in equalizing some form of accuracy. However, statistical fairness metrics often do not capture the substantive point of equality. I argue that equal accuracy is not only difficult to measure but also unsatisfactory for ensuring equal justice. In response, I introduce philosopher Elizabeth Anderson’s theory of relational equality as a fruitful alternative framework: to relate as equals, people need access to certain basic capabilities. I show that relational equality requires Affirmative PSPRA algorithms that lower risk scores for Black defendants. This is because fairness based on relational equality means considering the impact of PSPRA algorithms’ decisions on access to basic capabilities. This impact is racially asymmetric in an unjust society. I make three main contributions: (1) I illustrate the shortcomings of statistical fairness notions in their reliance on equalizing some form of accuracy; (2) I present the first comprehensive ethical defense of Affirmative PSPRA algorithms, based on fairness in terms of relational equality instead; and (3) I show that equalizing accuracy is neither sufficient nor necessary for fairness based on relational equality. Overall, this work serves narrowly as a reason to re-evaluate algorithmic fairness for PSPRA algorithms, and serves broadly as an example of how discussions of algorithmic fairness can benefit from egalitarian philosophical frameworks.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {495–507},
numpages = {13},
keywords = {algorithmic fairness, affirmative algorithms, pretrial risk assessments, philosophy, fairness, criminal justice, relational equality},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,Best vs. All: Equity and Accuracy of Standardized Test Score Reporting,https://dl.acm.org/doi/10.1145/3531146.3533121,"['Mingzi Niu', 'Sampath Kannan', 'Aaron Roth', 'Rakesh Vohra']",4,"We study a game theoretic model of standardized testing for college admissions. Students are of two types; High and Low. There is a college that would like to admit the High type students. Students take a potentially costly standardized exam which provides a noisy signal of their type. The students come from two populations, which are identical in talent (i.e. the type distribution is the same), but differ in their access to resources: the higher resourced population can at their option take the exam multiple times, whereas the lower resourced population can only take the exam once. We study two models of score reporting, which capture existing policies used by colleges. The first policy (sometimes known as ‚Äúsuper-scoring‚Äù) allows students to report the max of the scores they achieve. The other policy requires that all scores be reported. We find in our model that requiring that all scores be reported results in superior outcomes in equilibrium, both from the perspective of the college (the admissions rule is more accurate), and from the perspective of equity across populations: a student‚Äôs probability of admission is independent of their population, conditional on their type. In particular, the false positive rates and false negative rates are identical in this setting, across the highly and poorly resourced student populations. This is the case despite the fact that the more highly resourced students can‚Äîat their option‚Äîeither report a more accurate signal of their type, or pool with the lower resourced population under this policy. This represents an unusual situation in the algorithmic fairness literature where the goals of accuracy and equity are in alignment, and do not need to be traded off against one another.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3533121,
author = {Niu, Mingzi and Kannan, Sampath and Roth, Aaron and Vohra, Rakesh},
title = {Best vs. All: Equity and Accuracy of Standardized Test Score Reporting},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533121},
doi = {10.1145/3531146.3533121},
abstract = {We study a game theoretic model of standardized testing for college admissions. Students are of two types; High and Low. There is a college that would like to admit the High type students. Students take a potentially costly standardized exam which provides a noisy signal of their type. The students come from two populations, which are identical in talent (i.e. the type distribution is the same), but differ in their access to resources: the higher resourced population can at their option take the exam multiple times, whereas the lower resourced population can only take the exam once. We study two models of score reporting, which capture existing policies used by colleges. The first policy (sometimes known as “super-scoring”) allows students to report the max of the scores they achieve. The other policy requires that all scores be reported. We find in our model that requiring that all scores be reported results in superior outcomes in equilibrium, both from the perspective of the college (the admissions rule is more accurate), and from the perspective of equity across populations: a student’s probability of admission is independent of their population, conditional on their type. In particular, the false positive rates and false negative rates are identical in this setting, across the highly and poorly resourced student populations. This is the case despite the fact that the more highly resourced students can—at their option—either report a more accurate signal of their type, or pool with the lower resourced population under this policy. This represents an unusual situation in the algorithmic fairness literature where the goals of accuracy and equity are in alignment, and do not need to be traded off against one another.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {574–586},
numpages = {13},
keywords = {screen accuracy, score reporting policies, algorithm fairness},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,Fairness for AUC via Feature Augmentation,https://dl.acm.org/doi/10.1145/3531146.3533126,"['Hortense Fong', 'Vineet Kumar', 'Anay Mehrotra', 'Nisheeth K. Vishnoi']",2,"We study fairness in the context of classification where the performance is measured by the area under the curve (AUC) of the receiver operating characteristic. AUC is commonly used when both Type¬†I (false positive) and Type II (false negative) errors are important. However, the same classifier can have significantly varying AUCs for different protected groups and, in real-world applications, it is often desirable to reduce such cross-group differences. We address the problem of how to select additional features to most greatly improve AUC for the disadvantaged group. Our results establish that the unconditional variance of features does not inform us about AUC fairness but class-conditional variance does. Using this connection, we develop a novel approach, fairAUC, based on feature augmentation (adding features) to mitigate bias between identifiable groups. We evaluate fairAUC on synthetic and real-world (COMPAS) datasets and find that it significantly improves AUC for the disadvantaged group relative to benchmarks maximizing overall AUC and minimizing bias between groups.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3533126,
author = {Fong, Hortense and Kumar, Vineet and Mehrotra, Anay and Vishnoi, Nisheeth K.},
title = {Fairness for AUC via Feature Augmentation},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533126},
doi = {10.1145/3531146.3533126},
abstract = {We study fairness in the context of classification where the performance is measured by the area under the curve (AUC) of the receiver operating characteristic. AUC is commonly used when both Type I (false positive) and Type II (false negative) errors are important. However, the same classifier can have significantly varying AUCs for different protected groups and, in real-world applications, it is often desirable to reduce such cross-group differences. We address the problem of how to select additional features to most greatly improve AUC for the disadvantaged group. Our results establish that the unconditional variance of features does not inform us about AUC fairness but class-conditional variance does. Using this connection, we develop a novel approach, fairAUC, based on feature augmentation (adding features) to mitigate bias between identifiable groups. We evaluate fairAUC on synthetic and real-world (COMPAS) datasets and find that it significantly improves AUC for the disadvantaged group relative to benchmarks maximizing overall AUC and minimizing bias between groups.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {610},
numpages = {1},
keywords = {Classification, Area under the ROC curve (AUC), feature augmentation, data collection and curation},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,Subverting Fair Image Search with Generative Adversarial Perturbations,https://dl.acm.org/doi/10.1145/3531146.3533128,"['Avijit Ghosh', 'Matthew Jagielski', 'Christo Wilson']",0,"In this work we explore the intersection fairness and robustness in the context of ranking: when a ranking model has been calibrated to achieve some definition of fairness, is it possible for an external adversary to make the ranking model behave unfairly without having access to the model or training data? To investigate this question, we present a case study in which we develop and then attack a state-of-the-art, fairness-aware image search engine using images that have been maliciously modified using a Generative Adversarial Perturbation (GAP) model¬†[75]. These perturbations attempt to cause the fair re-ranking algorithm to unfairly boost the rank of images containing people from an adversary-selected subpopulation. We present results from extensive experiments demonstrating that our attacks can successfully confer significant unfair advantage to people from the majority class relative to fairly-ranked baseline search results. We demonstrate that our attacks are robust across a number of variables, that they have close to zero impact on the relevance of search results, and that they succeed under a strict threat model. Our findings highlight the danger of deploying fair machine learning algorithms in-the-wild when (1) the data necessary to achieve fairness may be adversarially manipulated, and (2) the models themselves are not robust against attacks.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3533128,
author = {Ghosh, Avijit and Jagielski, Matthew and Wilson, Christo},
title = {Subverting Fair Image Search with Generative Adversarial Perturbations},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533128},
doi = {10.1145/3531146.3533128},
abstract = {In this work we explore the intersection fairness and robustness in the context of ranking: when a ranking model has been calibrated to achieve some definition of fairness, is it possible for an external adversary to make the ranking model behave unfairly without having access to the model or training data? To investigate this question, we present a case study in which we develop and then attack a state-of-the-art, fairness-aware image search engine using images that have been maliciously modified using a Generative Adversarial Perturbation (GAP) model [75]. These perturbations attempt to cause the fair re-ranking algorithm to unfairly boost the rank of images containing people from an adversary-selected subpopulation. We present results from extensive experiments demonstrating that our attacks can successfully confer significant unfair advantage to people from the majority class relative to fairly-ranked baseline search results. We demonstrate that our attacks are robust across a number of variables, that they have close to zero impact on the relevance of search results, and that they succeed under a strict threat model. Our findings highlight the danger of deploying fair machine learning algorithms in-the-wild when (1) the data necessary to achieve fairness may be adversarially manipulated, and (2) the models themselves are not robust against attacks.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {637–650},
numpages = {14},
keywords = {Adversarial Machine Learning, Information Retrieval, Fair Ranking, Demographic Inference},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,Towards Intersectional Feminist and Participatory ML: A Case Study in Supporting Feminicide Counterdata Collection,https://dl.acm.org/doi/10.1145/3531146.3533132,"['Harini Suresh', 'Rajiv Movva', 'Amelia Lee Dogan', 'Rahul Bhargava', 'Isadora Cruxen', 'Angeles Martinez Cuba', 'Guilia Taurino', 'Wonyoung So', ""Catherine D'Ignazio""]",1,"Data ethics and fairness have emerged as important areas of research in recent years. However, much work in this area focuses on retroactively auditing and ‚Äúmitigating bias‚Äù in existing, potentially flawed systems, without interrogating the deeper structural inequalities underlying them. There are not yet examples of how to apply feminist and participatory methodologies from the start, to conceptualize and design machine learning-based tools that center and aim to challenge power inequalities. Our work targets this more prospective goal. Guided by the framework of data feminism, we co-design datasets and machine learning models to support the efforts of activists who collect and monitor data about feminicide¬†‚Äî¬†gender-based killings of women and girls. We describe how intersectional feminist goals and participatory processes shaped each stage of our approach, from problem conceptualization to data collection to model evaluation. We highlight several methodological contributions, including 1) an iterative data collection and annotation process that targets model weaknesses and interrogates framing concepts (such as who is included/excluded in ‚Äúfeminicide‚Äù), 2) models that explicitly focus on intersectional identities rather than statistical majorities, and 3) a multi-step evaluation process¬†‚Äî¬†with quantitative, qualitative and participatory steps¬†‚Äî¬†focused on context-specific relevance. We also distill insights and tensions that arise from bridging intersectional feminist goals with ML. These include reflections on how ML may challenge power, embrace pluralism, rethink binaries and consider context, as well as the inherent limitations of any technology-based solution to address durable structural inequalities.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3533132,
author = {Suresh, Harini and Movva, Rajiv and Dogan, Amelia Lee and Bhargava, Rahul and Cruxen, Isadora and Cuba, Angeles Martinez and Taurino, Guilia and So, Wonyoung and D'Ignazio, Catherine},
title = {Towards Intersectional Feminist and Participatory ML: A Case Study in Supporting Feminicide Counterdata Collection},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533132},
doi = {10.1145/3531146.3533132},
abstract = {Data ethics and fairness have emerged as important areas of research in recent years. However, much work in this area focuses on retroactively auditing and “mitigating bias” in existing, potentially flawed systems, without interrogating the deeper structural inequalities underlying them. There are not yet examples of how to apply feminist and participatory methodologies from the start, to conceptualize and design machine learning-based tools that center and aim to challenge power inequalities. Our work targets this more prospective goal. Guided by the framework of data feminism, we co-design datasets and machine learning models to support the efforts of activists who collect and monitor data about feminicide — gender-based killings of women and girls. We describe how intersectional feminist goals and participatory processes shaped each stage of our approach, from problem conceptualization to data collection to model evaluation. We highlight several methodological contributions, including 1) an iterative data collection and annotation process that targets model weaknesses and interrogates framing concepts (such as who is included/excluded in “feminicide”), 2) models that explicitly focus on intersectional identities rather than statistical majorities, and 3) a multi-step evaluation process — with quantitative, qualitative and participatory steps — focused on context-specific relevance. We also distill insights and tensions that arise from bridging intersectional feminist goals with ML. These include reflections on how ML may challenge power, embrace pluralism, rethink binaries and consider context, as well as the inherent limitations of any technology-based solution to address durable structural inequalities.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {667–678},
numpages = {12},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,Achieving Fairness via Post-Processing in Web-Scale Recommender Systems‚ú±,https://dl.acm.org/doi/10.1145/3531146.3533136,"['Preetam Nandy', 'Cyrus DiCiccio', 'Divya Venugopalan', 'Heloise Logan', 'Kinjal Basu', 'Noureddine El Karoui']",7,"Building fair recommender systems is a challenging and crucial area of study due to its immense impact on society. We extended the definitions of two commonly accepted notions of fairness to recommender systems, namely equality of opportunity and equalized odds. These fairness measures ensure that equally ‚Äúqualified‚Äù (or ‚Äúunqualified‚Äù) candidates are treated equally regardless of their protected attribute status (such as gender or race). We propose scalable methods for achieving equality of opportunity and equalized odds in rankings in the presence of position bias, which commonly plagues data generated from recommender systems. Our algorithms are model agnostic in the sense that they depend only on the final scores provided by a model, making them easily applicable to virtually all web-scale recommender systems. We conduct extensive simulations as well as real-world experiments to show the efficacy of our approach.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3533136,
author = {Nandy, Preetam and DiCiccio, Cyrus and Venugopalan, Divya and Logan, Heloise and Basu, Kinjal and El Karoui, Noureddine},
title = {Achieving Fairness via Post-Processing in Web-Scale Recommender Systems✱},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533136},
doi = {10.1145/3531146.3533136},
abstract = {Building fair recommender systems is a challenging and crucial area of study due to its immense impact on society. We extended the definitions of two commonly accepted notions of fairness to recommender systems, namely equality of opportunity and equalized odds. These fairness measures ensure that equally “qualified” (or “unqualified”) candidates are treated equally regardless of their protected attribute status (such as gender or race). We propose scalable methods for achieving equality of opportunity and equalized odds in rankings in the presence of position bias, which commonly plagues data generated from recommender systems. Our algorithms are model agnostic in the sense that they depend only on the final scores provided by a model, making them easily applicable to virtually all web-scale recommender systems. We conduct extensive simulations as well as real-world experiments to show the efficacy of our approach.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {715–725},
numpages = {11},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,Robots Enact Malignant Stereotypes,https://dl.acm.org/doi/10.1145/3531146.3533138,"['Andrew Hundt', 'William Agnew', 'Vicky Zeng', 'Severin Kacianka', 'Matthew Gombolay']",1,"Stereotypes, bias, and discrimination have been extensively documented in Machine Learning (ML) methods such as Computer Vision (CV)¬†[18, 80], Natural Language Processing (NLP)¬†[6], or both, in the case of large image and caption models such as OpenAI CLIP¬†[14]. In this paper, we evaluate how ML bias manifests in robots that physically and autonomously act within the world. We audit one of several recently published CLIP-powered robotic manipulation methods, presenting it with objects that have pictures of human faces on the surface which vary across race and gender, alongside task descriptions that contain terms associated with common stereotypes. Our experiments definitively show robots acting out toxic stereotypes with respect to gender, race, and scientifically-discredited physiognomy, at scale. Furthermore, the audited methods are less likely to recognize Women and People of Color. Our interdisciplinary sociotechnical analysis synthesizes across fields and applications such as Science Technology and Society (STS), Critical Studies, History, Safety, Robotics, and AI. We find that robots powered by large datasets and Dissolution Models (sometimes called ‚Äúfoundation models‚Äù, e.g. CLIP) that contain humans risk physically amplifying malignant stereotypes in general; and that merely correcting disparities will be insufficient for the complexity and scale of the problem. Instead, we recommend that robot learning methods that physically manifest stereotypes or other harmful outcomes be paused, reworked, or even wound down when appropriate, until outcomes can be proven safe, effective, and just. Finally, we discuss comprehensive policy changes and the potential of new interdisciplinary research on topics like Identity Safety Assessment Frameworks and Design Justice to better understand and address these harms.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3533138,
author = {Hundt, Andrew and Agnew, William and Zeng, Vicky and Kacianka, Severin and Gombolay, Matthew},
title = {Robots Enact Malignant Stereotypes},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533138},
doi = {10.1145/3531146.3533138},
abstract = {Stereotypes, bias, and discrimination have been extensively documented in Machine Learning (ML) methods such as Computer Vision (CV) [18, 80], Natural Language Processing (NLP) [6], or both, in the case of large image and caption models such as OpenAI CLIP [14]. In this paper, we evaluate how ML bias manifests in robots that physically and autonomously act within the world. We audit one of several recently published CLIP-powered robotic manipulation methods, presenting it with objects that have pictures of human faces on the surface which vary across race and gender, alongside task descriptions that contain terms associated with common stereotypes. Our experiments definitively show robots acting out toxic stereotypes with respect to gender, race, and scientifically-discredited physiognomy, at scale. Furthermore, the audited methods are less likely to recognize Women and People of Color. Our interdisciplinary sociotechnical analysis synthesizes across fields and applications such as Science Technology and Society (STS), Critical Studies, History, Safety, Robotics, and AI. We find that robots powered by large datasets and Dissolution Models (sometimes called “foundation models”, e.g. CLIP) that contain humans risk physically amplifying malignant stereotypes in general; and that merely correcting disparities will be insufficient for the complexity and scale of the problem. Instead, we recommend that robot learning methods that physically manifest stereotypes or other harmful outcomes be paused, reworked, or even wound down when appropriate, until outcomes can be proven safe, effective, and just. Finally, we discuss comprehensive policy changes and the potential of new interdisciplinary research on topics like Identity Safety Assessment Frameworks and Design Justice to better understand and address these harms.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {743–756},
numpages = {14},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,Imperfect Inferences: A Practical Assessment,https://dl.acm.org/doi/10.1145/3531146.3533140,"['Aaron Rieke', 'Vincent Southerland', 'Dan Svirsky', 'Mingwei Hsu']",1,"Measuring racial disparities is challenging, especially when demographic labels are unavailable. Recently, some researchers and advocates have argued that companies should infer race and other demographic factors to help them understand and address discrimination. Others have been more skeptical, emphasizing the inaccuracy of racial inferences, critiquing the conceptualization of demographic categories themselves, and arguing that the use of demographic data might encourage algorithmic tweaks where more radical interventions are needed. We conduct a novel empirical analysis that informs this debate, using a dataset of self-reported demographic information provided by users of the ride-hailing service Uber who consented to share this information for research purposes. As a threshold matter, we show how this data reflects the enduring power of racism in society. We find differences by race across a range of outcomes. For example, among self-reported African-American riders, we see racial differences on factors from iOS use to local pollution levels. We then turn to a practical assessment of racial inference methodologies and offer two key findings. First, every inference method we tested has significant errors, miscategorizing people relative to their self-reports (even as the self-reports themselves suffer from selection bias). Second, and most importantly, we found that the inference methods worked: they reliably confirmed directional racial disparities that we knew were reflected in our dataset. Our analysis also suggests that the choice of inference methods should be informed by the measurement task. For example, disparities that are geographic in nature might be best captured by inferences that rely on geography; discrimination based on a person‚Äôs name might be best detected by inferences that rely on names. In conclusion, our analysis shows that common racial inference methods have real and practical utility in shedding light on aggregate, directional disparities, despite their imperfections. While the recent literature has identified notable challenges regarding the collection and use of this data, these challenges should not be seen as dispositive.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3533140,
author = {Rieke, Aaron and Southerland, Vincent and Svirsky, Dan and Hsu, Mingwei},
title = {Imperfect Inferences: A Practical Assessment},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533140},
doi = {10.1145/3531146.3533140},
abstract = {Measuring racial disparities is challenging, especially when demographic labels are unavailable. Recently, some researchers and advocates have argued that companies should infer race and other demographic factors to help them understand and address discrimination. Others have been more skeptical, emphasizing the inaccuracy of racial inferences, critiquing the conceptualization of demographic categories themselves, and arguing that the use of demographic data might encourage algorithmic tweaks where more radical interventions are needed. We conduct a novel empirical analysis that informs this debate, using a dataset of self-reported demographic information provided by users of the ride-hailing service Uber who consented to share this information for research purposes. As a threshold matter, we show how this data reflects the enduring power of racism in society. We find differences by race across a range of outcomes. For example, among self-reported African-American riders, we see racial differences on factors from iOS use to local pollution levels. We then turn to a practical assessment of racial inference methodologies and offer two key findings. First, every inference method we tested has significant errors, miscategorizing people relative to their self-reports (even as the self-reports themselves suffer from selection bias). Second, and most importantly, we found that the inference methods worked: they reliably confirmed directional racial disparities that we knew were reflected in our dataset. Our analysis also suggests that the choice of inference methods should be informed by the measurement task. For example, disparities that are geographic in nature might be best captured by inferences that rely on geography; discrimination based on a person’s name might be best detected by inferences that rely on names. In conclusion, our analysis shows that common racial inference methods have real and practical utility in shedding light on aggregate, directional disparities, despite their imperfections. While the recent literature has identified notable challenges regarding the collection and use of this data, these challenges should not be seen as dispositive.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {767–777},
numpages = {11},
keywords = {demographics, fairness, race, civil rights, discrimination, inference},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,Trucks Don‚Äôt Mean Trump: Diagnosing Human Error in Image Analysis,https://dl.acm.org/doi/10.1145/3531146.3533145,"['J.D. Zamfirescu-Pereira', 'Jerry Chen', 'Emily Wen', 'Allison Koenecke', 'Nikhil Garg', 'Emma Pierson']",0,"Algorithms provide powerful tools for detecting and dissecting human bias and error. Here, we develop machine learning methods to to analyze how humans err in a particular high-stakes task: image interpretation. We leverage a unique dataset of 16,135,392 human predictions of whether a neighborhood voted for Donald Trump or Joe Biden in the 2020 US election, based on a Google Street View image. We show that by training a machine learning estimator of the Bayes optimal decision for each image, we can provide an actionable decomposition of human error into bias, variance, and noise terms, and further identify specific features (like pickup trucks) which lead humans astray. Our methods can be applied to ensure that human-in-the-loop decision-making is accurate and fair and are also applicable to black-box algorithmic systems.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3533145,
author = {Zamfirescu-Pereira, J.D. and Chen, Jerry and Wen, Emily and Koenecke, Allison and Garg, Nikhil and Pierson, Emma},
title = {Trucks Don’t Mean Trump: Diagnosing Human Error in Image Analysis},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533145},
doi = {10.1145/3531146.3533145},
abstract = {Algorithms provide powerful tools for detecting and dissecting human bias and error. Here, we develop machine learning methods to to analyze how humans err in a particular high-stakes task: image interpretation. We leverage a unique dataset of 16,135,392 human predictions of whether a neighborhood voted for Donald Trump or Joe Biden in the 2020 US election, based on a Google Street View image. We show that by training a machine learning estimator of the Bayes optimal decision for each image, we can provide an actionable decomposition of human error into bias, variance, and noise terms, and further identify specific features (like pickup trucks) which lead humans astray. Our methods can be applied to ensure that human-in-the-loop decision-making is accurate and fair and are also applicable to black-box algorithmic systems.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {799–813},
numpages = {15},
keywords = {human error, diagnosing bias, image analysis},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,Fair Representation Clustering with Several Protected Classes,https://dl.acm.org/doi/10.1145/3531146.3533146,"['Zhen Dai', 'Yury Makarychev', 'Ali Vakilian']",0,"We study the problem of fair k-median where each cluster is required to have a fair representation of individuals from different groups. In the fair representation k-median problem, we are given a set of points X in a metric space. Each point x ‚àà X belongs to one of ‚Ñì groups. Further, we are given fair representation parameters Œ±j and Œ≤j for each group j ‚àà [‚Ñì]. We say that a k-clustering C1, ‚ãÖ‚ãÖ‚ãÖ, Ck fairly represents all groups if the number of points from group j in cluster Ci is between Œ±j|Ci| and Œ≤j|Ci| for every j ‚àà [‚Ñì] and i ‚àà [k]. The goal is to find a set of k centers and an assignment such that the clustering defined by fairly represents all groups and minimizes the ‚Ñì1-objective ‚àëx ‚àà Xd(x, œï(x)). We present an O(log‚Äâk)-approximation algorithm that runs in time nO(‚Ñì). Note that the known algorithms for the problem either (i) violate the fairness constraints by an additive term or (ii) run in time that is exponential in both k and ‚Ñì. We also consider an important special case of the problem where and for all j ‚àà [‚Ñì]. For this special case, we present an O(log‚Äâk)-approximation algorithm that runs in time.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3533146,
author = {Dai, Zhen and Makarychev, Yury and Vakilian, Ali},
title = {Fair Representation Clustering with Several Protected Classes},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533146},
doi = {10.1145/3531146.3533146},
abstract = {We study the problem of fair k-median where each cluster is required to have a fair representation of individuals from different groups. In the fair representation k-median problem, we are given a set of points X in a metric space. Each point x ∈ X belongs to one of ℓ groups. Further, we are given fair representation parameters αj and βj for each group j ∈ [ℓ]. We say that a k-clustering C1, ⋅⋅⋅, Ck fairly represents all groups if the number of points from group j in cluster Ci is between αj|Ci| and βj|Ci| for every j ∈ [ℓ] and i ∈ [k]. The goal is to find a set of k centers and an assignment such that the clustering defined by fairly represents all groups and minimizes the ℓ1-objective ∑x ∈ Xd(x, ϕ(x)). We present an O(log k)-approximation algorithm that runs in time nO(ℓ). Note that the known algorithms for the problem either (i) violate the fairness constraints by an additive term or (ii) run in time that is exponential in both k and ℓ. We also consider an important special case of the problem where and for all j ∈ [ℓ]. For this special case, we present an O(log k)-approximation algorithm that runs in time.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {814–823},
numpages = {10},
keywords = {approximation algorithm, randomized algorithm, clustering, fair k-median},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,"Model Multiplicity: Opportunities, Concerns, and Solutions",https://dl.acm.org/doi/10.1145/3531146.3533149,"['Emily Black', 'Manish Raghavan', 'Solon Barocas']",0,"Recent scholarship has brought attention to the fact that there often exist multiple models for a given prediction task with equal accuracy that differ in their individual-level predictions or aggregate properties. This phenomenon‚Äîwhich we call model multiplicity‚Äîcan introduce a good deal of flexibility into the model selection process, creating a range of exciting opportunities. By demonstrating that there are many different ways of making equally accurate predictions, multiplicity gives model developers the freedom to prioritize other values in their model selection process without having to abandon their commitment to maximizing accuracy. However, multiplicity also brings to light a concerning truth: model selection on the basis of accuracy alone‚Äîthe default procedure in many deployment scenarios‚Äîfails to consider what might be meaningful differences between equally accurate models with respect to other criteria such as fairness, robustness, and interpretability. Unless these criteria are taken into account explicitly, developers might end up making unnecessary trade-offs or could even mask intentional discrimination. Furthermore, the prospect that there might exist another model of equal accuracy that flips a prediction for a particular individual may lead to a crisis in justifiability: why should an individual be subject to an adverse model outcome if there exists an equally accurate model that treats them more favorably? In this work, we investigate how to take advantage of the flexibility afforded by model multiplicity while addressing the concerns with justifiability that it might raise?",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3533149,
author = {Black, Emily and Raghavan, Manish and Barocas, Solon},
title = {Model Multiplicity: Opportunities, Concerns, and Solutions},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533149},
doi = {10.1145/3531146.3533149},
abstract = {Recent scholarship has brought attention to the fact that there often exist multiple models for a given prediction task with equal accuracy that differ in their individual-level predictions or aggregate properties. This phenomenon—which we call model multiplicity—can introduce a good deal of flexibility into the model selection process, creating a range of exciting opportunities. By demonstrating that there are many different ways of making equally accurate predictions, multiplicity gives model developers the freedom to prioritize other values in their model selection process without having to abandon their commitment to maximizing accuracy. However, multiplicity also brings to light a concerning truth: model selection on the basis of accuracy alone—the default procedure in many deployment scenarios—fails to consider what might be meaningful differences between equally accurate models with respect to other criteria such as fairness, robustness, and interpretability. Unless these criteria are taken into account explicitly, developers might end up making unnecessary trade-offs or could even mask intentional discrimination. Furthermore, the prospect that there might exist another model of equal accuracy that flips a prediction for a particular individual may lead to a crisis in justifiability: why should an individual be subject to an adverse model outcome if there exists an equally accurate model that treats them more favorably? In this work, we investigate how to take advantage of the flexibility afforded by model multiplicity while addressing the concerns with justifiability that it might raise?},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {850–863},
numpages = {14},
keywords = {Model multiplicity, recourse, procedural multiplicity, fairness, arbitrariness, predictive multiplicity, discrimination},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,Promoting Ethical Awareness in Communication Analysis: Investigating Potentials and Limits of Visual Analytics for Intelligence Applications,https://dl.acm.org/doi/10.1145/3531146.3533151,"['Maximilian T. Fischer', 'Simon David Hirsbrunner', 'Wolfgang Jentner', 'Matthias Miller', 'Daniel A. Keim', 'Paula Helm']",0,"Digital systems for analyzing human communication data have become prevalent in recent years. This may be related to the increasing abundance of data that can be harnessed but can hardly be managed manually. Intelligence analysis of communications data in investigative journalism, criminal intelligence, and law present particularly interesting cases, as they must take into account the often highly sensitive properties of the underlying operations and data. At the same time, these are areas where increasingly automated, sophisticated approaches and tailored systems can be particularly useful and relevant, especially in terms of Big Data manageability. However, by the shifting of responsibilities, this also poses dangers. In addition to privacy concerns, these dangers relate to uncertain or poor data quality, leading to discrimination and potentially misleading insights. Other problems relate to a lack of transparency and traceability, making it difficult to accurately identify problems and determine appropriate remedial strategies. Visual analytics combines machine learning methods with interactive visual interfaces to enable human sense- and decision-making. This technique can be key for designing and operating meaningful interactive communication analysis systems that consider these ethical challenges. In this interdisciplinary work, a joint endeavor of computer scientists, ethicists, and scholars in Science & Technology Studies, we investigate and evaluate opportunities and risks involved in using Visual analytics approaches for communication analysis in intelligence applications in particular. We introduce, at first, the common technological systems used in communication analysis, with a special focus on intelligence analysis in criminal investigations, further discussing the domain-specific ethical implications, tensions, and risks involved. We then make the case of how tailored Visual Analytics approaches may reduce and mitigate the described problems, both theoretically and through practical examples. Offering interactive analysis capabilities and what-if explorations while facilitating guidance, provenance generation, and bias awareness (through nudges, for example) can improve analysts‚Äô understanding of their data, increasing trustworthiness, accountability, and generating knowledge. We show that finding Visual Analytics design solutions for ethical issues is not a mere optimization task with an ideal final solution. Design solutions for specific ethical problems (e.g., privacy) often trigger new ethical issues (e.g., accountability) in other areas. Balancing out and negotiating these trade-offs has, as we argue, to be an integral aspect of the system design process from the outset. Finally, our work identifies existing gaps and highlights research opportunities, further describing how our results can be transferred to other domains. With this contribution, we aim at informing more ethically-aware approaches to communication analysis in intelligence operations.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3533151,
author = {Fischer, Maximilian T. and Hirsbrunner, Simon David and Jentner, Wolfgang and Miller, Matthias and Keim, Daniel A. and Helm, Paula},
title = {Promoting Ethical Awareness in Communication Analysis: Investigating Potentials and Limits of Visual Analytics for Intelligence Applications},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533151},
doi = {10.1145/3531146.3533151},
abstract = {Digital systems for analyzing human communication data have become prevalent in recent years. This may be related to the increasing abundance of data that can be harnessed but can hardly be managed manually. Intelligence analysis of communications data in investigative journalism, criminal intelligence, and law present particularly interesting cases, as they must take into account the often highly sensitive properties of the underlying operations and data. At the same time, these are areas where increasingly automated, sophisticated approaches and tailored systems can be particularly useful and relevant, especially in terms of Big Data manageability. However, by the shifting of responsibilities, this also poses dangers. In addition to privacy concerns, these dangers relate to uncertain or poor data quality, leading to discrimination and potentially misleading insights. Other problems relate to a lack of transparency and traceability, making it difficult to accurately identify problems and determine appropriate remedial strategies. Visual analytics combines machine learning methods with interactive visual interfaces to enable human sense- and decision-making. This technique can be key for designing and operating meaningful interactive communication analysis systems that consider these ethical challenges. In this interdisciplinary work, a joint endeavor of computer scientists, ethicists, and scholars in Science & Technology Studies, we investigate and evaluate opportunities and risks involved in using Visual analytics approaches for communication analysis in intelligence applications in particular. We introduce, at first, the common technological systems used in communication analysis, with a special focus on intelligence analysis in criminal investigations, further discussing the domain-specific ethical implications, tensions, and risks involved. We then make the case of how tailored Visual Analytics approaches may reduce and mitigate the described problems, both theoretically and through practical examples. Offering interactive analysis capabilities and what-if explorations while facilitating guidance, provenance generation, and bias awareness (through nudges, for example) can improve analysts’ understanding of their data, increasing trustworthiness, accountability, and generating knowledge. We show that finding Visual Analytics design solutions for ethical issues is not a mere optimization task with an ideal final solution. Design solutions for specific ethical problems (e.g., privacy) often trigger new ethical issues (e.g., accountability) in other areas. Balancing out and negotiating these trade-offs has, as we argue, to be an integral aspect of the system design process from the outset. Finally, our work identifies existing gaps and highlights research opportunities, further describing how our results can be transferred to other domains. With this contribution, we aim at informing more ethically-aware approaches to communication analysis in intelligence operations.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {877–889},
numpages = {13},
keywords = {Interdisciplinary Research, Critical Algorithm Studies, Machine Learning, Intelligence Analysis, Communication Analysis, Ethic Awareness, Visual Analytics, Science & Technology Studies, Critical Data Studies},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,On the Fairness of Machine-Assisted Human Decisions,https://dl.acm.org/doi/10.1145/3531146.3533152,"['Bryce McLaughlin', 'Jann Spiess', 'Talia Gillis']",6,"When machine-learning algorithms are deployed in high-stakes decisions, we want to ensure that their deployment leads to fair and equitable outcomes. This concern has motivated a fast-growing literature that focuses on diagnosing and addressing disparities in machine predictions. However, many machine predictions are deployed to assist in decisions where a human decision-maker retains the ultimate decision authority. In this article, we therefore consider how properties of machine predictions affect the resulting human decisions. We show in a formal model that the inclusion of a biased human decision-maker can revert common relationships between the structure of the algorithm and the qualities of resulting decisions. Specifically, we document that excluding information about protected groups from the prediction may fail to reduce, and may even increase, ultimate disparities. While our concrete results rely on specific assumptions about the data, algorithm, and decision-maker, they show more broadly that any study of critical properties of complex decision systems, such as the fairness of machine-assisted human decisions, should go beyond focusing on the underlying algorithmic predictions in isolation.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3533152,
author = {McLaughlin, Bryce and Spiess, Jann and Gillis, Talia},
title = {On the Fairness of Machine-Assisted Human Decisions},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533152},
doi = {10.1145/3531146.3533152},
abstract = {When machine-learning algorithms are deployed in high-stakes decisions, we want to ensure that their deployment leads to fair and equitable outcomes. This concern has motivated a fast-growing literature that focuses on diagnosing and addressing disparities in machine predictions. However, many machine predictions are deployed to assist in decisions where a human decision-maker retains the ultimate decision authority. In this article, we therefore consider how properties of machine predictions affect the resulting human decisions. We show in a formal model that the inclusion of a biased human decision-maker can revert common relationships between the structure of the algorithm and the qualities of resulting decisions. Specifically, we document that excluding information about protected groups from the prediction may fail to reduce, and may even increase, ultimate disparities. While our concrete results rely on specific assumptions about the data, algorithm, and decision-maker, they show more broadly that any study of critical properties of complex decision systems, such as the fairness of machine-assisted human decisions, should go beyond focusing on the underlying algorithmic predictions in isolation.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {890},
numpages = {1},
keywords = {Machine Learning, Decision Support Systems, Fairness, Human Computer Interaction, Protected Classes},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,Multi-disciplinary fairness considerations in machine learning for clinical trials,https://dl.acm.org/doi/10.1145/3531146.3533154,"['Isabel Chien', 'Nina Deliu', 'Richard Turner', 'Adrian Weller', 'Sofia Villar', 'Niki Kilbertus']",1,"While interest in the application of machine learning to improve healthcare has grown tremendously in recent years, a number of barriers prevent deployment in medical practice. A notable concern is the potential to exacerbate entrenched biases and existing health disparities in society. The area of fairness in machine learning seeks to address these issues of equity; however, appropriate approaches are context-dependent, necessitating domain-specific consideration. We focus on clinical trials, i.e., research studies conducted on humans to evaluate medical treatments. Clinical trials are a relatively under-explored application in machine learning for healthcare, in part due to complex ethical, legal, and regulatory requirements and high costs. Our aim is to provide a multi-disciplinary assessment of how fairness for machine learning fits into the context of clinical trials research and practice. We start by reviewing the current ethical considerations and guidelines for clinical trials and examine their relationship with common definitions of fairness in machine learning. We examine potential sources of unfairness in clinical trials, providing concrete examples, and discuss the role machine learning might play in either mitigating potential biases or exacerbating them when applied without care. Particular focus is given to adaptive clinical trials, which may employ machine learning. Finally, we highlight concepts that require further investigation and development, and emphasize new approaches to fairness that may be relevant to the design of clinical trials.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3533154,
author = {Chien, Isabel and Deliu, Nina and Turner, Richard and Weller, Adrian and Villar, Sofia and Kilbertus, Niki},
title = {Multi-Disciplinary Fairness Considerations in Machine Learning for Clinical Trials},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533154},
doi = {10.1145/3531146.3533154},
abstract = {While interest in the application of machine learning to improve healthcare has grown tremendously in recent years, a number of barriers prevent deployment in medical practice. A notable concern is the potential to exacerbate entrenched biases and existing health disparities in society. The area of fairness in machine learning seeks to address these issues of equity; however, appropriate approaches are context-dependent, necessitating domain-specific consideration. We focus on clinical trials, i.e., research studies conducted on humans to evaluate medical treatments. Clinical trials are a relatively under-explored application in machine learning for healthcare, in part due to complex ethical, legal, and regulatory requirements and high costs. Our aim is to provide a multi-disciplinary assessment of how fairness for machine learning fits into the context of clinical trials research and practice. We start by reviewing the current ethical considerations and guidelines for clinical trials and examine their relationship with common definitions of fairness in machine learning. We examine potential sources of unfairness in clinical trials, providing concrete examples, and discuss the role machine learning might play in either mitigating potential biases or exacerbating them when applied without care. Particular focus is given to adaptive clinical trials, which may employ machine learning. Finally, we highlight concepts that require further investigation and development, and emphasize new approaches to fairness that may be relevant to the design of clinical trials.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {906–924},
numpages = {19},
keywords = {machine learning for healthcare, adaptive clinical trials, health informatics, clinical trials},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,German AI Start-Ups and ‚ÄúAI Ethics‚Äù: Using A Social Practice Lens for Assessing and Implementing Socio-Technical Innovation,https://dl.acm.org/doi/10.1145/3531146.3533156,"['Mona Sloane', 'Janina Zakrzewski']",3,"The current AI ethics discourse focuses on developing computational interpretations of ethical concerns, normative frameworks, and concepts for socio-technical innovation. There is less emphasis on understanding how AI practitioners themselves understand ethics and socially organize to operationalize ethical concerns. This is particularly true for AI start-ups, despite their significance as a conduit for the cultural production of innovation and progress, especially in the US and European context. This gap in empirical research intensifies the risk of a disconnect between scholarly research, innovation and application. This risk materializes acutely as mounting pressures to identify and mitigate the potential harms of AI systems have created an urgent need to rapidly assess and implement socio-technical innovation focused on fairness, accountability, and transparency. In this paper, we address this need. Building on social practice theory, we propose a framework that allows AI researchers, practitioners, and regulators to systematically analyze existing cultural understandings, histories, and social practices of ‚Äúethical AI‚Äù to define appropriate strategies for effectively implementing socio-technical innovations. We argue that this approach is needed because socio-technical innovation ‚Äústicks‚Äù better if it sustains the cultural meaning of socially shared (ethical) AI practices, rather than breaking them. By doing so, it creates pathways for technical and socio-technical innovations to be integrated into already existing routines. Against that backdrop, our contributions are threefold: (1) we introduce a practice-based approach for understanding ‚Äúethical AI‚Äù; (2) we present empirical findings from our study on the operationalization of ‚Äúethics‚Äù in German AI start-ups to underline that AI ethics and social practices must be understood in their specific cultural and historical contexts; and (3) based on our empirical findings, suggest that ‚Äúethical AI‚Äù practices can be broken down into principles, needs, narratives, materializations, and cultural genealogies to form a useful backdrop for considering socio-technical innovations. We conclude with critical reflections and practical implications of our work, as well as recommendations for future research.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3533156,
author = {Sloane, Mona and Zakrzewski, Janina},
title = {German AI Start-Ups and “AI Ethics”: Using A Social Practice Lens for Assessing and Implementing Socio-Technical Innovation},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533156},
doi = {10.1145/3531146.3533156},
abstract = {The current AI ethics discourse focuses on developing computational interpretations of ethical concerns, normative frameworks, and concepts for socio-technical innovation. There is less emphasis on understanding how AI practitioners themselves understand ethics and socially organize to operationalize ethical concerns. This is particularly true for AI start-ups, despite their significance as a conduit for the cultural production of innovation and progress, especially in the US and European context. This gap in empirical research intensifies the risk of a disconnect between scholarly research, innovation and application. This risk materializes acutely as mounting pressures to identify and mitigate the potential harms of AI systems have created an urgent need to rapidly assess and implement socio-technical innovation focused on fairness, accountability, and transparency. In this paper, we address this need. Building on social practice theory, we propose a framework that allows AI researchers, practitioners, and regulators to systematically analyze existing cultural understandings, histories, and social practices of “ethical AI” to define appropriate strategies for effectively implementing socio-technical innovations. We argue that this approach is needed because socio-technical innovation “sticks” better if it sustains the cultural meaning of socially shared (ethical) AI practices, rather than breaking them. By doing so, it creates pathways for technical and socio-technical innovations to be integrated into already existing routines. Against that backdrop, our contributions are threefold: (1) we introduce a practice-based approach for understanding “ethical AI”; (2) we present empirical findings from our study on the operationalization of “ethics” in German AI start-ups to underline that AI ethics and social practices must be understood in their specific cultural and historical contexts; and (3) based on our empirical findings, suggest that “ethical AI” practices can be broken down into principles, needs, narratives, materializations, and cultural genealogies to form a useful backdrop for considering socio-technical innovations. We conclude with critical reflections and practical implications of our work, as well as recommendations for future research.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {935–947},
numpages = {13},
keywords = {socio-cultural history, AI ethics, start-ups, innovation, social practice, fairness, accountability, transparency, regulation, organizations},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,The Forgotten Margins of AI Ethics,https://dl.acm.org/doi/10.1145/3531146.3533157,"['Abeba Birhane', 'Elayne Ruane', 'Thomas Laurent', 'Matthew S. Brown', 'Johnathan Flowers', 'Anthony Ventresque', 'Christopher L. Dancy']",11,"How has recent AI Ethics literature addressed topics such as fairness and justice in the context of continued social and structural power asymmetries? We trace both the historical roots and current landmark work that have been shaping the field and categorize these works under three broad umbrellas: (i) those grounded in Western canonical philosophy, (ii) mathematical and statistical methods, and (iii) those emerging from critical data/algorithm/information studies. We also survey the field and explore emerging trends by examining the rapidly growing body of literature that falls under the broad umbrella of AI Ethics. To that end, we read and annotated peer-reviewed papers published over the past four years in two premier conferences: FAccT and AIES. We organize the literature based on an annotation scheme we developed according to three main dimensions: whether the paper deals with concrete applications, use-cases, and/or people‚Äôs lived experience; to what extent it addresses harmed, threatened, or otherwise marginalized groups; and if so, whether it explicitly names such groups. We note that although the goals of the majority of FAccT and AIES papers were often commendable, their consideration of the negative impacts of AI on traditionally marginalized groups remained shallow. Taken together, our conceptual analysis and the data from annotated papers indicate that the field would benefit from an increased focus on ethical analysis grounded in concrete use-cases, people‚Äôs experiences, and applications as well as from approaches that are sensitive to structural and historical power asymmetries.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3533157,
author = {Birhane, Abeba and Ruane, Elayne and Laurent, Thomas and S. Brown, Matthew and Flowers, Johnathan and Ventresque, Anthony and L. Dancy, Christopher},
title = {The Forgotten Margins of AI Ethics},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533157},
doi = {10.1145/3531146.3533157},
abstract = {How has recent AI Ethics literature addressed topics such as fairness and justice in the context of continued social and structural power asymmetries? We trace both the historical roots and current landmark work that have been shaping the field and categorize these works under three broad umbrellas: (i) those grounded in Western canonical philosophy, (ii) mathematical and statistical methods, and (iii) those emerging from critical data/algorithm/information studies. We also survey the field and explore emerging trends by examining the rapidly growing body of literature that falls under the broad umbrella of AI Ethics. To that end, we read and annotated peer-reviewed papers published over the past four years in two premier conferences: FAccT and AIES. We organize the literature based on an annotation scheme we developed according to three main dimensions: whether the paper deals with concrete applications, use-cases, and/or people’s lived experience; to what extent it addresses harmed, threatened, or otherwise marginalized groups; and if so, whether it explicitly names such groups. We note that although the goals of the majority of FAccT and AIES papers were often commendable, their consideration of the negative impacts of AI on traditionally marginalized groups remained shallow. Taken together, our conceptual analysis and the data from annotated papers indicate that the field would benefit from an increased focus on ethical analysis grounded in concrete use-cases, people’s experiences, and applications as well as from approaches that are sensitive to structural and historical power asymmetries.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {948–958},
numpages = {11},
keywords = {Trends, AI Ethics, AIES, Justice, FAccT},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,"Female, white, 27? Bias Evaluation on Data and Algorithms for Affect Recognition in Faces",https://dl.acm.org/doi/10.1145/3531146.3533159,"['Jaspar Pahl', 'Ines Rieger', 'Anna M√∂ller', 'Thomas Wittenberg', 'Ute Schmid']",1,"Nowadays, Artificial Intelligence (AI) algorithms show a strong performance for many use cases, making them desirable for real-world scenarios where the algorithms provide high-impact decisions. However, one major drawback of AI algorithms is their susceptibility to bias and resulting unfairness. This has a huge influence for their application, as they have a higher failure rate for certain subgroups. In this paper, we focus on the field of affective computing and particularly on the detection of bias for facial expressions. Depending on the deployment scenario, bias in facial expression models can have a disadvantageous impact and it is therefore essential to evaluate the bias and limitations of the model. In order to analyze the metadata distribution in affective computing datasets, we annotate several benchmark training datasets, containing both Action Units and categorical emotions, with age, gender, ethnicity, glasses, and beards. We show that there is a significantly skewed distribution, particularly for ethnicity and age. Based on this metadata annotation, we evaluate two trained state-of-the-art affective computing algorithms. Our evaluation shows that the strongest bias is in age, with the best performance for persons under 34 and a sharp decrease for older persons. Furthermore, we see an ethnicity bias with varying direction depending on the algorithm, a slight gender bias and worse performance for facial parts occluded by glasses.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3533159,
author = {Pahl, Jaspar and Rieger, Ines and M\""{o}ller, Anna and Wittenberg, Thomas and Schmid, Ute},
title = {Female, White, 27? Bias Evaluation on Data and Algorithms for Affect Recognition in Faces},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533159},
doi = {10.1145/3531146.3533159},
abstract = {Nowadays, Artificial Intelligence (AI) algorithms show a strong performance for many use cases, making them desirable for real-world scenarios where the algorithms provide high-impact decisions. However, one major drawback of AI algorithms is their susceptibility to bias and resulting unfairness. This has a huge influence for their application, as they have a higher failure rate for certain subgroups. In this paper, we focus on the field of affective computing and particularly on the detection of bias for facial expressions. Depending on the deployment scenario, bias in facial expression models can have a disadvantageous impact and it is therefore essential to evaluate the bias and limitations of the model. In order to analyze the metadata distribution in affective computing datasets, we annotate several benchmark training datasets, containing both Action Units and categorical emotions, with age, gender, ethnicity, glasses, and beards. We show that there is a significantly skewed distribution, particularly for ethnicity and age. Based on this metadata annotation, we evaluate two trained state-of-the-art affective computing algorithms. Our evaluation shows that the strongest bias is in age, with the best performance for persons under 34 and a sharp decrease for older persons. Furthermore, we see an ethnicity bias with varying direction depending on the algorithm, a slight gender bias and worse performance for facial parts occluded by glasses.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {973–987},
numpages = {15},
keywords = {algorithm evaluation, action units, bias, data evaluation, categorical emotions, fairness, metadata post-annotation, affective computing},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,Beyond Fairness: Reparative Algorithms to Address Historical Injustices of Housing Discrimination in the US,https://dl.acm.org/doi/10.1145/3531146.3533160,"['Wonyoung So', 'Pranay Lohia', 'Rakesh Pimplikar', 'A.E. Hosoi', ""Catherine D'Ignazio""]",0,"Fairness in Machine Learning (ML) has mostly focused on interrogating the fairness of a particular decision point with assumptions made that the people represented in the data have been fairly treated throughout history. However, fairness cannot be ultimately achieved if such assumptions are not valid. This is the case for mortgage lending discrimination in the US, which should be critically understood as the result of historically accumulated injustices that were enacted through public policies and private practices including redlining, racial covenants, exclusionary zoning, and predatory inclusion, among others. With the erroneous assumptions of historical fairness in ML, Black borrowers with low income and low wealth are considered as a given condition in a lending algorithm, thus rejecting loans to them would be considered a ‚Äúfair‚Äù decision even though Black borrowers were historically excluded from homeownership and wealth creation. To emphasize such issues, we introduce case studies using contemporary mortgage lending data as well as historical census data in the US. First, we show that historical housing discrimination has differentiated each racial group‚Äôs baseline wealth which is a critical input for algorithmically determining mortgage loans. The second case study estimates the cost of housing reparations in the algorithmic lending context to redress historical harms because of such discriminatory housing policies. Through these case studies, we envision what reparative algorithms would look like in the context of housing discrimination in the US. This work connects to emerging scholarship on how algorithmic systems can contribute to redressing past harms through engaging with reparations policies and programs.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3533160,
author = {So, Wonyoung and Lohia, Pranay and Pimplikar, Rakesh and Hosoi, A.E. and D'Ignazio, Catherine},
title = {Beyond Fairness: Reparative Algorithms to Address Historical Injustices of Housing Discrimination in the US},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533160},
doi = {10.1145/3531146.3533160},
abstract = {Fairness in Machine Learning (ML) has mostly focused on interrogating the fairness of a particular decision point with assumptions made that the people represented in the data have been fairly treated throughout history. However, fairness cannot be ultimately achieved if such assumptions are not valid. This is the case for mortgage lending discrimination in the US, which should be critically understood as the result of historically accumulated injustices that were enacted through public policies and private practices including redlining, racial covenants, exclusionary zoning, and predatory inclusion, among others. With the erroneous assumptions of historical fairness in ML, Black borrowers with low income and low wealth are considered as a given condition in a lending algorithm, thus rejecting loans to them would be considered a “fair” decision even though Black borrowers were historically excluded from homeownership and wealth creation. To emphasize such issues, we introduce case studies using contemporary mortgage lending data as well as historical census data in the US. First, we show that historical housing discrimination has differentiated each racial group’s baseline wealth which is a critical input for algorithmically determining mortgage loans. The second case study estimates the cost of housing reparations in the algorithmic lending context to redress historical harms because of such discriminatory housing policies. Through these case studies, we envision what reparative algorithms would look like in the context of housing discrimination in the US. This work connects to emerging scholarship on how algorithmic systems can contribute to redressing past harms through engaging with reparations policies and programs.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {988–1004},
numpages = {17},
keywords = {reparations, racial wealth gap, housing, mortgage lending, fairness},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,"Subverting machines, fluctuating identities: Re-learning human categorization",https://dl.acm.org/doi/10.1145/3531146.3533161,"['Christina Lu', 'Jackie Kay', 'Kevin McKee']",3,"Most machine learning systems that interact with humans construct some notion of a person‚Äôs ‚Äúidentity,‚Äù yet the default paradigm in AI research envisions identity with essential attributes that are discrete and static. In stark contrast, strands of thought within critical theory present a conception of identity as malleable and constructed entirely through interaction; a doing rather than a being. In this work, we distill some of these ideas for machine learning practitioners and introduce a theory of identity as autopoiesis, circular processes of formation and function. We argue that the default paradigm of identity used by the field immobilizes existing identity categories and the power differentials that co-occur, due to the absence of iterative feedback to our models. This includes a critique of emergent AI fairness practices that continue to impose the default paradigm. Finally, we apply our theory to sketch approaches to autopoietic identity through multilevel optimization and relational learning. While these ideas raise many open questions, we imagine the possibilities of machines that are capable of expressing human identity as a relationship perpetually in flux.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3533161,
author = {Lu, Christina and Kay, Jackie and McKee, Kevin},
title = {Subverting Machines, Fluctuating Identities: Re-Learning Human Categorization},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533161},
doi = {10.1145/3531146.3533161},
abstract = {Most machine learning systems that interact with humans construct some notion of a person’s “identity,” yet the default paradigm in AI research envisions identity with essential attributes that are discrete and static. In stark contrast, strands of thought within critical theory present a conception of identity as malleable and constructed entirely through interaction; a doing rather than a being. In this work, we distill some of these ideas for machine learning practitioners and introduce a theory of identity as autopoiesis, circular processes of formation and function. We argue that the default paradigm of identity used by the field immobilizes existing identity categories and the power differentials that co-occur, due to the absence of iterative feedback to our models. This includes a critique of emergent AI fairness practices that continue to impose the default paradigm. Finally, we apply our theory to sketch approaches to autopoietic identity through multilevel optimization and relational learning. While these ideas raise many open questions, we imagine the possibilities of machines that are capable of expressing human identity as a relationship perpetually in flux.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1005–1015},
numpages = {11},
keywords = {theories of identity, social construction, identity systems, algorithmic fairness},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,A Data-Driven Simulation of the New York State Foster Care System,https://dl.acm.org/doi/10.1145/3531146.3533165,"['Yuhao Du', 'Stefania Ionescu', 'Melanie Sage', 'Kenneth Joseph']",2,"We introduce an analytic pipeline to model and simulate youth trajectories through the New York state foster care system. Our goal in doing so is to forecast how proposed interventions may impact the foster care system‚Äôs ability to achieve it‚Äôs stated goals before these interventions are actually implemented and impact the lives of thousands of youth. Here, we focus on two specific stated goals of the system: racial equity, and, as codified most recently by the 2018 Family First Prevention Services Act (FFPSA), a focus on keeping all youth out of foster care. We also focus on one specific potential intervention‚Äî a predictive model, proposed in prior work and implemented elsewhere in the U.S., which aims to determine whether or not a youth is in need of care. We use our method to explore how the implementation of this predictive model in New York would impact racial equity and the number of youth in care. While our findings, as in any simulation model, ultimately rely on modeling assumptions, we find evidence that the model would not necessarily achieve either goal. Primarily, then, we aim to further promote the use of data-driven simulation to help understand the ramifications of algorithmic interventions in public systems.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3533165,
author = {Du, Yuhao and Ionescu, Stefania and Sage, Melanie and Joseph, Kenneth},
title = {A Data-Driven Simulation of the New York State Foster Care System},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533165},
doi = {10.1145/3531146.3533165},
abstract = {We introduce an analytic pipeline to model and simulate youth trajectories through the New York state foster care system. Our goal in doing so is to forecast how proposed interventions may impact the foster care system’s ability to achieve it’s stated goals before these interventions are actually implemented and impact the lives of thousands of youth. Here, we focus on two specific stated goals of the system: racial equity, and, as codified most recently by the 2018 Family First Prevention Services Act (FFPSA), a focus on keeping all youth out of foster care. We also focus on one specific potential intervention— a predictive model, proposed in prior work and implemented elsewhere in the U.S., which aims to determine whether or not a youth is in need of care. We use our method to explore how the implementation of this predictive model in New York would impact racial equity and the number of youth in care. While our findings, as in any simulation model, ultimately rely on modeling assumptions, we find evidence that the model would not necessarily achieve either goal. Primarily, then, we aim to further promote the use of data-driven simulation to help understand the ramifications of algorithmic interventions in public systems.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1028–1038},
numpages = {11},
keywords = {Social Policy, Simulation, Racial Equity, Child Welfare},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,"Net benefit, calibration, threshold selection, and training objectives for algorithmic fairness in healthcare",https://dl.acm.org/doi/10.1145/3531146.3533166,"['Stephen Pfohl', 'Yizhe Xu', 'Agata Foryciarz', 'Nikolaos Ignatiadis', 'Julian Genkins', 'Nigam Shah']",3,"A growing body of work uses the paradigm of algorithmic fairness to frame the development of techniques to anticipate and proactively mitigate the introduction or exacerbation of health inequities that may follow from the use of model-guided decision-making. We evaluate the interplay between measures of model performance, fairness, and the expected utility of decision-making to offer practical recommendations for the operationalization of algorithmic fairness principles for the development and evaluation of predictive models in healthcare. We conduct an empirical case-study via development of models to estimate the ten-year risk of atherosclerotic cardiovascular disease to inform statin initiation in accordance with clinical practice guidelines. We demonstrate that approaches that incorporate fairness considerations into the model training objective typically do not improve model performance or confer greater net benefit for any of the studied patient populations compared to the use of standard learning paradigms followed by threshold selection concordant with patient preferences, evidence of intervention effectiveness, and model calibration. These results hold when the measured outcomes are not subject to differential measurement error across patient populations and threshold selection is unconstrained, regardless of whether differences in model performance metrics, such as in true and false positive error rates, are present. In closing, we argue for focusing model development efforts on developing calibrated models that predict outcomes well for all patient populations while emphasizing that such efforts are complementary to transparent reporting, participatory design, and reasoning about the impact of model-informed interventions in context.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3533166,
author = {Pfohl, Stephen and Xu, Yizhe and Foryciarz, Agata and Ignatiadis, Nikolaos and Genkins, Julian and Shah, Nigam},
title = {Net Benefit, Calibration, Threshold Selection, and Training Objectives for Algorithmic Fairness in Healthcare},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533166},
doi = {10.1145/3531146.3533166},
abstract = {A growing body of work uses the paradigm of algorithmic fairness to frame the development of techniques to anticipate and proactively mitigate the introduction or exacerbation of health inequities that may follow from the use of model-guided decision-making. We evaluate the interplay between measures of model performance, fairness, and the expected utility of decision-making to offer practical recommendations for the operationalization of algorithmic fairness principles for the development and evaluation of predictive models in healthcare. We conduct an empirical case-study via development of models to estimate the ten-year risk of atherosclerotic cardiovascular disease to inform statin initiation in accordance with clinical practice guidelines. We demonstrate that approaches that incorporate fairness considerations into the model training objective typically do not improve model performance or confer greater net benefit for any of the studied patient populations compared to the use of standard learning paradigms followed by threshold selection concordant with patient preferences, evidence of intervention effectiveness, and model calibration. These results hold when the measured outcomes are not subject to differential measurement error across patient populations and threshold selection is unconstrained, regardless of whether differences in model performance metrics, such as in true and false positive error rates, are present. In closing, we argue for focusing model development efforts on developing calibrated models that predict outcomes well for all patient populations while emphasizing that such efforts are complementary to transparent reporting, participatory design, and reasoning about the impact of model-informed interventions in context.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1039–1052},
numpages = {14},
keywords = {cardiovascular disease, fairness, healthcare},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,FADE: FAir Double Ensemble Learning for Observable and Counterfactual Outcomes,https://dl.acm.org/doi/10.1145/3531146.3533167,"['Alan Mishler', 'Edward H. Kennedy']",8,"Methods for building fair predictors often involve tradeoffs between fairness and accuracy and between different fairness criteria. Recent work seeks to characterize these tradeoffs in specific problem settings, but these methods often do not accommodate users who wish to improve the fairness of an existing benchmark model without sacrificing accuracy, or vice versa. These results are also typically restricted to observable accuracy and fairness criteria. We develop a flexible framework for fair ensemble learning that allows users to efficiently explore the fairness-accuracy space or to improve the fairness or accuracy of a benchmark model. Our framework can simultaneously target multiple observable or counterfactual fairness criteria, and it enables users to combine a large number of previously trained and newly trained predictors. We provide theoretical guarantees that our estimators converge at fast rates. We apply our method on both simulated and real data, with respect to both observable and counterfactual accuracy and fairness criteria. We show that, surprisingly, multiple unfairness measures can sometimes be minimized simultaneously with little impact on accuracy, relative to unconstrained predictors or existing benchmark models.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3533167,
author = {Mishler, Alan and Kennedy, Edward H.},
title = {FADE: FAir Double Ensemble Learning for Observable and Counterfactual Outcomes},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533167},
doi = {10.1145/3531146.3533167},
abstract = {Methods for building fair predictors often involve tradeoffs between fairness and accuracy and between different fairness criteria. Recent work seeks to characterize these tradeoffs in specific problem settings, but these methods often do not accommodate users who wish to improve the fairness of an existing benchmark model without sacrificing accuracy, or vice versa. These results are also typically restricted to observable accuracy and fairness criteria. We develop a flexible framework for fair ensemble learning that allows users to efficiently explore the fairness-accuracy space or to improve the fairness or accuracy of a benchmark model. Our framework can simultaneously target multiple observable or counterfactual fairness criteria, and it enables users to combine a large number of previously trained and newly trained predictors. We provide theoretical guarantees that our estimators converge at fast rates. We apply our method on both simulated and real data, with respect to both observable and counterfactual accuracy and fairness criteria. We show that, surprisingly, multiple unfairness measures can sometimes be minimized simultaneously with little impact on accuracy, relative to unconstrained predictors or existing benchmark models.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1053},
numpages = {1},
keywords = {ensemble learning, semiparametric, fairness, counterfactual},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,"Tackling Algorithmic Disability Discrimination in the Hiring Process: An Ethical, Legal and Technical Analysis",https://dl.acm.org/doi/10.1145/3531146.3533169,"['Maarten Buyl', 'Christina Cociancig', 'Cristina Frattone', 'Nele Roekens']",2,"Tackling algorithmic discrimination against persons with disabilities (PWDs) demands a distinctive approach that is fundamentally different to that applied to other protected characteristics, due to particular ethical, legal, and technical challenges. We address these challenges specifically in the context of artificial intelligence (AI) systems used in hiring processes (or automated hiring systems, AHSs), in which automated assessment procedures are subject to unique ethical and legal considerations and have an undeniable adverse impact on PWDs. In this paper, we discuss concerns and opportunities raised by AI-driven hiring in relation to disability discrimination. Ultimately, we aim to encourage further research into this topic. Hence, we establish some starting points and design a roadmap for ethicists, lawmakers, advocates as well as AI practitioners alike.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3533169,
author = {Buyl, Maarten and Cociancig, Christina and Frattone, Cristina and Roekens, Nele},
title = {Tackling Algorithmic Disability Discrimination in the Hiring Process: An Ethical, Legal and Technical Analysis},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533169},
doi = {10.1145/3531146.3533169},
abstract = {Tackling algorithmic discrimination against persons with disabilities (PWDs) demands a distinctive approach that is fundamentally different to that applied to other protected characteristics, due to particular ethical, legal, and technical challenges. We address these challenges specifically in the context of artificial intelligence (AI) systems used in hiring processes (or automated hiring systems, AHSs), in which automated assessment procedures are subject to unique ethical and legal considerations and have an undeniable adverse impact on PWDs. In this paper, we discuss concerns and opportunities raised by AI-driven hiring in relation to disability discrimination. Ultimately, we aim to encourage further research into this topic. Hence, we establish some starting points and design a roadmap for ethicists, lawmakers, advocates as well as AI practitioners alike.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1071–1082},
numpages = {12},
keywords = {social justice, data protection law, automated hiring systems, algorithmic discrimination, persons with disabilities, reasonable accommodation, Artificial Intelligence Act, equality law, ethics of discrimination},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,Trade-offs between Group Fairness Metrics in Societal Resource Allocation,https://dl.acm.org/doi/10.1145/3531146.3533171,"['Tasfia Mashiat', 'Xavier Gitiaux', 'Huzefa Rangwala', 'Patrick Fowler', 'Sanmay Das']",6,"We consider social resource allocations that deliver an array of scarce supports to a diverse population. Such allocations pervade social service delivery, such as provision of homeless services and assignment of refugees to cities, among others. At issue is whether allocations are fair across sociodemographic groups and intersectional identities. Our paper shows that necessary trade-offs exist for fairness in the context of scarcity; many reasonable definitions of equitable outcomes cannot hold simultaneously except under stringent conditions. For example, defining fairness in terms of improvement over a baseline inherently conflicts with defining fairness in terms of loss compared with the best possible outcome. Moreover, we demonstrate that the fairness trade-offs stem from heterogeneity across groups in intervention responses. Administrative records on homeless service delivery offer a real-world example. Building on prior work, we measure utilities for each household as the probability of reentry into homeless services if given three homeless services. Heterogeneity in utility distributions (conditional on received services) for several sociodemographic groups (e.g. single women with children versus without children) generates divergence across fairness metrics. We argue that such heterogeneity, and thus, fairness trade-offs, pervade many social policy contexts.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3533171,
author = {Mashiat, Tasfia and Gitiaux, Xavier and Rangwala, Huzefa and Fowler, Patrick and Das, Sanmay},
title = {Trade-Offs between Group Fairness Metrics in Societal Resource Allocation},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533171},
doi = {10.1145/3531146.3533171},
abstract = {We consider social resource allocations that deliver an array of scarce supports to a diverse population. Such allocations pervade social service delivery, such as provision of homeless services and assignment of refugees to cities, among others. At issue is whether allocations are fair across sociodemographic groups and intersectional identities. Our paper shows that necessary trade-offs exist for fairness in the context of scarcity; many reasonable definitions of equitable outcomes cannot hold simultaneously except under stringent conditions. For example, defining fairness in terms of improvement over a baseline inherently conflicts with defining fairness in terms of loss compared with the best possible outcome. Moreover, we demonstrate that the fairness trade-offs stem from heterogeneity across groups in intervention responses. Administrative records on homeless service delivery offer a real-world example. Building on prior work, we measure utilities for each household as the probability of reentry into homeless services if given three homeless services. Heterogeneity in utility distributions (conditional on received services) for several sociodemographic groups (e.g. single women with children versus without children) generates divergence across fairness metrics. We argue that such heterogeneity, and thus, fairness trade-offs, pervade many social policy contexts.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1095–1105},
numpages = {11},
keywords = {algorithmic fairness, Resource allocation, fairness metrics},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,Smallset Timelines: A Visual Representation of Data Preprocessing Decisions,https://dl.acm.org/doi/10.1145/3531146.3533175,"['Lydia R. Lucchesi', 'Petra M. Kuhnert', 'Jenny L. Davis', 'Lexing Xie']",0,"Data preprocessing is a crucial stage in the data analysis pipeline, with both technical and social aspects to consider. Yet, the attention it receives is often lacking in research practice and dissemination. We present the Smallset Timeline, a visualisation to help reflect on and communicate data preprocessing decisions. A ‚ÄúSmallset‚Äù is a small selection of rows from the original dataset containing instances of dataset alterations. The Timeline is comprised of Smallset snapshots representing different points in the preprocessing stage and captions to describe the alterations visualised at each point. Edits, additions, and deletions to the dataset are highlighted with colour. We develop the R software package, smallsets, that can create Smallset Timelines from R and Python data preprocessing scripts. Constructing the figure asks practitioners to reflect on and revise decisions as necessary, while sharing it aims to make the process accessible to a diverse range of audiences. We present two case studies to illustrate use of the Smallset Timeline for visualising preprocessing decisions. Case studies include software defect data and income survey benchmark data, in which preprocessing affects levels of data loss and group fairness in prediction tasks, respectively. We envision Smallset Timelines as a go-to data provenance tool, enabling better documentation and communication of preprocessing tasks at large.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3533175,
author = {Lucchesi, Lydia R. and Kuhnert, Petra M. and Davis, Jenny L. and Xie, Lexing},
title = {Smallset Timelines: A Visual Representation of Data Preprocessing Decisions},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533175},
doi = {10.1145/3531146.3533175},
abstract = {Data preprocessing is a crucial stage in the data analysis pipeline, with both technical and social aspects to consider. Yet, the attention it receives is often lacking in research practice and dissemination. We present the Smallset Timeline, a visualisation to help reflect on and communicate data preprocessing decisions. A “Smallset” is a small selection of rows from the original dataset containing instances of dataset alterations. The Timeline is comprised of Smallset snapshots representing different points in the preprocessing stage and captions to describe the alterations visualised at each point. Edits, additions, and deletions to the dataset are highlighted with colour. We develop the R software package, smallsets, that can create Smallset Timelines from R and Python data preprocessing scripts. Constructing the figure asks practitioners to reflect on and revise decisions as necessary, while sharing it aims to make the process accessible to a diverse range of audiences. We present two case studies to illustrate use of the Smallset Timeline for visualising preprocessing decisions. Case studies include software defect data and income survey benchmark data, in which preprocessing affects levels of data loss and group fairness in prediction tasks, respectively. We envision Smallset Timelines as a go-to data provenance tool, enabling better documentation and communication of preprocessing tasks at large.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1136–1153},
numpages = {18},
keywords = {open-source software, reflexivity, data preprocessing, visualization, communication},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,Brain Computer Interfaces and Human Rights: Brave new rights for a brave new world,https://dl.acm.org/doi/10.1145/3531146.3533176,['Marietjie Wilhelmina Maria Botes'],1,"Digital health applications include a wide range of wearable, implantable, injectable and ingestible digital medical devices. Many of these devices use machine learning algorithms to assist medical prognosis and decision-making. One of the most compelling digital medical device developments is brain-computer interfaces (BCIs) which entails the connecting of a person's brain to a computer, or to another device outside the human body. BCIs allow bidirectional communication and control between the human brain and the outside world by exporting brain data or altering brain activity. Although being marveled at for its clinical promises, this technological advancement also raises novel ethical, legal, social and technical implications (ELSTI). Debates in this regard centers around patient autonomy, equity, trustworthiness in healthcare, data protection and security, risks of dehumanization, the limitations of machine learning-based decision-making, and the influence that BCIs have on what it means to be human and human rights. Since the adoption of the Universal Declaration of Human Rights (UDHR) after World War II, the landscape that give rise to these human rights has evolved enormously. Human life and humans‚Äô role in society are being transformed and threatened by technologies that were never imagined at the time the UDHR was adopted. BCIs, in particular, harbor the greatest possibility of social and individual disruption through its capability to record, interpret, manipulate, or alter brain activity that may potentially alter what it means to be human and how we control humans in future. Cutting edge technological innovations that increasingly blur the lines between human and computer beg the rethinking and extension of existing human rights to remain relevant in a digitized world. In this paper sui generis human rights such as mental privacy, the right to identity or self, agency or free will and fair access to cognitive augmentation will be discussed and how a regulatory framework must be adapted to act as technology enablers, whilst ensuring fairness, accountability, and transparency in sociotechnical systems.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3533176,
author = {Botes, Marietjie Wilhelmina Maria},
title = {Brain Computer Interfaces and Human Rights: Brave New Rights for a Brave New World},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533176},
doi = {10.1145/3531146.3533176},
abstract = {Digital health applications include a wide range of wearable, implantable, injectable and ingestible digital medical devices. Many of these devices use machine learning algorithms to assist medical prognosis and decision-making. One of the most compelling digital medical device developments is brain-computer interfaces (BCIs) which entails the connecting of a person's brain to a computer, or to another device outside the human body. BCIs allow bidirectional communication and control between the human brain and the outside world by exporting brain data or altering brain activity. Although being marveled at for its clinical promises, this technological advancement also raises novel ethical, legal, social and technical implications (ELSTI). Debates in this regard centers around patient autonomy, equity, trustworthiness in healthcare, data protection and security, risks of dehumanization, the limitations of machine learning-based decision-making, and the influence that BCIs have on what it means to be human and human rights. Since the adoption of the Universal Declaration of Human Rights (UDHR) after World War II, the landscape that give rise to these human rights has evolved enormously. Human life and humans’ role in society are being transformed and threatened by technologies that were never imagined at the time the UDHR was adopted. BCIs, in particular, harbor the greatest possibility of social and individual disruption through its capability to record, interpret, manipulate, or alter brain activity that may potentially alter what it means to be human and how we control humans in future. Cutting edge technological innovations that increasingly blur the lines between human and computer beg the rethinking and extension of existing human rights to remain relevant in a digitized world. In this paper sui generis human rights such as mental privacy, the right to identity or self, agency or free will and fair access to cognitive augmentation will be discussed and how a regulatory framework must be adapted to act as technology enablers, whilst ensuring fairness, accountability, and transparency in sociotechnical systems.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1154–1161},
numpages = {8},
keywords = {identity, brain computer interfaces, human rights, neurological privacy, autonomy},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,Multi Stage Screening: Enforcing Fairness and Maximizing Efficiency in a Pre-Existing Pipeline,https://dl.acm.org/doi/10.1145/3531146.3533178,"['Avrim Blum', 'Kevin Stangl', 'Ali Vakilian']",2,"Consider an actor making selection decisions (e.g., hiring) using a series of classifiers, which we term a sequential screening process. The early stages (e.g. resume screen, coding screen, phone interview) filter out some of the applicants, and in the final stage an expensive but accurate test (e.g. a full interview) is applied to those individuals that make it to the final stage. Since the final stage is expensive, if there are multiple groups with different fractions of positives in them at the penultimate stage (even if a slight gap), then the firm may naturally only choose to apply the final (interview) stage solely to the highest precision group which would be clearly unfair to the other groups. Even if the firm is required to interview all those who pass to the final round, the tests themselves could have the property that qualified individuals from some groups pass more easily than qualified individuals from others. Accordingly, we consider requiring Equality of Opportunity (qualified members of each group have the same chance of reaching the final stage and being interviewed). We then examine the goal of maximizing quantities of interest to the decision maker subject to this constraint, via modification of the probabilities of promotion through the screening process at each stage based on performance at the previous stage. We exhibit algorithms for satisfying Equal Opportunity over the selection process and maximizing precision (the fraction of interviews that yield qualified candidates) as well as linear combinations of precision and recall (recall determines the number of applicants needed per hire) at the end of the final stage. We also present examples showing that the solution space is non-convex, which motivate our combinatorial exact and (FPTAS) approximation algorithms for maximizing the linear combination of precision and recall. Finally, we discuss the ‚Äòprice of‚Äô adding additional restrictions, such as not allowing the decision-maker to use group membership in its decision process.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3533178,
author = {Blum, Avrim and Stangl, Kevin and Vakilian, Ali},
title = {Multi Stage Screening: Enforcing Fairness and Maximizing Efficiency in a Pre-Existing Pipeline},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533178},
doi = {10.1145/3531146.3533178},
abstract = {Consider an actor making selection decisions (e.g., hiring) using a series of classifiers, which we term a sequential screening process. The early stages (e.g. resume screen, coding screen, phone interview) filter out some of the applicants, and in the final stage an expensive but accurate test (e.g. a full interview) is applied to those individuals that make it to the final stage. Since the final stage is expensive, if there are multiple groups with different fractions of positives in them at the penultimate stage (even if a slight gap), then the firm may naturally only choose to apply the final (interview) stage solely to the highest precision group which would be clearly unfair to the other groups. Even if the firm is required to interview all those who pass to the final round, the tests themselves could have the property that qualified individuals from some groups pass more easily than qualified individuals from others. Accordingly, we consider requiring Equality of Opportunity (qualified members of each group have the same chance of reaching the final stage and being interviewed). We then examine the goal of maximizing quantities of interest to the decision maker subject to this constraint, via modification of the probabilities of promotion through the screening process at each stage based on performance at the previous stage. We exhibit algorithms for satisfying Equal Opportunity over the selection process and maximizing precision (the fraction of interviews that yield qualified candidates) as well as linear combinations of precision and recall (recall determines the number of applicants needed per hire) at the end of the final stage. We also present examples showing that the solution space is non-convex, which motivate our combinatorial exact and (FPTAS) approximation algorithms for maximizing the linear combination of precision and recall. Finally, we discuss the ‘price of’ adding additional restrictions, such as not allowing the decision-maker to use group membership in its decision process.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1178–1193},
numpages = {16},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,The Road to Explainability is Paved with Bias: Measuring the Fairness of Explanations,https://dl.acm.org/doi/10.1145/3531146.3533179,"['Aparna Balagopalan', 'Haoran Zhang', 'Kimia Hamidieh', 'Thomas Hartvigsen', 'Frank Rudzicz', 'Marzyeh Ghassemi']",14,"Machine learning models in safety-critical settings like healthcare are often ‚Äúblackboxes‚Äù: they contain a large number of parameters which are not transparent to users. Post-hoc explainability methods where a simple, human-interpretable model imitates the behavior of these blackbox models are often proposed to help users trust model predictions. In this work, we audit the quality of such explanations for different protected subgroups using real data from four settings in finance, healthcare, college admissions, and the US justice system. Across two different blackbox model architectures and four popular explainability methods, we find that the approximation quality of explanation models, also known as the fidelity, differs significantly between subgroups. We also demonstrate that pairing explainability methods with recent advances in robust machine learning can improve explanation fairness in some settings. However, we highlight the importance of communicating details of non-zero fidelity gaps to users, since a single solution might not exist across all settings. Finally, we discuss the implications of unfair explanation models as a challenging and understudied problem facing the machine learning community.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3533179,
author = {Balagopalan, Aparna and Zhang, Haoran and Hamidieh, Kimia and Hartvigsen, Thomas and Rudzicz, Frank and Ghassemi, Marzyeh},
title = {The Road to Explainability is Paved with Bias: Measuring the Fairness of Explanations},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533179},
doi = {10.1145/3531146.3533179},
abstract = {Machine learning models in safety-critical settings like healthcare are often “blackboxes”: they contain a large number of parameters which are not transparent to users. Post-hoc explainability methods where a simple, human-interpretable model imitates the behavior of these blackbox models are often proposed to help users trust model predictions. In this work, we audit the quality of such explanations for different protected subgroups using real data from four settings in finance, healthcare, college admissions, and the US justice system. Across two different blackbox model architectures and four popular explainability methods, we find that the approximation quality of explanation models, also known as the fidelity, differs significantly between subgroups. We also demonstrate that pairing explainability methods with recent advances in robust machine learning can improve explanation fairness in some settings. However, we highlight the importance of communicating details of non-zero fidelity gaps to users, since a single solution might not exist across all settings. Finally, we discuss the implications of unfair explanation models as a challenging and understudied problem facing the machine learning community.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1194–1206},
numpages = {13},
keywords = {fairness, explainability, machine learning},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,Multiaccurate Proxies for Downstream Fairness,https://dl.acm.org/doi/10.1145/3531146.3533180,"['Emily Diana', 'Wesley Gill', 'Michael Kearns', 'Krishnaram Kenthapadi', 'Aaron Roth', 'Saeed Sharifi-Malvajerdi']",8,"We study the problem of training a model that must obey demographic fairness conditions when the sensitive features are not available at training time ‚Äî in other words, how can we train a model to be fair by race when we don‚Äôt have data about race? We adopt a fairness pipeline perspective, in which an ‚Äúupstream‚Äù learner that does have access to the sensitive features will learn a proxy model for these features from the other attributes. The goal of the proxy is to allow a general ‚Äúdownstream‚Äù learner ‚Äî with minimal assumptions on their prediction task ‚Äî to be able to use the proxy to train a model that is fair with respect to the true sensitive features. We show that obeying multiaccuracy constraints with respect to the downstream model class suffices for this purpose, provide sample- and oracle efficient-algorithms and generalization bounds for learning such proxies, and conduct an experimental evaluation. In general, multiaccuracy is much easier to satisfy than classification accuracy, and can be satisfied even when the sensitive features are hard to predict.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3533180,
author = {Diana, Emily and Gill, Wesley and Kearns, Michael and Kenthapadi, Krishnaram and Roth, Aaron and Sharifi-Malvajerdi, Saeed},
title = {Multiaccurate Proxies for Downstream Fairness},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533180},
doi = {10.1145/3531146.3533180},
abstract = {We study the problem of training a model that must obey demographic fairness conditions when the sensitive features are not available at training time — in other words, how can we train a model to be fair by race when we don’t have data about race? We adopt a fairness pipeline perspective, in which an “upstream” learner that does have access to the sensitive features will learn a proxy model for these features from the other attributes. The goal of the proxy is to allow a general “downstream” learner — with minimal assumptions on their prediction task — to be able to use the proxy to train a model that is fair with respect to the true sensitive features. We show that obeying multiaccuracy constraints with respect to the downstream model class suffices for this purpose, provide sample- and oracle efficient-algorithms and generalization bounds for learning such proxies, and conduct an experimental evaluation. In general, multiaccuracy is much easier to satisfy than classification accuracy, and can be satisfied even when the sensitive features are hard to predict.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1207–1239},
numpages = {33},
keywords = {proxy variables, game theory, multiaccuracy, algorithmic fairness},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,Learning Resource Allocation Policies from Observational Data with an Application to Homeless Services Delivery,https://dl.acm.org/doi/10.1145/3531146.3533181,"['Aida Rahmattalabi', 'Phebe Vayanos', 'Kathryn Dullerud', 'Eric Rice']",6,"We study the problem of learning, from observational data, fair and interpretable policies that effectively match heterogeneous individuals to scarce resources of different types. We model this problem as a multi-class multi-server queuing system where both individuals and resources arrive stochastically over time. Each individual, upon arrival, is assigned to a queue where they wait to be matched to a resource. The resources are assigned in a first come first served (FCFS) fashion according to an eligibility structure that encodes the resource types that serve each queue. We propose a methodology based on techniques in modern causal inference to construct the individual queues as well as learn the matching outcomes and provide a mixed-integer optimization (MIO) formulation to optimize the eligibility structure. The MIO problem maximizes policy outcome subject to wait time and fairness constraints. It is very flexible, allowing for additional linear domain constraints. We conduct extensive analyses using synthetic and real-world data. In particular, we evaluate our framework using data from the U.S. Homeless Management Information System (HMIS). We obtain wait times as low as an FCFS policy while improving the rate of exit from homelessness for underserved or vulnerable groups (7% higher for the Black individuals and 15% higher for those below 17 years old) and overall.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3533181,
author = {Rahmattalabi, Aida and Vayanos, Phebe and Dullerud, Kathryn and Rice, Eric},
title = {Learning Resource Allocation Policies from Observational Data with an Application to Homeless Services Delivery},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533181},
doi = {10.1145/3531146.3533181},
abstract = {We study the problem of learning, from observational data, fair and interpretable policies that effectively match heterogeneous individuals to scarce resources of different types. We model this problem as a multi-class multi-server queuing system where both individuals and resources arrive stochastically over time. Each individual, upon arrival, is assigned to a queue where they wait to be matched to a resource. The resources are assigned in a first come first served (FCFS) fashion according to an eligibility structure that encodes the resource types that serve each queue. We propose a methodology based on techniques in modern causal inference to construct the individual queues as well as learn the matching outcomes and provide a mixed-integer optimization (MIO) formulation to optimize the eligibility structure. The MIO problem maximizes policy outcome subject to wait time and fairness constraints. It is very flexible, allowing for additional linear domain constraints. We conduct extensive analyses using synthetic and real-world data. In particular, we evaluate our framework using data from the U.S. Homeless Management Information System (HMIS). We obtain wait times as low as an FCFS policy while improving the rate of exit from homelessness for underserved or vulnerable groups (7% higher for the Black individuals and 15% higher for those below 17 years old) and overall.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1240–1256},
numpages = {17},
keywords = {Causal Inference, Observational Data, Mixed-integer Optimization, Fairness in AI},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,Markedness in Visual Semantic AI,https://dl.acm.org/doi/10.1145/3531146.3533183,"['Robert Wolfe', 'Aylin Caliskan']",3,"We evaluate the state-of-the-art multimodal ‚Äùvisual semantic‚Äù model CLIP (‚ÄùContrastive Language Image Pretraining‚Äù) for biases related to the marking of age, gender, and race or ethnicity. Given the option to label an image as ‚Äùa photo of a person‚Äù or to select a label denoting race or ethnicity, CLIP chooses the ‚Äùperson‚Äù label 47.9% of the time for White individuals, compared with 5.0% or less for individuals who are Black, East Asian, Southeast Asian, Indian, or Latino or Hispanic. The model is also more likely to rank the unmarked ‚Äùperson‚Äù label higher than labels denoting gender for Male individuals (26.7% of the time) vs. Female individuals (15.2% of the time). Age also affects whether an individual is marked by the model: Female individuals under the age of 20 are more likely than Male individuals to be marked with a gender label, but less likely to be marked with an age label, while Female individuals over the age of 40 are more likely to be marked based on age than Male individuals. We trace our results back to the CLIP embedding space by examining the self-similarity (mean pairwise cosine similarity) for each social group, where higher self-similarity denotes greater attention directed by CLIP to the shared characteristics (i.e., age, race, or gender) of the social group. The results indicate that, as age increases, the self-similarity of representations of Female individuals increases at a higher rate than for Male individuals, with the disparity most pronounced at the ‚Äùmore than 70‚Äù age range. Six of the ten least self-similar social groups are individuals who are White and Male, while all ten of the most self-similar social groups are individuals under the age of 10 or over the age of 70, and six of the ten are Female individuals. Our results yield evidence that bias in CLIP is intersectional: existing biases of self-similarity and markedness between Male and Female gender groups are further exacerbated when the groups compared are individuals who are White and Male and individuals who are Black and Female. CLIP is an English-language model trained on internet content gathered based on a query list generated from an American website (Wikipedia), and results indicate that CLIP reflects the biases of the language and society which produced this training data.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3533183,
author = {Wolfe, Robert and Caliskan, Aylin},
title = {Markedness in Visual Semantic AI},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533183},
doi = {10.1145/3531146.3533183},
abstract = {We evaluate the state-of-the-art multimodal ”visual semantic” model CLIP (”Contrastive Language Image Pretraining”) for biases related to the marking of age, gender, and race or ethnicity. Given the option to label an image as ”a photo of a person” or to select a label denoting race or ethnicity, CLIP chooses the ”person” label 47.9% of the time for White individuals, compared with 5.0% or less for individuals who are Black, East Asian, Southeast Asian, Indian, or Latino or Hispanic. The model is also more likely to rank the unmarked ”person” label higher than labels denoting gender for Male individuals (26.7% of the time) vs. Female individuals (15.2% of the time). Age also affects whether an individual is marked by the model: Female individuals under the age of 20 are more likely than Male individuals to be marked with a gender label, but less likely to be marked with an age label, while Female individuals over the age of 40 are more likely to be marked based on age than Male individuals. We trace our results back to the CLIP embedding space by examining the self-similarity (mean pairwise cosine similarity) for each social group, where higher self-similarity denotes greater attention directed by CLIP to the shared characteristics (i.e., age, race, or gender) of the social group. The results indicate that, as age increases, the self-similarity of representations of Female individuals increases at a higher rate than for Male individuals, with the disparity most pronounced at the ”more than 70” age range. Six of the ten least self-similar social groups are individuals who are White and Male, while all ten of the most self-similar social groups are individuals under the age of 10 or over the age of 70, and six of the ten are Female individuals. Our results yield evidence that bias in CLIP is intersectional: existing biases of self-similarity and markedness between Male and Female gender groups are further exacerbated when the groups compared are individuals who are White and Male and individuals who are Black and Female. CLIP is an English-language model trained on internet content gathered based on a query list generated from an American website (Wikipedia), and results indicate that CLIP reflects the biases of the language and society which produced this training data.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1269–1279},
numpages = {11},
keywords = {language-and-vision AI, bias in AI, age bias, visual semantics, multimodal, markedness},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,The Algorithmic Imprint,https://dl.acm.org/doi/10.1145/3531146.3533186,"['Upol Ehsan', 'Ranjit Singh', 'Jacob Metcalf', 'Mark Riedl']",6,"When algorithmic harms emerge, a reasonable response is to stop using the algorithm to resolve concerns related to fairness, accountability, transparency, and ethics (FATE). However, just because an algorithm is removed does not imply its FATE-related issues cease to exist. In this paper, we introduce the notion of the ‚Äúalgorithmic imprint‚Äù to illustrate how merely removing an algorithm does not necessarily undo or mitigate its consequences. We operationalize this concept and its implications through the 2020 events surrounding the algorithmic grading of the General Certificate of Education (GCE) Advanced (A) Level exams, an internationally recognized UK-based high school diploma exam administered in over 160 countries. While the algorithmic standardization was ultimately removed due to global protests, we show how the removal failed to undo the algorithmic imprint on the sociotechnical infrastructures that shape students‚Äô, teachers‚Äô, and parents‚Äô lives. These events provide a rare chance to analyze the state of the world both with and without algorithmic mediation. We situate our case study in Bangladesh to illustrate how algorithms made in the Global North disproportionately impact stakeholders in the Global South. Chronicling more than a year-long community engagement consisting of 47 interviews, we present the first coherent timeline of ‚Äúwhat‚Äù happened in Bangladesh, contextualizing ‚Äúwhy‚Äù and ‚Äúhow‚Äù they happened through the lenses of the algorithmic imprint and situated algorithmic fairness. Analyzing these events, we highlight how the contours of the algorithmic imprints can be inferred at the infrastructural, social, and individual levels. We share conceptual and practical implications around how imprint-awareness can (a) broaden the boundaries of how we think about algorithmic impact, (b) inform how we design algorithms, and (c) guide us in AI governance. The imprint-aware design mindset can make the algorithmic development process more human-centered and sociotechnically-informed.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3533186,
author = {Ehsan, Upol and Singh, Ranjit and Metcalf, Jacob and Riedl, Mark},
title = {The Algorithmic Imprint},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533186},
doi = {10.1145/3531146.3533186},
abstract = {When algorithmic harms emerge, a reasonable response is to stop using the algorithm to resolve concerns related to fairness, accountability, transparency, and ethics (FATE). However, just because an algorithm is removed does not imply its FATE-related issues cease to exist. In this paper, we introduce the notion of the “algorithmic imprint” to illustrate how merely removing an algorithm does not necessarily undo or mitigate its consequences. We operationalize this concept and its implications through the 2020 events surrounding the algorithmic grading of the General Certificate of Education (GCE) Advanced (A) Level exams, an internationally recognized UK-based high school diploma exam administered in over 160 countries. While the algorithmic standardization was ultimately removed due to global protests, we show how the removal failed to undo the algorithmic imprint on the sociotechnical infrastructures that shape students’, teachers’, and parents’ lives. These events provide a rare chance to analyze the state of the world both with and without algorithmic mediation. We situate our case study in Bangladesh to illustrate how algorithms made in the Global North disproportionately impact stakeholders in the Global South. Chronicling more than a year-long community engagement consisting of 47 interviews, we present the first coherent timeline of “what” happened in Bangladesh, contextualizing “why” and “how” they happened through the lenses of the algorithmic imprint and situated algorithmic fairness. Analyzing these events, we highlight how the contours of the algorithmic imprints can be inferred at the infrastructural, social, and individual levels. We share conceptual and practical implications around how imprint-awareness can (a) broaden the boundaries of how we think about algorithmic impact, (b) inform how we design algorithms, and (c) guide us in AI governance. The imprint-aware design mindset can make the algorithmic development process more human-centered and sociotechnically-informed.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1305–1317},
numpages = {13},
keywords = {Algorithmic Imprint, Situated Fairness, Infrastructure, Algorithmic Impact Assessment, Global South, Folk Theories of Algorithms, User Perceptions},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,Confronting Power and Corporate Capture at the FAccT Conference,https://dl.acm.org/doi/10.1145/3531146.3533194,"['Meg Young', 'Michael Katell', 'P.M. Krafft']",3,"Fields such as medicine and public health attest to deep conflict of interest concerns present when private companies fund evaluation of their own products and services. We draw on these lessons to consider corporate capture of the ACM Fairness, Accountability, and Transparency (FAccT) conference. We situate our analysis within scholarship on the entanglement of industry and academia and focus on the silences it produces in the research record. Our analysis of the institutional design at FAccT indicates the conference‚Äôs neglect of those people most negatively impacted by algorithmic systems. We focus on a 2021 paper by Wilson et al., ‚ÄúBuilding and auditing fair algorithms: A case study in candidate screening‚Äù as a key example of conflicted research accepted via peer review at FAccT. We call on the conference to (1) lead on models for how to manage conflicts of interest in the field of computing beyond individual disclosure of funding sources, (2) hold space for advocates and activists able to speak directly to questions of algorithmic harm, and (3) reconstitute the conference with attention to fostering agonistic dissensus‚Äîun-making the present manufactured consensus and nurturing challenges to power. These changes will position our community to contend with the political dimensions of research on AI harms.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3533194,
author = {Young, Meg and Katell, Michael and Krafft, P.M.},
title = {Confronting Power and Corporate Capture at the FAccT Conference},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533194},
doi = {10.1145/3531146.3533194},
abstract = {Fields such as medicine and public health attest to deep conflict of interest concerns present when private companies fund evaluation of their own products and services. We draw on these lessons to consider corporate capture of the ACM Fairness, Accountability, and Transparency (FAccT) conference. We situate our analysis within scholarship on the entanglement of industry and academia and focus on the silences it produces in the research record. Our analysis of the institutional design at FAccT indicates the conference’s neglect of those people most negatively impacted by algorithmic systems. We focus on a 2021 paper by Wilson et al., “Building and auditing fair algorithms: A case study in candidate screening” as a key example of conflicted research accepted via peer review at FAccT. We call on the conference to (1) lead on models for how to manage conflicts of interest in the field of computing beyond individual disclosure of funding sources, (2) hold space for advocates and activists able to speak directly to questions of algorithmic harm, and (3) reconstitute the conference with attention to fostering agonistic dissensus—un-making the present manufactured consensus and nurturing challenges to power. These changes will position our community to contend with the political dimensions of research on AI harms.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1375–1386},
numpages = {12},
keywords = {pymetrics, agonism, corporate capture, research funding, industry engagement, conflict of interest},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,Towards Fair Unsupervised Learning,https://dl.acm.org/doi/10.1145/3531146.3533197,"['Francois Buet-Golfouse', 'Islam Utyagulov']",3,"Bias-mitigating techniques are now well established in the supervised learning literature and have shown their ability to tackle fairness-accuracy, as well as fairness-fairness trade-offs. These are usually predicated on different conceptions of fairness, such as demographic parity or equal odds that depend on the available labels in the dataset. However, it is often the case in practice that unsupervised learning is used as part of a machine learning pipeline (for instance, to perform dimensionality reduction or representation learning via SVD) or as a standalone model (for example, to derive a customer segmentation via k-means). It is thus crucial to develop approaches towards fair unsupervised learning. This work investigates fair unsupervised learning within the broad framework of generalised low-rank models (GLRM). Importantly, we introduce the concept of fairness functional that encompasses both traditional unsupervised learning techniques and min-max algorithms (whereby one minimises the maximum group loss). To do so, we design straightforward alternate convex search or biconvex gradient descent algorithms that also provide partial debiasing techniques. Finally, we show on benchmark datasets that our fair generalised low-rank models (‚ÄúfGLRM‚Äù) perform well and help reduce disparity amongst groups while only incurring small runtime overheads.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3533197,
author = {Buet-Golfouse, Francois and Utyagulov, Islam},
title = {Towards Fair Unsupervised Learning},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533197},
doi = {10.1145/3531146.3533197},
abstract = {Bias-mitigating techniques are now well established in the supervised learning literature and have shown their ability to tackle fairness-accuracy, as well as fairness-fairness trade-offs. These are usually predicated on different conceptions of fairness, such as demographic parity or equal odds that depend on the available labels in the dataset. However, it is often the case in practice that unsupervised learning is used as part of a machine learning pipeline (for instance, to perform dimensionality reduction or representation learning via SVD) or as a standalone model (for example, to derive a customer segmentation via k-means). It is thus crucial to develop approaches towards fair unsupervised learning. This work investigates fair unsupervised learning within the broad framework of generalised low-rank models (GLRM). Importantly, we introduce the concept of fairness functional that encompasses both traditional unsupervised learning techniques and min-max algorithms (whereby one minimises the maximum group loss). To do so, we design straightforward alternate convex search or biconvex gradient descent algorithms that also provide partial debiasing techniques. Finally, we show on benchmark datasets that our fair generalised low-rank models (“fGLRM”) perform well and help reduce disparity amongst groups while only incurring small runtime overheads.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1399–1409},
numpages = {11},
keywords = {PCA, Clustering, Unsupervised Learning, Fairness},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,Decision Time: Normative Dimensions of Algorithmic Speed,https://dl.acm.org/doi/10.1145/3531146.3533198,['Daniel Susser'],5,"Existing discussions about automated decision-making focus primarily on its inputs and outputs, raising questions about data collection and privacy on one hand and accuracy and fairness on the other. Less attention has been devoted to critically examining the temporality of decision-making processes‚Äîthe speed at which automated decisions are reached. In this paper, I identify four dimensions of algorithmic speed that merit closer analysis. Duration (how much time it takes to reach a judgment), timing (when automated systems intervene in the activity being evaluated), frequency (how often evaluations are performed), and lived time (the human experience of algorithmic speed) are interrelated, but distinct, features of automated decision-making. Choices about the temporal structure of automated decision-making systems have normative implications, which I describe in terms of ‚Äùdisruption,‚Äù ‚Äùdisplacement,‚Äù ‚Äùre-calibration,‚Äù and ‚Äùtemporal fairness,‚Äù with values such as accuracy, fairness, accountability, and legitimacy hanging in the balance. As computational tools are increasingly tasked with making judgments about human activities and practices, the designers of decision-making systems will have to reckon, I argue, with when‚Äîand how fast‚Äîjudgments ought to be rendered. Though computers are capable of reaching decisions at incredible speeds, failing to account for the temporality of automated decision-making risks misapprehending the costs and benefits automation promises.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3533198,
author = {Susser, Daniel},
title = {Decision Time: Normative Dimensions of Algorithmic Speed},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533198},
doi = {10.1145/3531146.3533198},
abstract = {Existing discussions about automated decision-making focus primarily on its inputs and outputs, raising questions about data collection and privacy on one hand and accuracy and fairness on the other. Less attention has been devoted to critically examining the temporality of decision-making processes—the speed at which automated decisions are reached. In this paper, I identify four dimensions of algorithmic speed that merit closer analysis. Duration (how much time it takes to reach a judgment), timing (when automated systems intervene in the activity being evaluated), frequency (how often evaluations are performed), and lived time (the human experience of algorithmic speed) are interrelated, but distinct, features of automated decision-making. Choices about the temporal structure of automated decision-making systems have normative implications, which I describe in terms of ”disruption,” ”displacement,” ”re-calibration,” and ”temporal fairness,” with values such as accuracy, fairness, accountability, and legitimacy hanging in the balance. As computational tools are increasingly tasked with making judgments about human activities and practices, the designers of decision-making systems will have to reckon, I argue, with when—and how fast—judgments ought to be rendered. Though computers are capable of reaching decisions at incredible speeds, failing to account for the temporality of automated decision-making risks misapprehending the costs and benefits automation promises.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1410–1420},
numpages = {11},
keywords = {AI ethics, speed, automated decision-making, automation, time, temporality, data ethics},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,Don‚Äôt Throw it Away! The Utility of Unlabeled Data in Fair Decision Making,https://dl.acm.org/doi/10.1145/3531146.3533199,"['Miriam Rateike', 'Ayan Majumdar', 'Olga Mineeva', 'Krishna P. Gummadi', 'Isabel Valera']",1,"Decision making algorithms, in practice, are often trained on data that exhibits a variety of biases. Decision-makers often aim to take decisions based on some ground-truth target that is assumed or expected to be unbiased, i.e., equally distributed across socially salient groups. In many practical settings, the ground-truth cannot be directly observed, and instead, we have to rely on a biased proxy measure of the ground-truth, i.e., biased labels, in the data. In addition, data is often selectively labeled, i.e., even the biased labels are only observed for a small fraction of the data that received a positive decision. To overcome label and selection biases, recent work proposes to learn stochastic, exploring decision policies via i) online training of new policies at each time-step and ii) enforcing fairness as a constraint on performance. However, the existing approach uses only labeled data, disregarding a large amount of unlabeled data, and thereby suffers from high instability and variance in the learned decision policies at different times. In this paper, we propose a novel method based on a variational autoencoder for practical fair decision-making. Our method learns an unbiased data representation leveraging both labeled and unlabeled data and uses the representations to learn a policy in an online process. Using synthetic data, we empirically validate that our method converges to the optimal (fair) policy according to the ground-truth with low variance. In real-world experiments, we further show that our training approach not only offers a more stable learning process but also yields policies with higher fairness as well as utility than previous approaches.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3533199,
author = {Rateike, Miriam and Majumdar, Ayan and Mineeva, Olga and Gummadi, Krishna P. and Valera, Isabel},
title = {Don’t Throw It Away! The Utility of Unlabeled Data in Fair Decision Making},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533199},
doi = {10.1145/3531146.3533199},
abstract = {Decision making algorithms, in practice, are often trained on data that exhibits a variety of biases. Decision-makers often aim to take decisions based on some ground-truth target that is assumed or expected to be unbiased, i.e., equally distributed across socially salient groups. In many practical settings, the ground-truth cannot be directly observed, and instead, we have to rely on a biased proxy measure of the ground-truth, i.e., biased labels, in the data. In addition, data is often selectively labeled, i.e., even the biased labels are only observed for a small fraction of the data that received a positive decision. To overcome label and selection biases, recent work proposes to learn stochastic, exploring decision policies via i) online training of new policies at each time-step and ii) enforcing fairness as a constraint on performance. However, the existing approach uses only labeled data, disregarding a large amount of unlabeled data, and thereby suffers from high instability and variance in the learned decision policies at different times. In this paper, we propose a novel method based on a variational autoencoder for practical fair decision-making. Our method learns an unbiased data representation leveraging both labeled and unlabeled data and uses the representations to learn a policy in an online process. Using synthetic data, we empirically validate that our method converges to the optimal (fair) policy according to the ground-truth with low variance. In real-world experiments, we further show that our training approach not only offers a more stable learning process but also yields policies with higher fairness as well as utility than previous approaches.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1421–1433},
numpages = {13},
keywords = {label bias, fairness, fair representation, selection bias, decision making, variational autoencoder},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,Algorithmic Fairness and Vertical Equity: Income Fairness with IRS Tax Audit Models,https://dl.acm.org/doi/10.1145/3531146.3533204,"['Emily Black', 'Hadi Elzayn', 'Alexandra Chouldechova', 'Jacob Goldin', 'Daniel Ho']",1,"This study examines issues of algorithmic fairness in the context of systems that inform tax audit selection by the United States Internal Revenue Service (IRS). While the field of algorithmic fairness has developed primarily around notions of treating like individuals alike, we instead explore the concept of vertical equity‚Äîappropriately accounting for relevant differences across individuals‚Äîwhich is a central component of fairness in many public policy settings. Applied to the design of the U.S. individual income tax system, vertical equity relates to the fair allocation of tax and enforcement burdens across taxpayers of different income levels. Through a unique collaboration with the Treasury Department and IRS, we use access to detailed, anonymized individual taxpayer microdata, risk-selected audits, and random audits from 2010-14 to study vertical equity in tax administration. In particular, we assess how the adoption of modern machine learning methods for selecting taxpayer audits may affect vertical equity. Our paper makes four contributions. First, we show how the adoption of more flexible machine learning (classification) methods‚Äîas opposed to simpler models‚Äîshapes vertical equity by shifting audit burdens from high to middle-income taxpayers. Second, given concerns about high audit rates of low-income taxpayers, we investigate how existing algorithmic fairness techniques would change the audit distribution. We find that such methods can mitigate some disparities across income buckets, but that these come at a steep cost to performance. Third, we show that the choice of whether to treat risk of underreporting as a classification or regression problem is highly consequential. Moving from a classification approach to a regression approach to predict the expected magnitude of underreporting shifts the audit burden substantially toward high income individuals, while increasing revenue. Last, we investigate the role of differential audit cost in shaping the distribution of audits. Audits of lower income taxpayers, for instance, are typically conducted by mail and hence pose much lower cost to the IRS. We show that a narrow focus on return-on-investment can undermine vertical equity. Our results have implications for ongoing policy debates and the design of algorithmic tools across the public sector.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3533204,
author = {Black, Emily and Elzayn, Hadi and Chouldechova, Alexandra and Goldin, Jacob and Ho, Daniel},
title = {Algorithmic Fairness and Vertical Equity: Income Fairness with IRS Tax Audit Models},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533204},
doi = {10.1145/3531146.3533204},
abstract = {This study examines issues of algorithmic fairness in the context of systems that inform tax audit selection by the United States Internal Revenue Service (IRS). While the field of algorithmic fairness has developed primarily around notions of treating like individuals alike, we instead explore the concept of vertical equity—appropriately accounting for relevant differences across individuals—which is a central component of fairness in many public policy settings. Applied to the design of the U.S. individual income tax system, vertical equity relates to the fair allocation of tax and enforcement burdens across taxpayers of different income levels. Through a unique collaboration with the Treasury Department and IRS, we use access to detailed, anonymized individual taxpayer microdata, risk-selected audits, and random audits from 2010-14 to study vertical equity in tax administration. In particular, we assess how the adoption of modern machine learning methods for selecting taxpayer audits may affect vertical equity. Our paper makes four contributions. First, we show how the adoption of more flexible machine learning (classification) methods—as opposed to simpler models—shapes vertical equity by shifting audit burdens from high to middle-income taxpayers. Second, given concerns about high audit rates of low-income taxpayers, we investigate how existing algorithmic fairness techniques would change the audit distribution. We find that such methods can mitigate some disparities across income buckets, but that these come at a steep cost to performance. Third, we show that the choice of whether to treat risk of underreporting as a classification or regression problem is highly consequential. Moving from a classification approach to a regression approach to predict the expected magnitude of underreporting shifts the audit burden substantially toward high income individuals, while increasing revenue. Last, we investigate the role of differential audit cost in shaping the distribution of audits. Audits of lower income taxpayers, for instance, are typically conducted by mail and hence pose much lower cost to the IRS. We show that a narrow focus on return-on-investment can undermine vertical equity. Our results have implications for ongoing policy debates and the design of algorithmic tools across the public sector.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1479–1503},
numpages = {25},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,"Justice in Misinformation Detection Systems: An Analysis of Algorithms, Stakeholders, and Potential Harms",https://dl.acm.org/doi/10.1145/3531146.3533205,"['Terrence Neumann', 'Maria De-Arteaga', 'Sina Fazelpour']",6,"Faced with the scale and surge of misinformation on social media, many platforms and fact-checking organizations have turned to algorithms for automating key parts of misinformation detection pipelines. While offering a promising solution to the challenge of scale, the ethical and societal risks associated with algorithmic misinformation detection are not well-understood. In this paper, we employ and extend upon the notion of informational justice to develop a framework for explicating issues of justice relating to representation, participation, distribution of benefits and burdens, and credibility in the misinformation detection pipeline. Drawing on the framework: (1) we show how injustices materialize for stakeholders across three algorithmic stages in the pipeline; (2) we suggest empirical measures for assessing these injustices; and (3) we identify potential sources of these harms. This framework should help researchers, policymakers, and practitioners reason about potential harms or risks associated with these algorithms and provide conceptual guidance for the design of algorithmic fairness audits in this domain.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3533205,
author = {Neumann, Terrence and De-Arteaga, Maria and Fazelpour, Sina},
title = {Justice in Misinformation Detection Systems: An Analysis of Algorithms, Stakeholders, and Potential Harms},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533205},
doi = {10.1145/3531146.3533205},
abstract = {Faced with the scale and surge of misinformation on social media, many platforms and fact-checking organizations have turned to algorithms for automating key parts of misinformation detection pipelines. While offering a promising solution to the challenge of scale, the ethical and societal risks associated with algorithmic misinformation detection are not well-understood. In this paper, we employ and extend upon the notion of informational justice to develop a framework for explicating issues of justice relating to representation, participation, distribution of benefits and burdens, and credibility in the misinformation detection pipeline. Drawing on the framework: (1) we show how injustices materialize for stakeholders across three algorithmic stages in the pipeline; (2) we suggest empirical measures for assessing these injustices; and (3) we identify potential sources of these harms. This framework should help researchers, policymakers, and practitioners reason about potential harms or risks associated with these algorithms and provide conceptual guidance for the design of algorithmic fairness audits in this domain.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1504–1515},
numpages = {12},
keywords = {machine learning, informational justice, justice, algorithmic fairness, misinformation detection},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,Critical Tools for Machine Learning: Working with Intersectional Critical Concepts in Machine Learning Systems Design,https://dl.acm.org/doi/10.1145/3531146.3533207,"['Goda Klumbytƒó', 'Claude Draude', 'Alex S. Taylor']",2,"This paper investigates how intersectional critical theoretical concepts from social sciences and humanities research can be worked with in machine learning systems design. It does so by presenting a case study of a series of speculative design workshops, conducted in 2021. These workshops drew on intersectional feminist methodologies to construct interdisciplinary interventions in the design of machine learning systems, towards more inclusive, accountable, and contextualized systems design. The concepts of ‚Äúsituating/situated knowledges‚Äù, ""figuration"", ""diffraction"", and ‚Äúcritical fabulation/speculation‚Äù were taken up as theoretical and methodological tools for concept-led design workshops. This paper presents the design framework of the workshops and highlights tensions and possibilities with regards to interdisciplinary machine learning systems design towards more inclusive, contextualized, and accountable systems. It discusses the role that critical theoretical concepts can play in a design process and shows how such concepts can work as methodological tools that nonetheless require an open-ended experimental space to function. It presents insights and discussion points regarding what it means to work with critical intersectional knowledge that is inextricably connected to its historical and socio-political roots, and how this reframes what it might mean to design fair and accountable systems.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3533207,
author = {Klumbytundefined, Goda and Draude, Claude and Taylor, Alex S.},
title = {Critical Tools for Machine Learning: Working with Intersectional Critical Concepts in Machine Learning Systems Design},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533207},
doi = {10.1145/3531146.3533207},
abstract = {This paper investigates how intersectional critical theoretical concepts from social sciences and humanities research can be worked with in machine learning systems design. It does so by presenting a case study of a series of speculative design workshops, conducted in 2021. These workshops drew on intersectional feminist methodologies to construct interdisciplinary interventions in the design of machine learning systems, towards more inclusive, accountable, and contextualized systems design. The concepts of “situating/situated knowledges”, ""figuration"", ""diffraction"", and “critical fabulation/speculation” were taken up as theoretical and methodological tools for concept-led design workshops. This paper presents the design framework of the workshops and highlights tensions and possibilities with regards to interdisciplinary machine learning systems design towards more inclusive, contextualized, and accountable systems. It discusses the role that critical theoretical concepts can play in a design process and shows how such concepts can work as methodological tools that nonetheless require an open-ended experimental space to function. It presents insights and discussion points regarding what it means to work with critical intersectional knowledge that is inextricably connected to its historical and socio-political roots, and how this reframes what it might mean to design fair and accountable systems.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1528–1541},
numpages = {14},
keywords = {Intersectionality, Machine learning systems design, Experimental practice, Interdisciplinary methodologies, Feminist epistemologies},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,On the Power of Randomization in Fair Classification and Representation,https://dl.acm.org/doi/10.1145/3531146.3533209,"['Sushant Agarwal', 'Amit Deshpande']",3,"Fair classification and fair representation learning are two important problems in supervised and unsupervised fair machine learning, respectively. Fair classification asks for a classifier that maximizes accuracy on a given data distribution subject to fairness constraints. Fair representation maps a given data distribution over the original feature space to a distribution over a new representation space such that all classifiers over the representation satisfy fairness. In this paper, we examine the power of randomization in both these problems to minimize the loss of accuracy that results when we impose fairness constraints. Previous work on fair classification has characterized the optimal fair classifiers on a given data distribution that maximize accuracy subject to fairness constraints, e.g., Demographic Parity (DP), Equal Opportunity (EO), and Predictive Equality (PE). We refine these characterizations to demonstrate when the optimal randomized fair classifiers can surpass their deterministic counterparts in accuracy. We also show how the optimal randomized fair classifier that we characterize can be obtained as a solution to a convex optimization problem. Recent work has provided techniques to construct fair representations for a given data distribution such that any classifier over this representation satisfies DP. However, the classifiers on these fair representations either come with no or weak accuracy guarantees when compared to the optimal fair classifier on the original data distribution. Extending our ideas for randomized fair classification, we improve on these works, and construct DP-fair, EO-fair, and PE-fair representations that have provably optimal accuracy and suffer no accuracy loss compared to the optimal DP-fair, EO-fair, and PE-fair classifiers respectively on the original data distribution.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3533209,
author = {Agarwal, Sushant and Deshpande, Amit},
title = {On the Power of Randomization in Fair Classification and Representation},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533209},
doi = {10.1145/3531146.3533209},
abstract = {Fair classification and fair representation learning are two important problems in supervised and unsupervised fair machine learning, respectively. Fair classification asks for a classifier that maximizes accuracy on a given data distribution subject to fairness constraints. Fair representation maps a given data distribution over the original feature space to a distribution over a new representation space such that all classifiers over the representation satisfy fairness. In this paper, we examine the power of randomization in both these problems to minimize the loss of accuracy that results when we impose fairness constraints. Previous work on fair classification has characterized the optimal fair classifiers on a given data distribution that maximize accuracy subject to fairness constraints, e.g., Demographic Parity (DP), Equal Opportunity (EO), and Predictive Equality (PE). We refine these characterizations to demonstrate when the optimal randomized fair classifiers can surpass their deterministic counterparts in accuracy. We also show how the optimal randomized fair classifier that we characterize can be obtained as a solution to a convex optimization problem. Recent work has provided techniques to construct fair representations for a given data distribution such that any classifier over this representation satisfies DP. However, the classifiers on these fair representations either come with no or weak accuracy guarantees when compared to the optimal fair classifier on the original data distribution. Extending our ideas for randomized fair classification, we improve on these works, and construct DP-fair, EO-fair, and PE-fair representations that have provably optimal accuracy and suffer no accuracy loss compared to the optimal DP-fair, EO-fair, and PE-fair classifiers respectively on the original data distribution.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1542–1551},
numpages = {10},
keywords = {randomization, classification, representation, fairness, equal opportunity, demographic parity, machine learning},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,ABCinML: Anticipatory Bias Correction in Machine Learning Applications,https://dl.acm.org/doi/10.1145/3531146.3533211,"['Abdulaziz A. Almuzaini', 'Chidansh A. Bhatt', 'David M. Pennock', 'Vivek K. Singh']",1,"The idealization of a static machine-learned model, trained once and deployed forever, is not practical. As input distributions change over time, the model will not only lose accuracy, any constraints to reduce bias against a protected class may fail to work as intended. Thus, researchers have begun to explore ways to maintain algorithmic fairness over time. One line of work focuses on dynamic learning: retraining after each batch, and the other on robust learning which tries to make algorithms robust against all possible future changes. Dynamic learning seeks to reduce biases soon after they have occurred and robust learning often yields (overly) conservative models. We propose an anticipatory dynamic learning approach for correcting the algorithm to mitigate bias before it occurs. Specifically, we make use of anticipations regarding the relative distributions of population subgroups (e.g., relative ratios of male and female applicants) in the next cycle to identify the right parameters for an importance weighing fairness approach. Results from experiments over multiple real-world datasets suggest that this approach has promise for anticipatory bias correction.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3533211,
author = {Almuzaini, Abdulaziz A. and Bhatt, Chidansh A. and Pennock, David M. and Singh, Vivek K.},
title = {ABCinML: Anticipatory Bias Correction in Machine Learning Applications},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533211},
doi = {10.1145/3531146.3533211},
abstract = {The idealization of a static machine-learned model, trained once and deployed forever, is not practical. As input distributions change over time, the model will not only lose accuracy, any constraints to reduce bias against a protected class may fail to work as intended. Thus, researchers have begun to explore ways to maintain algorithmic fairness over time. One line of work focuses on dynamic learning: retraining after each batch, and the other on robust learning which tries to make algorithms robust against all possible future changes. Dynamic learning seeks to reduce biases soon after they have occurred and robust learning often yields (overly) conservative models. We propose an anticipatory dynamic learning approach for correcting the algorithm to mitigate bias before it occurs. Specifically, we make use of anticipations regarding the relative distributions of population subgroups (e.g., relative ratios of male and female applicants) in the next cycle to identify the right parameters for an importance weighing fairness approach. Results from experiments over multiple real-world datasets suggest that this approach has promise for anticipatory bias correction.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1552–1560},
numpages = {9},
keywords = {algorithmic bias, classification, fairness},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,Algorithms Off-limits?: If digital trade law restricts access to source code of software then accountability will suffer,https://dl.acm.org/doi/10.1145/3531146.3533212,['Kristina Irion'],1,"Free trade agreements are increasingly used to construct an additional layer of protection for source code of software. This comes in the shape of a new prohibition for governments to require access to, or transfer of, source code of software, subject to certain exceptions. A clause on software source code is also part and parcel of an ambitious set of new rules on trade-related aspects of electronic commerce currently negotiated by 86 members of the World Trade Organization. Our understanding to date of how such a commitment inside trade law impacts on governments right to regulate digital technologies and the policy space that is allowed under trade law is limited. Access to software source code is for example necessary to meet regulatory and judicial needs in order to ensure that digital technologies are in conformity with individuals‚Äô human rights and societal values. This article will unpack and analyze the implications of such a source code clause for current and future digital policies by governments that aim to ensure transparency, fairness and accountability of computer and machine learning algorithms.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3533212,
author = {Irion, Kristina},
title = {Algorithms Off-Limits? If Digital Trade Law Restricts Access to Source Code of Software Then Accountability Will Suffer},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533212},
doi = {10.1145/3531146.3533212},
abstract = {Free trade agreements are increasingly used to construct an additional layer of protection for source code of software. This comes in the shape of a new prohibition for governments to require access to, or transfer of, source code of software, subject to certain exceptions. A clause on software source code is also part and parcel of an ambitious set of new rules on trade-related aspects of electronic commerce currently negotiated by 86 members of the World Trade Organization. Our understanding to date of how such a commitment inside trade law impacts on governments right to regulate digital technologies and the policy space that is allowed under trade law is limited. Access to software source code is for example necessary to meet regulatory and judicial needs in order to ensure that digital technologies are in conformity with individuals’ human rights and societal values. This article will unpack and analyze the implications of such a source code clause for current and future digital policies by governments that aim to ensure transparency, fairness and accountability of computer and machine learning algorithms.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1561–1570},
numpages = {10},
keywords = {Fairness, Accountability, International trade law, Application Programming Interface, Transparency, Software, Computer algorithms, Source code, Digital policy},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,Assessing Annotator Identity Sensitivity via Item Response Theory: A Case Study in a Hate Speech Corpus,https://dl.acm.org/doi/10.1145/3531146.3533216,"['Pratik S. Sachdeva', 'Renata Barreto', 'Claudia von Vacano', 'Chris J. Kennedy']",1,"Content Warning: This paper contains content considered profane, hateful, and offensive. Annotators, by labeling data samples, play an essential role in the production of machine learning datasets. Their role is increasingly prevalent for more complex tasks such as hate speech or disinformation classification, where labels may be particularly subjective, as evidenced by low inter-annotator agreement statistics. Annotators may exhibit observable differences in their labeling patterns when grouped by their self-reported demographic identities, such as race, gender, etc. We frame these patterns as annotator identity sensitivities, referring to an annotator‚Äôs increased likelihood of assigning a particular label on a data sample, conditional on a self-reported identity group. We purposefully refrain from using the term annotator bias, which we argue is problematic terminology in such subjective scenarios. Since annotator identity sensitivities can play a role in the patterns learned by machine learning algorithms, quantifying and characterizing them is of paramount importance for fairness and accountability in machine learning. In this work, we utilize item response theory (IRT), a methodological approach developed for measurement theory, to quantify annotator identity sensitivity. IRT models can be constructed to incorporate diverse factors that influence a label on a specific data sample, such as the data sample itself, the annotator, and the labeling instrument‚Äôs wording and response options. An IRT model captures the contributions of these facets to the label via a latent-variable probabilistic model, thereby allowing the direct quantification of annotator sensitivity. As a case study, we examine a hate speech corpus containing over 50,000 social media comments from Reddit, YouTube, and Twitter, rated by 10,000 annotators on 10 components of hate speech (e.g., sentiment, respect, violence, dehumanization, etc.). We leverage three different IRT techniques which are complementary in that they quantify sensitivity from different perspectives: separated measurements, annotator-level interactions, and group-level interactions. We use these techniques to assess whether an annotator‚Äôs racial identity is associated with their ratings on comments that target different racial identities. We find that, after controlling for the estimated hatefulness of social media comments, annotators tended to be more sensitive when rating comments targeting a group they identify with. Specifically, annotators were more likely to rate comments targeting their own racial identity as possessing elements of hate speech. Our results identify a correspondence between annotator identity and the target identity of hate speech comments, and provide a set of tools that can assess annotator identity sensitivity in machine learning datasets at large.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3533216,
author = {Sachdeva, Pratik S. and Barreto, Renata and von Vacano, Claudia and Kennedy, Chris J.},
title = {Assessing Annotator Identity Sensitivity via Item Response Theory: A Case Study in a Hate Speech Corpus},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533216},
doi = {10.1145/3531146.3533216},
abstract = {Content Warning: This paper contains content considered profane, hateful, and offensive. Annotators, by labeling data samples, play an essential role in the production of machine learning datasets. Their role is increasingly prevalent for more complex tasks such as hate speech or disinformation classification, where labels may be particularly subjective, as evidenced by low inter-annotator agreement statistics. Annotators may exhibit observable differences in their labeling patterns when grouped by their self-reported demographic identities, such as race, gender, etc. We frame these patterns as annotator identity sensitivities, referring to an annotator’s increased likelihood of assigning a particular label on a data sample, conditional on a self-reported identity group. We purposefully refrain from using the term annotator bias, which we argue is problematic terminology in such subjective scenarios. Since annotator identity sensitivities can play a role in the patterns learned by machine learning algorithms, quantifying and characterizing them is of paramount importance for fairness and accountability in machine learning. In this work, we utilize item response theory (IRT), a methodological approach developed for measurement theory, to quantify annotator identity sensitivity. IRT models can be constructed to incorporate diverse factors that influence a label on a specific data sample, such as the data sample itself, the annotator, and the labeling instrument’s wording and response options. An IRT model captures the contributions of these facets to the label via a latent-variable probabilistic model, thereby allowing the direct quantification of annotator sensitivity. As a case study, we examine a hate speech corpus containing over 50,000 social media comments from Reddit, YouTube, and Twitter, rated by 10,000 annotators on 10 components of hate speech (e.g., sentiment, respect, violence, dehumanization, etc.). We leverage three different IRT techniques which are complementary in that they quantify sensitivity from different perspectives: separated measurements, annotator-level interactions, and group-level interactions. We use these techniques to assess whether an annotator’s racial identity is associated with their ratings on comments that target different racial identities. We find that, after controlling for the estimated hatefulness of social media comments, annotators tended to be more sensitive when rating comments targeting a group they identify with. Specifically, annotators were more likely to rate comments targeting their own racial identity as possessing elements of hate speech. Our results identify a correspondence between annotator identity and the target identity of hate speech comments, and provide a set of tools that can assess annotator identity sensitivity in machine learning datasets at large.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1585–1603},
numpages = {19},
keywords = {differential rater functioning, item response theory, annotation, hate speech, annotator sensitivity},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,‚ÄúThere Is Not Enough Information‚Äù: On the Effects of Explanations on Perceptions of Informational Fairness and Trustworthiness in Automated Decision-Making,https://dl.acm.org/doi/10.1145/3531146.3533218,"['Jakob Schoeffer', 'Niklas Kuehl', 'Yvette Machowski']",4,"Automated decision systems (ADS) are increasingly used for consequential decision-making. These systems often rely on sophisticated yet opaque machine learning models, which do not allow for understanding how a given decision was arrived at. In this work, we conduct a human subject study to assess people‚Äôs perceptions of informational fairness (i.e., whether people think they are given adequate information on and explanation of the process and its outcomes) and trustworthiness of an underlying ADS when provided with varying types of information about the system. More specifically, we instantiate an ADS in the area of automated loan approval and generate different explanations that are commonly used in the literature. We randomize the amount of information that study participants get to see by providing certain groups of people with the same explanations as others plus additional explanations. From our quantitative analyses, we observe that different amounts of information as well as people‚Äôs (self-assessed) AI literacy significantly influence the perceived informational fairness, which, in turn, positively relates to perceived trustworthiness of the ADS. A comprehensive analysis of qualitative feedback sheds light on people‚Äôs desiderata for explanations, among which are (i) consistency (both with people‚Äôs expectations and across different explanations), (ii) disclosure of monotonic relationships between features and outcome, and (iii) actionability of recommendations.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3533218,
author = {Schoeffer, Jakob and Kuehl, Niklas and Machowski, Yvette},
title = {“There Is Not Enough Information”: On the Effects of Explanations on Perceptions of Informational Fairness and Trustworthiness in Automated Decision-Making},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533218},
doi = {10.1145/3531146.3533218},
abstract = {Automated decision systems (ADS) are increasingly used for consequential decision-making. These systems often rely on sophisticated yet opaque machine learning models, which do not allow for understanding how a given decision was arrived at. In this work, we conduct a human subject study to assess people’s perceptions of informational fairness (i.e., whether people think they are given adequate information on and explanation of the process and its outcomes) and trustworthiness of an underlying ADS when provided with varying types of information about the system. More specifically, we instantiate an ADS in the area of automated loan approval and generate different explanations that are commonly used in the literature. We randomize the amount of information that study participants get to see by providing certain groups of people with the same explanations as others plus additional explanations. From our quantitative analyses, we observe that different amounts of information as well as people’s (self-assessed) AI literacy significantly influence the perceived informational fairness, which, in turn, positively relates to perceived trustworthiness of the ADS. A comprehensive analysis of qualitative feedback sheds light on people’s desiderata for explanations, among which are (i) consistency (both with people’s expectations and across different explanations), (ii) disclosure of monotonic relationships between features and outcome, and (iii) actionability of recommendations.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1616–1628},
numpages = {13},
keywords = {informational fairness, trustworthiness, Automated decision-making, machine learning, explanations, perceptions},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,Characterizing Properties and Trade-offs of Centralized Delegation Mechanisms in Liquid Democracy,https://dl.acm.org/doi/10.1145/3531146.3533219,"['Brian Brubach', 'Audrey Ballarin', 'Heeba Nazeer']",4,"Liquid democracy is a form of transitive delegative democracy that has received a flurry of scholarly attention from the computer science community in recent years. In its simplest form, every agent starts with one vote and may have other votes assigned to them via delegation from other agents. They can choose to delegate all votes assigned to them to another agent or vote directly with all votes assigned to them. However, many proposed realizations of liquid democracy allow for agents to express their delegation/voting preferences in more complex ways (e.g., a ranked list of potential delegates) and employ a centralized delegation mechanism to compute the final vote tally. In doing so, centralized delegation mechanisms can make decisions that affect the outcome of a vote and where/whether agents are able to delegate their votes. Much of the analysis thus far has focused on the ability of these mechanisms to make a correct choice. We extend this analysis by introducing and formalizing other important properties of a centralized delegation mechanism in liquid democracy with respect to crucial features such as accountability, transparency, explainability, fairness, and user agency. In addition, we evaluate existing methods in terms of these properties, show how some prior work can be augmented to achieve desirable properties, prove impossibility results for achieving certain sets of properties simultaneously, and highlight directions for future work.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3533219,
author = {Brubach, Brian and Ballarin, Audrey and Nazeer, Heeba},
title = {Characterizing Properties and Trade-Offs of Centralized Delegation Mechanisms in Liquid Democracy},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533219},
doi = {10.1145/3531146.3533219},
abstract = {Liquid democracy is a form of transitive delegative democracy that has received a flurry of scholarly attention from the computer science community in recent years. In its simplest form, every agent starts with one vote and may have other votes assigned to them via delegation from other agents. They can choose to delegate all votes assigned to them to another agent or vote directly with all votes assigned to them. However, many proposed realizations of liquid democracy allow for agents to express their delegation/voting preferences in more complex ways (e.g., a ranked list of potential delegates) and employ a centralized delegation mechanism to compute the final vote tally. In doing so, centralized delegation mechanisms can make decisions that affect the outcome of a vote and where/whether agents are able to delegate their votes. Much of the analysis thus far has focused on the ability of these mechanisms to make a correct choice. We extend this analysis by introducing and formalizing other important properties of a centralized delegation mechanism in liquid democracy with respect to crucial features such as accountability, transparency, explainability, fairness, and user agency. In addition, we evaluate existing methods in terms of these properties, show how some prior work can be augmented to achieve desirable properties, prove impossibility results for achieving certain sets of properties simultaneously, and highlight directions for future work.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1629–1638},
numpages = {10},
keywords = {accountability, voting, fairness, computational social choice, liquid democracy, transparency},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,Human-Algorithm Collaboration: Achieving Complementarity and Avoiding Unfairness,https://dl.acm.org/doi/10.1145/3531146.3533221,"['Kate Donahue', 'Alexandra Chouldechova', 'Krishnaram Kenthapadi']",2,"Much of machine learning research focuses on predictive accuracy: given a task, create a machine learning model (or algorithm) that maximizes accuracy. In many settings, however, the final prediction or decision of a system is under the control of a human, who uses an algorithm‚Äôs output along with their own personal expertise in order to produce a combined prediction. One ultimate goal of such collaborative systems is complementarity: that is, to produce lower loss (equivalently, greater payoff or utility) than either the human or algorithm alone. However, experimental results have shown that even in carefully-designed systems, complementary performance can be elusive. Our work provides three key contributions. First, we provide a theoretical framework for modeling simple human-algorithm systems and demonstrate that multiple prior analyses can be expressed within it. Next, we use this model to prove conditions where complementarity is impossible, and give constructive examples of where complementarity is achievable. Finally, we discuss the implications of our findings, especially with respect to the fairness of a classifier. In sum, these results deepen our understanding of key factors influencing the combined performance of human-algorithm systems, giving insight into how algorithmic tools can best be designed for collaborative environments.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3533221,
author = {Donahue, Kate and Chouldechova, Alexandra and Kenthapadi, Krishnaram},
title = {Human-Algorithm Collaboration: Achieving Complementarity and Avoiding Unfairness},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533221},
doi = {10.1145/3531146.3533221},
abstract = {Much of machine learning research focuses on predictive accuracy: given a task, create a machine learning model (or algorithm) that maximizes accuracy. In many settings, however, the final prediction or decision of a system is under the control of a human, who uses an algorithm’s output along with their own personal expertise in order to produce a combined prediction. One ultimate goal of such collaborative systems is complementarity: that is, to produce lower loss (equivalently, greater payoff or utility) than either the human or algorithm alone. However, experimental results have shown that even in carefully-designed systems, complementary performance can be elusive. Our work provides three key contributions. First, we provide a theoretical framework for modeling simple human-algorithm systems and demonstrate that multiple prior analyses can be expressed within it. Next, we use this model to prove conditions where complementarity is impossible, and give constructive examples of where complementarity is achievable. Finally, we discuss the implications of our findings, especially with respect to the fairness of a classifier. In sum, these results deepen our understanding of key factors influencing the combined performance of human-algorithm systems, giving insight into how algorithmic tools can best be designed for collaborative environments.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1639–1656},
numpages = {18},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,Fairness-aware Model-agnostic Positive and Unlabeled Learning,https://dl.acm.org/doi/10.1145/3531146.3533225,"['Ziwei Wu', 'Jingrui He']",1,"With the increasing application of machine learning in high-stake decision-making problems, potential algorithmic bias towards people from certain social groups poses negative impacts on individuals and our society at large. In the real-world scenario, many such problems involve positive and unlabeled data such as medical diagnosis, criminal risk assessment and recommender systems. For instance, in medical diagnosis, only the diagnosed diseases will be recorded (positive) while others will not (unlabeled). Despite the large amount of existing work on fairness-aware machine learning in the (semi-)supervised and unsupervised settings, the fairness issue is largely under-explored in the aforementioned Positive and Unlabeled Learning (PUL) context, where it is usually more severe. In this paper, to alleviate this tension, we propose a fairness-aware PUL method named FairPUL. In particular, for binary classification over individuals from two populations, we aim to achieve similar true positive rates and false positive rates in both populations as our fairness metric. Based on the analysis of the optimal fair classifier for PUL, we design a model-agnostic post-processing framework, leveraging both the positive examples and unlabeled ones. Our framework is proven to be statistically consistent in terms of both the classification error and the fairness metric. Experiments on the synthetic and real-world data sets demonstrate that our framework outperforms state-of-the-art in both PUL and fair classification.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3287560.3287586,
author = {Celis, L. Elisa and Huang, Lingxiao and Keswani, Vijay and Vishnoi, Nisheeth K.},
title = {Classification with Fairness Constraints: A Meta-Algorithm with Provable Guarantees},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287586},
doi = {10.1145/3287560.3287586},
abstract = {Developing classification algorithms that are fair with respect to sensitive attributes of the data is an important problem due to the increased deployment of classification algorithms in societal contexts. Several recent works have focused on studying classification with respect to specific fairness metrics, modeled the corresponding fair classification problem as constrained optimization problems, and developed tailored algorithms to solve them. Despite this, there still remain important metrics for which there are no fair classifiers with theoretical guarantees; primarily because the resulting optimization problem is non-convex. The main contribution of this paper is a meta-algorithm for classification that can take as input a general class of fairness constraints with respect to multiple non-disjoint and multi-valued sensitive attributes, and which comes with provable guarantees. In particular, our algorithm can handle non-convex ""linear fractional"" constraints (which includes fairness constraints such as predictive parity) for which no prior algorithm was known. Key to our results is an algorithm for a family of classification problems with convex constraints along with a reduction from classification problems with linear fractional constraints to this family. Empirically, we observe that our algorithm is fast, can achieve near-perfect fairness with respect to various fairness metrics, and the loss in accuracy due to the imposed fairness constraints is often small.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {319–328},
numpages = {10},
keywords = {Algorithmic Fairness, Classification},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}"
FACCT,2022,Demographic-Reliant Algorithmic Fairness: Characterizing the Risks of Demographic Data Collection in the Pursuit of Fairness,https://dl.acm.org/doi/10.1145/3531146.3533226,"['McKane Andrus', 'Sarah Villeneuve']",7,"Most proposed algorithmic fairness techniques require access to demographic data in order to make performance comparisons and standardizations across groups, however this data is largely unavailable in practice, hindering the widespread adoption of algorithmic fairness. Through this paper, we consider calls to collect more data on demographics to enable algorithmic fairness and challenge the notion that discrimination can be overcome with smart enough technical methods and sufficient data. We show how these techniques largely ignore broader questions of data governance and systemic oppression when categorizing individuals for the purpose of fairer algorithmic processing. In this work, we explore under what conditions demographic data should be collected and used to enable algorithmic fairness methods by characterizing a range of social risks to individuals and communities. For the risks to individuals we consider the unique privacy risks of sensitive attributes, the possible harms of miscategorization and misrepresentation, and the use of sensitive data beyond data subjects‚Äô expectations. Looking more broadly, the risks to entire groups and communities include the expansion of surveillance infrastructure in the name of fairness, misrepresenting and mischaracterizing what it means to be part of a demographic group, and ceding the ability to define what constitutes biased or unfair treatment. We argue that, by confronting these questions before and during the collection of demographic data, algorithmic fairness methods are more likely to actually mitigate harmful treatment disparities without reinforcing systems of oppression. Towards this end, we assess privacy-focused methods of data collection and use and participatory data governance structures as proposals for more responsibly collecting demographic data.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3533226,
author = {Andrus, McKane and Villeneuve, Sarah},
title = {Demographic-Reliant Algorithmic Fairness: Characterizing the Risks of Demographic Data Collection in the Pursuit of Fairness},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533226},
doi = {10.1145/3531146.3533226},
abstract = {Most proposed algorithmic fairness techniques require access to demographic data in order to make performance comparisons and standardizations across groups, however this data is largely unavailable in practice, hindering the widespread adoption of algorithmic fairness. Through this paper, we consider calls to collect more data on demographics to enable algorithmic fairness and challenge the notion that discrimination can be overcome with smart enough technical methods and sufficient data. We show how these techniques largely ignore broader questions of data governance and systemic oppression when categorizing individuals for the purpose of fairer algorithmic processing. In this work, we explore under what conditions demographic data should be collected and used to enable algorithmic fairness methods by characterizing a range of social risks to individuals and communities. For the risks to individuals we consider the unique privacy risks of sensitive attributes, the possible harms of miscategorization and misrepresentation, and the use of sensitive data beyond data subjects’ expectations. Looking more broadly, the risks to entire groups and communities include the expansion of surveillance infrastructure in the name of fairness, misrepresenting and mischaracterizing what it means to be part of a demographic group, and ceding the ability to define what constitutes biased or unfair treatment. We argue that, by confronting these questions before and during the collection of demographic data, algorithmic fairness methods are more likely to actually mitigate harmful treatment disparities without reinforcing systems of oppression. Towards this end, we assess privacy-focused methods of data collection and use and participatory data governance structures as proposals for more responsibly collecting demographic data.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1709–1721},
numpages = {13},
keywords = {categorization, demographic data, measurement, race, sensitive data, gender, identity, discrimination, fairness, sexuality},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,Adversarial Scrutiny of Evidentiary Statistical Software,https://dl.acm.org/doi/10.1145/3531146.3533228,"['Rediet Abebe', 'Moritz Hardt', 'Angela Jin', 'John Miller', 'Ludwig Schmidt', 'Rebecca Wexler']",1,"The U.S. criminal legal system increasingly relies on software output to convict and incarcerate people. In a large number of cases each year, the government makes these consequential decisions based on evidence from statistical software‚Äîsuch as probabilistic genotyping, environmental audio detection and toolmark analysis tools‚Äîthat the defense counsel cannot fully cross-examine or scrutinize. This undermines the commitments of the adversarial criminal legal system, which relies on the defense‚Äôs ability to probe and test the prosecution‚Äôs case to safeguard individual rights. Responding to this need to adversarially scrutinize output from such software, we propose robust adversarial testing as a framework to examine the validity of evidentiary statistical software. We define and operationalize this notion of robust adversarial testing for defense use by drawing on a large body of recent work in robust machine learning and algorithmic fairness. We demonstrate how this framework both standardizes the process for scrutinizing such tools and empowers defense lawyers to examine their validity for instances most relevant to the case at hand. We further discuss existing structural and institutional challenges within the U.S. criminal legal system which may create barriers for implementing this framework and close with a discussion on policy changes that could help address these concerns.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3533228,
author = {Abebe, Rediet and Hardt, Moritz and Jin, Angela and Miller, John and Schmidt, Ludwig and Wexler, Rebecca},
title = {Adversarial Scrutiny of Evidentiary Statistical Software},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533228},
doi = {10.1145/3531146.3533228},
abstract = {The U.S. criminal legal system increasingly relies on software output to convict and incarcerate people. In a large number of cases each year, the government makes these consequential decisions based on evidence from statistical software—such as probabilistic genotyping, environmental audio detection and toolmark analysis tools—that the defense counsel cannot fully cross-examine or scrutinize. This undermines the commitments of the adversarial criminal legal system, which relies on the defense’s ability to probe and test the prosecution’s case to safeguard individual rights. Responding to this need to adversarially scrutinize output from such software, we propose robust adversarial testing as a framework to examine the validity of evidentiary statistical software. We define and operationalize this notion of robust adversarial testing for defense use by drawing on a large body of recent work in robust machine learning and algorithmic fairness. We demonstrate how this framework both standardizes the process for scrutinizing such tools and empowers defense lawyers to examine their validity for instances most relevant to the case at hand. We further discuss existing structural and institutional challenges within the U.S. criminal legal system which may create barriers for implementing this framework and close with a discussion on policy changes that could help address these concerns.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1733–1746},
numpages = {14},
keywords = {robust machine learning, evidentiary software, statistical software, black-box software, adversarial scrutiny},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,Models for understanding and quantifying feedback in societal systems,https://dl.acm.org/doi/10.1145/3531146.3533230,"['Lydia Reader', 'Pegah Nokhiz', 'Cathleen Power', 'Neal Patwari', 'Suresh Venkatasubramanian', 'Sorelle Friedler']",2,"When it comes to long-term fairness in decision-making settings, many studies have focused on closed systems with a specific appointed decision-maker and certain engagement rules in place. However, if the objective is to achieve equity in a broader societal system, studying the system in isolation is insufficient. In a societal system, neither a singular decision maker nor defined agent behavior rules exist. Additionally, analysis of societal systems can be complicated by the presence of feedback, in which historical and current inequities influence future inequity. In this paper, we present a model to quantify feedback in social systems so that the long-term effects of a policy or decision process may be investigated, even when the feedback mechanisms are not individually characterized. We explore the dynamics of real social systems and find that many examples of feedback are qualitatively similar in their temporal characteristics. Using a key idea in linear systems theory, namely proportional-integral-derivative (PID) feedback, we propose a model to quantify three types of feedback. We illustrate how different components of the PID capture analogous aspects of societal dynamics such as the persistence of current inequity, the cumulative effects of long-term inequity, and the response to the speed at which society is changing. Our model does not attempt to describe underlying systems or capture individual actions. It is a system-based approach to study inequity in feedback loops, and as a result unlocks a direction to study social systems that would otherwise be almost impossible to model and can only be observed. Our framework helps elucidate the ability of fair policies to produce and sustain equity in the long-term.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3533230,
author = {Reader, Lydia and Nokhiz, Pegah and Power, Cathleen and Patwari, Neal and Venkatasubramanian, Suresh and Friedler, Sorelle},
title = {Models for Understanding and Quantifying Feedback in Societal Systems},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533230},
doi = {10.1145/3531146.3533230},
abstract = {When it comes to long-term fairness in decision-making settings, many studies have focused on closed systems with a specific appointed decision-maker and certain engagement rules in place. However, if the objective is to achieve equity in a broader societal system, studying the system in isolation is insufficient. In a societal system, neither a singular decision maker nor defined agent behavior rules exist. Additionally, analysis of societal systems can be complicated by the presence of feedback, in which historical and current inequities influence future inequity. In this paper, we present a model to quantify feedback in social systems so that the long-term effects of a policy or decision process may be investigated, even when the feedback mechanisms are not individually characterized. We explore the dynamics of real social systems and find that many examples of feedback are qualitatively similar in their temporal characteristics. Using a key idea in linear systems theory, namely proportional-integral-derivative (PID) feedback, we propose a model to quantify three types of feedback. We illustrate how different components of the PID capture analogous aspects of societal dynamics such as the persistence of current inequity, the cumulative effects of long-term inequity, and the response to the speed at which society is changing. Our model does not attempt to describe underlying systems or capture individual actions. It is a system-based approach to study inequity in feedback loops, and as a result unlocks a direction to study social systems that would otherwise be almost impossible to model and can only be observed. Our framework helps elucidate the ability of fair policies to produce and sustain equity in the long-term.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1765–1775},
numpages = {11},
keywords = {inequity, feedback, societal systems},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,Marrying Fairness and Explainability in Supervised Learning,https://dl.acm.org/doi/10.1145/3531146.3533236,"['Przemyslaw A. Grabowicz', 'Nicholas Perello', 'Aarshee Mishra']",5,"Machine learning algorithms that aid human decision-making may inadvertently discriminate against certain protected groups. Therefore, we formalize direct discrimination as a direct causal effect of the protected attributes on the decisions, while induced discrimination as a change in the causal influence of non-protected features associated with the protected attributes. The measurements of marginal direct effect (MDE) and SHapley Additive exPlanations (SHAP) reveal that state-of-the-art fair learning methods can induce discrimination via association or reverse discrimination in synthetic and real-world datasets. To inhibit discrimination in algorithmic systems, we propose to nullify the influence of the protected attribute on the output of the system, while preserving the influence of remaining features. We introduce and study post-processing methods achieving such objectives, finding that they yield relatively high model accuracy, prevent direct discrimination, and diminishes various disparity measures, e.g., demographic disparity.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3533236,
author = {Grabowicz, Przemyslaw A. and Perello, Nicholas and Mishra, Aarshee},
title = {Marrying Fairness and Explainability in Supervised Learning},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533236},
doi = {10.1145/3531146.3533236},
abstract = {Machine learning algorithms that aid human decision-making may inadvertently discriminate against certain protected groups. Therefore, we formalize direct discrimination as a direct causal effect of the protected attributes on the decisions, while induced discrimination as a change in the causal influence of non-protected features associated with the protected attributes. The measurements of marginal direct effect (MDE) and SHapley Additive exPlanations (SHAP) reveal that state-of-the-art fair learning methods can induce discrimination via association or reverse discrimination in synthetic and real-world datasets. To inhibit discrimination in algorithmic systems, we propose to nullify the influence of the protected attribute on the output of the system, while preserving the influence of remaining features. We introduce and study post-processing methods achieving such objectives, finding that they yield relatively high model accuracy, prevent direct discrimination, and diminishes various disparity measures, e.g., demographic disparity.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1905–1916},
numpages = {12},
keywords = {algorithmic fairness, supervised learning, machine learning, explainability, discrimination},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,How Platform-User Power Relations Shape Algorithmic Accountability: A Case Study of Instant Loan Platforms and Financially Stressed Users in India,https://dl.acm.org/doi/10.1145/3531146.3533237,"['Divya Ramesh', 'Vaishnav Kameswaran', 'Ding Wang', 'Nithya Sambasivan']",2,"Accountability, a requisite for responsible AI, can be facilitated through transparency mechanisms such as audits and explainability. However, prior work suggests that the success of these mechanisms may be limited to Global North contexts; understanding the limitations of current interventions in varied socio-political conditions is crucial to help policymakers facilitate wider accountability. To do so, we examined the mediation of accountability in the existing interactions between vulnerable users and a ‚Äòhigh-risk‚Äô AI system in a Global South setting. We report on a qualitative study with 29 financially-stressed users of instant loan platforms in India. We found that users experienced intense feelings of indebtedness for the ‚Äòboon‚Äô of instant loans, and perceived huge obligations towards loan platforms. Users fulfilled obligations by accepting harsh terms and conditions, over-sharing sensitive data, and paying high fees to unknown and unverified lenders. Users demonstrated a dependence on loan platforms by persisting with such behaviors despite risks of harms such as abuse, recurring debts, discrimination, privacy harms, and self-harm to them. Instead of being enraged with loan platforms, users assumed responsibility for their negative experiences, thus releasing the high-powered loan platforms from accountability obligations. We argue that accountability is shaped by platform-user power relations, and urge caution to policymakers in adopting a purely technical approach to fostering algorithmic accountability. Instead, we call for situated interventions that enhance agency of users, enable meaningful transparency, reconfigure designer-user relations, and prompt a critical reflection in practitioners towards wider accountability. We conclude with implications for responsibly deploying AI in FinTech applications in India and beyond.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3533237,
author = {Ramesh, Divya and Kameswaran, Vaishnav and Wang, Ding and Sambasivan, Nithya},
title = {How Platform-User Power Relations Shape Algorithmic Accountability: A Case Study of Instant Loan Platforms and Financially Stressed Users in India},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533237},
doi = {10.1145/3531146.3533237},
abstract = {Accountability, a requisite for responsible AI, can be facilitated through transparency mechanisms such as audits and explainability. However, prior work suggests that the success of these mechanisms may be limited to Global North contexts; understanding the limitations of current interventions in varied socio-political conditions is crucial to help policymakers facilitate wider accountability. To do so, we examined the mediation of accountability in the existing interactions between vulnerable users and a ‘high-risk’ AI system in a Global South setting. We report on a qualitative study with 29 financially-stressed users of instant loan platforms in India. We found that users experienced intense feelings of indebtedness for the ‘boon’ of instant loans, and perceived huge obligations towards loan platforms. Users fulfilled obligations by accepting harsh terms and conditions, over-sharing sensitive data, and paying high fees to unknown and unverified lenders. Users demonstrated a dependence on loan platforms by persisting with such behaviors despite risks of harms such as abuse, recurring debts, discrimination, privacy harms, and self-harm to them. Instead of being enraged with loan platforms, users assumed responsibility for their negative experiences, thus releasing the high-powered loan platforms from accountability obligations. We argue that accountability is shaped by platform-user power relations, and urge caution to policymakers in adopting a purely technical approach to fostering algorithmic accountability. Instead, we call for situated interventions that enhance agency of users, enable meaningful transparency, reconfigure designer-user relations, and prompt a critical reflection in practitioners towards wider accountability. We conclude with implications for responsibly deploying AI in FinTech applications in India and beyond.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1917–1928},
numpages = {12},
keywords = {socio-technical systems, instant loans, human-ai interaction, algorithmic fairness, algorithmic accountability},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,"Fair ranking: a critical review, challenges, and future directions",https://dl.acm.org/doi/10.1145/3531146.3533238,"['Gourab K. Patro', 'Lorenzo Porcaro', 'Laura Mitchell', 'Qiuyue Zhang', 'Meike Zehlike', 'Nikhil Garg']",11,"Ranking, recommendation, and retrieval systems are widely used in online platforms and other societal systems, including e-commerce, media-streaming, admissions, gig platforms, and hiring. In the recent past, a large ‚Äúfair ranking‚Äù research literature has been developed around making these systems fair to the individuals, providers, or content that are being ranked. Most of this literature defines fairness for a single instance of retrieval, or as a simple additive notion for multiple instances of retrievals over time. This work provides a critical overview of this literature, detailing the often context-specific concerns that such approaches miss: the gap between high ranking placements and true provider utility, spillovers and compounding effects over time, induced strategic incentives, and the effect of statistical uncertainty. We then provide a path forward for a more holistic and impact-oriented fair ranking research agenda, including methodological lessons from other fields and the role of the broader stakeholder community in overcoming data bottlenecks and designing effective regulatory environments.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3533238,
author = {Patro, Gourab K. and Porcaro, Lorenzo and Mitchell, Laura and Zhang, Qiuyue and Zehlike, Meike and Garg, Nikhil},
title = {Fair Ranking: A Critical Review, Challenges, and Future Directions},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533238},
doi = {10.1145/3531146.3533238},
abstract = {Ranking, recommendation, and retrieval systems are widely used in online platforms and other societal systems, including e-commerce, media-streaming, admissions, gig platforms, and hiring. In the recent past, a large “fair ranking” research literature has been developed around making these systems fair to the individuals, providers, or content that are being ranked. Most of this literature defines fairness for a single instance of retrieval, or as a simple additive notion for multiple instances of retrievals over time. This work provides a critical overview of this literature, detailing the often context-specific concerns that such approaches miss: the gap between high ranking placements and true provider utility, spillovers and compounding effects over time, induced strategic incentives, and the effect of statistical uncertainty. We then provide a path forward for a more holistic and impact-oriented fair ranking research agenda, including methodological lessons from other fields and the role of the broader stakeholder community in overcoming data bottlenecks and designing effective regulatory environments.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1929–1942},
numpages = {14},
keywords = {Exposure, Recommendation, Algorithmic Impact Assessment, Ranking, Fairness, Strategic Behaviour},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,CounterFAccTual: How FAccT Undermines Its Organizing Principles,https://dl.acm.org/doi/10.1145/3531146.3533241,"['Ben Gansky', 'Sean McDonald']",6,"This essay joins recent scholarship in arguing that FAccT's fundamental framing of the potential to achieve the normative conditions for justice through bettering the design of algorithmic systems is counterproductive to achieving said justice in practice. Insofar as the FAccT community's research tends to prioritize design-stage interventions, it ignores the fact that the majority of the contextual factors that practically determine FAccT outcomes happen in the implementation and impact stages of AI/ML lifecycles. We analyze an emergent and widely-cited movement within the FAccT community for attempting to honor the centrality of contextual factors in shaping social outcomes, a set of strategies we term ‚Äòmetadata maximalism‚Äô. Symptomatic of design-centered approaches, metadata maximalism abstracts away its reliance on institutions and structures of justice that are, by every observable metric, already struggling (where not failing) to provide accessible, enforceable rights. These justice infrastructures, moreover, are currently wildly under-equipped to manage the disputes arising from digital transformation and machine learning. The political economy of AI/ML implementation provides further obstructions to realizing rights. Data and software supply chains, in tandem with intellectual property protections, introduce structural sources of opacity. Where duties of care to vulnerable persons should reign, profit incentives are given legal and regulatory primacy. Errors are inevitable and inextricable from the development of machine learning systems. In the face of these realities, FAccT programs, including metadata maximalism, tend to project their efforts in a fundamentally counter-factual universe: one in which functioning institutions and processes for due diligence in implementation and for redress of harms are working and ready to interoperate with. Unfortunately, in our world, these institutions and processes have been captured by the interests they are meant to hold accountable, intentionally hollowed-out, and/or were never designed to function in today's sociotechnical landscape. Continuing to produce (fair! accountable! transparent!) data-enabled systems that operate in high-impact areas, irrespective of this landscape's radically insufficient paths to justice, given the unavoidability of errors and/or intentional misuse in implementation, and the exhaustively-demonstrated disproportionate distribution of resulting harms onto already-marginalized communities, is a choice - a choice to be CounterFAccTual.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3533241,
author = {Gansky, Ben and McDonald, Sean},
title = {CounterFAccTual: How FAccT Undermines Its Organizing Principles},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533241},
doi = {10.1145/3531146.3533241},
abstract = {This essay joins recent scholarship in arguing that FAccT's fundamental framing of the potential to achieve the normative conditions for justice through bettering the design of algorithmic systems is counterproductive to achieving said justice in practice. Insofar as the FAccT community's research tends to prioritize design-stage interventions, it ignores the fact that the majority of the contextual factors that practically determine FAccT outcomes happen in the implementation and impact stages of AI/ML lifecycles.We analyze an emergent and widely-cited movement within the FAccT community for attempting to honor the centrality of contextual factors in shaping social outcomes, a set of strategies we term ‘metadata maximalism’. Symptomatic of design-centered approaches, metadata maximalism abstracts away its reliance on institutions and structures of justice that are, by every observable metric, already struggling (where not failing) to provide accessible, enforceable rights. These justice infrastructures, moreover, are currently wildly under-equipped to manage the disputes arising from digital transformation and machine learning. The political economy of AI/ML implementation provides further obstructions to realizing rights. Data and software supply chains, in tandem with intellectual property protections, introduce structural sources of opacity. Where duties of care to vulnerable persons should reign, profit incentives are given legal and regulatory primacy. Errors are inevitable and inextricable from the development of machine learning systems.In the face of these realities, FAccT programs, including metadata maximalism, tend to project their efforts in a fundamentally counter-factual universe: one in which functioning institutions and processes for due diligence in implementation and for redress of harms are working and ready to interoperate with. Unfortunately, in our world, these institutions and processes have been captured by the interests they are meant to hold accountable, intentionally hollowed-out, and/or were never designed to function in today's sociotechnical landscape. Continuing to produce (fair! accountable! transparent!) data-enabled systems that operate in high-impact areas, irrespective of this landscape's radically insufficient paths to justice, given the unavoidability of errors and/or intentional misuse in implementation, and the exhaustively-demonstrated disproportionate distribution of resulting harms onto already-marginalized communities, is a choice - a choice to be CounterFAccTual.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1982–1992},
numpages = {11},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,What is Proxy Discrimination?,https://dl.acm.org/doi/10.1145/3531146.3533242,['Michael Carl Tschantz'],0,"The near universal condemnation of proxy discrimination hides a disagreement over what it is. This work surveys various notions of proxy and proxy discrimination found in prior work and represents them in a common framework. These notions variously turn on statistical dependencies, causal effects, and intentions. It discusses the limitations and uses of each notation and of the concept as a whole.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3533242,
author = {Tschantz, Michael Carl},
title = {What is Proxy Discrimination?},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533242},
doi = {10.1145/3531146.3533242},
abstract = {The near universal condemnation of proxy discrimination hides a disagreement over what it is. This work surveys various notions of proxy and proxy discrimination found in prior work and represents them in a common framework. These notions variously turn on statistical dependencies, causal effects, and intentions. It discusses the limitations and uses of each notation and of the concept as a whole.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1993–2003},
numpages = {11},
keywords = {discrimination, proxy, conceptual analysis},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,Uncertainty and the Social Planner‚Äôs Problem: Why Sample Complexity Matters,https://dl.acm.org/doi/10.1145/3531146.3533243,['Cyrus Cousins'],3,"Welfare measures overall utility across a population, whereas malfare measures overall disutility, and the social planner‚Äôs problem can be cast either as maximizing the former or minimizing the latter. We show novel bounds on the expectations and tail probabilities of estimators of welfare, malfare, and regret of per-group (dis)utility values, where estimates are made from a finite sample drawn from each group. In particular, we consider estimating these quantities for individual functions (e.g., allocations or classifiers) with standard probabilistic bounds, and optimizing and bounding generalization error over hypothesis classes (i.e., we quantify overfitting) using Rademacher averages. We then study algorithmic fairness through the lens of sample complexity, finding that because marginalized or minority groups are often understudied, and fewer data are therefore available, the social planner is more likely to overfit to these groups, thus even models that seem fair in training can be systematically biased against such groups. We argue that this effect can be mitigated by ensuring sufficient sample sizes for each group, and our sample complexity analysis characterizes these sample sizes. Motivated by these conclusions, we present progressive sampling algorithms to efficiently optimize various fairness objectives.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3533243,
author = {Cousins, Cyrus},
title = {Uncertainty and the Social Planner’s Problem: Why Sample Complexity Matters},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533243},
doi = {10.1145/3531146.3533243},
abstract = {Welfare measures overall utility across a population, whereas malfare measures overall disutility, and the social planner’s problem can be cast either as maximizing the former or minimizing the latter. We show novel bounds on the expectations and tail probabilities of estimators of welfare, malfare, and regret of per-group (dis)utility values, where estimates are made from a finite sample drawn from each group. In particular, we consider estimating these quantities for individual functions (e.g., allocations or classifiers) with standard probabilistic bounds, and optimizing and bounding generalization error over hypothesis classes (i.e., we quantify overfitting) using Rademacher averages. We then study algorithmic fairness through the lens of sample complexity, finding that because marginalized or minority groups are often understudied, and fewer data are therefore available, the social planner is more likely to overfit to these groups, thus even models that seem fair in training can be systematically biased against such groups. We argue that this effect can be mitigated by ensuring sufficient sample sizes for each group, and our sample complexity analysis characterizes these sample sizes. Motivated by these conclusions, we present progressive sampling algorithms to efficiently optimize various fairness objectives.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {2004–2015},
numpages = {12},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,Is calibration a fairness requirement?: An argument from the point of view of moral philosophy and decision theory,https://dl.acm.org/doi/10.1145/3531146.3533245,"['Michele Loi', 'Christoph Heitz']",1,"In this paper, we provide a moral analysis of two criteria of statistical fairness debated in the machine learning literature: 1) calibration between groups and 2) equality of false positive and false negative rates between groups. In our paper, we focus on moral arguments in support of either measure. The conflict between group calibration vs. false positive and false negative rate equality is one of the core issues in the debate about group fairness definitions among practitioners. For any thorough moral analysis, the meaning of the term ‚Äúfairness‚Äù has to be made explicit and defined properly. For our paper, we equate fairness with (non-)discrimination, which is a legitimate understanding in the discussion about group fairness. More specifically, we equate it with ‚Äúprima facie wrongful discrimination‚Äù in the sense this is used in Prof. Lippert-Rasmussen's treatment of this definition. In this paper, we argue that a violation of group calibration may be unfair in some cases, but not unfair in others. Our argument analyzes in great detail two specific hypothetical examples of usage of predictions in decision making. The most important practical implication is that between-group calibration is defensible as a bias standard in some cases but not others; we show this by referring to examples in which the violation of between-group calibration is discriminatory, and others in which it is not. This is in line with claims already advanced in the literature, that algorithmic fairness should be defined in a way that is sensitive to context. The most important practical implication is that arguments based on examples in which fairness requires between-group calibration, or equality in the false-positive/false-negative rates, do no generalize. For it may be that group calibration is a fairness requirement in one case, but not in another.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3533245,
author = {Loi, Michele and Heitz, Christoph},
title = {Is Calibration a Fairness Requirement? An Argument from the Point of View of Moral Philosophy and Decision Theory},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533245},
doi = {10.1145/3531146.3533245},
abstract = {In this paper, we provide a moral analysis of two criteria of statistical fairness debated in the machine learning literature: 1) calibration between groups and 2) equality of false positive and false negative rates between groups. In our paper, we focus on moral arguments in support of either measure. The conflict between group calibration vs. false positive and false negative rate equality is one of the core issues in the debate about group fairness definitions among practitioners. For any thorough moral analysis, the meaning of the term “fairness” has to be made explicit and defined properly. For our paper, we equate fairness with (non-)discrimination, which is a legitimate understanding in the discussion about group fairness. More specifically, we equate it with “prima facie wrongful discrimination” in the sense this is used in Prof. Lippert-Rasmussen's treatment of this definition. In this paper, we argue that a violation of group calibration may be unfair in some cases, but not unfair in others. Our argument analyzes in great detail two specific hypothetical examples of usage of predictions in decision making. The most important practical implication is that between-group calibration is defensible as a bias standard in some cases but not others; we show this by referring to examples in which the violation of between-group calibration is discriminatory, and others in which it is not. This is in line with claims already advanced in the literature, that algorithmic fairness should be defined in a way that is sensitive to context. The most important practical implication is that arguments based on examples in which fairness requires between-group calibration, or equality in the false-positive/false-negative rates, do no generalize. For it may be that group calibration is a fairness requirement in one case, but not in another.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {2026–2034},
numpages = {9},
keywords = {fairness, opportunity, equalized odds, calibration, prediction},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,Promoting Fairness in Learned Models by Learning to Active Learn under Parity Constraints,https://dl.acm.org/doi/10.1145/3531146.3534632,"['Amr Sharaf', 'Hal Daume III', 'Renkun Ni']",10,"Machine learning models can have consequential effects when used to automate decisions, and disparities between groups of people in the error rates of those decisions can lead to harms suffered more by some groups than others. Past algorithmic approaches aim to enforce parity across groups given a fixed set of training data; instead, we ask: what if we can gather more data to mitigate disparities? We develop a meta-learning algorithm for parity-constrained active learning that learns a policy to decide which labels to query so as to maximize accuracy subject to parity constraints. To optimize the active learning policy, our proposed algorithm formulates the parity-constrained active learning task as a bi-level optimization problem. The inner level corresponds to training a classifier on a subset of labeled examples. The outer level corresponds to updating the selection policy choosing this subset to achieve a desired fairness and accuracy behavior on the trained classifier. To solve this constrained bi-level optimization problem, we employ the Forward-Backward Splitting optimization method. Empirically, across several parity metrics and classification tasks, our approach outperforms alternatives by a large margin.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3534632,
author = {Sharaf, Amr and Daume III, Hal and Ni, Renkun},
title = {Promoting Fairness in Learned Models by Learning to Active Learn under Parity Constraints},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3534632},
doi = {10.1145/3531146.3534632},
abstract = {Machine learning models can have consequential effects when used to automate decisions, and disparities between groups of people in the error rates of those decisions can lead to harms suffered more by some groups than others. Past algorithmic approaches aim to enforce parity across groups given a fixed set of training data; instead, we ask: what if we can gather more data to mitigate disparities? We develop a meta-learning algorithm for parity-constrained active learning that learns a policy to decide which labels to query so as to maximize accuracy subject to parity constraints. To optimize the active learning policy, our proposed algorithm formulates the parity-constrained active learning task as a bi-level optimization problem. The inner level corresponds to training a classifier on a subset of labeled examples. The outer level corresponds to updating the selection policy choosing this subset to achieve a desired fairness and accuracy behavior on the trained classifier. To solve this constrained bi-level optimization problem, we employ the Forward-Backward Splitting optimization method. Empirically, across several parity metrics and classification tasks, our approach outperforms alternatives by a large margin.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {2149–2156},
numpages = {8},
keywords = {active learning, meta-learning},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,Fast online ranking with fairness of exposure,https://dl.acm.org/doi/10.1145/3531146.3534633,"['Nicolas Usunier', 'Virginie Do', 'Elvis Dohmatob']",3,"As recommender systems become increasingly central for sorting and prioritizing the content available online, they have a growing impact on the opportunities or revenue of their items producers. For instance, they influence which recruiter a resume is recommended to, or to whom and how much a music track, video or news article is being exposed. This calls for recommendation approaches that not only maximize (a proxy of) user satisfaction, but also consider some notion of fairness in the exposure of items or groups of items. Formally, such recommendations are usually obtained by maximizing a concave objective function in the space of randomized rankings. When the total exposure of an item is defined as the sum of its exposure over users, the optimal rankings of every users become coupled, which makes the optimization process challenging. Existing approaches to find these rankings either solve the global optimization problem in a batch setting, i.e., for all users at once, which makes them inapplicable at scale, or are based on heuristics that have weak theoretical guarantees. In this paper, we propose the first efficient online algorithm to optimize concave objective functions in the space of rankings which applies to every concave and smooth objective function, such as the ones found for fairness of exposure. Based on online variants of the Frank-Wolfe algorithm, we show that our algorithm is computationally fast, generating rankings on-the-fly with computation cost dominated by the sort operation, memory efficient, and has strong theoretical guarantees. Compared to baseline policies that only maximize user-side performance, our algorithm allows to incorporate complex fairness of exposure criteria in the recommendations with negligible computational overhead. We present experiments on artificial music and movie recommendation tasks using Last.fm and MovieLens datasets which suggest that in practice, the algorithm rapidly reaches good performances on three different objectives representing different fairness of exposure criteria.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3534633,
author = {Usunier, Nicolas and Do, Virginie and Dohmatob, Elvis},
title = {Fast Online Ranking with Fairness of Exposure},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3534633},
doi = {10.1145/3531146.3534633},
abstract = {As recommender systems become increasingly central for sorting and prioritizing the content available online, they have a growing impact on the opportunities or revenue of their items producers. For instance, they influence which recruiter a resume is recommended to, or to whom and how much a music track, video or news article is being exposed. This calls for recommendation approaches that not only maximize (a proxy of) user satisfaction, but also consider some notion of fairness in the exposure of items or groups of items. Formally, such recommendations are usually obtained by maximizing a concave objective function in the space of randomized rankings. When the total exposure of an item is defined as the sum of its exposure over users, the optimal rankings of every users become coupled, which makes the optimization process challenging. Existing approaches to find these rankings either solve the global optimization problem in a batch setting, i.e., for all users at once, which makes them inapplicable at scale, or are based on heuristics that have weak theoretical guarantees. In this paper, we propose the first efficient online algorithm to optimize concave objective functions in the space of rankings which applies to every concave and smooth objective function, such as the ones found for fairness of exposure. Based on online variants of the Frank-Wolfe algorithm, we show that our algorithm is computationally fast, generating rankings on-the-fly with computation cost dominated by the sort operation, memory efficient, and has strong theoretical guarantees. Compared to baseline policies that only maximize user-side performance, our algorithm allows to incorporate complex fairness of exposure criteria in the recommendations with negligible computational overhead. We present experiments on artificial music and movie recommendation tasks using Last.fm and MovieLens datasets which suggest that in practice, the algorithm rapidly reaches good performances on three different objectives representing different fairness of exposure criteria.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {2157–2167},
numpages = {11},
keywords = {fairness, online ranking, recommender systems},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,The Long Arc of Fairness: Formalisations and Ethical Discourse,https://dl.acm.org/doi/10.1145/3531146.3534635,"['Pola Schw√∂bel', 'Peter Remmers']",3,"In recent years, the idea of formalising and modelling fairness for algorithmic decision making (ADM) has advanced to a point of sophisticated specialisation. However, the relations between technical (formalised) and ethical discourse on fairness are not always clear and productive. Arguing for an alternative perspective, we review existing fairness metrics and discuss some common issues. For instance, the fairness of procedures and distributions is often formalised and discussed statically, disregarding both structural preconditions of the status quo and downstream effects of a given intervention. We then introduce dynamic fairness modelling, a more comprehensive approach that realigns formal fairness metrics with arguments from the ethical discourse. A dynamic fairness model incorporates (1) ethical goals, (2) formal metrics to quantify decision procedures and outcomes and (3) mid-term or long-term downstream effects. By contextualising these elements of fairness-related processes, dynamic fairness modelling explicates formerly latent ethical aspects and thereby provides a helpful tool to navigate trade-offs between different fairness interventions. To illustrate the framework, we discuss an example application ‚Äì the current European efforts to increase the number of women on company boards, e¬†.g.¬† via quota solutions ‚Äì and present early technical work that fits within our framework.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3534635,
author = {Schw\""{o}bel, Pola and Remmers, Peter},
title = {The Long Arc of Fairness: Formalisations and Ethical Discourse},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3534635},
doi = {10.1145/3531146.3534635},
abstract = {In recent years, the idea of formalising and modelling fairness for algorithmic decision making (ADM) has advanced to a point of sophisticated specialisation. However, the relations between technical (formalised) and ethical discourse on fairness are not always clear and productive. Arguing for an alternative perspective, we review existing fairness metrics and discuss some common issues. For instance, the fairness of procedures and distributions is often formalised and discussed statically, disregarding both structural preconditions of the status quo and downstream effects of a given intervention. We then introduce dynamic fairness modelling, a more comprehensive approach that realigns formal fairness metrics with arguments from the ethical discourse. A dynamic fairness model incorporates (1) ethical goals, (2) formal metrics to quantify decision procedures and outcomes and (3) mid-term or long-term downstream effects. By contextualising these elements of fairness-related processes, dynamic fairness modelling explicates formerly latent ethical aspects and thereby provides a helpful tool to navigate trade-offs between different fairness interventions. To illustrate the framework, we discuss an example application – the current European efforts to increase the number of women on company boards, e .g.  via quota solutions – and present early technical work that fits within our framework.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {2179–2188},
numpages = {10},
keywords = {dynamic fairness modelling, algorithmic fairness, algorithmic decision making, fairness metrics, ethics of machine learning},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,Measuring Fairness of Rankings under Noisy Sensitive Information,https://dl.acm.org/doi/10.1145/3531146.3534641,"['Azin Ghazimatin', 'Matthaus Kleindessner', 'Chris Russell', 'Ziawasch Abedjan', 'Jacek Golebiowski']",2,"Metrics commonly used to assess group fairness in ranking require the knowledge of group membership labels (e.g., whether a job applicant is male or female). Obtaining accurate group membership labels, however, may be costly, operationally difficult, or even infeasible. Where it is not possible to obtain these labels, one common solution is to use proxy labels in their place, which are typically predicted by machine learning models. Proxy labels are susceptible to systematic biases, and using them for fairness estimation can thus lead to unreliable assessments. We investigate the problem of measuring group fairness in ranking for a suite of divergence-based metrics in the presence of proxy labels. We show that under certain assumptions, fairness of a ranking can reliably be measured from the proxy labels. We formalize two assumptions and provide a theoretical analysis for each showing how the true metric values can be derived from the estimates based on proxy labels. We prove that without such assumptions fairness assessment based on proxy labels is impossible. Through extensive experiments on both synthetic and real datasets, we demonstrate the effectiveness of our proposed methods for recovering reliable fairness assessments in rankings.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3534641,
author = {Ghazimatin, Azin and Kleindessner, Matthaus and Russell, Chris and Abedjan, Ziawasch and Golebiowski, Jacek},
title = {Measuring Fairness of Rankings under Noisy Sensitive Information},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3534641},
doi = {10.1145/3531146.3534641},
abstract = {Metrics commonly used to assess group fairness in ranking require the knowledge of group membership labels (e.g., whether a job applicant is male or female). Obtaining accurate group membership labels, however, may be costly, operationally difficult, or even infeasible. Where it is not possible to obtain these labels, one common solution is to use proxy labels in their place, which are typically predicted by machine learning models. Proxy labels are susceptible to systematic biases, and using them for fairness estimation can thus lead to unreliable assessments. We investigate the problem of measuring group fairness in ranking for a suite of divergence-based metrics in the presence of proxy labels. We show that under certain assumptions, fairness of a ranking can reliably be measured from the proxy labels. We formalize two assumptions and provide a theoretical analysis for each showing how the true metric values can be derived from the estimates based on proxy labels. We prove that without such assumptions fairness assessment based on proxy labels is impossible. Through extensive experiments on both synthetic and real datasets, we demonstrate the effectiveness of our proposed methods for recovering reliable fairness assessments in rankings.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {2263–2279},
numpages = {17},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,People are not coins: Morally distinct types of predictions necessitate different fairness constraints,https://dl.acm.org/doi/10.1145/3531146.3534643,"['Eleonora Vigan√≤', 'Corinna Hertweck', 'Christoph Heitz', 'Michele Loi']",0,"In a recent paper [1], Brian Hedden has argued that most of the group fairness constraints discussed in the machine learning literature are not necessary conditions for the fairness of predictions, and hence that there are no genuine fairness metrics. This is proven by discussing a special case of a fair prediction. In our paper, we show that Hedden's argument does not hold for the most common kind of predictions used in data science, which are about people and based on data from similar people; we call these ‚Äúhuman-group-based practices.‚Äù We argue that there is a morally salient distinction between human-group-based practices and those that are based on data of only one person, which we call ‚Äúhuman-individual-based practices.‚Äù Thus, what may be a necessary condition for the fairness of human-group-based practices may not be a necessary condition for the fairness of human-individual-based practices, on which Hedden's argument is based. Accordingly, the group fairness metrics discussed in the machine learning literature may still be relevant for most applications of prediction-based decision making.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3534643,
author = {Vigan\`{o}, Eleonora and Hertweck, Corinna and Heitz, Christoph and Loi, Michele},
title = {People Are Not Coins: Morally Distinct Types of Predictions Necessitate Different Fairness Constraints},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3534643},
doi = {10.1145/3531146.3534643},
abstract = {In a recent paper [1], Brian Hedden has argued that most of the group fairness constraints discussed in the machine learning literature are not necessary conditions for the fairness of predictions, and hence that there are no genuine fairness metrics. This is proven by discussing a special case of a fair prediction. In our paper, we show that Hedden's argument does not hold for the most common kind of predictions used in data science, which are about people and based on data from similar people; we call these “human-group-based practices.” We argue that there is a morally salient distinction between human-group-based practices and those that are based on data of only one person, which we call “human-individual-based practices.” Thus, what may be a necessary condition for the fairness of human-group-based practices may not be a necessary condition for the fairness of human-individual-based practices, on which Hedden's argument is based. Accordingly, the group fairness metrics discussed in the machine learning literature may still be relevant for most applications of prediction-based decision making.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {2293–2301},
numpages = {9},
keywords = {fairness metrics, fair prediction, algorithmic decision making, moral principles},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,Data augmentation for fairness-aware machine learning: Preventing algorithmic bias in law enforcement systems,https://dl.acm.org/doi/10.1145/3531146.3534644,"['Ioannis Pastaltzidis', 'Nikolaos Dimitriou', 'Katherine Quezada-Tavarez', 'Stergios Aidinlis', 'Thomas Marquenie', 'Agata Gurzawska', 'Dimitrios Tzovaras']",5,"Researchers and practitioners in the fairness community have highlighted the ethical and legal challenges of using biased datasets in data-driven systems, with algorithmic bias being a major concern. Despite the rapidly growing body of literature on fairness in algorithmic decision-making, there remains a paucity of fairness scholarship on machine learning algorithms for the real-time detection of crime. This contribution presents an approach for fairness-aware machine learning to mitigate the algorithmic bias / discrimination issues posed by the reliance on biased data when building law enforcement technology. Our analysis is based on RWF-2000, which has served as the basis for violent activity recognition tasks in data-driven law enforcement projects. We reveal issues of overrepresentation of minority subjects in violence situations that limit the external validity of the dataset for real-time crime detection systems and propose data augmentation techniques to rebalance the dataset. The experiments on real world data show the potential to create more balanced datasets by synthetically generated samples, thus mitigating bias and discrimination concerns in law enforcement applications.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3534644,
author = {Pastaltzidis, Ioannis and Dimitriou, Nikolaos and Quezada-Tavarez, Katherine and Aidinlis, Stergios and Marquenie, Thomas and Gurzawska, Agata and Tzovaras, Dimitrios},
title = {Data Augmentation for Fairness-Aware Machine Learning: Preventing Algorithmic Bias in Law Enforcement Systems},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3534644},
doi = {10.1145/3531146.3534644},
abstract = {Researchers and practitioners in the fairness community have highlighted the ethical and legal challenges of using biased datasets in data-driven systems, with algorithmic bias being a major concern. Despite the rapidly growing body of literature on fairness in algorithmic decision-making, there remains a paucity of fairness scholarship on machine learning algorithms for the real-time detection of crime. This contribution presents an approach for fairness-aware machine learning to mitigate the algorithmic bias / discrimination issues posed by the reliance on biased data when building law enforcement technology. Our analysis is based on RWF-2000, which has served as the basis for violent activity recognition tasks in data-driven law enforcement projects. We reveal issues of overrepresentation of minority subjects in violence situations that limit the external validity of the dataset for real-time crime detection systems and propose data augmentation techniques to rebalance the dataset. The experiments on real world data show the potential to create more balanced datasets by synthetically generated samples, thus mitigating bias and discrimination concerns in law enforcement applications.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {2302–2314},
numpages = {13},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,Enforcing Group Fairness in Algorithmic Decision Making: Utility Maximization Under Sufficiency,https://dl.acm.org/doi/10.1145/3531146.3534645,"['Joachim Baumann', 'Anik√≥ Hann√°k', 'Christoph Heitz']",4,"Binary decision making classifiers are not fair by default. Fairness requirements are an additional element to the decision making rationale, which is typically driven by maximizing some utility function. In that sense, algorithmic fairness can be formulated as a constrained optimization problem. This paper contributes to the discussion on how to implement fairness, focusing on the fairness concepts of positive predictive value (PPV) parity, false omission rate (FOR) parity, and sufficiency (which combines the former two). We show that group-specific threshold rules are optimal for PPV parity and FOR parity, similar to well-known results for other group fairness criteria. However, depending on the underlying population distributions and the utility function, we find that sometimes an upper-bound threshold rule for one group is optimal: utility maximization under PPV parity (or FOR parity) might thus lead to selecting the individuals with the smallest utility for one group, instead of selecting the most promising individuals. This result is counter-intuitive and in contrast to the analogous solutions for statistical parity and equality of opportunity. We also provide a solution for the optimal decision rules satisfying the fairness constraint sufficiency. We show that more complex decision rules are required and that this leads to within-group unfairness for all but one of the groups. We illustrate our findings based on simulated and real data.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3534645,
author = {Baumann, Joachim and Hann\'{a}k, Anik\'{o} and Heitz, Christoph},
title = {Enforcing Group Fairness in Algorithmic Decision Making: Utility Maximization Under Sufficiency},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3534645},
doi = {10.1145/3531146.3534645},
abstract = {Binary decision making classifiers are not fair by default. Fairness requirements are an additional element to the decision making rationale, which is typically driven by maximizing some utility function. In that sense, algorithmic fairness can be formulated as a constrained optimization problem. This paper contributes to the discussion on how to implement fairness, focusing on the fairness concepts of positive predictive value (PPV) parity, false omission rate (FOR) parity, and sufficiency (which combines the former two). We show that group-specific threshold rules are optimal for PPV parity and FOR parity, similar to well-known results for other group fairness criteria. However, depending on the underlying population distributions and the utility function, we find that sometimes an upper-bound threshold rule for one group is optimal: utility maximization under PPV parity (or FOR parity) might thus lead to selecting the individuals with the smallest utility for one group, instead of selecting the most promising individuals. This result is counter-intuitive and in contrast to the analogous solutions for statistical parity and equality of opportunity. We also provide a solution for the optimal decision rules satisfying the fairness constraint sufficiency. We show that more complex decision rules are required and that this leads to within-group unfairness for all but one of the groups. We illustrate our findings based on simulated and real data.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {2315–2326},
numpages = {12},
keywords = {algorithmic fairness, prediction-based decision making, machine learning, sufficiency, group fairness metrics, constrained utility optimization, fairness trade-offs},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
FACCT,2022,Locality of Technical Objects and the Role of Structural Interventions for Systemic Change,https://dl.acm.org/doi/10.1145/3531146.3534646,"['Efr√©n Cruz Cort√©s', 'Sarah Rajtmajer', 'Debashis Ghosh']",1,"Technical objects, like algorithms, exhibit causal capacities both in terms of their internal makeup and the position they occupy in relation to other objects and processes within a system. At the same time, systems encompassing technical objects interact with other systems themselves, producing a multi-scale structural composition. In the framework of fair artificial intelligence, typical causal inference interventions focus on the internal workings of technical objects (fairness constraints), and often forsake structural properties of the system. However, these interventions are often not sufficient to capture forms of discrimination and harm at a systemic level. To complement this approach we introduce the notion of locality and define structural interventions. We compare the effect of structural interventions on a system compared to local, structure-preserving interventions on technical objects. We focus on comparing interventions on generating mechanisms (representing social dynamics giving rise to discrimination) with constraining algorithms to satisfy some measure of fairness. This framework allows us to identify bias outside the algorithmic stage and propose joint interventions on social dynamics and algorithm design. We show how, for a model of financial lending, structural interventions can drive the system towards equality even when algorithmic interventions are unable to do so. This suggests that the responsibility of decision makers extends beyond ensuring that local fairness metrics are satisfied to an ecosystem that fosters equity for all.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3531146.3534646,
author = {Cruz Cort\'{e}s, Efr\'{e}n and Rajtmajer, Sarah and Ghosh, Debashis},
title = {Locality of Technical Objects and the Role of Structural Interventions for Systemic Change},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3534646},
doi = {10.1145/3531146.3534646},
abstract = {Technical objects, like algorithms, exhibit causal capacities both in terms of their internal makeup and the position they occupy in relation to other objects and processes within a system. At the same time, systems encompassing technical objects interact with other systems themselves, producing a multi-scale structural composition. In the framework of fair artificial intelligence, typical causal inference interventions focus on the internal workings of technical objects (fairness constraints), and often forsake structural properties of the system. However, these interventions are often not sufficient to capture forms of discrimination and harm at a systemic level. To complement this approach we introduce the notion of locality and define structural interventions. We compare the effect of structural interventions on a system compared to local, structure-preserving interventions on technical objects. We focus on comparing interventions on generating mechanisms (representing social dynamics giving rise to discrimination) with constraining algorithms to satisfy some measure of fairness. This framework allows us to identify bias outside the algorithmic stage and propose joint interventions on social dynamics and algorithm design. We show how, for a model of financial lending, structural interventions can drive the system towards equality even when algorithmic interventions are unable to do so. This suggests that the responsibility of decision makers extends beyond ensuring that local fairness metrics are satisfied to an ecosystem that fosters equity for all.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {2327–2341},
numpages = {15},
keywords = {system dynamics, fairness, lending, sociotechnical systems, ethics, interventions},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
AIES,2019,Popularity Bias in Ranking and Recommendation,https://dl.acm.org/doi/10.1145/3306618.3314309,['Himan Abdollahpouri'],52,"Many recommender systems suffer from popularity bias: popular items are recommended frequently while less popular, niche products, are recommended rarely or not at all. However, recommending the ignored products in the ""long tail"" is critical for businesses as they are less likely to be discovered. Popularity bias is also against social justice where the entities need to have a fair chance of being served and represented. In this work, I investigate the problem of popularity bias in recommender systems and develop algorithms to address this problem.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3306618.3314309,
author = {Abdollahpouri, Himan},
title = {Popularity Bias in Ranking and Recommendation},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314309},
doi = {10.1145/3306618.3314309},
abstract = {Many recommender systems suffer from popularity bias: popular items are recommended frequently while less popular, niche products, are recommended rarely or not at all. However, recommending the ignored products in the ""long tail"" is critical for businesses as they are less likely to be discovered. Popularity bias is also against social justice where the entities need to have a fair chance of being served and represented. In this work, I investigate the problem of popularity bias in recommender systems and develop algorithms to address this problem.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {529–530},
numpages = {2},
keywords = {information retrieval, ranking, popularity bias, recommender systems},
location = {Honolulu, HI, USA},
series = {AIES '19}
}
  
"
AIES,2019,Risk Assessments and Fairness Under Missingness and Confounding,https://dl.acm.org/doi/10.1145/3306618.3314310,['Amanda Coston'],0,"Fairness in machine learning has become a significant area of research as risk assessments and other algorithmic decision-making systems are increasingly used in high-stakes applications such as criminal justice, consumer lending, and child welfare screening decisions. Two significant challenges to achieving fair decision-making systems are 1) access to the protected attribute may be limited and 2) the outcome may be confounded or selectively observed depending on the historical data generating process. To address the former challenge, we propose two methods for overcoming limited access to the protected attribute and empirically evaluate their success on three datasets. To address the later challenge, we develop counterfactual risk assessments that account for the effect of historical interventions on the outcome. We analyze the performance of our counterfactual risk assessments in criminal sentencing decisions in Pennsylvania. We compare our model against observational risk assessments.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3306618.3314310,
author = {Coston, Amanda},
title = {Risk Assessments and Fairness Under Missingness and Confounding},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314310},
doi = {10.1145/3306618.3314310},
abstract = {Fairness in machine learning has become a significant area of research as risk assessments and other algorithmic decision-making systems are increasingly used in high-stakes applications such as criminal justice, consumer lending, and child welfare screening decisions. Two significant challenges to achieving fair decision-making systems are 1) access to the protected attribute may be limited and 2) the outcome may be confounded or selectively observed depending on the historical data generating process. To address the former challenge, we propose two methods for overcoming limited access to the protected attribute and empirically evaluate their success on three datasets. To address the later challenge, we develop counterfactual risk assessments that account for the effect of historical interventions on the outcome. We analyze the performance of our counterfactual risk assessments in criminal sentencing decisions in Pennsylvania. We compare our model against observational risk assessments.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {531},
numpages = {1},
keywords = {decision-making, fairness, machine learning, risk assessments},
location = {Honolulu, HI, USA},
series = {AIES '19}
}
  
"
AIES,2019,"Fairness, Accountability and Transparency in Artificial Intelligence: A Case Study of Logical Predictive Models",https://dl.acm.org/doi/10.1145/3306618.3314316,['Kacper Sokol'],2,NOT,,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3306618.3314316,
author = {Sokol, Kacper},
title = {Fairness, Accountability and Transparency in Artificial Intelligence: A Case Study of Logical Predictive Models},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314316},
doi = {10.1145/3306618.3314316},
abstract = {Machine learning -- the part of artificial intelligence aimed at eliciting knowledge from data and automated decision making without explicit instructions -- is making great strides, with new algorithms being invented every day. These algorithms find myriads of applications, but their ubiquity often comes at the expense of limited interpretability, hidden biases and unexpected vulnerabilities. Whenever one of these factors is a priority, the learning algorithm of choice is often a method considered to be inherently interpretable, e.g. logical models such as decision trees. In my research I challenge this assumption and highlight (quite common) cases when the assumed interpretability fails to deliver. To restore interpretability of logical machine learning models (decision trees and their ensembles in particular) I propose to explain them with class-contrastive counterfactual statements, which are a very common type of explanation in human interactions, well-grounded in social science research. To evaluate transparency of such models I collate explainability desiderata that can be used to systematically assess and compare such methods as an addition to user studies. Given contrastive explanations, I investigate their influence on the model's security, in particular gaming and stealing the model. Finally, I evaluate model fairness, where I am interested in choosing the most fair model among all the models with equal performance.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {541–542},
numpages = {2},
keywords = {accountability, transparency, logical models, fairness},
location = {Honolulu, HI, USA},
series = {AIES '19}
}
  
"
AIES,2020,From Bad Users and Failed Uses to Responsible Technologies: A Call to Expand the AI Ethics Toolkit,https://dl.acm.org/doi/10.1145/3375627.3377141,['Gina Neff'],8,"Recent advances in artificial intelligence applications have sparked scholarly and public attention to the challenges of the ethical design of technologies. These conversations about ethics have been targeted largely at technology designers and concerned with helping to inform building better and fairer AI tools and technologies. This approach, however, addresses only a small part of the problem of responsible use and will not be adequate for describing or redressing the problems that will arise as more types of AI technologies are more widely used. Many of the tools being developed today have potentially enormous and historic impacts on how people work, how society organises, stores and distributes information, where and how people interact with one another, and how people's work is valued and compensated. And yet, our ethical attention has looked at a fairly narrow range of questions about expanding the access to, fairness of, and accountability for existing tools. Instead, I argue that scholars should develop much broader questions of about the reconfiguration of societal power, for which AI technologies form a crucial component. This talk will argue that AI ethics needs to expand its theoretical and methodological toolkit in order to move away from prioritizing notions of good design that privilege the work of good and ethical technology designers. Instead, using approaches from feminist theory, organization studies, and science and technology, I argue for expanding how we evaluate uses of AI. This approach begins with the assumption of socially informed technological affordances, or ""imagined affordances"" [1] shaping how people understand and use technologies in practice. It also gives centrality to the power of social institutions for shaping technologies-in-practice.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3375627.3377141,
author = {Neff, Gina},
title = {From Bad Users and Failed Uses to Responsible Technologies: A Call to Expand the AI Ethics Toolkit},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3377141},
doi = {10.1145/3375627.3377141},
abstract = {Recent advances in artificial intelligence applications have sparked scholarly and public attention to the challenges of the ethical design of technologies. These conversations about ethics have been targeted largely at technology designers and concerned with helping to inform building better and fairer AI tools and technologies. This approach, however, addresses only a small part of the problem of responsible use and will not be adequate for describing or redressing the problems that will arise as more types of AI technologies are more widely used.Many of the tools being developed today have potentially enormous and historic impacts on how people work, how society organises, stores and distributes information, where and how people interact with one another, and how people's work is valued and compensated. And yet, our ethical attention has looked at a fairly narrow range of questions about expanding the access to, fairness of, and accountability for existing tools. Instead, I argue that scholars should develop much broader questions of about the reconfiguration of societal power, for which AI technologies form a crucial component.This talk will argue that AI ethics needs to expand its theoretical and methodological toolkit in order to move away from prioritizing notions of good design that privilege the work of good and ethical technology designers. Instead, using approaches from feminist theory, organization studies, and science and technology, I argue for expanding how we evaluate uses of AI. This approach begins with the assumption of socially informed technological affordances, or ""imagined affordances"" [1] shaping how people understand and use technologies in practice. It also gives centrality to the power of social institutions for shaping technologies-in-practice.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {5–6},
numpages = {2},
keywords = {social sciences, theory, ai ethics, work and organizations, data work, sts, feminist theory},
location = {New York, NY, USA},
series = {AIES '20}
}
  
"
AIES,2020,Normative Principles for Evaluating Fairness in Machine Learning,https://dl.acm.org/doi/10.1145/3375627.3375808,['Derek Leben'],27,"There are many incompatible ways to measure fair outcomes for machine learning algorithms. The goal of this paper is to characterize rates of success and error across protected groups (race, gender, sexual orientation) as a distribution problem, and describe the possible solutions to this problem according to different normative principles from moral and political philosophy. These normative principles are based on various competing attributes within a distribution problem: intentions, compensation, desert, consent, and consequences. Each principle will be applied to a sample risk-assessment classifier to demonstrate the philosophical arguments underlying different sets of fairness metrics.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3375627.3375808,
author = {Leben, Derek},
title = {Normative Principles for Evaluating Fairness in Machine Learning},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375808},
doi = {10.1145/3375627.3375808},
abstract = {There are many incompatible ways to measure fair outcomes for machine learning algorithms. The goal of this paper is to characterize rates of success and error across protected groups (race, gender, sexual orientation) as a distribution problem, and describe the possible solutions to this problem according to different normative principles from moral and political philosophy. These normative principles are based on various competing attributes within a distribution problem: intentions, compensation, desert, consent, and consequences. Each principle will be applied to a sample risk-assessment classifier to demonstrate the philosophical arguments underlying different sets of fairness metrics.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {86–92},
numpages = {7},
keywords = {political philosophy, discrimination, machine learning, fairness, algorithmic decision-making},
location = {New York, NY, USA},
series = {AIES '20}
}
  
"
AIES,2020,"Towards Just, Fair and Interpretable Methods for Judicial Subset Selection",https://dl.acm.org/doi/10.1145/3375627.3375848,"['Lingxiao Huang', 'Julia Wei', 'Elisa Celis']",5,"In many judicial systems -- including the United States courts of appeals, the European Court of Justice, the UK Supreme Court and the Supreme Court of Canada -- a subset of judges is selected from the entire judicial body for each case in order to hear the arguments and decide the judgment. Ideally, the subset selected is representative, i.e., the decision of the subset would match what the decision of the entire judicial body would have been had they all weighed in on the case. Further, the process should be fair in that all judges should have similar workloads, and the selection process should not allow for certain judge's opinions to be silenced or amplified via case assignments. Lastly, in order to be practical and trustworthy, the process should also be interpretable, easy to use, and (if algorithmic) computationally efficient. In this paper, we propose an algorithmic method for the judicial subset selection problem that satisfies all of the above criteria. The method satisfies fairness by design, and we prove that it has optimal representativeness asymptotically for a large range of parameters and under noisy information models about judge opinions -- something no existing methods can provably achieve. We then assess the benefits of our approach empirically by counterfactually comparing against the current practice and recent alternative algorithmic approaches using cases from the United States courts of appeals database.",,,Maybe,Algorithms,,"Contextualized, subset selection",,,,,,,,,,,"@inproceedings{10.1145/3375627.3375848,
author = {Huang, Lingxiao and Wei, Julia and Celis, Elisa},
title = {Towards Just, Fair and Interpretable Methods for Judicial Subset Selection},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375848},
doi = {10.1145/3375627.3375848},
abstract = {In many judicial systems -- including the United States courts of appeals, the European Court of Justice, the UK Supreme Court and the Supreme Court of Canada -- a subset of judges is selected from the entire judicial body for each case in order to hear the arguments and decide the judgment. Ideally, the subset selected is representative, i.e., the decision of the subset would match what the decision of the entire judicial body would have been had they all weighed in on the case. Further, the process should be fair in that all judges should have similar workloads, and the selection process should not allow for certain judge's opinions to be silenced or amplified via case assignments. Lastly, in order to be practical and trustworthy, the process should also be interpretable, easy to use, and (if algorithmic) computationally efficient. In this paper, we propose an algorithmic method for the judicial subset selection problem that satisfies all of the above criteria. The method satisfies fairness by design, and we prove that it has optimal representativeness asymptotically for a large range of parameters and under noisy information models about judge opinions -- something no existing methods can provably achieve. We then assess the benefits of our approach empirically by counterfactually comparing against the current practice and recent alternative algorithmic approaches using cases from the United States courts of appeals database.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {293–299},
numpages = {7},
keywords = {representative, interpretable, judicial subset selection, fair},
location = {New York, NY, USA},
series = {AIES '20}
}
  
"
AIES,2020,"""The Global South is everywhere, but also always somewhere"": National Policy Narratives and AI Justice",https://dl.acm.org/doi/10.1145/3375627.3375859,['Amba Kak'],7,"There is more attention than ever on the social implications of AI. In contrast to universalized paradigms of ethics and fairness, a growing body of critical work highlights bias and discrimination in AI within the frame of social justice and human rights (""AI justice""). However, the geographical location of much of this critique in the West could be engendering its own blind spots. The global supply chain of AI (data, computational power, natural resources, labor) today replicates historical colonial inequities, and the continued subordination of Global South countries. This paper draws attention to official narratives from the Indian government and the United Nations Conference on Trade and Development (UNCTAD) advocating for the role (and place) of these regions in the AI economy. Domestically, these policies are being contested for their top-down formulation, and reflect narrow industry interests. This underscores the need to approach the political economy of AI from varying altitudes - global, national, and from the perspective of communities whose lives and livelihoods are most directly impacted in this economy. Without a deliberate effort at centering this conversation it is inevitable that mainstream discourse on AI justice will grow parallel to (and potentially undercut) demands emanating from Global South governments and communities",,,NOT,,,,,,,,,,,,,,"@inproceedings{10.1145/3375627.3375859,
author = {Kak, Amba},
title = {""The Global South is Everywhere, but Also Always Somewhere"": National Policy Narratives and AI Justice},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375859},
doi = {10.1145/3375627.3375859},
abstract = {There is more attention than ever on the social implications of AI. In contrast to universalized paradigms of ethics and fairness, a growing body of critical work highlights bias and discrimination in AI within the frame of social justice and human rights (""AI justice""). However, the geographical location of much of this critique in the West could be engendering its own blind spots. The global supply chain of AI (data, computational power, natural resources, labor) today replicates historical colonial inequities, and the continued subordination of Global South countries. This paper draws attention to official narratives from the Indian government and the United Nations Conference on Trade and Development (UNCTAD) advocating for the role (and place) of these regions in the AI economy. Domestically, these policies are being contested for their top-down formulation, and reflect narrow industry interests. This underscores the need to approach the political economy of AI from varying altitudes - global, national, and from the perspective of communities whose lives and livelihoods are most directly impacted in this economy. Without a deliberate effort at centering this conversation it is inevitable that mainstream discourse on AI justice will grow parallel to (and potentially undercut) demands emanating from Global South governments and communities},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {307–312},
numpages = {6},
keywords = {data flows, global south, decolonial, political economy, ai accountability},
location = {New York, NY, USA},
series = {AIES '20}
}
  
"
AIES,2021,Participatory Algorithmic Management: Elicitation Methods for Worker Well-Being Models,https://dl.acm.org/doi/10.1145/3461702.3462628,"['Min Kyung Lee', 'Ishan Nigam', 'Angie Zhang', 'Joel Afriyie', 'Zhizhen Qin', 'Sicun Gao']",5,"Artificial intelligence is increasingly being used to manage the workforce. Algorithmic management promises organizational efficiency, but often undermines worker well-being. How can we computationally model worker well-being so that algorithmic management can be optimized for and assessed in terms of worker well-being? Toward this goal, we propose a participatory approach for worker well-being models. We first define worker well-being models: Work preference models---preferences about work and working conditions, and managerial fairness models---beliefs about fair resource allocation among multiple workers. We then propose elicitation methods to enable workers to build their own well-being models leveraging pairwise comparisons and ranking. As a case study, we evaluate our methods in the context of algorithmic work scheduling with 25 shift workers and 3 managers. The findings show that workers expressed idiosyncratic work preference models and more uniform managerial fairness models, and the elicitation methods helped workers discover their preferences and gave them a sense of empowerment. Our work provides a method and initial evidence for enabling participatory algorithmic management for worker well-being.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3461702.3462628,
author = {Lee, Min Kyung and Nigam, Ishan and Zhang, Angie and Afriyie, Joel and Qin, Zhizhen and Gao, Sicun},
title = {Participatory Algorithmic Management: Elicitation Methods for Worker Well-Being Models},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462628},
doi = {10.1145/3461702.3462628},
abstract = {Artificial intelligence is increasingly being used to manage the workforce. Algorithmic management promises organizational efficiency, but often undermines worker well-being. How can we computationally model worker well-being so that algorithmic management can be optimized for and assessed in terms of worker well-being? Toward this goal, we propose a participatory approach for worker well-being models. We first define worker well-being models: Work preference models---preferences about work and working conditions, and managerial fairness models---beliefs about fair resource allocation among multiple workers. We then propose elicitation methods to enable workers to build their own well-being models leveraging pairwise comparisons and ranking. As a case study, we evaluate our methods in the context of algorithmic work scheduling with 25 shift workers and 3 managers. The findings show that workers expressed idiosyncratic work preference models and more uniform managerial fairness models, and the elicitation methods helped workers discover their preferences and gave them a sense of empowerment. Our work provides a method and initial evidence for enabling participatory algorithmic management for worker well-being.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {715–726},
numpages = {12},
keywords = {algorithmic management, algorithmic fairness, preference elicitation, participatory design, worker well-being},
location = {Virtual Event, USA},
series = {AIES '21}
}
  
"
AIES,2021,Comparing Equity and Effectiveness of Different Algorithms in an Application for the Room Rental Market,https://dl.acm.org/doi/10.1145/3461702.3462600,"['David Solans', 'Francesco Fabbri', 'Caterina Calsamiglia', 'Carlos Castillo', 'Francesco Bonchi']",1,"Machine Learning (ML) techniques have been increasingly adopted by the real estate market in the last few years. Applications include, among many others, predicting the market value of a property or an area, advanced systems for managing marketing and ads campaigns, and recommendation systems based on user preferences. While these techniques can provide important benefits to the business owners and the users of the platforms, algorithmic biases can result in inequalities and loss of opportunities for groups of people who are already disadvantaged in their access to housing. In this work, we present a comprehensive and independent algorithmic evaluation of a recommender system for the real estate market, designed specifically for finding shared apartments in metropolitan areas. We were granted full access to the internals of the platform, including details on algorithms and usage data during a period of 2 years. We analyze the performance of the various algorithms which are deployed for the recommender system and asses their effect across different population groups. Our analysis reveals that introducing a recommender system algorithm facilitates finding an appropriate tenant or a desirable room to rent, but at the same time, it strengthen performance inequalities between groups, further reducing opportunities of finding a rental for certain minorities.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3461702.3462600,
author = {Solans, David and Fabbri, Francesco and Calsamiglia, Caterina and Castillo, Carlos and Bonchi, Francesco},
title = {Comparing Equity and Effectiveness of Different Algorithms in an Application for the Room Rental Market},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462600},
doi = {10.1145/3461702.3462600},
abstract = {Machine Learning (ML) techniques have been increasingly adopted by the real estate market in the last few years. Applications include, among many others, predicting the market value of a property or an area, advanced systems for managing marketing and ads campaigns, and recommendation systems based on user preferences. While these techniques can provide important benefits to the business owners and the users of the platforms, algorithmic biases can result in inequalities and loss of opportunities for groups of people who are already disadvantaged in their access to housing. In this work, we present a comprehensive and independent algorithmic evaluation of a recommender system for the real estate market, designed specifically for finding shared apartments in metropolitan areas. We were granted full access to the internals of the platform, including details on algorithms and usage data during a period of 2 years.We analyze the performance of the various algorithms which are deployed for the recommender system and asses their effect across different population groups.Our analysis reveals that introducing a recommender system algorithm facilitates finding an appropriate tenant or a desirable room to rent, but at the same time, it strengthen performance inequalities between groups, further reducing opportunities of finding a rental for certain minorities.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {978–988},
numpages = {11},
keywords = {performance, demographics, recommender system, fairness},
location = {Virtual Event, USA},
series = {AIES '21}
}
  
"
AIES,2022,Governing AI Applications To Monitoring and Managing Our Global Environmental Commons,https://dl.acm.org/doi/10.1145/3514094.3539540,['Melissa Chapman'],0,"Artificial Intelligence (AI) is playing a rapidly growing role in our response to global environmental change, from how we collect and process ecological and earth system data to how we make and enforce management decisions. Importantly, AI permits the use of increasingly high-resolution data, promising more accurate monitoring of environmental change and more targeted interventions. But, while AI promises several potential contributions to addressing global environmental change, how do we ensure these technologies stand to enhance equity rather than exasperate existing environmental injustices and power asymmetries?",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3514094.3539540,
author = {Chapman, Melissa},
title = {Governing AI Applications To Monitoring and Managing Our Global Environmental Commons},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3539540},
doi = {10.1145/3514094.3539540},
abstract = {Artificial Intelligence (AI) is playing a rapidly growing role in our response to global environmental change, from how we collect and process ecological and earth system data to how we make and enforce management decisions. Importantly, AI permits the use of increasingly high-resolution data, promising more accurate monitoring of environmental change and more targeted interventions. But, while AI promises several potential contributions to addressing global environmental change, how do we ensure these technologies stand to enhance equity rather than exasperate existing environmental injustices and power asymmetries?},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {893},
numpages = {1},
keywords = {social-environmental systems, environmental policy, climate change, conservation by algorithm, biodiversity},
location = {Oxford, United Kingdom},
series = {AIES '22}
}
  
"
AIES,2022,Bias in Hate Speech and Toxicity Detection,https://dl.acm.org/doi/10.1145/3514094.3539519,['Paula Reyero Lobo'],0,"Many Artificial Intelligence (AI) systems rely on finding patterns in large datasets, which are prone to bias and exacerbate existing segregation and inequalities of marginalised communities. Due to their socio-technical impact, bias in AI has become a pressing issue. In this work, we investigate discrimination prevention methods on the assumption that disparities of specific populations in the training samples are reproduced or even amplified in the AI system outcomes. We aim to identify the information from vulnerable groups in the training data, uncover potential inequalities in how data capture these groups and provide additional information about them to alleviate inequalities, e.g., stereotypical and generalised views that lead to learning discriminatory associations. We develop data preprocessing techniques in automated moderation (AI systems to flag or filter online abuse) due to its substantial social implications and existing challenges common to many AI applications.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3514094.3539519,
author = {Reyero Lobo, Paula},
title = {Bias in Hate Speech and Toxicity Detection},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3539519},
doi = {10.1145/3514094.3539519},
abstract = {Many Artificial Intelligence (AI) systems rely on finding patterns in large datasets, which are prone to bias and exacerbate existing segregation and inequalities of marginalised communities. Due to their socio-technical impact, bias in AI has become a pressing issue. In this work, we investigate discrimination prevention methods on the assumption that disparities of specific populations in the training samples are reproduced or even amplified in the AI system outcomes. We aim to identify the information from vulnerable groups in the training data, uncover potential inequalities in how data capture these groups and provide additional information about them to alleviate inequalities, e.g., stereotypical and generalised views that lead to learning discriminatory associations. We develop data preprocessing techniques in automated moderation (AI systems to flag or filter online abuse) due to its substantial social implications and existing challenges common to many AI applications.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {910},
numpages = {1},
keywords = {toxic speech, semantic web, bias, artificial intelligence},
location = {Oxford, United Kingdom},
series = {AIES '22}
}
  
"
AIES,2022,"Fair, Robust, and Data-Efficient Machine Learning in Healthcare",https://dl.acm.org/doi/10.1145/3514094.3539552,['Harvineet Singh'],0,"While machine learning systems have shown improvements, often, in carefully curated settings, challenges still exist to their wider deployment, especially for making consequential decisions. The research described here explores three challenges, particularly, emphasizing the interesting issues that arise at their intersection. How do we design machine learning systems to account for the systemic biases of the world, to act reliably under unseen settings, and to handle limited availability of data? Human-facing applications of machine learning such as personalized health commonly encounter these challenges, thus, these are important to address. The research has three components addressing different parts of the above central question. Here, we describe the work done on two components of the above central question and highlight the future work planned as part of the third one. We draw from methods in causal inference, algorithmic fairness, and interactive learning, and apply them to applications in health.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3514094.3539552,
author = {Singh, Harvineet},
title = {Fair, Robust, and Data-Efficient Machine Learning in Healthcare},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3539552},
doi = {10.1145/3514094.3539552},
abstract = {While machine learning systems have shown improvements, often, in carefully curated settings, challenges still exist to their wider deployment, especially for making consequential decisions. The research described here explores three challenges, particularly, emphasizing the interesting issues that arise at their intersection.How do we design machine learning systems to account for the systemic biases of the world, to act reliably under unseen settings, and to handle limited availability of data?Human-facing applications of machine learning such as personalized health commonly encounter these challenges, thus, these are important to address. The research has three components addressing different parts of the above central question. Here, we describe the work done on two components of the above central question and highlight the future work planned as part of the third one. We draw from methods in causal inference, algorithmic fairness, and interactive learning, and apply them to applications in health.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {914},
numpages = {1},
keywords = {robust learning, policy evaluation, active learning, algorithmic fairness},
location = {Oxford, United Kingdom},
series = {AIES '22}
}
  
"
EAAMO,2021,Advancing social justice through linguistic justice: Strategies for building equity fluent NLP technology,https://dl.acm.org/doi/10.1145/3465416.3483301,"['Julia Nee', 'Genevieve Macfarlane Smith', 'Alicia Sheares', 'Ishita Rustagi']",3,"Language and social reality are mutually reinforcing; as a result, natural language processing (NLP) presents a unique opportunity to shift social reality at scale, advancing social justice by promoting linguistic justice. We provide an overview of how language and bias are intertwined and implications for building NLP tools that actively advance equity and inclusion. Then, we present a framework for centering inclusion and social justice in NLP design at four overlapping layers of linguistic structure. The goal is to provide a foundation for adopting equity-centered principles in the creation of NLP tools that don‚Äôt simply mitigate social biases, but actively advance inclusion and social justice through language. This work aims to be practical and builds from a partnership between researchers at the Center for Equity, Gender, and Leadership at the UC Berkeley Haas School of Business and leaders and practitioners at a large Silicon Valley tech firm. This framework can foster equity-centered thinking to lead to greater ‚Äúequity fluent‚Äù NLP tools that have the potential to advance justice more broadly.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3465416.3483301,
author = {Nee, Julia and Macfarlane Smith, Genevieve and Sheares, Alicia and Rustagi, Ishita},
title = {Advancing Social Justice through Linguistic Justice: Strategies for Building Equity Fluent NLP Technology},
year = {2021},
isbn = {9781450385534},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3465416.3483301},
doi = {10.1145/3465416.3483301},
abstract = {Language and social reality are mutually reinforcing; as a result, natural language processing (NLP) presents a unique opportunity to shift social reality at scale, advancing social justice by promoting linguistic justice. We provide an overview of how language and bias are intertwined and implications for building NLP tools that actively advance equity and inclusion. Then, we present a framework for centering inclusion and social justice in NLP design at four overlapping layers of linguistic structure. The goal is to provide a foundation for adopting equity-centered principles in the creation of NLP tools that don’t simply mitigate social biases, but actively advance inclusion and social justice through language. This work aims to be practical and builds from a partnership between researchers at the Center for Equity, Gender, and Leadership at the UC Berkeley Haas School of Business and leaders and practitioners at a large Silicon Valley tech firm. This framework can foster equity-centered thinking to lead to greater “equity fluent” NLP tools that have the potential to advance justice more broadly.},
booktitle = {Equity and Access in Algorithms, Mechanisms, and Optimization},
articleno = {13},
numpages = {9},
keywords = {linguistic justice, equity-centered design, bias, discrimination, language ideologies},
location = {--, NY, USA},
series = {EAAMO '21}
}
  
"
EAAMO,2021,A Public Transit Network Optimization Model for Equitable Access to Social Services,https://dl.acm.org/doi/10.1145/3465416.3483288,"['Adam Rumpf', 'Hemanshu Kaul']",1,"We present a flexible public transit network design model which optimizes a social access objective while guaranteeing that the system‚Äôs costs and transit times remain within a preset margin of their current levels. The purpose of the model is to find a set of minor, immediate modifications to an existing bus network that can give more communities access to the chosen services while having a minimal impact on the current network‚Äôs operator costs and user costs. Design decisions consist of reallocation of existing resources in order to adjust line frequencies and capacities. We present a hybrid tabu search/simulated annealing algorithm for the solution of this optimization-based model. As a case study we apply the model to the problem of improving equity of access to primary health care facilities in the Chicago metropolitan area. The results of the model suggest that it is possible to achieve better primary care access equity through reassignment of existing buses and implementation of express runs, while leaving overall service levels relatively unaffected.",,,,,,,,,,,,,,,,,"@inproceedings{10.1145/3465416.3483288,
author = {Rumpf, Adam and Kaul, Hemanshu},
title = {A Public Transit Network Optimization Model for Equitable Access to Social Services},
year = {2021},
isbn = {9781450385534},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3465416.3483288},
doi = {10.1145/3465416.3483288},
abstract = {We present a flexible public transit network design model which optimizes a social access objective while guaranteeing that the system’s costs and transit times remain within a preset margin of their current levels. The purpose of the model is to find a set of minor, immediate modifications to an existing bus network that can give more communities access to the chosen services while having a minimal impact on the current network’s operator costs and user costs. Design decisions consist of reallocation of existing resources in order to adjust line frequencies and capacities. We present a hybrid tabu search/simulated annealing algorithm for the solution of this optimization-based model. As a case study we apply the model to the problem of improving equity of access to primary health care facilities in the Chicago metropolitan area. The results of the model suggest that it is possible to achieve better primary care access equity through reassignment of existing buses and implementation of express runs, while leaving overall service levels relatively unaffected.},
booktitle = {Equity and Access in Algorithms, Mechanisms, and Optimization},
articleno = {16},
numpages = {17},
location = {--, NY, USA},
series = {EAAMO '21}
}
  
"
ICML,2017,Active Learning for Top $K$ Rank Aggregation from Noisy Comparisons,https://proceedings.mlr.press/v70/mohajer17a.html,"['Soheil Mohajer', 'Changho Suh', 'Adel Elmahdy']",152,"We explore an active top-$K$ ranking problem based on pairwise comparisons that are collected possibly in a sequential manner as per our design choice. We consider two settings: (1) top-$K$ sorting in which the goal is to recover the top-$K$ items in order out of $n$ items; (2) top-$K$ partitioning where only the set of top-$K$ items is desired. Under a fairly general model which subsumes as special cases various models (e.g., Strong Stochastic Transitivity model, BTL model and uniform noise model), we characterize upper bounds on the sample size required for top-$K$ sorting as well as for top-$K$ partitioning. As a consequence, we demonstrate that active ranking can offer significant multiplicative gains in sample complexity over passive ranking. Depending on the underlying stochastic noise model, such gain varies from around $\frac{\log n}{\log \log n}$ to $\frac{ n^2 \log n }{\log \log n}$. We also present an algorithm that is applicable to both settings.",NOT,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v70-mohajer17a,
  title = 	 {Active Learning for Top-$K$ Rank Aggregation from Noisy Comparisons},
  author =       {Soheil Mohajer and Changho Suh and Adel Elmahdy},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {2488--2497},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/mohajer17a/mohajer17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/mohajer17a.html},
  abstract = 	 {We explore an active top-$K$ ranking problem based on pairwise comparisons that are collected possibly in a sequential manner as per our design choice. We consider two settings: (1) <em>top-$K$ sorting</em> in which the goal is to recover the top-$K$ items in order out of $n$ items; (2) <em>top-$K$ partitioning</em> where only the set of top-$K$ items is desired. Under a fairly general model which subsumes as special cases various models (e.g., Strong Stochastic Transitivity model, BTL model and uniform noise model), we characterize upper bounds on the sample size required for top-$K$ sorting as well as for top-$K$ partitioning. As a consequence, we demonstrate that active ranking can offer significant multiplicative gains in sample complexity over passive ranking. Depending on the underlying stochastic noise model, such gain varies from around $\frac{\log n}{\log \log n}$ to $\frac{ n^2 \log n }{\log \log n}$. We also present an algorithm that is applicable to both settings.}
}
"
ICML,2018,A Reductions Approach to Fair Classification,https://proceedings.mlr.press/v80/agarwal18a.html,"['Agarwal, Alekh', 'Beygelzimer, Alina', 'Dudik, Miroslav', 'Langford, John', 'Wallach, Hanna']",752,"We present a systematic approach for achieving fairness in a binary classification setting. While we focus on two well-known quantitative definitions of fairness, our approach encompasses many other previously studied definitions as special cases. The key idea is to reduce fair classification to a sequence of cost-sensitive classification problems, whose solutions yield a randomized classifier with the lowest (empirical) error subject to the desired constraints. We introduce two reductions that work for any representation of the cost-sensitive classifier and compare favorably to prior baselines on a variety of data sets, while overcoming several of their disadvantages.",TRAD,,Algorithm,,,,,,,,,,,,,,"
@InProceedings{pmlr-v80-agarwal18a,
  title = 	 {A Reductions Approach to Fair Classification},
  author =       {Agarwal, Alekh and Beygelzimer, Alina and Dudik, Miroslav and Langford, John and Wallach, Hanna},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {60--69},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/agarwal18a/agarwal18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/agarwal18a.html},
  abstract = 	 {We present a systematic approach for achieving fairness in a binary classification setting. While we focus on two well-known quantitative definitions of fairness, our approach encompasses many other previously studied definitions as special cases. The key idea is to reduce fair classification to a sequence of cost-sensitive classification problems, whose solutions yield a randomized classifier with the lowest (empirical) error subject to the desired constraints. We introduce two reductions that work for any representation of the cost-sensitive classifier and compare favorably to prior baselines on a variety of data sets, while overcoming several of their disadvantages.}
}
"
ICML,2018,"Proportional Allocation: Simple, Distributed, and Diverse Matching with High Entropy",https://proceedings.mlr.press/v80/agrawal18b.html,"['Agrawal, Shipra', 'Zadimoghaddam, Morteza', 'Mirrokni, Vahab']",29,"Inspired by many applications of bipartite matching in online advertising and machine learning, we study a simple and natural iterative proportional allocation algorithm: Maintain a priority score $\priority_a$ for each node $a\in \mathds{A}$ on one side of the bipartition, initialized as $\priority_a=1$. Iteratively allocate the nodes $i\in \impressions$ on the other side to eligible nodes in $\mathds{A}$ in proportion of their priority scores. After each round, for each node $a\in \mathds{A}$, decrease or increase the score $\priority_a$ based on whether it is over- or under- allocated. Our first result is that this simple, distributed algorithm converges to a $(1-\epsilon)$-approximate fractional $b$-matching solution in $O({\log n\over \epsilon^2} )$ rounds. We also extend the proportional allocation algorithm and convergence results to the maximum weighted matching problem, and show that the algorithm can be naturally tuned to produce maximum matching with high entropy. High entropy, in turn, implies additional desirable properties of this matching, e.g., it satisfies certain diversity and fairness (aka anonymity) properties that are desirable in a variety of applications in online advertising and machine learning.",SCOPE,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v80-agrawal18b,
  title = 	 {Proportional Allocation: Simple, Distributed, and Diverse Matching with High Entropy},
  author =       {Agrawal, Shipra and Zadimoghaddam, Morteza and Mirrokni, Vahab},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {99--108},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/agrawal18b/agrawal18b.pdf},
  url = 	 {https://proceedings.mlr.press/v80/agrawal18b.html},
  abstract = 	 {Inspired by many applications of bipartite matching in online advertising and machine learning, we study a simple and natural iterative proportional allocation algorithm: Maintain a priority score $\priority_a$ for each node $a\in \mathds{A}$ on one side of the bipartition, initialized as $\priority_a=1$. Iteratively allocate the nodes $i\in \impressions$ on the other side to eligible nodes in $\mathds{A}$ in proportion of their priority scores. After each round, for each node $a\in \mathds{A}$, decrease or increase the score $\priority_a$ based on whether it is over- or under- allocated. Our first result is that this simple, distributed algorithm converges to a $(1-\epsilon)$-approximate fractional $b$-matching solution in $O({\log n\over \epsilon^2} )$ rounds. We also extend the proportional allocation algorithm and convergence results to the maximum weighted matching problem, and show that the algorithm can be naturally tuned to produce maximum matching with <em>high entropy</em>. High entropy, in turn, implies additional desirable properties of this matching, e.g., it satisfies certain diversity and fairness (aka anonymity) properties that are desirable in a variety of applications in online advertising and machine learning.}
}
"
ICML,2018,Fair and Diverse {DPP} Based Data Summarization,https://proceedings.mlr.press/v80/celis18a.html,"['Celis, Elisa', 'Keswani, Vijay', 'Straszak, Damian', 'Deshpande, Amit', 'Kathuria, Tarun', 'Vishnoi, Nisheeth']",93,"Sampling methods that choose a subset of the data proportional to its diversity in the feature space are popular for data summarization. However, recent studies have noted the occurrence of bias {‚Äì} e.g., under or over representation of a particular gender or ethnicity {‚Äì} in such data summarization methods. In this paper we initiate a study of the problem of outputting a diverse and fair summary of a given dataset. We work with a well-studied determinantal measure of diversity and corresponding distributions (DPPs) and present a framework that allows us to incorporate a general class of fairness constraints into such distributions. Designing efficient algorithms to sample from these constrained determinantal distributions, however, suffers from a complexity barrier; we present a fast sampler that is provably good when the input vectors satisfy a natural property. Our empirical results on both real-world and synthetic datasets show that the diversity of the samples produced by adding fairness constraints is not too far from the unconstrained case.",SCOPE,,Algorithm,,,Sampling (Data summarization),,,,,,,,,,,"
@InProceedings{pmlr-v80-celis18a,
  title = 	 {Fair and Diverse {DPP}-Based Data Summarization},
  author =       {Celis, Elisa and Keswani, Vijay and Straszak, Damian and Deshpande, Amit and Kathuria, Tarun and Vishnoi, Nisheeth},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {716--725},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/celis18a/celis18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/celis18a.html},
  abstract = 	 {Sampling methods that choose a subset of the data proportional to its diversity in the feature space are popular for data summarization. However, recent studies have noted the occurrence of bias {–} e.g., under or over representation of a particular gender or ethnicity {–} in such data summarization methods. In this paper we initiate a study of the problem of outputting a diverse and fair summary of a given dataset. We work with a well-studied determinantal measure of diversity and corresponding distributions (DPPs) and present a framework that allows us to incorporate a general class of fairness constraints into such distributions. Designing efficient algorithms to sample from these constrained determinantal distributions, however, suffers from a complexity barrier; we present a fast sampler that is provably good when the input vectors satisfy a natural property. Our empirical results on both real-world and synthetic datasets show that the diversity of the samples produced by adding fairness constraints is not too far from the unconstrained case.}
}
"
ICML,2018,Fairness Without Demographics in Repeated Loss Minimization,https://proceedings.mlr.press/v80/hashimoto18a.html,"['Hashimoto, Tatsunori', 'Srivastava, Megha', 'Namkoong, Hongseok', 'Liang, Percy']",372,"Machine learning models (e.g., speech recognizers) trained on average loss suffer from representation disparity‚Äîminority groups (e.g., non-native speakers) carry less weight in the training objective, and thus tend to suffer higher loss. Worse, as model accuracy affects user retention, a minority group can shrink over time. In this paper, we first show that the status quo of empirical risk minimization (ERM) amplifies representation disparity over time, which can even turn initially fair models unfair. To mitigate this, we develop an approach based on distributionally robust optimization (DRO), which minimizes the worst case risk over all distributions close to the empirical distribution. We prove that this approach controls the risk of the minority group at each time step, in the spirit of Rawlsian distributive justice, while remaining oblivious to the identity of the groups. We demonstrate that DRO prevents disparity amplification on examples where ERM fails, and show improvements in minority group user satisfaction in a real-world text autocomplete task.",YES,,Algorithm,Optimization,Problem Identification,Mitigation,,,,,,,,,,,"
@InProceedings{pmlr-v80-hashimoto18a,
  title =          {Fairness Without Demographics in Repeated Loss Minimization},
  author =       {Hashimoto, Tatsunori and Srivastava, Megha and Namkoong, Hongseok and Liang, Percy},
  booktitle =          {Proceedings of the 35th International Conference on Machine Learning},
  pages =          {1929--1938},
  year =          {2018},
  editor =          {Dy, Jennifer and Krause, Andreas},
  volume =          {80},
  series =          {Proceedings of Machine Learning Research},
  month =          {10--15 Jul},
  publisher =    {PMLR},
  pdf =          {http://proceedings.mlr.press/v80/hashimoto18a/hashimoto18a.pdf},
  url =          {https://proceedings.mlr.press/v80/hashimoto18a.html},
  abstract =          {Machine learning models (e.g., speech recognizers) trained on average loss suffer from representation disparity—minority groups (e.g., non-native speakers) carry less weight in the training objective, and thus tend to suffer higher loss. Worse, as model accuracy affects user retention, a minority group can shrink over time. In this paper, we first show that the status quo of empirical risk minimization (ERM) amplifies representation disparity over time, which can even turn initially fair models unfair. To mitigate this, we develop an approach based on distributionally robust optimization (DRO), which minimizes the worst case risk over all distributions close to the empirical distribution. We prove that this approach controls the risk of the minority group at each time step, in the spirit of Rawlsian distributive justice, while remaining oblivious to the identity of the groups. We demonstrate that DRO prevents disparity amplification on examples where ERM fails, and show improvements in minority group user satisfaction in a real-world text autocomplete task.}
}
"
ICML,2018,Multicalibration: Calibration for the ({C}omputationally Identifiable) Masses,https://proceedings.mlr.press/v80/hebert-johnson18a.html,"['Hebert-Johnson, Ursula', 'Kim, Michael', 'Reingold, Omer', 'Rothblum, Guy']",251,"We develop and study multicalibration as a new measure of fairness in machine learning that aims to mitigate inadvertent or malicious discrimination that is introduced at training time (even from ground truth data). Multicalibration guarantees meaningful (calibrated) predictions for every subpopulation that can be identified within a specified class of computations. The specified class can be quite rich; in particular, it can contain many overlapping subgroups of a protected group. We demonstrate that in many settings this strong notion of protection from discrimination is provably attainable and aligned with the goal of obtaining accurate predictions. Along the way, we present algorithms for learning a multicalibrated predictor, study the computational complexity of this task, and illustrate tight connections to the agnostic learning model.",TRAD,,Metrics,Algorithm,,,,,,,,,,,,,"
@InProceedings{pmlr-v80-hebert-johnson18a,
  title = 	 {Multicalibration: Calibration for the ({C}omputationally-Identifiable) Masses},
  author =       {Hebert-Johnson, Ursula and Kim, Michael and Reingold, Omer and Rothblum, Guy},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {1939--1948},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/hebert-johnson18a/hebert-johnson18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/hebert-johnson18a.html},
  abstract = 	 {We develop and study multicalibration as a new measure of fairness in machine learning that aims to mitigate inadvertent or malicious discrimination that is introduced at training time (even from ground truth data). Multicalibration guarantees meaningful (calibrated) predictions for every subpopulation that can be identified within a specified class of computations. The specified class can be quite rich; in particular, it can contain many overlapping subgroups of a protected group. We demonstrate that in many settings this strong notion of protection from discrimination is provably attainable and aligned with the goal of obtaining accurate predictions. Along the way, we present algorithms for learning a multicalibrated predictor, study the computational complexity of this task, and illustrate tight connections to the agnostic learning model.}
}
"
ICML,2018,Residual Unfairness in Fair Machine Learning from Prejudiced Data,https://proceedings.mlr.press/v80/kallus18a.html,"['Kallus, Nathan', 'Zhou, Angela']",107,"Recent work in fairness in machine learning has proposed adjusting for fairness by equalizing accuracy metrics across groups and has also studied how datasets affected by historical prejudices may lead to unfair decision policies. We connect these lines of work and study the residual unfairness that arises when a fairness-adjusted predictor is not actually fair on the target population due to systematic censoring of training data by existing biased policies. This scenario is particularly common in the same applications where fairness is a concern. We characterize theoretically the impact of such censoring on standard fairness metrics for binary classifiers and provide criteria for when residual unfairness may or may not appear. We prove that, under certain conditions, fairness-adjusted classifiers will in fact induce residual unfairness that perpetuates the same injustices, against the same groups, that biased the data to begin with, thus showing that even state-of-the-art fair machine learning can have a ""bias in, bias out"" property. When certain benchmark data is available, we show how sample reweighting can estimate and adjust fairness metrics while accounting for censoring. We use this to study the case of Stop, Question, and Frisk (SQF) and demonstrate that attempting to adjust for fairness perpetuates the same injustices that the policy is infamous for.",YES,,Data,,Problem Identification,,,,,,,,,,,,"
@InProceedings{pmlr-v80-kallus18a,
  title =          {Residual Unfairness in Fair Machine Learning from Prejudiced Data},
  author =       {Kallus, Nathan and Zhou, Angela},
  booktitle =          {Proceedings of the 35th International Conference on Machine Learning},
  pages =          {2439--2448},
  year =          {2018},
  editor =          {Dy, Jennifer and Krause, Andreas},
  volume =          {80},
  series =          {Proceedings of Machine Learning Research},
  month =          {10--15 Jul},
  publisher =    {PMLR},
  pdf =          {http://proceedings.mlr.press/v80/kallus18a/kallus18a.pdf},
  url =          {https://proceedings.mlr.press/v80/kallus18a.html},
  abstract =          {Recent work in fairness in machine learning has proposed adjusting for fairness by equalizing accuracy metrics across groups and has also studied how datasets affected by historical prejudices may lead to unfair decision policies. We connect these lines of work and study the residual unfairness that arises when a fairness-adjusted predictor is not actually fair on the target population due to systematic censoring of training data by existing biased policies. This scenario is particularly common in the same applications where fairness is a concern. We characterize theoretically the impact of such censoring on standard fairness metrics for binary classifiers and provide criteria for when residual unfairness may or may not appear. We prove that, under certain conditions, fairness-adjusted classifiers will in fact induce residual unfairness that perpetuates the same injustices, against the same groups, that biased the data to begin with, thus showing that even state-of-the-art fair machine learning can have a ""bias in, bias out"" property. When certain benchmark data is available, we show how sample reweighting can estimate and adjust fairness metrics while accounting for censoring. We use this to study the case of Stop, Question, and Frisk (SQF) and demonstrate that attempting to adjust for fairness perpetuates the same injustices that the policy is infamous for.}
}
"
ICML,2018,Scalable Deletion Robust Submodular Maximization: Data Summarization with Privacy and Fairness Constraints,https://proceedings.mlr.press/v80/kazemi18a.html,"['Kazemi, Ehsan', 'Zadimoghaddam, Morteza', 'Karbasi, Amin']",56,"Can we efficiently extract useful information from a large user-generated dataset while protecting the privacy of the users and/or ensuring fairness in representation? We cast this problem as an instance of a deletion-robust submodular maximization where part of the data may be deleted or masked due to privacy concerns or fairness criteria. We propose the first memory-efficient centralized, streaming, and distributed methods with constant-factor approximation guarantees against any number of adversarial deletions. We extensively evaluate the performance of our algorithms on real-world applications, including (i) Uber-pick up locations with location privacy constraints; (ii) feature selection with fairness constraints for income prediction and crime rate prediction; and (iii) robust to deletion summarization of census data, consisting of 2,458,285 feature vectors. Our experiments show that our solution is robust against even $80%$ of data deletion.",SCOPE,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v80-kazemi18a,
  title = 	 {Scalable Deletion-Robust Submodular Maximization: Data Summarization with Privacy and Fairness Constraints},
  author =       {Kazemi, Ehsan and Zadimoghaddam, Morteza and Karbasi, Amin},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {2544--2553},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/kazemi18a/kazemi18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/kazemi18a.html},
  abstract = 	 {Can we efficiently extract useful information from a large user-generated dataset while protecting the privacy of the users and/or ensuring fairness in representation? We cast this problem as an instance of a deletion-robust submodular maximization where part of the data may be deleted or masked due to privacy concerns or fairness criteria. We propose the first memory-efficient centralized, streaming, and distributed methods with constant-factor approximation guarantees against <em>any</em> number of adversarial deletions. We extensively evaluate the performance of our algorithms on real-world applications, including (i) Uber-pick up locations with location privacy constraints; (ii) feature selection with fairness constraints for income prediction and crime rate prediction; and (iii) robust to deletion summarization of census data, consisting of 2,458,285 feature vectors. Our experiments show that our solution is robust against even $80%$ of data deletion.}
}
"
ICML,2018,Preventing Fairness Gerrymandering: Auditing and Learning for Subgroup Fairness,https://proceedings.mlr.press/v80/kearns18a.html,"['Kearns, Michael', 'Neel, Seth', 'Roth, Aaron', 'Wu, Zhiwei Steven']",558,"The most prevalent notions of fairness in machine learning fix a small collection of pre-defined groups (such as race or gender), and then ask for approximate parity of some statistic of the classifier (such as false positive rate) across these groups. Constraints of this form are susceptible to fairness gerrymandering, in which a classifier is fair on each individual group, but badly violates the fairness constraint on structured subgroups, such as certain combinations of protected attribute values. We thus consider fairness across exponentially or infinitely many subgroups, defined by a structured class of functions over the protected attributes. We first prove that the problem of auditing subgroup fairness for both equality of false positive rates and statistical parity is computationally equivalent to the problem of weak agnostic learning ‚Äî which means it is hard in the worst case, even for simple structured subclasses. However, it also suggests that common heuristics for learning can be applied to successfully solve the auditing problem in practice. We then derive an algorithm that provably converges in a polynomial number of steps to the best subgroup-fair distribution over classifiers, given access to an oracle which can solve the agnostic learning problem. The algorithm is based on a formulation of subgroup fairness as a zero-sum game between a Learner (the primal player) and an Auditor (the dual player). We implement a variant of this algorithm using heuristic oracles, and show that we can effectively both audit and learn fair classifiers on a real dataset.",TRAD,,Algorithm,,Mitigation,,,,,,,,,,,,"
@InProceedings{pmlr-v80-kearns18a,
  title = 	 {Preventing Fairness Gerrymandering: Auditing and Learning for Subgroup Fairness},
  author =       {Kearns, Michael and Neel, Seth and Roth, Aaron and Wu, Zhiwei Steven},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {2564--2572},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/kearns18a/kearns18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/kearns18a.html},
  abstract = 	 {The most prevalent notions of fairness in machine learning fix a small collection of pre-defined groups (such as race or gender), and then ask for approximate parity of some statistic of the classifier (such as false positive rate) across these groups. Constraints of this form are susceptible to fairness gerrymandering, in which a classifier is fair on each individual group, but badly violates the fairness constraint on structured subgroups, such as certain combinations of protected attribute values. We thus consider fairness across exponentially or infinitely many subgroups, defined by a structured class of functions over the protected attributes. We first prove that the problem of auditing subgroup fairness for both equality of false positive rates and statistical parity is computationally equivalent to the problem of weak agnostic learning — which means it is hard in the worst case, even for simple structured subclasses. However, it also suggests that common heuristics for learning can be applied to successfully solve the auditing problem in practice. We then derive an algorithm that provably converges in a polynomial number of steps to the best subgroup-fair distribution over classifiers, given access to an oracle which can solve the agnostic learning problem. The algorithm is based on a formulation of subgroup fairness as a zero-sum game between a Learner (the primal player) and an Auditor (the dual player). We implement a variant of this algorithm using heuristic oracles, and show that we can effectively both audit and learn fair classifiers on a real dataset.}
}
"
ICML,2018,Blind Justice: Fairness with Encrypted Sensitive Attributes,https://proceedings.mlr.press/v80/kilbertus18a.html,"['Kilbertus, Niki', 'Gascon, Adria', 'Kusner, Matt', 'Veale, Michael', 'Gummadi, Krishna', 'Weller, Adrian']",115,"Recent work has explored how to train machine learning models which do not discriminate against any subgroup of the population as determined by sensitive attributes such as gender or race. To avoid disparate treatment, sensitive attributes should not be considered. On the other hand, in order to avoid disparate impact, sensitive attributes must be examined, e.g., in order to learn a fair model, or to check if a given model is fair. We introduce methods from secure multi-party computation which allow us to avoid both. By encrypting sensitive attributes, we show how an outcome-based fair model may be learned, checked, or have its outputs verified and held to account, without users revealing their sensitive attributes.",TRAD,,Algorithm,,Mitigation,,,,,,,,,,,,"
@InProceedings{pmlr-v80-kilbertus18a,
  title = 	 {Blind Justice: Fairness with Encrypted Sensitive Attributes},
  author =       {Kilbertus, Niki and Gascon, Adria and Kusner, Matt and Veale, Michael and Gummadi, Krishna and Weller, Adrian},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {2630--2639},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/kilbertus18a/kilbertus18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/kilbertus18a.html},
  abstract = 	 {Recent work has explored how to train machine learning models which do not discriminate against any subgroup of the population as determined by sensitive attributes such as gender or race. To avoid disparate treatment, sensitive attributes should not be considered. On the other hand, in order to avoid disparate impact, sensitive attributes must be examined, e.g., in order to learn a fair model, or to check if a given model is fair. We introduce methods from secure multi-party computation which allow us to avoid both. By encrypting sensitive attributes, we show how an outcome-based fair model may be learned, checked, or have its outputs verified and held to account, without users revealing their sensitive attributes.}
}
"
ICML,2018,Nonconvex Optimization for Regression with Fairness Constraints,https://proceedings.mlr.press/v80/komiyama18a.html,"['Komiyama, Junpei', 'Takeda, Akiko', 'Honda, Junya', 'Shimao, Hajime']",89,"The unfairness of a regressor is evaluated by measuring the correlation between the estimator and the sensitive attribute (e.g., race, gender, age), and the coefficient of determination (CoD) is a natural extension of the correlation coefficient when more than one sensitive attribute exists. As is well known, there is a trade-off between fairness and accuracy of a regressor, which implies a perfectly fair optimizer does not always yield a useful prediction. Taking this into consideration, we optimize the accuracy of the estimation subject to a user-defined level of fairness. However, a fairness level as a constraint induces a nonconvexity of the feasible region, which disables the use of an off-the-shelf convex optimizer. Despite such nonconvexity, we show an exact solution is available by using tools of global optimization theory. Furthermore, we propose a nonlinear extension of the method by kernel representation. Unlike most of existing fairness-aware machine learning methods, our method allows us to deal with numeric and multiple sensitive attributes.",TRAD,,Algorithm,,,,,,,,,,,,,,"
@InProceedings{pmlr-v80-komiyama18a,
  title = 	 {Nonconvex Optimization for Regression with Fairness Constraints},
  author =       {Komiyama, Junpei and Takeda, Akiko and Honda, Junya and Shimao, Hajime},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {2737--2746},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/komiyama18a/komiyama18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/komiyama18a.html},
  abstract = 	 {The unfairness of a regressor is evaluated by measuring the correlation between the estimator and the sensitive attribute (e.g., race, gender, age), and the coefficient of determination (CoD) is a natural extension of the correlation coefficient when more than one sensitive attribute exists. As is well known, there is a trade-off between fairness and accuracy of a regressor, which implies a perfectly fair optimizer does not always yield a useful prediction. Taking this into consideration, we optimize the accuracy of the estimation subject to a user-defined level of fairness. However, a fairness level as a constraint induces a nonconvexity of the feasible region, which disables the use of an off-the-shelf convex optimizer. Despite such nonconvexity, we show an exact solution is available by using tools of global optimization theory. Furthermore, we propose a nonlinear extension of the method by kernel representation. Unlike most of existing fairness-aware machine learning methods, our method allows us to deal with numeric and multiple sensitive attributes.}
}
"
ICML,2018,Delayed Impact of Fair Machine Learning,https://proceedings.mlr.press/v80/liu18c.html,"['Liu, Lydia T.', 'Dean, Sarah', 'Rolf, Esther', 'Simchowitz, Max', 'Hardt, Moritz']",374,"Fairness in machine learning has predominantly been studied in static classification settings without concern for how decisions change the underlying population over time. Conventional wisdom suggests that fairness criteria promote the long-term well-being of those groups they aim to protect. We study how static fairness criteria interact with temporal indicators of well-being, such as long-term improvement, stagnation, and decline in a variable of interest. We demonstrate that even in a one-step feedback model, common fairness criteria in general do not promote improvement over time, and may in fact cause harm in cases where an unconstrained objective would not. We completely characterize the delayed impact of three standard criteria, contrasting the regimes in which these exhibit qualitatively different behavior. In addition, we find that a natural form of measurement error broadens the regime in which fairness criteria perform favorably. Our results highlight the importance of measurement and temporal modeling in the evaluation of fairness criteria, suggesting a range of new challenges and trade-offs.",Maybe,,,,Problem Identification,Fairness over time leads to unfairness,,,,,,,,,,,"
@InProceedings{pmlr-v80-liu18c,
  title = 	 {Delayed Impact of Fair Machine Learning},
  author =       {Liu, Lydia T. and Dean, Sarah and Rolf, Esther and Simchowitz, Max and Hardt, Moritz},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {3150--3158},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/liu18c/liu18c.pdf},
  url = 	 {https://proceedings.mlr.press/v80/liu18c.html},
  abstract = 	 {Fairness in machine learning has predominantly been studied in static classification settings without concern for how decisions change the underlying population over time. Conventional wisdom suggests that fairness criteria promote the long-term well-being of those groups they aim to protect. We study how static fairness criteria interact with temporal indicators of well-being, such as long-term improvement, stagnation, and decline in a variable of interest. We demonstrate that even in a one-step feedback model, common fairness criteria in general do not promote improvement over time, and may in fact cause harm in cases where an unconstrained objective would not. We completely characterize the delayed impact of three standard criteria, contrasting the regimes in which these exhibit qualitatively different behavior. In addition, we find that a natural form of measurement error broadens the regime in which fairness criteria perform favorably. Our results highlight the importance of measurement and temporal modeling in the evaluation of fairness criteria, suggesting a range of new challenges and trade-offs.}
}
"
ICML,2018,Learning Adversarially Fair and Transferable Representations,https://proceedings.mlr.press/v80/madras18a.html,"['Madras, David', 'Creager, Elliot', 'Pitassi, Toniann', 'Zemel, Richard']",494,"In this paper, we advocate for representation learning as the key to mitigating unfair prediction outcomes downstream. Motivated by a scenario where learned representations are used by third parties with unknown objectives, we propose and explore adversarial representation learning as a natural method of ensuring those parties act fairly. We connect group fairness (demographic parity, equalized odds, and equal opportunity) to different adversarial objectives. Through worst-case theoretical guarantees and experimental validation, we show that the choice of this objective is crucial to fair prediction. Furthermore, we present the first in-depth experimental demonstration of fair transfer learning and demonstrate empirically that our learned representations admit fair predictions on new tasks while maintaining utility, an essential goal of fair representation learning.",TRAD,,Algorithm,,Mitigation,,,,,,,,,,,,"
@InProceedings{pmlr-v80-madras18a,
  title = 	 {Learning Adversarially Fair and Transferable Representations},
  author =       {Madras, David and Creager, Elliot and Pitassi, Toniann and Zemel, Richard},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {3384--3393},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/madras18a/madras18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/madras18a.html},
  abstract = 	 {In this paper, we advocate for representation learning as the key to mitigating unfair prediction outcomes downstream. Motivated by a scenario where learned representations are used by third parties with unknown objectives, we propose and explore adversarial representation learning as a natural method of ensuring those parties act fairly. We connect group fairness (demographic parity, equalized odds, and equal opportunity) to different adversarial objectives. Through worst-case theoretical guarantees and experimental validation, we show that the choice of this objective is crucial to fair prediction. Furthermore, we present the first in-depth experimental demonstration of fair transfer learning and demonstrate empirically that our learned representations admit fair predictions on new tasks while maintaining utility, an essential goal of fair representation learning.}
}
"
ICML,2018,Design of Experiments for Model Discrimination Hybridising Analytical and Data Driven Approaches,https://proceedings.mlr.press/v80/olofsson18a.html,"['Olofsson, Simon', 'Deisenroth, Marc', 'Misener, Ruth']",9,"Healthcare companies must submit pharmaceutical drugs or medical device to regulatory bodies before marketing new technology. Regulatory bodies frequently require transparent and interpretable computational modelling to justify a new healthcare technology, but researchers may have several competing models for a biological system and too little data to discriminate between the models. In design of experiments for model discrimination, where the goal is to design maximally informative physical experiments in order to discriminate between rival predictive models, research has focused either on analytical approaches, which cannot manage all functions, or on data-driven approaches, which may have computational difficulties or lack interpretable marginal predictive distributions. We develop a methodology for introducing Gaussian process surrogates in lieu of the original mechanistic models. This allows us to extend existing design and model discrimination methods developed for analytical models to cases of non-analytical models.",YES,,Validation,Selection,Case Study,,,,,,,,,,,,"
@InProceedings{pmlr-v80-olofsson18a,
  title =          {Design of Experiments for Model Discrimination Hybridising Analytical and Data-Driven Approaches},
  author =       {Olofsson, Simon and Deisenroth, Marc and Misener, Ruth},
  booktitle =          {Proceedings of the 35th International Conference on Machine Learning},
  pages =          {3908--3917},
  year =          {2018},
  editor =          {Dy, Jennifer and Krause, Andreas},
  volume =          {80},
  series =          {Proceedings of Machine Learning Research},
  month =          {10--15 Jul},
  publisher =    {PMLR},
  pdf =          {http://proceedings.mlr.press/v80/olofsson18a/olofsson18a.pdf},
  url =          {https://proceedings.mlr.press/v80/olofsson18a.html},
  abstract =          {Healthcare companies must submit pharmaceutical drugs or medical device to regulatory bodies before marketing new technology. Regulatory bodies frequently require transparent and interpretable computational modelling to justify a new healthcare technology, but researchers may have several competing models for a biological system and too little data to discriminate between the models. In design of experiments for model discrimination, where the goal is to design maximally informative physical experiments in order to discriminate between rival predictive models, research has focused either on analytical approaches, which cannot manage all functions, or on data-driven approaches, which may have computational difficulties or lack interpretable marginal predictive distributions. We develop a methodology for introducing Gaussian process surrogates in lieu of the original mechanistic models. This allows us to extend existing design and model discrimination methods developed for analytical models to cases of non-analytical models.}
}
"
ICML,2018,Probably Approximately Metric Fair Learning,https://proceedings.mlr.press/v80/yona18a.html,"['Yona, Gal', 'Rothblum, Guy']",53,"The seminal work of Dwork et al. [ITCS 2012] introduced a metric-based notion of individual fairness: given a task-specific similarity metric, their notion required that every pair of similar individuals should be treated similarly. In the context of machine learning, however, individual fairness does not generalize from a training set to the underlying population. We show that this can lead to computational intractability even for simple fair-learning tasks. With this motivation in mind, we introduce and study a relaxed notion of approximate metric-fairness: for a random pair of individuals sampled from the population, with all but a small probability of error, if they are similar then they should be treated similarly. We formalize the goal of achieving approximate metric-fairness simultaneously with best-possible accuracy as Probably Approximately Correct and Fair (PACF) Learning. We show that approximate metric-fairness does generalize, and leverage these generalization guarantees to construct polynomial-time PACF learning algorithms for the classes of linear and logistic predictors.",TRAD,,Algorithm,,,,,,,,,,,,,,"
@InProceedings{pmlr-v80-yona18a,
  title = 	 {Probably Approximately Metric-Fair Learning},
  author =       {Yona, Gal and Rothblum, Guy},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {5680--5688},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/yona18a/yona18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/yona18a.html},
  abstract = 	 {The seminal work of Dwork <em>et al.</em> [ITCS 2012] introduced a metric-based notion of individual fairness: given a task-specific similarity metric, their notion required that every pair of similar individuals should be treated similarly. In the context of machine learning, however, individual fairness does not generalize from a training set to the underlying population. We show that this can lead to computational intractability even for simple fair-learning tasks. With this motivation in mind, we introduce and study a relaxed notion of <em>approximate metric-fairness</em>: for a random pair of individuals sampled from the population, with all but a small probability of error, if they are similar then they should be treated similarly. We formalize the goal of achieving approximate metric-fairness simultaneously with best-possible accuracy as Probably Approximately Correct and Fair (PACF) Learning. We show that approximate metric-fairness <em>does</em> generalize, and leverage these generalization guarantees to construct polynomial-time PACF learning algorithms for the classes of linear and logistic predictors.}
}
"
ICML,2019,Fair Regression: Quantitative Definitions and Reduction Based Algorithms,https://proceedings.mlr.press/v97/agarwal19d.html,"['Agarwal, Alekh', 'Dudik, Miroslav', 'Wu, Zhiwei Steven']",151,"In this paper, we study the prediction of a real-valued target, such as a risk score or recidivism rate, while guaranteeing a quantitative notion of fairness with respect to a protected attribute such as gender or race. We call this class of problems fair regression. We propose general schemes for fair regression under two notions of fairness: (1) statistical parity, which asks that the prediction be statistically independent of the protected attribute, and (2) bounded group loss, which asks that the prediction error restricted to any protected group remain below some pre-determined level. While we only study these two notions of fairness, our schemes are applicable to arbitrary Lipschitz-continuous losses, and so they encompass least-squares regression, logistic regression, quantile regression, and many other tasks. Our schemes only require access to standard risk minimization algorithms (such as standard classification or least-squares regression) while providing theoretical guarantees on the optimality and fairness of the obtained solutions. In addition to analyzing theoretical properties of our schemes, we empirically demonstrate their ability to uncover fairness‚Äìaccuracy frontiers on several standard datasets.",TRAD,,Algorithm,,,,,,,,,,,,,,"
@InProceedings{pmlr-v97-agarwal19d,
  title = 	 {Fair Regression: Quantitative Definitions and Reduction-Based Algorithms},
  author =       {Agarwal, Alekh and Dudik, Miroslav and Wu, Zhiwei Steven},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {120--129},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/agarwal19d/agarwal19d.pdf},
  url = 	 {https://proceedings.mlr.press/v97/agarwal19d.html},
  abstract = 	 {In this paper, we study the prediction of a real-valued target, such as a risk score or recidivism rate, while guaranteeing a quantitative notion of fairness with respect to a protected attribute such as gender or race. We call this class of problems fair regression. We propose general schemes for fair regression under two notions of fairness: (1) statistical parity, which asks that the prediction be statistically independent of the protected attribute, and (2) bounded group loss, which asks that the prediction error restricted to any protected group remain below some pre-determined level. While we only study these two notions of fairness, our schemes are applicable to arbitrary Lipschitz-continuous losses, and so they encompass least-squares regression, logistic regression, quantile regression, and many other tasks. Our schemes only require access to standard risk minimization algorithms (such as standard classification or least-squares regression) while providing theoretical guarantees on the optimality and fairness of the obtained solutions. In addition to analyzing theoretical properties of our schemes, we empirically demonstrate their ability to uncover fairness–accuracy frontiers on several standard datasets.}
}
"
ICML,2019,Fairwashing: the risk of rationalization,https://proceedings.mlr.press/v97/aivodji19a.html,"['Aivodji, Ulrich', 'Arai, Hiromi', 'Fortineau, Olivier', ""Gambs, S{\\'e}bastien"", 'Hara, Satoshi', 'Tapp, Alain']",101,"Black-box explanation is the problem of explaining how a machine learning model ‚Äì whose internal logic is hidden to the auditor and generally complex ‚Äì produces its outcomes. Current approaches for solving this problem include model explanation, outcome explanation as well as model inspection. While these techniques can be beneficial by providing interpretability, they can be used in a negative manner to perform fairwashing, which we define as promoting the false perception that a machine learning model respects some ethical values. In particular, we demonstrate that it is possible to systematically rationalize decisions taken by an unfair black-box model using the model explanation as well as the outcome explanation approaches with a given fairness metric. Our solution, LaundryML, is based on a regularized rule list enumeration algorithm whose objective is to search for fair rule lists approximating an unfair black-box model. We empirically evaluate our rationalization technique on black-box models trained on real-world datasets and show that one can obtain rule lists with high fidelity to the black-box model while being considerably less unfair at the same time.",NOT,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v97-aivodji19a,
  title = 	 {Fairwashing: the risk of rationalization},
  author =       {Aivodji, Ulrich and Arai, Hiromi and Fortineau, Olivier and Gambs, S{\'e}bastien and Hara, Satoshi and Tapp, Alain},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {161--170},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/aivodji19a/aivodji19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/aivodji19a.html},
  abstract = 	 {Black-box explanation is the problem of explaining how a machine learning model – whose internal logic is hidden to the auditor and generally complex – produces its outcomes. Current approaches for solving this problem include model explanation, outcome explanation as well as model inspection. While these techniques can be beneficial by providing interpretability, they can be used in a negative manner to perform fairwashing, which we define as promoting the false perception that a machine learning model respects some ethical values. In particular, we demonstrate that it is possible to systematically rationalize decisions taken by an unfair black-box model using the model explanation as well as the outcome explanation approaches with a given fairness metric. Our solution, LaundryML, is based on a regularized rule list enumeration algorithm whose objective is to search for fair rule lists approximating an unfair black-box model. We empirically evaluate our rationalization technique on black-box models trained on real-world datasets and show that one can obtain rule lists with high fidelity to the black-box model while being considerably less unfair at the same time.}
}
"
ICML,2019,Scalable Fair Clustering,https://proceedings.mlr.press/v97/backurs19a.html,"['Backurs, Arturs', 'Indyk, Piotr', 'Onak, Krzysztof', 'Schieber, Baruch', 'Vakilian, Ali', 'Wagner, Tal']",149,"We study the fair variant of the classic k-median problem introduced by (Chierichetti et al., NeurIPS 2017) in which the points are colored, and the goal is to minimize the same average distance objective as in the standard $k$-median problem while ensuring that all clusters have an ‚Äúapproximately equal‚Äù number of points of each color. (Chierichetti et al., NeurIPS 2017) proposed a two-phase algorithm for fair $k$-clustering. In the first step, the pointset is partitioned into subsets called fairlets that satisfy the fairness requirement and approximately preserve the k-median objective. In the second step, fairlets are merged into k clusters by one of the existing k-median algorithms. The running time of this algorithm is dominated by the first step, which takes super-quadratic time. In this paper, we present a practical approximate fairlet decomposition algorithm that runs in nearly linear time.",SCOPE,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v97-backurs19a,
  title = 	 {Scalable Fair Clustering},
  author =       {Backurs, Arturs and Indyk, Piotr and Onak, Krzysztof and Schieber, Baruch and Vakilian, Ali and Wagner, Tal},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {405--413},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/backurs19a/backurs19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/backurs19a.html},
  abstract = 	 {We study the fair variant of the classic k-median problem introduced by (Chierichetti et al., NeurIPS 2017) in which the points are colored, and the goal is to minimize the same average distance objective as in the standard $k$-median problem while ensuring that all clusters have an “approximately equal” number of points of each color. (Chierichetti et al., NeurIPS 2017) proposed a two-phase algorithm for fair $k$-clustering. In the first step, the pointset is partitioned into subsets called fairlets that satisfy the fairness requirement and approximately preserve the k-median objective. In the second step, fairlets are merged into k clusters by one of the existing k-median algorithms. The running time of this algorithm is dominated by the first step, which takes super-quadratic time. In this paper, we present a practical approximate fairlet decomposition algorithm that runs in nearly linear time.}
}
"
ICML,2019,Compositional Fairness Constraints for Graph Embeddings,https://proceedings.mlr.press/v97/bose19a.html,"['Bose, Avishek', 'Hamilton, William']",151,"Learning high-quality node embeddings is a key building block for machine learning models that operate on graph data, such as social networks and recommender systems. However, existing graph embedding techniques are unable to cope with fairness constraints, e.g., ensuring that the learned representations do not correlate with certain attributes, such as age or gender. Here, we introduce an adversarial framework to enforce fairness constraints on graph embeddings. Our approach is compositional‚Äîmeaning that it can flexibly accommodate different combinations of fairness constraints during inference. For instance, in the context of social recommendations, our framework would allow one user to request that their recommendations are invariant to both their age and gender, while also allowing another user to request invariance to just their age. Experiments on standard knowledge graph and recommender system benchmarks highlight the utility of our proposed framework.",Maybe/SCOPE,,Data,Algorithm,Mitigation,Way of learning fair graph embeddings,,,,,,,,,,,"
@InProceedings{pmlr-v97-bose19a,
  title = 	 {Compositional Fairness Constraints for Graph Embeddings},
  author =       {Bose, Avishek and Hamilton, William},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {715--724},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/bose19a/bose19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/bose19a.html},
  abstract = 	 {Learning high-quality node embeddings is a key building block for machine learning models that operate on graph data, such as social networks and recommender systems. However, existing graph embedding techniques are unable to cope with fairness constraints, e.g., ensuring that the learned representations do not correlate with certain attributes, such as age or gender. Here, we introduce an adversarial framework to enforce fairness constraints on graph embeddings. Our approach is <em>compositional</em>—meaning that it can flexibly accommodate different combinations of fairness constraints during inference. For instance, in the context of social recommendations, our framework would allow one user to request that their recommendations are invariant to both their age and gender, while also allowing another user to request invariance to just their age. Experiments on standard knowledge graph and recommender system benchmarks highlight the utility of our proposed framework.}
}
"
ICML,2019,Proportionally Fair Clustering,https://proceedings.mlr.press/v97/chen19d.html,"['Chen, Xingyu', 'Fain, Brandon', 'Lyu, Liang', 'Munagala, Kamesh']",109,"We extend the fair machine learning literature by considering the problem of proportional centroid clustering in a metric context. For clustering n points with k centers, we define fairness as proportionality to mean that any n/k points are entitled to form their own cluster if there is another center that is closer in distance for all n/k points. We seek clustering solutions to which there are no such justified complaints from any subsets of agents, without assuming any a priori notion of protected subsets. We present and analyze algorithms to efficiently compute, optimize, and audit proportional solutions. We conclude with an empirical examination of the tradeoff between proportional solutions and the k-means objective.",SCOPE,,,,,Unsupervised,,,,,,,,,,,"
@InProceedings{pmlr-v97-chen19d,
  title = 	 {Proportionally Fair Clustering},
  author =       {Chen, Xingyu and Fain, Brandon and Lyu, Liang and Munagala, Kamesh},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {1032--1041},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/chen19d/chen19d.pdf},
  url = 	 {https://proceedings.mlr.press/v97/chen19d.html},
  abstract = 	 {We extend the fair machine learning literature by considering the problem of proportional centroid clustering in a metric context. For clustering n points with k centers, we define fairness as proportionality to mean that any n/k points are entitled to form their own cluster if there is another center that is closer in distance for all n/k points. We seek clustering solutions to which there are no such justified complaints from any subsets of agents, without assuming any a priori notion of protected subsets. We present and analyze algorithms to efficiently compute, optimize, and audit proportional solutions. We conclude with an empirical examination of the tradeoff between proportional solutions and the k-means objective.}
}
"
ICML,2019,Training Well Generalizing Classifiers for Fairness Metrics and Other Data Dependent Constraints,https://proceedings.mlr.press/v97/cotter19b.html,"['Cotter, Andrew', 'Gupta, Maya', 'Jiang, Heinrich', 'Srebro, Nathan', 'Sridharan, Karthik', 'Wang, Serena', 'Woodworth, Blake', 'You, Seungil']",74,"Classifiers can be trained with data-dependent constraints to satisfy fairness goals, reduce churn, achieve a targeted false positive rate, or other policy goals. We study the generalization performance for such constrained optimization problems, in terms of how well the constraints are satisfied at evaluation time, given that they are satisfied at training time. To improve generalization, we frame the problem as a two-player game where one player optimizes the model parameters on a training dataset, and the other player enforces the constraints on an independent validation dataset. We build on recent work in two-player constrained optimization to show that if one uses this two-dataset approach, then constraint generalization can be significantly improved. As we illustrate experimentally, this approach works not only in theory, but also in practice.",TRAD,,Algorithm,,,,,,,,,,,,,,"
@InProceedings{pmlr-v97-cotter19b,
  title = 	 {Training Well-Generalizing Classifiers for Fairness Metrics and Other Data-Dependent Constraints},
  author =       {Cotter, Andrew and Gupta, Maya and Jiang, Heinrich and Srebro, Nathan and Sridharan, Karthik and Wang, Serena and Woodworth, Blake and You, Seungil},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {1397--1405},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/cotter19b/cotter19b.pdf},
  url = 	 {https://proceedings.mlr.press/v97/cotter19b.html},
  abstract = 	 {Classifiers can be trained with data-dependent constraints to satisfy fairness goals, reduce churn, achieve a targeted false positive rate, or other policy goals. We study the generalization performance for such constrained optimization problems, in terms of how well the constraints are satisfied at evaluation time, given that they are satisfied at training time. To improve generalization, we frame the problem as a two-player game where one player optimizes the model parameters on a training dataset, and the other player enforces the constraints on an independent validation dataset. We build on recent work in two-player constrained optimization to show that if one uses this two-dataset approach, then constraint generalization can be significantly improved. As we illustrate experimentally, this approach works not only in theory, but also in practice.}
}
"
ICML,2019,Flexibly Fair Representation Learning by Disentanglement,https://proceedings.mlr.press/v97/creager19a.html,"['Creager, Elliot', 'Madras, David', 'Jacobsen, Joern-Henrik', 'Weis, Marissa', 'Swersky, Kevin', 'Pitassi, Toniann', 'Zemel, Richard']",226,"We consider the problem of learning representations that achieve group and subgroup fairness with respect to multiple sensitive attributes. Taking inspiration from the disentangled representation learning literature, we propose an algorithm for learning compact representations of datasets that are useful for reconstruction and prediction, but are also flexibly fair, meaning they can be easily modified at test time to achieve subgroup demographic parity with respect to multiple sensitive attributes and their conjunctions. We show empirically that the resulting encoder‚Äîwhich does not require the sensitive attributes for inference‚Äîallows for the adaptation of a single representation to a variety of fair classification tasks with new target labels and subgroup definitions.",YES,,Data,Algorithm,Mitigation,Mitigation,,,,,,,,,,,"
@InProceedings{pmlr-v97-creager19a,
  title =          {Flexibly Fair Representation Learning by Disentanglement},
  author =       {Creager, Elliot and Madras, David and Jacobsen, Joern-Henrik and Weis, Marissa and Swersky, Kevin and Pitassi, Toniann and Zemel, Richard},
  booktitle =          {Proceedings of the 36th International Conference on Machine Learning},
  pages =          {1436--1445},
  year =          {2019},
  editor =          {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume =          {97},
  series =          {Proceedings of Machine Learning Research},
  month =          {09--15 Jun},
  publisher =    {PMLR},
  pdf =          {http://proceedings.mlr.press/v97/creager19a/creager19a.pdf},
  url =          {https://proceedings.mlr.press/v97/creager19a.html},
  abstract =          {We consider the problem of learning representations that achieve group and subgroup fairness with respect to multiple sensitive attributes. Taking inspiration from the disentangled representation learning literature, we propose an algorithm for learning compact representations of datasets that are useful for reconstruction and prediction, but are also <em>flexibly fair</em>, meaning they can be easily modified at test time to achieve subgroup demographic parity with respect to multiple sensitive attributes and their conjunctions. We show empirically that the resulting encoder—which does not require the sensitive attributes for inference—allows for the adaptation of a single representation to a variety of fair classification tasks with new target labels and subgroup definitions.}
}
"
ICML,2019,Analyzing and Improving Representations with the Soft Nearest Neighbor Loss,https://proceedings.mlr.press/v97/frosst19a.html,"['Frosst, Nicholas', 'Papernot, Nicolas', 'Hinton, Geoffrey']",86,"We explore and expand the Soft Nearest Neighbor Loss to measure the entanglement of class manifolds in representation space: i.e., how close pairs of points from the same class are relative to pairs of points from different classes. We demonstrate several use cases of the loss. As an analytical tool, it provides insights into the evolution of class similarity structures during learning. Surprisingly, we find that maximizing the entanglement of representations of different classes in the hidden layers is beneficial for discrimination in the final layer, possibly because it encourages representations to identify class-independent similarity structures. Maximizing the soft nearest neighbor loss in the hidden layers leads not only to better-calibrated estimates of uncertainty on outlier data but also marginally improved generalization. Data that is not from the training distribution can be recognized by observing that in the hidden layers, it has fewer than the normal number of neighbors from the predicted class.",Maybe,,Algorithm,,Mitigation,Mitigation,,,,,,,,,,,"
@InProceedings{pmlr-v97-frosst19a,
  title = 	 {Analyzing and Improving Representations with the Soft Nearest Neighbor Loss},
  author =       {Frosst, Nicholas and Papernot, Nicolas and Hinton, Geoffrey},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {2012--2020},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/frosst19a/frosst19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/frosst19a.html},
  abstract = 	 {We explore and expand the Soft Nearest Neighbor Loss to measure the entanglement of class manifolds in representation space: i.e., how close pairs of points from the same class are relative to pairs of points from different classes. We demonstrate several use cases of the loss. As an analytical tool, it provides insights into the evolution of class similarity structures during learning. Surprisingly, we find that maximizing the entanglement of representations of different classes in the hidden layers is beneficial for discrimination in the final layer, possibly because it encourages representations to identify class-independent similarity structures. Maximizing the soft nearest neighbor loss in the hidden layers leads not only to better-calibrated estimates of uncertainty on outlier data but also marginally improved generalization. Data that is not from the training distribution can be recognized by observing that in the hidden layers, it has fewer than the normal number of neighbors from the predicted class.}
}
"
ICML,2019,Diagnosing Bottlenecks in Deep Q learning Algorithms,https://proceedings.mlr.press/v97/fu19a.html,"['Fu, Justin', 'Kumar, Aviral', 'Soh, Matthew', 'Levine, Sergey']",103,"Q-learning methods are a common class of algorithms used in reinforcement learning (RL). However, their behavior with function approximation, especially with neural networks, is poorly understood theoretically and empirically. In this work, we aim to experimentally investigate potential issues in Q-learning, by means of a ""unit testing"" framework where we can utilize oracles to disentangle sources of error. Specifically, we investigate questions related to function approximation, sampling error and nonstationarity, and where available, verify if trends found in oracle settings hold true with deep RL methods. We find that large neural network architectures have many benefits with regards to learning stability; offer several practical compensations for overfitting; and develop a novel sampling method based on explicitly compensating for function approximation error that yields fair improvement on high-dimensional continuous control domains.",NOT,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v97-fu19a,
  title = 	 {Diagnosing Bottlenecks in Deep Q-learning Algorithms},
  author =       {Fu, Justin and Kumar, Aviral and Soh, Matthew and Levine, Sergey},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {2021--2030},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/fu19a/fu19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/fu19a.html},
  abstract = 	 {Q-learning methods are a common class of algorithms used in reinforcement learning (RL). However, their behavior with function approximation, especially with neural networks, is poorly understood theoretically and empirically. In this work, we aim to experimentally investigate potential issues in Q-learning, by means of a ""unit testing"" framework where we can utilize oracles to disentangle sources of error. Specifically, we investigate questions related to function approximation, sampling error and nonstationarity, and where available, verify if trends found in oracle settings hold true with deep RL methods. We find that large neural network architectures have many benefits with regards to learning stability; offer several practical compensations for overfitting; and develop a novel sampling method based on explicitly compensating for function approximation error that yields fair improvement on high-dimensional continuous control domains.}
}
"
ICML,2019,Obtaining Fairness using Optimal Transport Theory,https://proceedings.mlr.press/v97/gordaliza19a.html,"['Gordaliza, Paula', 'Barrio, Eustasio Del', 'Fabrice, Gamboa', 'Loubes, Jean-Michel']",123,"In the fair classification setup, we recast the links between fairness and predictability in terms of probability metrics. We analyze repair methods based on mapping conditional distributions to the Wasserstein barycenter. We propose a Random Repair which yields a tradeoff between minimal information loss and a certain amount of fairness.",TRAD,,Algorithm,Metrics,Mitigation,Mitigation,,,,,,,,,,,"
@InProceedings{pmlr-v97-gordaliza19a,
  title = 	 {Obtaining Fairness using Optimal Transport Theory},
  author =       {Gordaliza, Paula and Barrio, Eustasio Del and Fabrice, Gamboa and Loubes, Jean-Michel},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {2357--2365},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/gordaliza19a/gordaliza19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/gordaliza19a.html},
  abstract = 	 {In the fair classification setup, we recast the links between fairness and predictability in terms of probability metrics. We analyze repair methods based on mapping conditional distributions to the Wasserstein barycenter. We propose a Random Repair which yields a tradeoff between minimal information loss and a certain amount of fairness.}
}
"
ICML,2019,On the Long term Impact of Algorithmic Decision Policies: Effort Unfairness and Feature Segregation through Social Learning,https://proceedings.mlr.press/v97/heidari19a.html,"['Heidari, Hoda', 'Nanda, Vedant', 'Gummadi, Krishna']",47,"Most existing notions of algorithmic fairness are one-shot: they ensure some form of allocative equality at the time of decision making, but do not account for the adverse impact of the algorithmic decisions today on the long-term welfare and prosperity of certain segments of the population. We take a broader perspective on algorithmic fairness. We propose an effort-based measure of fairness and present a data-driven framework for characterizing the long-term impact of algorithmic policies on reshaping the underlying population. Motivated by the psychological literature on social learning and the economic literature on equality of opportunity, we propose a micro-scale model of how individuals may respond to decision-making algorithms. We employ existing measures of segregation from sociology and economics to quantify the resulting macro- scale population-level change. Importantly, we observe that different models may shift the group- conditional distribution of qualifications in different directions. Our findings raise a number of important questions regarding the formalization of fairness for decision-making models.",TRAD,,Metrics,,,,only include if you're including defining new metrics,,,,,,,,,,"
@InProceedings{pmlr-v97-heidari19a,
  title = 	 {On the Long-term Impact of Algorithmic Decision Policies: Effort Unfairness and Feature Segregation through Social Learning},
  author =       {Heidari, Hoda and Nanda, Vedant and Gummadi, Krishna},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {2692--2701},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/heidari19a/heidari19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/heidari19a.html},
  abstract = 	 {Most existing notions of algorithmic fairness are one-shot: they ensure some form of allocative equality at the time of decision making, but do not account for the adverse impact of the algorithmic decisions today on the long-term welfare and prosperity of certain segments of the population. We take a broader perspective on algorithmic fairness. We propose an effort-based measure of fairness and present a data-driven framework for characterizing the long-term impact of algorithmic policies on reshaping the underlying population. Motivated by the psychological literature on social learning and the economic literature on equality of opportunity, we propose a micro-scale model of how individuals may respond to decision-making algorithms. We employ existing measures of segregation from sociology and economics to quantify the resulting macro- scale population-level change. Importantly, we observe that different models may shift the group- conditional distribution of qualifications in different directions. Our findings raise a number of important questions regarding the formalization of fairness for decision-making models.}
}
"
ICML,2019,Stable and Fair Classification,https://proceedings.mlr.press/v97/huang19e.html,"['Huang, Lingxiao', 'Vishnoi, Nisheeth']",49,"In a recent study, Friedler et al. pointed out that several fair classification algorithms are not stable with respect to variations in the training set ‚Äì a crucial consideration in several applications. Motivated by their work, we study the problem of designing classification algorithms that are both fair and stable. We propose an extended framework based on fair classification algorithms that are formulated as optimization problems, by introducing a stability-focused regularization term. Theoretically, we prove an additional stability guarantee, that was lacking in fair classification algorithms, and also provide an accuracy guarantee for our extended framework. Our accuracy guarantee can be used to inform the selection of the regularization parameter in our framework. We assess the benefits of our approach empirically by extending several fair classification algorithms that are shown to achieve the best balance between fairness and accuracy over the \textbf{Adult} dataset. Our empirical results show that our extended framework indeed improves the stability at only a slight sacrifice in accuracy.",TRAD,,Algorithm,Metrics,Mitigation,Mitigation,makes stable fairness improving algos (in response to friedler),,,,,,,,,,"
@InProceedings{pmlr-v97-huang19e,
  title = 	 {Stable and Fair Classification},
  author =       {Huang, Lingxiao and Vishnoi, Nisheeth},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {2879--2890},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/huang19e/huang19e.pdf},
  url = 	 {https://proceedings.mlr.press/v97/huang19e.html},
  abstract = 	 {In a recent study, Friedler et al. pointed out that several fair classification algorithms are not stable with respect to variations in the training set – a crucial consideration in several applications. Motivated by their work, we study the problem of designing classification algorithms that are both fair and stable. We propose an extended framework based on fair classification algorithms that are formulated as optimization problems, by introducing a stability-focused regularization term. Theoretically, we prove an additional stability guarantee, that was lacking in fair classification algorithms, and also provide an accuracy guarantee for our extended framework. Our accuracy guarantee can be used to inform the selection of the regularization parameter in our framework. We assess the benefits of our approach empirically by extending several fair classification algorithms that are shown to achieve the best balance between fairness and accuracy over the \textbf{Adult} dataset. Our empirical results show that our extended framework indeed improves the stability at only a slight sacrifice in accuracy.}
}
"
ICML,2019,Differentially Private Fair Learning,https://proceedings.mlr.press/v97/jagielski19a.html,"['Jagielski, Matthew', 'Kearns, Michael', 'Mao, Jieming', 'Oprea, Alina', 'Roth, Aaron', '-Malvajerdi, Saeed Sharifi', 'Ullman, Jonathan']",104,"Motivated by settings in which predictive models may be required to be non-discriminatory with respect to certain attributes (such as race), but even collecting the sensitive attribute may be forbidden or restricted, we initiate the study of fair learning under the constraint of differential privacy. Our first algorithm is a private implementation of the equalized odds post-processing approach of (Hardt et al., 2016). This algorithm is appealingly simple, but must be able to use protected group membership explicitly at test time, which can be viewed as a form of ‚Äúdisparate treatment‚Äù. Our second algorithm is a differentially private version of the oracle-efficient in-processing approach of (Agarwal et al., 2018) which is more complex but need not have access to protected group membership at test time. We identify new tradeoffs between fairness, accuracy, and privacy that emerge only when requiring all three properties, and show that these tradeoffs can be milder if group membership may be used at test time. We conclude with a brief experimental evaluation.",TRAD,,Algorithm,,Mitigation,Mitigation,fair AND differentially private,,,,,,,,,,"
@InProceedings{pmlr-v97-jagielski19a,
  title = 	 {Differentially Private Fair Learning},
  author =       {Jagielski, Matthew and Kearns, Michael and Mao, Jieming and Oprea, Alina and Roth, Aaron and -Malvajerdi, Saeed Sharifi and Ullman, Jonathan},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {3000--3008},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/jagielski19a/jagielski19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/jagielski19a.html},
  abstract = 	 {Motivated by settings in which predictive models may be required to be non-discriminatory with respect to certain attributes (such as race), but even collecting the sensitive attribute may be forbidden or restricted, we initiate the study of fair learning under the constraint of differential privacy. Our first algorithm is a private implementation of the equalized odds post-processing approach of (Hardt et al., 2016). This algorithm is appealingly simple, but must be able to use protected group membership explicitly at test time, which can be viewed as a form of “disparate treatment”. Our second algorithm is a differentially private version of the oracle-efficient in-processing approach of (Agarwal et al., 2018) which is more complex but need not have access to protected group membership at test time. We identify new tradeoffs between fairness, accuracy, and privacy that emerge only when requiring all three properties, and show that these tradeoffs can be milder if group membership may be used at test time. We conclude with a brief experimental evaluation.}
}
"
ICML,2019,A Deep Reinforcement Learning Perspective on Internet Congestion Control,https://proceedings.mlr.press/v97/jay19a.html,"['Jay, Nathan', 'Rotman, Noga', 'Godfrey, Brighten', 'Schapira, Michael', 'Tamar, Aviv']",211,"We present and investigate a novel and timely application domain for deep reinforcement learning (RL): Internet congestion control. Congestion control is the core networking task of modulating traffic sources‚Äô data-transmission rates to efficiently utilize network capacity, and is the subject of extensive attention in light of the advent of Internet services such as live video, virtual reality, Internet-of-Things, and more. We show that casting congestion control as RL enables training deep network policies that capture intricate patterns in data traffic and network conditions, and leverage this to outperform the state-of-the-art. We also highlight significant challenges facing real-world adoption of RL-based congestion control, including fairness, safety, and generalization, which are not trivial to address within conventional RL formalism. To facilitate further research and reproducibility of our results, we present a test suite for RL-guided congestion control based on the OpenAI Gym interface.",NOT,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v97-jay19a,
  title = 	 {A Deep Reinforcement Learning Perspective on Internet Congestion Control},
  author =       {Jay, Nathan and Rotman, Noga and Godfrey, Brighten and Schapira, Michael and Tamar, Aviv},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {3050--3059},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/jay19a/jay19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/jay19a.html},
  abstract = 	 {We present and investigate a novel and timely application domain for deep reinforcement learning (RL): Internet congestion control. Congestion control is the core networking task of modulating traffic sources’ data-transmission rates to efficiently utilize network capacity, and is the subject of extensive attention in light of the advent of Internet services such as live video, virtual reality, Internet-of-Things, and more. We show that casting congestion control as RL enables training deep network policies that capture intricate patterns in data traffic and network conditions, and leverage this to outperform the state-of-the-art. We also highlight significant challenges facing real-world adoption of RL-based congestion control, including fairness, safety, and generalization, which are not trivial to address within conventional RL formalism. To facilitate further research and reproducibility of our results, we present a test suite for RL-guided congestion control based on the OpenAI Gym interface.}
}
"
ICML,2019,Fair k Center Clustering for Data Summarization,https://proceedings.mlr.press/v97/kleindessner19a.html,"['Kleindessner, Matth{\\""a}us', 'Awasthi, Pranjal', 'Morgenstern, Jamie']",124,"In data summarization we want to choose $k$ prototypes in order to summarize a data set. We study a setting where the data set comprises several demographic groups and we are restricted to choose $k_i$ prototypes belonging to group $i$. A common approach to the problem without the fairness constraint is to optimize a centroid-based clustering objective such as $k$-center. A natural extension then is to incorporate the fairness constraint into the clustering problem. Existing algorithms for doing so run in time super-quadratic in the size of the data set, which is in contrast to the standard $k$-center problem being approximable in linear time. In this paper, we resolve this gap by providing a simple approximation algorithm for the $k$-center problem under the fairness constraint with running time linear in the size of the data set and $k$. If the number of demographic groups is small, the approximation guarantee of our algorithm only incurs a constant-factor overhead.",SCOPE,,Algorithm,,Mitigation,Mitigation,fair data summarization,,,,,,,,,,"
@InProceedings{pmlr-v97-kleindessner19a,
  title = 	 {Fair k-Center Clustering for Data Summarization},
  author =       {Kleindessner, Matth{\""a}us and Awasthi, Pranjal and Morgenstern, Jamie},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {3448--3457},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/kleindessner19a/kleindessner19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/kleindessner19a.html},
  abstract = 	 {In data summarization we want to choose $k$ prototypes in order to summarize a data set. We study a setting where the data set comprises several demographic groups and we are restricted to choose $k_i$ prototypes belonging to group $i$. A common approach to the problem without the fairness constraint is to optimize a centroid-based clustering objective such as $k$-center. A natural extension then is to incorporate the fairness constraint into the clustering problem. Existing algorithms for doing so run in time super-quadratic in the size of the data set, which is in contrast to the standard $k$-center problem being approximable in linear time. In this paper, we resolve this gap by providing a simple approximation algorithm for the $k$-center problem under the fairness constraint with running time linear in the size of the data set and $k$. If the number of demographic groups is small, the approximation guarantee of our algorithm only incurs a constant-factor overhead.}
}
"
ICML,2019,Guarantees for Spectral Clustering with Fairness Constraints,https://proceedings.mlr.press/v97/kleindessner19b.html,"['Kleindessner, Matth{\\""a}us', 'Samadi, Samira', 'Awasthi, Pranjal', 'Morgenstern, Jamie']",109,"Given the widespread popularity of spectral clustering (SC) for partitioning graph data, we study a version of constrained SC in which we try to incorporate the fairness notion proposed by Chierichetti et al. (2017). According to this notion, a clustering is fair if every demographic group is approximately proportionally represented in each cluster. To this end, we develop variants of both normalized and unnormalized constrained SC and show that they help find fairer clusterings on both synthetic and real data. We also provide a rigorous theoretical analysis of our algorithms on a natural variant of the stochastic block model, where $h$ groups have strong inter-group connectivity, but also exhibit a ‚Äúnatural‚Äù clustering structure which is fair. We prove that our algorithms can recover this fair clustering with high probability.",SCOPE,,Algorithm,,Mitigation,Mitigation,fair data summarization,,,,,,,,,,"
@InProceedings{pmlr-v97-kleindessner19b,
  title = 	 {Guarantees for Spectral Clustering with Fairness Constraints},
  author =       {Kleindessner, Matth{\""a}us and Samadi, Samira and Awasthi, Pranjal and Morgenstern, Jamie},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {3458--3467},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/kleindessner19b/kleindessner19b.pdf},
  url = 	 {https://proceedings.mlr.press/v97/kleindessner19b.html},
  abstract = 	 {Given the widespread popularity of spectral clustering (SC) for partitioning graph data, we study a version of constrained SC in which we try to incorporate the fairness notion proposed by Chierichetti et al. (2017). According to this notion, a clustering is fair if every demographic group is approximately proportionally represented in each cluster. To this end, we develop variants of both normalized and unnormalized constrained SC and show that they help find fairer clusterings on both synthetic and real data. We also provide a rigorous theoretical analysis of our algorithms on a natural variant of the stochastic block model, where $h$ groups have strong inter-group connectivity, but also exhibit a “natural” clustering structure which is fair. We prove that our algorithms can recover this fair clustering with high probability.}
}
"
ICML,2019,Making Decisions that Reduce Discriminatory Impacts,https://proceedings.mlr.press/v97/kusner19a.html,"['Kusner, Matt', 'Russell, Chris', 'Loftus, Joshua', 'Silva, Ricardo']",23,"As machine learning algorithms move into real-world settings, it is crucial to ensure they are aligned with societal values. There has been much work on one aspect of this, namely the discriminatory prediction problem: How can we reduce discrimination in the predictions themselves? While an important question, solutions to this problem only apply in a restricted setting, as we have full control over the predictions. Often we care about the non-discrimination of quantities we do not have full control over. Thus, we describe another key aspect of this challenge, the discriminatory impact problem: How can we reduce discrimination arising from the real-world impact of decisions? To address this, we describe causal methods that model the relevant parts of the real-world system in which the decisions are made. Unlike previous approaches, these models not only allow us to map the causal pathway of a single decision, but also to model the effect of interference‚Äìhow the impact on an individual depends on decisions made about other people. Often, the goal of decision policies is to maximize a beneficial impact overall. To reduce the discrimination of these benefits, we devise a constraint inspired by recent work in counterfactual fairness, and give an efficient procedure to solve the constrained optimization problem. We demonstrate our approach with an example: how to increase students taking college entrance exams in New York City public schools.",SCOPE,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v97-kusner19a,
  title = 	 {Making Decisions that Reduce Discriminatory Impacts},
  author =       {Kusner, Matt and Russell, Chris and Loftus, Joshua and Silva, Ricardo},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {3591--3600},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/kusner19a/kusner19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/kusner19a.html},
  abstract = 	 {As machine learning algorithms move into real-world settings, it is crucial to ensure they are aligned with societal values. There has been much work on one aspect of this, namely the discriminatory prediction problem: How can we reduce discrimination in the predictions themselves? While an important question, solutions to this problem only apply in a restricted setting, as we have full control over the predictions. Often we care about the non-discrimination of quantities we do not have full control over. Thus, we describe another key aspect of this challenge, the discriminatory impact problem: How can we reduce discrimination arising from the real-world impact of decisions? To address this, we describe causal methods that model the relevant parts of the real-world system in which the decisions are made. Unlike previous approaches, these models not only allow us to map the causal pathway of a single decision, but also to model the effect of interference–how the impact on an individual depends on decisions made about other people. Often, the goal of decision policies is to maximize a beneficial impact overall. To reduce the discrimination of these benefits, we devise a constraint inspired by recent work in counterfactual fairness, and give an efficient procedure to solve the constrained optimization problem. We demonstrate our approach with an example: how to increase students taking college entrance exams in New York City public schools.}
}
"
ICML,2019,Self Attention Graph Pooling,https://proceedings.mlr.press/v97/lee19c.html,"['Lee, Junhyun', 'Lee, Inyeop', 'Kang, Jaewoo']",695,"Advanced methods of applying deep learning to structured data such as graphs have been proposed in recent years. In particular, studies have focused on generalizing convolutional neural networks to graph data, which includes redefining the convolution and the downsampling (pooling) operations for graphs. The method of generalizing the convolution operation to graphs has been proven to improve performance and is widely used. However, the method of applying downsampling to graphs is still difficult to perform and has room for improvement. In this paper, we propose a graph pooling method based on self-attention. Self-attention using graph convolution allows our pooling method to consider both node features and graph topology. To ensure a fair comparison, the same training procedures and model architectures were used for the existing pooling methods and our method. The experimental results demonstrate that our method achieves superior graph classification performance on the benchmark datasets using a reasonable number of parameters.",NOT,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v97-lee19c,
  title = 	 {Self-Attention Graph Pooling},
  author =       {Lee, Junhyun and Lee, Inyeop and Kang, Jaewoo},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {3734--3743},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/lee19c/lee19c.pdf},
  url = 	 {https://proceedings.mlr.press/v97/lee19c.html},
  abstract = 	 {Advanced methods of applying deep learning to structured data such as graphs have been proposed in recent years. In particular, studies have focused on generalizing convolutional neural networks to graph data, which includes redefining the convolution and the downsampling (pooling) operations for graphs. The method of generalizing the convolution operation to graphs has been proven to improve performance and is widely used. However, the method of applying downsampling to graphs is still difficult to perform and has room for improvement. In this paper, we propose a graph pooling method based on self-attention. Self-attention using graph convolution allows our pooling method to consider both node features and graph topology. To ensure a fair comparison, the same training procedures and model architectures were used for the existing pooling methods and our method. The experimental results demonstrate that our method achieves superior graph classification performance on the benchmark datasets using a reasonable number of parameters.}
}
"
ICML,2019,The Implicit Fairness Criterion of Unconstrained Learning,https://proceedings.mlr.press/v97/liu19f.html,"['Liu, Lydia T.', 'Simchowitz, Max', 'Hardt, Moritz']",56,"We clarify what fairness guarantees we can and cannot expect to follow from unconstrained machine learning. Specifically, we show that in many settings, unconstrained learning on its own implies group calibration, that is, the outcome variable is conditionally independent of group membership given the score. A lower bound confirms the optimality of our upper bound. Moreover, we prove that as the excess risk of the learned score decreases, the more strongly it violates separation and independence, two other standard fairness criteria. Our results challenge the view that group calibration necessitates an active intervention, suggesting that often we ought to think of it as a byproduct of unconstrained machine learning.",NOT,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v97-liu19f,
  title = 	 {The Implicit Fairness Criterion of Unconstrained Learning},
  author =       {Liu, Lydia T. and Simchowitz, Max and Hardt, Moritz},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {4051--4060},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/liu19f/liu19f.pdf},
  url = 	 {https://proceedings.mlr.press/v97/liu19f.html},
  abstract = 	 {We clarify what fairness guarantees we can and cannot expect to follow from unconstrained machine learning. Specifically, we show that in many settings, unconstrained learning on its own implies group calibration, that is, the outcome variable is conditionally independent of group membership given the score. A lower bound confirms the optimality of our upper bound. Moreover, we prove that as the excess risk of the learned score decreases, the more strongly it violates separation and independence, two other standard fairness criteria. Our results challenge the view that group calibration necessitates an active intervention, suggesting that often we ought to think of it as a byproduct of unconstrained machine learning.}
}
"
ICML,2019,Composable Core sets for Determinant Maximization: A Simple Near Optimal Algorithm,https://proceedings.mlr.press/v97/mahabadi19a.html,"['Mahabadi, Sepideh', 'Indyk, Piotr', 'Gharan, Shayan Oveis', 'Rezaei, Alireza']",12,"‚ÄúComposable core-sets‚Äù are an efficient framework for solving optimization problems in massive data models. In this work, we consider efficient construction of composable core-sets for the determinant maximization problem. This can also be cast as the MAP inference task for ‚Äúdeterminantal point processes"", that have recently gained a lot of interest for modeling diversity and fairness. The problem was recently studied in \cite{indyk2018composable}, where they designed composable core-sets with the optimal approximation bound of $O(k)^k$. On the other hand, the more practical ‚ÄúGreedy"" algorithm has been previously used in similar contexts. In this work, first we provide a theoretical approximation guarantee of $C^{k^2}$ for the Greedy algorithm in the context of composable core-sets; Further, we propose to use a ‚ÄúLocal Search"" based algorithm that while being still practical, achieves a nearly optimal approximation bound of $O(k)^{2k}$; Finally, we implement all three algorithms and show the effectiveness of our proposed algorithm on standard data sets.",NOT,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v97-mahabadi19a,
  title = 	 {Composable Core-sets for Determinant Maximization: A Simple Near-Optimal Algorithm},
  author =       {Mahabadi, Sepideh and Indyk, Piotr and Gharan, Shayan Oveis and Rezaei, Alireza},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {4254--4263},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/mahabadi19a/mahabadi19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/mahabadi19a.html},
  abstract = 	 {“Composable core-sets” are an efficient framework for solving optimization problems in massive data models. In this work, we consider efficient construction of composable core-sets for the determinant maximization problem. This can also be cast as the MAP inference task for “determinantal point processes"", that have recently gained a lot of interest for modeling diversity and fairness. The problem was recently studied in \cite{indyk2018composable}, where they designed composable core-sets with the optimal approximation bound of $O(k)^k$. On the other hand, the more practical “Greedy"" algorithm has been previously used in similar contexts. In this work, first we provide a theoretical approximation guarantee of $C^{k^2}$ for the Greedy algorithm in the context of composable core-sets; Further, we propose to use a “Local Search"" based algorithm that while being still practical, achieves a nearly optimal approximation bound of $O(k)^{2k}$; Finally, we implement all three algorithms and show the effectiveness of our proposed algorithm on standard data sets.}
}
"
ICML,2019,Fairness Aware Learning for Continuous Attributes and Treatments,https://proceedings.mlr.press/v97/mary19a.html,"['Mary, Jeremie', ""Calauz{\\`e}nes, Cl{\\'e}ment"", 'Karoui, Noureddine El']",73,"We address the problem of algorithmic fairness: ensuring that the outcome of a classifier is not biased towards certain values of sensitive variables such as age, race or gender. As common fairness metrics can be expressed as measures of (conditional) independence between variables, we propose to use the R√©nyi maximum correlation coefficient to generalize fairness measurement to continuous variables. We exploit Witsenhausen‚Äôs characterization of the R√©nyi correlation coefficient to propose a differentiable implementation linked to $f$-divergences. This allows us to generalize fairness-aware learning to continuous variables by using a penalty that upper bounds this coefficient. Theses allows fairness to be extented to variables such as mixed ethnic groups or financial status without thresholds effects. This penalty can be estimated on mini-batches allowing to use deep nets. Experiments show favorable comparisons to state of the art on binary variables and prove the ability to protect continuous ones",TRAD,,Algorithm,Metrics,Mitigation,,,,,,,,,,,,"
@InProceedings{pmlr-v97-mary19a,
  title = 	 {Fairness-Aware Learning for Continuous Attributes and Treatments},
  author =       {Mary, Jeremie and Calauz{\`e}nes, Cl{\'e}ment and Karoui, Noureddine El},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {4382--4391},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/mary19a/mary19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/mary19a.html},
  abstract = 	 {We address the problem of algorithmic fairness: ensuring that the outcome of a classifier is not biased towards certain values of sensitive variables such as age, race or gender. As common fairness metrics can be expressed as measures of (conditional) independence between variables, we propose to use the Rényi maximum correlation coefficient to generalize fairness measurement to continuous variables. We exploit Witsenhausen’s characterization of the Rényi correlation coefficient to propose a differentiable implementation linked to $f$-divergences. This allows us to generalize fairness-aware learning to continuous variables by using a penalty that upper bounds this coefficient. Theses allows fairness to be extented to variables such as mixed ethnic groups or financial status without thresholds effects. This penalty can be estimated on mini-batches allowing to use deep nets. Experiments show favorable comparisons to state of the art on binary variables and prove the ability to protect continuous ones}
}
"
ICML,2019,Toward Controlling Discrimination in Online Ad Auctions,https://proceedings.mlr.press/v97/mehrotra19a.html,"['Celis, Elisa', 'Mehrotra, Anay', 'Vishnoi, Nisheeth']",40,"Online advertising platforms are thriving due to the customizable audiences they offer advertisers. However, recent studies show that advertisements can be discriminatory with respect to the gender or race of the audience that sees the ad, and may inadvertently cross ethical and/or legal boundaries. To prevent this, we propose a constrained ad auction framework that maximizes the platform‚Äôs revenue conditioned on ensuring that the audience seeing an advertiser‚Äôs ad is distributed appropriately across sensitive types such as gender or race. Building upon Myerson‚Äôs classic work, we first present an optimal auction mechanism for a large class of fairness constraints. Finding the parameters of this optimal auction, however, turns out to be a non-convex problem. We show that this non-convex problem can be reformulated as a more structured non-convex problem with no saddle points or local-maxima; this allows us to develop a gradient-descent-based algorithm to solve it. Our empirical results on the A1 Yahoo! dataset demonstrate that our algorithm can obtain uniform coverage across different user types for each advertiser at a minor loss to the revenue of the platform, and a small change to the size of the audience each advertiser reaches.",SCOPE/Maybe,,Algorithm,,Mitigation,,How to make a fair auction,,,,,,,,,,"
@InProceedings{pmlr-v97-mehrotra19a,
  title = 	 {Toward Controlling Discrimination in Online Ad Auctions},
  author =       {Celis, Elisa and Mehrotra, Anay and Vishnoi, Nisheeth},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {4456--4465},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/mehrotra19a/mehrotra19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/mehrotra19a.html},
  abstract = 	 {Online advertising platforms are thriving due to the customizable audiences they offer advertisers. However, recent studies show that advertisements can be discriminatory with respect to the gender or race of the audience that sees the ad, and may inadvertently cross ethical and/or legal boundaries. To prevent this, we propose a constrained ad auction framework that maximizes the platform’s revenue conditioned on ensuring that the audience seeing an advertiser’s ad is distributed appropriately across sensitive types such as gender or race. Building upon Myerson’s classic work, we first present an optimal auction mechanism for a large class of fairness constraints. Finding the parameters of this optimal auction, however, turns out to be a non-convex problem. We show that this non-convex problem can be reformulated as a more structured non-convex problem with no saddle points or local-maxima; this allows us to develop a gradient-descent-based algorithm to solve it. Our empirical results on the A1 Yahoo! dataset demonstrate that our algorithm can obtain uniform coverage across different user types for each advertiser at a minor loss to the revenue of the platform, and a small change to the size of the audience each advertiser reaches.}
}
"
ICML,2019,Agnostic Federated Learning,https://proceedings.mlr.press/v97/mohri19a.html,"['Mohri, Mehryar', 'Sivek, Gary', 'Suresh, Ananda Theertha']",556,"A key learning scenario in large-scale applications is that of federated learning, where a centralized model is trained based on data originating from a large number of clients. We argue that, with the existing training and inference, federated models can be biased towards different clients. Instead, we propose a new framework of agnostic federated learning, where the centralized model is optimized for any target distribution formed by a mixture of the client distributions. We further show that this framework naturally yields a notion of fairness. We present data-dependent Rademacher complexity guarantees for learning with this objective, which guide the definition of an algorithm for agnostic federated learning. We also give a fast stochastic optimization algorithm for solving the corresponding optimization problem, for which we prove convergence bounds, assuming a convex loss function and a convex hypothesis set. We further empirically demonstrate the benefits of our approach in several datasets. Beyond federated learning, our framework and algorithm can be of interest to other learning scenarios such as cloud computing, domain adaptation, drifting, and other contexts where the training and test distributions do not coincide.",NOT,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v97-mohri19a,
  title = 	 {Agnostic Federated Learning},
  author =       {Mohri, Mehryar and Sivek, Gary and Suresh, Ananda Theertha},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {4615--4625},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/mohri19a/mohri19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/mohri19a.html},
  abstract = 	 {A key learning scenario in large-scale applications is that of federated learning, where a centralized model is trained based on data originating from a large number of clients. We argue that, with the existing training and inference, federated models can be biased towards different clients. Instead, we propose a new framework of agnostic federated learning, where the centralized model is optimized for any target distribution formed by a mixture of the client distributions. We further show that this framework naturally yields a notion of fairness. We present data-dependent Rademacher complexity guarantees for learning with this objective, which guide the definition of an algorithm for agnostic federated learning. We also give a fast stochastic optimization algorithm for solving the corresponding optimization problem, for which we prove convergence bounds, assuming a convex loss function and a convex hypothesis set. We further empirically demonstrate the benefits of our approach in several datasets. Beyond federated learning, our framework and algorithm can be of interest to other learning scenarios such as cloud computing, domain adaptation, drifting, and other contexts where the training and test distributions do not coincide.}
}
"
ICML,2019,Learning Optimal Fair Policies,https://proceedings.mlr.press/v97/nabi19a.html,"['Nabi, Razieh', 'Malinsky, Daniel', 'Shpitser, Ilya']",64,"Systematic discriminatory biases present in our society influence the way data is collected and stored, the way variables are defined, and the way scientific findings are put into practice as policy. Automated decision procedures and learning algorithms applied to such data may serve to perpetuate existing injustice or unfairness in our society. In this paper, we consider how to make optimal but fair decisions, which ‚Äúbreak the cycle of injustice‚Äù by correcting for the unfair dependence of both decisions and outcomes on sensitive features (e.g., variables that correspond to gender, race, disability, or other protected attributes). We use methods from causal inference and constrained optimization to learn optimal policies in a way that addresses multiple potential biases which afflict data analysis in sensitive contexts, extending the approach of Nabi & Shpitser (2018). Our proposal comes equipped with the theoretical guarantee that the chosen fair policy will induce a joint distribution for new instances that satisfies given fairness constraints. We illustrate our approach with both synthetic data and real criminal justice data.",YES/Maybe,,Algorithm,,Mitigation,,NEEDS A SECOND LOOK,,,,,,,,,,"
@InProceedings{pmlr-v97-nabi19a,
  title =          {Learning Optimal Fair Policies},
  author =       {Nabi, Razieh and Malinsky, Daniel and Shpitser, Ilya},
  booktitle =          {Proceedings of the 36th International Conference on Machine Learning},
  pages =          {4674--4682},
  year =          {2019},
  editor =          {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume =          {97},
  series =          {Proceedings of Machine Learning Research},
  month =          {09--15 Jun},
  publisher =    {PMLR},
  pdf =          {http://proceedings.mlr.press/v97/nabi19a/nabi19a.pdf},
  url =          {https://proceedings.mlr.press/v97/nabi19a.html},
  abstract =          {Systematic discriminatory biases present in our society influence the way data is collected and stored, the way variables are defined, and the way scientific findings are put into practice as policy. Automated decision procedures and learning algorithms applied to such data may serve to perpetuate existing injustice or unfairness in our society. In this paper, we consider how to make optimal but fair decisions, which “break the cycle of injustice” by correcting for the unfair dependence of both decisions and outcomes on sensitive features (e.g., variables that correspond to gender, race, disability, or other protected attributes). We use methods from causal inference and constrained optimization to learn optimal policies in a way that addresses multiple potential biases which afflict data analysis in sensitive contexts, extending the approach of Nabi &amp; Shpitser (2018). Our proposal comes equipped with the theoretical guarantee that the chosen fair policy will induce a joint distribution for new instances that satisfies given fairness constraints. We illustrate our approach with both synthetic data and real criminal justice data.}
}
"
ICML,2019,Mixture Models for Diverse Machine Translation: Tricks of the Trade,https://proceedings.mlr.press/v97/shen19c.html,"['Shen, Tianxiao', 'Ott, Myle', 'Auli, Michael', ""Ranzato, Marc'Aurelio""]",90,"Mixture models trained via EM are among the simplest, most widely used and well understood latent variable models in the machine learning literature. Surprisingly, these models have been hardly explored in text generation applications such as machine translation. In principle, they provide a latent variable to control generation and produce a diverse set of hypotheses. In practice, however, mixture models are prone to degeneracies‚Äîoften only one component gets trained or the latent variable is simply ignored. We find that disabling dropout noise in responsibility computation is critical to successful training. In addition, the design choices of parameterization, prior distribution, hard versus soft EM and online versus offline assignment can dramatically affect model performance. We develop an evaluation protocol to assess both quality and diversity of generations against multiple references, and provide an extensive empirical study of several mixture model variants. Our analysis shows that certain types of mixture models are more robust and offer the best trade-off between translation quality and diversity compared to variational models and diverse decoding approaches.\footnote{Code to reproduce the results in this paper is available at \url{https://github.com/pytorch/fairseq}}",NOT,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v97-shen19c,
  title = 	 {Mixture Models for Diverse Machine Translation: Tricks of the Trade},
  author =       {Shen, Tianxiao and Ott, Myle and Auli, Michael and Ranzato, Marc'Aurelio},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {5719--5728},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/shen19c/shen19c.pdf},
  url = 	 {https://proceedings.mlr.press/v97/shen19c.html},
  abstract = 	 {Mixture models trained via EM are among the simplest, most widely used and well understood latent variable models in the machine learning literature. Surprisingly, these models have been hardly explored in text generation applications such as machine translation. In principle, they provide a latent variable to control generation and produce a diverse set of hypotheses. In practice, however, mixture models are prone to degeneracies—often only one component gets trained or the latent variable is simply ignored. We find that disabling dropout noise in responsibility computation is critical to successful training. In addition, the design choices of parameterization, prior distribution, hard versus soft EM and online versus offline assignment can dramatically affect model performance. We develop an evaluation protocol to assess both quality and diversity of generations against multiple references, and provide an extensive empirical study of several mixture model variants. Our analysis shows that certain types of mixture models are more robust and offer the best trade-off between translation quality and diversity compared to variational models and diverse decoding approaches.\footnote{Code to reproduce the results in this paper is available at \url{https://github.com/pytorch/fairseq}}}
}
"
ICML,2019,Fairness without Harm: Decoupled Classifiers with Preference Guarantees,https://proceedings.mlr.press/v97/ustun19a.html,"['Ustun, Berk', 'Liu, Yang', 'Parkes, David']",83,"In domains such as medicine, it can be acceptable for machine learning models to include sensitive attributes such as gender and ethnicity. In this work, we argue that when there is this kind of treatment disparity, then it should be in the best interest of each group. Drawing on ethical principles such as beneficence (""do the best"") and non-maleficence (""do no harm""), we show how to use sensitive attributes to train decoupled classifiers that satisfy preference guarantees. These guarantees ensure the majority of individuals in each group prefer their assigned classifier to (i) a pooled model that ignores group membership (rationality), and (ii) the model assigned to any other group (envy-freeness). We introduce a recursive procedure that adaptively selects group attributes for decoupling, and present formal conditions to ensure preference guarantees in terms of generalization error. We validate the effectiveness of the procedure on real-world datasets, showing that it improves accuracy without violating preference guarantees on test data.",TRAD,,Algorithm,Metrics,,,trains envy free decoupled classifiers,,,,,,,,,,"
@InProceedings{pmlr-v97-ustun19a,
  title = 	 {Fairness without Harm: Decoupled Classifiers with Preference Guarantees},
  author =       {Ustun, Berk and Liu, Yang and Parkes, David},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {6373--6382},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/ustun19a/ustun19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/ustun19a.html},
  abstract = 	 {In domains such as medicine, it can be acceptable for machine learning models to include <em>sensitive attributes</em> such as gender and ethnicity. In this work, we argue that when there is this kind of treatment disparity, then it should be in the best interest of each group. Drawing on ethical principles such as beneficence (""do the best"") and non-maleficence (""do no harm""), we show how to use sensitive attributes to train decoupled classifiers that satisfy preference guarantees. These guarantees ensure the majority of individuals in each group prefer their assigned classifier to (i) a pooled model that ignores group membership (rationality), and (ii) the model assigned to any other group (envy-freeness). We introduce a recursive procedure that adaptively selects group attributes for decoupling, and present formal conditions to ensure preference guarantees in terms of generalization error. We validate the effectiveness of the procedure on real-world datasets, showing that it improves accuracy without violating preference guarantees on test data.}
}
"
ICML,2019,Repairing without Retraining: Avoiding Disparate Impact with Counterfactual Distributions,https://proceedings.mlr.press/v97/wang19l.html,"['Wang, Hao', 'Ustun, Berk', 'Calmon, Flavio']",58,"When the performance of a machine learning model varies over groups defined by sensitive attributes (e.g., gender or ethnicity), the performance disparity can be expressed in terms of the probability distributions of the input and output variables over each group. In this paper, we exploit this fact to reduce the disparate impact of a fixed classification model over a population of interest. Given a black-box classifier, we aim to eliminate the performance gap by perturbing the distribution of input variables for the disadvantaged group. We refer to the perturbed distribution as a counterfactual distribution, and characterize its properties for common fairness criteria. We introduce a descent algorithm to learn a counterfactual distribution from data. We then discuss how the estimated distribution can be used to build a data preprocessor that can reduce disparate impact without training a new model. We validate our approach through experiments on real-world datasets, showing that it can repair different forms of disparity without a significant drop in accuracy.",YES/Maybe,,Data,Algorithm,,,"NEEDS A SECOND LOOK--- is it pipeline or no? its putting something on top of system, not really looking inside?",,,,,,,,,,"
@InProceedings{pmlr-v97-wang19l,
  title =          {Repairing without Retraining: Avoiding Disparate Impact with Counterfactual Distributions},
  author =       {Wang, Hao and Ustun, Berk and Calmon, Flavio},
  booktitle =          {Proceedings of the 36th International Conference on Machine Learning},
  pages =          {6618--6627},
  year =          {2019},
  editor =          {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume =          {97},
  series =          {Proceedings of Machine Learning Research},
  month =          {09--15 Jun},
  publisher =    {PMLR},
  pdf =          {http://proceedings.mlr.press/v97/wang19l/wang19l.pdf},
  url =          {https://proceedings.mlr.press/v97/wang19l.html},
  abstract =          {When the performance of a machine learning model varies over groups defined by sensitive attributes (e.g., gender or ethnicity), the performance disparity can be expressed in terms of the probability distributions of the input and output variables over each group. In this paper, we exploit this fact to reduce the disparate impact of a fixed classification model over a population of interest. Given a black-box classifier, we aim to eliminate the performance gap by perturbing the distribution of input variables for the disadvantaged group. We refer to the perturbed distribution as a counterfactual distribution, and characterize its properties for common fairness criteria. We introduce a descent algorithm to learn a counterfactual distribution from data. We then discuss how the estimated distribution can be used to build a data preprocessor that can reduce disparate impact without training a new model. We validate our approach through experiments on real-world datasets, showing that it can repair different forms of disparity without a significant drop in accuracy.}
}
"
ICML,2019,Fairness risk measures,https://proceedings.mlr.press/v97/williamson19a.html,"['Williamson, Robert', 'Menon, Aditya']",91,"Ensuring that classifiers are non-discriminatory or fair with respect to a sensitive feature (e.g., race or gender) is a topical problem. Progress in this task requires fixing a definition of fairness, and there have been several proposals in this regard over the past few years. Several of these, however, assume either binary sensitive features (thus precluding categorical or real-valued sensitive groups), or result in non-convex objectives (thus adversely affecting the optimisation landscape). In this paper, we propose a new definition of fairness that generalises some existing proposals, while allowing for generic sensitive features and resulting in a convex objective. The key idea is to enforce that the expected losses (or risks) across each subgroup induced by the sensitive feature are commensurate. We show how this relates to the rich literature on risk measures from mathematical finance. As a special case, this leads to a new convex fairness-aware objective based on minimising the conditional value at risk (CVaR).",TRAD,,Metrics,,,,only include if including metrics,,,,,,,,,,"
@InProceedings{pmlr-v97-williamson19a,
  title = 	 {Fairness risk measures},
  author =       {Williamson, Robert and Menon, Aditya},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {6786--6797},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/williamson19a/williamson19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/williamson19a.html},
  abstract = 	 {Ensuring that classifiers are non-discriminatory or fair with respect to a sensitive feature (e.g., race or gender) is a topical problem. Progress in this task requires fixing a definition of fairness, and there have been several proposals in this regard over the past few years. Several of these, however, assume either binary sensitive features (thus precluding categorical or real-valued sensitive groups), or result in non-convex objectives (thus adversely affecting the optimisation landscape). In this paper, we propose a new definition of fairness that generalises some existing proposals, while allowing for generic sensitive features and resulting in a convex objective. The key idea is to enforce that the expected losses (or risks) across each subgroup induced by the sensitive feature are commensurate. We show how this relates to the rich literature on risk measures from mathematical finance. As a special case, this leads to a new convex fairness-aware objective based on minimising the conditional value at risk (CVaR).}
}
"
ICML,2019,Bridging Theory and Algorithm for Domain Adaptation,https://proceedings.mlr.press/v97/zhang19i.html,"['Zhang, Yuchen', 'Liu, Tianle', 'Long, Mingsheng', 'Jordan, Michael']",421,"This paper addresses the problem of unsupervised domain adaption from theoretical and algorithmic perspectives. Existing domain adaptation theories naturally imply minimax optimization algorithms, which connect well with the domain adaptation methods based on adversarial learning. However, several disconnections still exist and form the gap between theory and algorithm. We extend previous theories (Mansour et al., 2009c; Ben-David et al., 2010) to multiclass classification in domain adaptation, where classifiers based on the scoring functions and margin loss are standard choices in algorithm design. We introduce Margin Disparity Discrepancy, a novel measurement with rigorous generalization bounds, tailored to the distribution comparison with the asymmetric margin loss, and to the minimax optimization for easier training. Our theory can be seamlessly transformed into an adversarial learning algorithm for domain adaptation, successfully bridging the gap between theory and algorithm. A series of empirical studies show that our algorithm achieves the state of the art accuracies on challenging domain adaptation tasks.",NOT,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v97-zhang19i,
  title = 	 {Bridging Theory and Algorithm for Domain Adaptation},
  author =       {Zhang, Yuchen and Liu, Tianle and Long, Mingsheng and Jordan, Michael},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {7404--7413},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/zhang19i/zhang19i.pdf},
  url = 	 {https://proceedings.mlr.press/v97/zhang19i.html},
  abstract = 	 {This paper addresses the problem of unsupervised domain adaption from theoretical and algorithmic perspectives. Existing domain adaptation theories naturally imply minimax optimization algorithms, which connect well with the domain adaptation methods based on adversarial learning. However, several disconnections still exist and form the gap between theory and algorithm. We extend previous theories (Mansour et al., 2009c; Ben-David et al., 2010) to multiclass classification in domain adaptation, where classifiers based on the scoring functions and margin loss are standard choices in algorithm design. We introduce Margin Disparity Discrepancy, a novel measurement with rigorous generalization bounds, tailored to the distribution comparison with the asymmetric margin loss, and to the minimax optimization for easier training. Our theory can be seamlessly transformed into an adversarial learning algorithm for domain adaptation, successfully bridging the gap between theory and algorithm. A series of empirical studies show that our algorithm achieves the state of the art accuracies on challenging domain adaptation tasks.}
}
"
ICML,2020,Fairwashing explanations with off manifold detergent,https://proceedings.mlr.press/v119/anders20a.html,"['Anders, Christopher', 'Pasliev, Plamen', 'Dombrowski, Ann-Kathrin', 'M{\\""u}ller, Klaus-Robert', 'Kessel, Pan']",55,"Explanation methods promise to make black-box classifiers more transparent. As a result, it is hoped that they can act as proof for a sensible, fair and trustworthy decision-making process of the algorithm and thereby increase its acceptance by the end-users. In this paper, we show both theoretically and experimentally that these hopes are presently unfounded. Specifically, we show that, for any classifier $g$, one can always construct another classifier $\tilde{g}$ which has the same behavior on the data (same train, validation, and test error) but has arbitrarily manipulated explanation maps. We derive this statement theoretically using differential geometry and demonstrate it experimentally for various explanation methods, architectures, and datasets. Motivated by our theoretical insights, we then propose a modification of existing explanation methods which makes them significantly more robust.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v119-anders20a,
  title = 	 {Fairwashing explanations with off-manifold detergent},
  author =       {Anders, Christopher and Pasliev, Plamen and Dombrowski, Ann-Kathrin and M{\""u}ller, Klaus-Robert and Kessel, Pan},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {314--323},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/anders20a/anders20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/anders20a.html},
  abstract = 	 {Explanation methods promise to make black-box classifiers more transparent. As a result, it is hoped that they can act as proof for a sensible, fair and trustworthy decision-making process of the algorithm and thereby increase its acceptance by the end-users. In this paper, we show both theoretically and experimentally that these hopes are presently unfounded. Specifically, we show that, for any classifier $g$, one can always construct another classifier $\tilde{g}$ which has the same behavior on the data (same train, validation, and test error) but has arbitrarily manipulated explanation maps. We derive this statement theoretically using differential geometry and demonstrate it experimentally for various explanation methods, architectures, and datasets. Motivated by our theoretical insights, we then propose a modification of existing explanation methods which makes them significantly more robust.}
}
"
ICML,2020,My Fair Bandit: Distributed Learning of Max Min Fairness with Multi player Bandits,https://proceedings.mlr.press/v119/bistritz20a.html,"['Bistritz, Ilai', 'Baharav, Tavor', 'Leshem, Amir', 'Bambos, Nicholas']",19,"Consider N cooperative but non-communicating players where each plays one out of M arms for T turns. Players have different utilities for each arm, representable as an NxM matrix. These utilities are unknown to the players. In each turn players receive noisy observations of their utility for their selected arm. However, if any other players selected the same arm that turn, they will all receive zero utility due to the conflict. No other communication or coordination between the players is possible. Our goal is to design a distributed algorithm that learns the matching between players and arms that achieves max-min fairness while minimizing the regret. We present an algorithm and prove that it is regret optimal up to a \log\log T factor. This is the first max-min fairness multi-player bandit algorithm with (near) order optimal regret.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v119-bistritz20a,
  title = 	 {My Fair Bandit: Distributed Learning of Max-Min Fairness with Multi-player Bandits},
  author =       {Bistritz, Ilai and Baharav, Tavor and Leshem, Amir and Bambos, Nicholas},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {930--940},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/bistritz20a/bistritz20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/bistritz20a.html},
  abstract = 	 {Consider N cooperative but non-communicating players where each plays one out of M arms for T turns. Players have different utilities for each arm, representable as an NxM matrix. These utilities are unknown to the players. In each turn players receive noisy observations of their utility for their selected arm. However, if any other players selected the same arm that turn, they will all receive zero utility due to the conflict. No other communication or coordination between the players is possible. Our goal is to design a distributed algorithm that learns the matching between players and arms that achieves max-min fairness while minimizing the regret. We present an algorithm and prove that it is regret optimal up to a \log\log T factor. This is the first max-min fairness multi-player bandit algorithm with (near) order optimal regret.}
}
"
ICML,2020,{T}ask{N}orm: Rethinking Batch Normalization for Meta Learning,https://proceedings.mlr.press/v119/bronskill20a.html,"['Bronskill, John', 'Gordon, Jonathan', 'Requeima, James', 'Nowozin, Sebastian', 'Turner, Richard']",75,"Modern meta-learning approaches for image classification rely on increasingly deep networks to achieve state-of-the-art performance, making batch normalization an essential component of meta-learning pipelines. However, the hierarchical nature of the meta-learning setting presents several challenges that can render conventional batch normalization ineffective, giving rise to the need to rethink normalization in this setting. We evaluate a range of approaches to batch normalization for meta-learning scenarios, and develop a novel approach that we call TaskNorm. Experiments on fourteen datasets demonstrate that the choice of batch normalization has a dramatic effect on both classification accuracy and training time for both gradient based- and gradient-free meta-learning approaches. Importantly, TaskNorm is found to consistently improve performance. Finally, we provide a set of best practices for normalization that will allow fair comparison of meta-learning algorithms.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v119-bronskill20a,
  title = 	 {{T}ask{N}orm: Rethinking Batch Normalization for Meta-Learning},
  author =       {Bronskill, John and Gordon, Jonathan and Requeima, James and Nowozin, Sebastian and Turner, Richard},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {1153--1164},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/bronskill20a/bronskill20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/bronskill20a.html},
  abstract = 	 {Modern meta-learning approaches for image classification rely on increasingly deep networks to achieve state-of-the-art performance, making batch normalization an essential component of meta-learning pipelines. However, the hierarchical nature of the meta-learning setting presents several challenges that can render conventional batch normalization ineffective, giving rise to the need to rethink normalization in this setting. We evaluate a range of approaches to batch normalization for meta-learning scenarios, and develop a novel approach that we call TaskNorm. Experiments on fourteen datasets demonstrate that the choice of batch normalization has a dramatic effect on both classification accuracy and training time for both gradient based- and gradient-free meta-learning approaches. Importantly, TaskNorm is found to consistently improve performance. Finally, we provide a set of best practices for normalization that will allow fair comparison of meta-learning algorithms.}
}
"
ICML,2020,A Pairwise Fair and Community preserving Approach to k Center Clustering,https://proceedings.mlr.press/v119/brubach20a.html,"['Brubach, Brian', 'Chakrabarti, Darshan', 'Dickerson, John', 'Khuller, Samir', 'Srinivasan, Aravind', 'Tsepenekas, Leonidas']",25,"Clustering is a foundational problem in machine learning with numerous applications. As machine learning increases in ubiquity as a backend for automated systems, concerns about fairness arise. Much of the current literature on fairness deals with discrimination against protected classes in supervised learning (group fairness). We define a different notion of fair clustering wherein the probability that two points (or a community of points) become separated is bounded by an increasing function of their pairwise distance (or community diameter). We capture the situation where data points represent people who gain some benefit from being clustered together. Unfairness arises when certain points are deterministically separated, either arbitrarily or by someone who intends to harm them as in the case of gerrymandering election districts. In response, we formally define two new types of fairness in the clustering setting, pairwise fairness and community preservation. To explore the practicality of our fairness goals, we devise an approach for extending existing $k$-center algorithms to satisfy these fairness constraints. Analysis of this approach proves that reasonable approximations can be achieved while maintaining fairness. In experiments, we compare the effectiveness of our approach to classical $k$-center algorithms/heuristics and explore the tradeoff between optimal clustering and fairness.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v119-brubach20a,
  title = 	 {A Pairwise Fair and Community-preserving Approach to k-Center Clustering},
  author =       {Brubach, Brian and Chakrabarti, Darshan and Dickerson, John and Khuller, Samir and Srinivasan, Aravind and Tsepenekas, Leonidas},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {1178--1189},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/brubach20a/brubach20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/brubach20a.html},
  abstract = 	 {Clustering is a foundational problem in machine learning with numerous applications. As machine learning increases in ubiquity as a backend for automated systems, concerns about fairness arise. Much of the current literature on fairness deals with discrimination against protected classes in supervised learning (group fairness). We define a different notion of fair clustering wherein the probability that two points (or a community of points) become separated is bounded by an increasing function of their pairwise distance (or community diameter). We capture the situation where data points represent people who gain some benefit from being clustered together. Unfairness arises when certain points are deterministically separated, either arbitrarily or by someone who intends to harm them as in the case of gerrymandering election districts. In response, we formally define two new types of fairness in the clustering setting, pairwise fairness and community preservation. To explore the practicality of our fairness goals, we devise an approach for extending existing $k$-center algorithms to satisfy these fairness constraints. Analysis of this approach proves that reasonable approximations can be achieved while maintaining fairness. In experiments, we compare the effectiveness of our approach to classical $k$-center algorithms/heuristics and explore the tradeoff between optimal clustering and fairness.}
}
"
ICML,2020,{D}e{B}ayes: a {B}ayesian Method for Debiasing Network Embeddings,https://proceedings.mlr.press/v119/buyl20a.html,"['Buyl, Maarten', 'De Bie, Tijl']",38,"As machine learning algorithms are increasingly deployed for high-impact automated decision making, ethical and increasingly also legal standards demand that they treat all individuals fairly, without discrimination based on their age, gender, race or other sensitive traits. In recent years much progress has been made on ensuring fairness and reducing bias in standard machine learning settings. Yet, for network embedding, with applications in vulnerable domains ranging from social network analysis to recommender systems, current options remain limited both in number and performance. We thus propose DeBayes: a conceptually elegant Bayesian method that is capable of learning debiased embeddings by using a biased prior. Our experiments show that these representations can then be used to perform link prediction that is significantly more fair in terms of popular metrics such as demographic parity and equalized opportunity.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v119-buyl20a,
  title =          {{D}e{B}ayes: a {B}ayesian Method for Debiasing Network Embeddings},
  author =       {Buyl, Maarten and De Bie, Tijl},
  booktitle =          {Proceedings of the 37th International Conference on Machine Learning},
  pages =          {1220--1229},
  year =          {2020},
  editor =          {III, Hal Daumé and Singh, Aarti},
  volume =          {119},
  series =          {Proceedings of Machine Learning Research},
  month =          {13--18 Jul},
  publisher =    {PMLR},
  pdf =          {http://proceedings.mlr.press/v119/buyl20a/buyl20a.pdf},
  url =          {https://proceedings.mlr.press/v119/buyl20a.html},
  abstract =          {As machine learning algorithms are increasingly deployed for high-impact automated decision making, ethical and increasingly also legal standards demand that they treat all individuals fairly, without discrimination based on their age, gender, race or other sensitive traits. In recent years much progress has been made on ensuring fairness and reducing bias in standard machine learning settings. Yet, for network embedding, with applications in vulnerable domains ranging from social network analysis to recommender systems, current options remain limited both in number and performance. We thus propose DeBayes: a conceptually elegant Bayesian method that is capable of learning debiased embeddings by using a biased prior. Our experiments show that these representations can then be used to perform link prediction that is significantly more fair in terms of popular metrics such as demographic parity and equalized opportunity.}
}
"
ICML,2020,Data preprocessing to mitigate bias: A maximum entropy based approach,https://proceedings.mlr.press/v119/celis20a.html,"['Celis, L. Elisa', 'Keswani, Vijay', 'Vishnoi, Nisheeth']",24,"Data containing human or social attributes may over- or under-represent groups with respect to salient social attributes such as gender or race, which can lead to biases in downstream applications. This paper presents an algorithmic framework that can be used as a data preprocessing method towards mitigating such bias. Unlike prior work, it can efficiently learn distributions over large domains, controllably adjust the representation rates of protected groups and achieve target fairness metrics such as statistical parity, yet remains close to the empirical distribution induced by the given dataset. Our approach leverages the principle of maximum entropy {‚Äì} amongst all distributions satisfying a given set of constraints, we should choose the one closest in KL-divergence to a given prior. While maximum entropy distributions can succinctly encode distributions over large domains, they can be difficult to compute. Our main contribution is an instantiation of this framework for our set of constraints and priors, which encode our bias mitigation goals, and that runs in time polynomial in the dimension of the data. Empirically, we observe that samples from the learned distribution have desired representation rates and statistical rates, and when used for training a classifier incurs only a slight loss in accuracy while maintaining fairness properties.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v119-celis20a,
  title =          {Data preprocessing to mitigate bias: A maximum entropy based approach},
  author =       {Celis, L. Elisa and Keswani, Vijay and Vishnoi, Nisheeth},
  booktitle =          {Proceedings of the 37th International Conference on Machine Learning},
  pages =          {1349--1359},
  year =          {2020},
  editor =          {III, Hal Daumé and Singh, Aarti},
  volume =          {119},
  series =          {Proceedings of Machine Learning Research},
  month =          {13--18 Jul},
  publisher =    {PMLR},
  pdf =          {http://proceedings.mlr.press/v119/celis20a/celis20a.pdf},
  url =          {https://proceedings.mlr.press/v119/celis20a.html},
  abstract =          {Data containing human or social attributes may over- or under-represent groups with respect to salient social attributes such as gender or race, which can lead to biases in downstream applications. This paper presents an algorithmic framework that can be used as a data preprocessing method towards mitigating such bias. Unlike prior work, it can efficiently learn distributions over large domains, controllably adjust the representation rates of protected groups and achieve target fairness metrics such as statistical parity, yet remains close to the empirical distribution induced by the given dataset. Our approach leverages the principle of maximum entropy {–} amongst all distributions satisfying a given set of constraints, we should choose the one closest in KL-divergence to a given prior. While maximum entropy distributions can succinctly encode distributions over large domains, they can be difficult to compute. Our main contribution is an instantiation of this framework for our set of constraints and priors, which encode our bias mitigation goals, and that runs in time polynomial in the dimension of the data. Empirically, we observe that samples from the learned distribution have desired representation rates and statistical rates, and when used for training a classifier incurs only a slight loss in accuracy while maintaining fairness properties.}
}
"
ICML,2020,How to Solve Fair k Center in Massive Data Models,https://proceedings.mlr.press/v119/chiplunkar20a.html,"['Chiplunkar, Ashish', 'Kale, Sagar', 'Ramamoorthy, Sivaramakrishnan Natarajan']",18,"Fueled by massive data, important decision making is being automated with the help of algorithms, therefore, fairness in algorithms has become an especially important research topic. In this work, we design new streaming and distributed algorithms for the fair k-center problem that models fair data summarization. The streaming and distributed models of computation have an attractive feature of being able to handle massive data sets that do not fit into main memory. Our main contributions are: (a) the first distributed algorithm; which has provably constant approximation ratio and is extremely parallelizable, and (b) a two-pass streaming algorithm with a provable approximation guarantee matching the best known algorithm (which is not a streaming algorithm). Our algorithms have the advantages of being easy to implement in practice, being fast with linear running times, having very small working memory and communication, and outperforming existing algorithms on several real and synthetic data sets. To complement our distributed algorithm, we also give a hardness result for natural distributed algorithms, which holds for even the special case of k-center.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v119-chiplunkar20a,
  title = 	 {How to Solve Fair k-Center in Massive Data Models},
  author =       {Chiplunkar, Ashish and Kale, Sagar and Ramamoorthy, Sivaramakrishnan Natarajan},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {1877--1886},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/chiplunkar20a/chiplunkar20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/chiplunkar20a.html},
  abstract = 	 {Fueled by massive data, important decision making is being automated with the help of algorithms, therefore, fairness in algorithms has become an especially important research topic. In this work, we design new streaming and distributed algorithms for the fair k-center problem that models fair data summarization. The streaming and distributed models of computation have an attractive feature of being able to handle massive data sets that do not fit into main memory. Our main contributions are: (a) the first distributed algorithm; which has provably constant approximation ratio and is extremely parallelizable, and (b) a two-pass streaming algorithm with a provable approximation guarantee matching the best known algorithm (which is not a streaming algorithm). Our algorithms have the advantages of being easy to implement in practice, being fast with linear running times, having very small working memory and communication, and outperforming existing algorithms on several real and synthetic data sets. To complement our distributed algorithm, we also give a hardness result for natural distributed algorithms, which holds for even the special case of k-center.}
}
"
ICML,2020,Fair Generative Modeling via Weak Supervision,https://proceedings.mlr.press/v119/choi20a.html,"['Choi, Kristy', 'Grover, Aditya', 'Singh, Trisha', 'Shu, Rui', 'Ermon, Stefano']",53,"Real-world datasets are often biased with respect to key demographic factors such as race and gender. Due to the latent nature of the underlying factors, detecting and mitigating bias is especially challenging for unsupervised machine learning. We present a weakly supervised algorithm for overcoming dataset bias for deep generative models. Our approach requires access to an additional small, unlabeled reference dataset as the supervision signal, thus sidestepping the need for explicit labels on the underlying bias factors. Using this supplementary dataset, we detect the bias in existing datasets via a density ratio technique and learn generative models which efficiently achieve the twin goals of: 1) data efficiency by using training examples from both biased and reference datasets for learning; and 2) data generation close in distribution to the reference dataset at test time. Empirically, we demonstrate the efficacy of our approach which reduces bias w.r.t. latent factors by an average of up to 34.6% over baselines for comparable image generation using generative adversarial networks.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v119-choi20a,
  title =          {Fair Generative Modeling via Weak Supervision},
  author =       {Choi, Kristy and Grover, Aditya and Singh, Trisha and Shu, Rui and Ermon, Stefano},
  booktitle =          {Proceedings of the 37th International Conference on Machine Learning},
  pages =          {1887--1898},
  year =          {2020},
  editor =          {III, Hal Daumé and Singh, Aarti},
  volume =          {119},
  series =          {Proceedings of Machine Learning Research},
  month =          {13--18 Jul},
  publisher =    {PMLR},
  pdf =          {http://proceedings.mlr.press/v119/choi20a/choi20a.pdf},
  url =          {https://proceedings.mlr.press/v119/choi20a.html},
  abstract =          {Real-world datasets are often biased with respect to key demographic factors such as race and gender. Due to the latent nature of the underlying factors, detecting and mitigating bias is especially challenging for unsupervised machine learning. We present a weakly supervised algorithm for overcoming dataset bias for deep generative models. Our approach requires access to an additional small, unlabeled reference dataset as the supervision signal, thus sidestepping the need for explicit labels on the underlying bias factors. Using this supplementary dataset, we detect the bias in existing datasets via a density ratio technique and learn generative models which efficiently achieve the twin goals of: 1) data efficiency by using training examples from both biased and reference datasets for learning; and 2) data generation close in distribution to the reference dataset at test time. Empirically, we demonstrate the efficacy of our approach which reduces bias w.r.t. latent factors by an average of up to 34.6% over baselines for comparable image generation using generative adversarial networks.}
}
"
ICML,2020,Causal Modeling for Fairness In Dynamical Systems,https://proceedings.mlr.press/v119/creager20a.html,"['Creager, Elliot', 'Madras, David', 'Pitassi, Toniann', 'Zemel, Richard']",37,"In many applications areas‚Äîlending, education, and online recommenders, for example‚Äîfairness and equity concerns emerge when a machine learning system interacts with a dynamically changing environment to produce both immediate and long-term effects for individuals and demographic groups. We discuss causal directed acyclic graphs (DAGs) as a unifying framework for the recent literature on fairness in such dynamical systems. We show that this formulation affords several new directions of inquiry to the modeler, where sound causal assumptions can be expressed and manipulated. We emphasize the importance of computing interventional quantities in the dynamical fairness setting, and show how causal assumptions enable simulation (when environment dynamics are known) and estimation by adjustment (when dynamics are unknown) of intervention on short- and long-term outcomes, at both the group and individual levels.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v119-creager20a,
  title = 	 {Causal Modeling for Fairness In Dynamical Systems},
  author =       {Creager, Elliot and Madras, David and Pitassi, Toniann and Zemel, Richard},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {2185--2195},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/creager20a/creager20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/creager20a.html},
  abstract = 	 {In many applications areas—lending, education, and online recommenders, for example—fairness and equity concerns emerge when a machine learning system interacts with a dynamically changing environment to produce both immediate and long-term effects for individuals and demographic groups. We discuss causal directed acyclic graphs (DAGs) as a unifying framework for the recent literature on fairness in such dynamical systems. We show that this formulation affords several new directions of inquiry to the modeler, where sound causal assumptions can be expressed and manipulated. We emphasize the importance of computing interventional quantities in the dynamical fairness setting, and show how causal assumptions enable simulation (when environment dynamics are known) and estimation by adjustment (when dynamics are unknown) of intervention on short- and long-term outcomes, at both the group and individual levels.}
}
"
ICML,2020,Is There a Trade Off Between Fairness and Accuracy? {A} Perspective Using Mismatched Hypothesis Testing,https://proceedings.mlr.press/v119/dutta20a.html,"['Dutta, Sanghamitra', 'Wei, Dennis', 'Yueksel, Hazar', 'Chen, Pin-Yu', 'Liu, Sijia', 'Varshney, Kush']",70,"A trade-off between accuracy and fairness is almost taken as a given in the existing literature on fairness in machine learning. Yet, it is not preordained that accuracy should decrease with increased fairness. Novel to this work, we examine fair classification through the lens of mismatched hypothesis testing: trying to find a classifier that distinguishes between two ideal distributions when given two mismatched distributions that are biased. Using Chernoff information, a tool in information theory, we theoretically demonstrate that, contrary to popular belief, there always exist ideal distributions such that optimal fairness and accuracy (with respect to the ideal distributions) are achieved simultaneously: there is no trade-off. Moreover, the same classifier yields the lack of a trade-off with respect to ideal distributions while yielding a trade-off when accuracy is measured with respect to the given (possibly biased) dataset. To complement our main result, we formulate an optimization to find ideal distributions and derive fundamental limits to explain why a trade-off exists on the given biased dataset. We also derive conditions under which active data collection can alleviate the fairness-accuracy trade-off in the real world. Our results lead us to contend that it is problematic to measure accuracy with respect to data that reflects bias, and instead, we should be considering accuracy with respect to ideal, unbiased data.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v119-dutta20a,
  title = 	 {Is There a Trade-Off Between Fairness and Accuracy? {A} Perspective Using Mismatched Hypothesis Testing},
  author =       {Dutta, Sanghamitra and Wei, Dennis and Yueksel, Hazar and Chen, Pin-Yu and Liu, Sijia and Varshney, Kush},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {2803--2813},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/dutta20a/dutta20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/dutta20a.html},
  abstract = 	 {A trade-off between accuracy and fairness is almost taken as a given in the existing literature on fairness in machine learning. Yet, it is not preordained that accuracy should decrease with increased fairness. Novel to this work, we examine fair classification through the lens of mismatched hypothesis testing: trying to find a classifier that distinguishes between two ideal distributions when given two mismatched distributions that are biased. Using Chernoff information, a tool in information theory, we theoretically demonstrate that, contrary to popular belief, there always exist ideal distributions such that optimal fairness and accuracy (with respect to the ideal distributions) are achieved simultaneously: there is no trade-off. Moreover, the same classifier yields the lack of a trade-off with respect to ideal distributions while yielding a trade-off when accuracy is measured with respect to the given (possibly biased) dataset. To complement our main result, we formulate an optimization to find ideal distributions and derive fundamental limits to explain why a trade-off exists on the given biased dataset. We also derive conditions under which active data collection can alleviate the fairness-accuracy trade-off in the real world. Our results lead us to contend that it is problematic to measure accuracy with respect to data that reflects bias, and instead, we should be considering accuracy with respect to ideal, unbiased data.}
}
"
ICML,2020,Towards Non Parametric Drift Detection via Dynamic Adapting Window Independence Drift Detection ({DAWIDD}),https://proceedings.mlr.press/v119/hinder20a.html,"['Hinder, Fabian', ""Artelt, Andr{\\'e}"", 'Hammer, Barbara']",18,"The notion of concept drift refers to the phenomenon that the distribution, which is underlying the observed data, changes over time; as a consequence machine learning models may become inaccurate and need adjustment. Many online learning schemes include drift detection to actively detect and react to observed changes. Yet, reliable drift detection constitutes a challenging problem in particular in the context of high dimensional data, varying drift characteristics, and the absence of a parametric model such as a classification scheme which reflects the drift. In this paper we present a novel concept drift detection method, Dynamic Adapting Window Independence Drift Detection (DAWIDD), which aims for non-parametric drift detection of diverse drift characteristics. For this purpose, we establish a mathematical equivalence of the presence of drift to the dependency of specific random variables in an according drift process. This allows us to rely on independence tests rather than parametric models or the classification loss, resulting in a fairly robust scheme to universally detect different types of drift, as it is also confirmed in experiments.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v119-hinder20a,
  title =          {Towards Non-Parametric Drift Detection via Dynamic Adapting Window Independence Drift Detection ({DAWIDD})},
  author =       {Hinder, Fabian and Artelt, Andr{\'e} and Hammer, Barbara},
  booktitle =          {Proceedings of the 37th International Conference on Machine Learning},
  pages =          {4249--4259},
  year =          {2020},
  editor =          {III, Hal Daumé and Singh, Aarti},
  volume =          {119},
  series =          {Proceedings of Machine Learning Research},
  month =          {13--18 Jul},
  publisher =    {PMLR},
  pdf =          {http://proceedings.mlr.press/v119/hinder20a/hinder20a.pdf},
  url =          {https://proceedings.mlr.press/v119/hinder20a.html},
  abstract =          {The notion of concept drift refers to the phenomenon that the distribution, which is underlying the observed data, changes over time; as a consequence machine learning models may become inaccurate and need adjustment. Many online learning schemes include drift detection to actively detect and react to observed changes. Yet, reliable drift detection constitutes a challenging problem in particular in the context of high dimensional data, varying drift characteristics, and the absence of a parametric model such as a classification scheme which reflects the drift. In this paper we present a novel concept drift detection method, Dynamic Adapting Window Independence Drift Detection (DAWIDD), which aims for non-parametric drift detection of diverse drift characteristics. For this purpose, we establish a mathematical equivalence of the presence of drift to the dependency of specific random variables in an according drift process. This allows us to rely on independence tests rather than parametric models or the classification loss, resulting in a fairly robust scheme to universally detect different types of drift, as it is also confirmed in experiments.}
}
"
ICML,2020,Fair k Centers via Maximum Matching,https://proceedings.mlr.press/v119/jones20a.html,"['Jones, Matthew', 'Nguyen, Huy', 'Nguyen, Thy']",22,"The field of algorithms has seen a push for fairness, or the removal of inherent bias, in recent history. In data summarization, where a much smaller subset of a data set is chosen to represent the whole of the data, fairness can be introduced by guaranteeing each ""demographic group"" a specific portion of the representative subset. Specifically, this paper examines this fair variant of the k-centers problem, where a subset of the data with cardinality k is chosen to minimize distance to the rest of the data. Previous papers working on this problem presented both a 3-approximation algorithm with a super-linear runtime and a linear-time algorithm whose approximation factor is exponential in the number of demographic groups. This paper combines the best of each algorithm by presenting a linear-time algorithm with a guaranteed 3-approximation factor and provides empirical evidence of both the algorithm‚Äôs runtime and effectiveness.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v119-jones20a,
  title = 	 {Fair k-Centers via Maximum Matching},
  author =       {Jones, Matthew and Nguyen, Huy and Nguyen, Thy},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {4940--4949},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/jones20a/jones20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/jones20a.html},
  abstract = 	 {The field of algorithms has seen a push for fairness, or the removal of inherent bias, in recent history. In data summarization, where a much smaller subset of a data set is chosen to represent the whole of the data, fairness can be introduced by guaranteeing each ""demographic group"" a specific portion of the representative subset. Specifically, this paper examines this fair variant of the k-centers problem, where a subset of the data with cardinality k is chosen to minimize distance to the rest of the data. Previous papers working on this problem presented both a 3-approximation algorithm with a super-linear runtime and a linear-time algorithm whose approximation factor is exponential in the number of demographic groups. This paper combines the best of each algorithm by presenting a linear-time algorithm with a guaranteed 3-approximation factor and provides empirical evidence of both the algorithm’s runtime and effectiveness.}
}
"
ICML,2020,{FACT}: A Diagnostic for Group Fairness Trade offs,https://proceedings.mlr.press/v119/kim20a.html,"['Kim, Joon Sik', 'Chen, Jiahao', 'Talwalkar, Ameet']",22,"Group fairness, a class of fairness notions that measure how different groups of individuals are treated differently according to their protected attributes, has been shown to conflict with one another, often with a necessary cost in loss of model‚Äôs predictive performance. We propose a general diagnostic that enables systematic characterization of these trade-offs in group fairness. We observe that the majority of group fairness notions can be expressed via the fairness-confusion tensor, which is the confusion matrix split according to the protected attribute values. We frame several optimization problems that directly optimize both accuracy and fairness objectives over the elements of this tensor, which yield a general perspective for understanding multiple trade-offs including group fairness incompatibilities. It also suggests an alternate post-processing method for designing fair classifiers. On synthetic and real datasets, we demonstrate the use cases of our diagnostic, particularly on understanding the trade-off landscape between accuracy and fairness.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v119-kim20a,
  title = 	 {{FACT}: A Diagnostic for Group Fairness Trade-offs},
  author =       {Kim, Joon Sik and Chen, Jiahao and Talwalkar, Ameet},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {5264--5274},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/kim20a/kim20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/kim20a.html},
  abstract = 	 {Group fairness, a class of fairness notions that measure how different groups of individuals are treated differently according to their protected attributes, has been shown to conflict with one another, often with a necessary cost in loss of model’s predictive performance. We propose a general diagnostic that enables systematic characterization of these trade-offs in group fairness. We observe that the majority of group fairness notions can be expressed via the fairness-confusion tensor, which is the confusion matrix split according to the protected attribute values. We frame several optimization problems that directly optimize both accuracy and fairness objectives over the elements of this tensor, which yield a general perspective for understanding multiple trade-offs including group fairness incompatibilities. It also suggests an alternate post-processing method for designing fair classifiers. On synthetic and real datasets, we demonstrate the use cases of our diagnostic, particularly on understanding the trade-off landscape between accuracy and fairness.}
}
"
ICML,2020,Finite Time Last Iterate Convergence for Multi Agent Learning in Games,https://proceedings.mlr.press/v119/lin20h.html,"['Lin, Tianyi', 'Zhou, Zhengyuan', 'Mertikopoulos, Panayotis', 'Jordan, Michael']",28,"In this paper, we consider multi-agent learning via online gradient descent in a class of games called $\lambda$-cocoercive games, a fairly broad class of games that admits many Nash equilibria and that properly includes unconstrained strongly monotone games. We characterize the finite-time last-iterate convergence rate for joint OGD learning on $\lambda$-cocoercive games; further, building on this result, we develop a fully adaptive OGD learning algorithm that does not require any knowledge of problem parameter (e.g. cocoercive constant $\lambda$) and show, via a novel double-stopping time technique, that this adaptive algorithm achieves same finite-time last-iterate convergence rate as non-adaptive counterpart. Subsequently, we extend OGD learning to the noisy gradient feedback case and establish last-iterate convergence results‚Äìfirst qualitative almost sure convergence, then quantitative finite-time convergence rates‚Äì all under non-decreasing step-sizes. To our knowledge, we provide the first set of results that fill in several gaps of the existing multi-agent online learning literature, where three aspects‚Äìfinite-time convergence rates, non-decreasing step-sizes, and fully adaptive algorithms have been unexplored before.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v119-lin20h,
  title = 	 {Finite-Time Last-Iterate Convergence for Multi-Agent Learning in Games},
  author =       {Lin, Tianyi and Zhou, Zhengyuan and Mertikopoulos, Panayotis and Jordan, Michael},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {6161--6171},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/lin20h/lin20h.pdf},
  url = 	 {https://proceedings.mlr.press/v119/lin20h.html},
  abstract = 	 {In this paper, we consider multi-agent learning via online gradient descent in a class of games called $\lambda$-cocoercive games, a fairly broad class of games that admits many Nash equilibria and that properly includes unconstrained strongly monotone games. We characterize the finite-time last-iterate convergence rate for joint OGD learning on $\lambda$-cocoercive games; further, building on this result, we develop a fully adaptive OGD learning algorithm that does not require any knowledge of problem parameter (e.g. cocoercive constant $\lambda$) and show, via a novel double-stopping time technique, that this adaptive algorithm achieves same finite-time last-iterate convergence rate as non-adaptive counterpart. Subsequently, we extend OGD learning to the noisy gradient feedback case and establish last-iterate convergence results–first qualitative almost sure convergence, then quantitative finite-time convergence rates– all under non-decreasing step-sizes. To our knowledge, we provide the first set of results that fill in several gaps of the existing multi-agent online learning literature, where three aspects–finite-time convergence rates, non-decreasing step-sizes, and fully adaptive algorithms have been unexplored before.}
}
"
ICML,2020,Weakly Supervised Disentanglement Without Compromises,https://proceedings.mlr.press/v119/locatello20a.html,"['Locatello, Francesco', 'Poole, Ben', 'Raetsch, Gunnar', 'Sch{\\""o}lkopf, Bernhard', 'Bachem, Olivier', 'Tschannen, Michael']",178,"Intelligent agents should be able to learn useful representations by observing changes in their environment. We model such observations as pairs of non-i.i.d. images sharing at least one of the underlying factors of variation. First, we theoretically show that only knowing how many factors have changed, but not which ones, is sufficient to learn disentangled representations. Second, we provide practical algorithms that learn disentangled representations from pairs of images without requiring annotation of groups, individual factors, or the number of factors that have changed. Third, we perform a large-scale empirical study and show that such pairs of observations are sufficient to reliably learn disentangled representations on several benchmark data sets. Finally, we evaluate our learned representations and find that they are simultaneously useful on a diverse suite of tasks, including generalization under covariate shifts, fairness, and abstract reasoning. Overall, our results demonstrate that weak supervision enables learning of useful disentangled representations in realistic scenarios.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v119-locatello20a,
  title = 	 {Weakly-Supervised Disentanglement Without Compromises},
  author =       {Locatello, Francesco and Poole, Ben and Raetsch, Gunnar and Sch{\""o}lkopf, Bernhard and Bachem, Olivier and Tschannen, Michael},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {6348--6359},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/locatello20a/locatello20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/locatello20a.html},
  abstract = 	 {Intelligent agents should be able to learn useful representations by observing changes in their environment. We model such observations as pairs of non-i.i.d. images sharing at least one of the underlying factors of variation. First, we theoretically show that only knowing how many factors have changed, but not which ones, is sufficient to learn disentangled representations. Second, we provide practical algorithms that learn disentangled representations from pairs of images without requiring annotation of groups, individual factors, or the number of factors that have changed. Third, we perform a large-scale empirical study and show that such pairs of observations are sufficient to reliably learn disentangled representations on several benchmark data sets. Finally, we evaluate our learned representations and find that they are simultaneously useful on a diverse suite of tasks, including generalization under covariate shifts, fairness, and abstract reasoning. Overall, our results demonstrate that weak supervision enables learning of useful disentangled representations in realistic scenarios.}
}
"
ICML,2020,Too Relaxed to Be Fair,https://proceedings.mlr.press/v119/lohaus20a.html,"['Lohaus, Michael', 'Perrot, Michael', 'Luxburg, Ulrike Von']",33,"We address the problem of classification under fairness constraints. Given a notion of fairness, the goal is to learn a classifier that is not discriminatory against a group of individuals. In the literature, this problem is often formulated as a constrained optimization problem and solved using relaxations of the fairness constraints. We show that many existing relaxations are unsatisfactory: even if a model satisfies the relaxed constraint, it can be surprisingly unfair. We propose a principled framework to solve this problem. This new approach uses a strongly convex formulation and comes with theoretical guarantees on the fairness of its solution. In practice, we show that this method gives promising results on real data.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v119-lohaus20a,
  title =          {Too Relaxed to Be Fair},
  author =       {Lohaus, Michael and Perrot, Michael and Luxburg, Ulrike Von},
  booktitle =          {Proceedings of the 37th International Conference on Machine Learning},
  pages =          {6360--6369},
  year =          {2020},
  editor =          {III, Hal Daumé and Singh, Aarti},
  volume =          {119},
  series =          {Proceedings of Machine Learning Research},
  month =          {13--18 Jul},
  publisher =    {PMLR},
  pdf =          {http://proceedings.mlr.press/v119/lohaus20a/lohaus20a.pdf},
  url =          {https://proceedings.mlr.press/v119/lohaus20a.html},
  abstract =          {We address the problem of classification under fairness constraints. Given a notion of fairness, the goal is to learn a classifier that is not discriminatory against a group of individuals. In the literature, this problem is often formulated as a constrained optimization problem and solved using relaxations of the fairness constraints. We show that many existing relaxations are unsatisfactory: even if a model satisfies the relaxed constraint, it can be surprisingly unfair. We propose a principled framework to solve this problem. This new approach uses a strongly convex formulation and comes with theoretical guarantees on the fairness of its solution. In practice, we show that this method gives promising results on real data.}
}
"
ICML,2020,Quadratically Regularized Subgradient Methods for Weakly Convex Optimization with Weakly Convex Constraints,https://proceedings.mlr.press/v119/ma20d.html,"['Ma, Runchao', 'Lin, Qihang', 'Yang, Tianbao']",7,"Optimization models with non-convex constraints arise in many tasks in machine learning, e.g., learning with fairness constraints or Neyman-Pearson classification with non-convex loss. Although many efficient methods have been developed with theoretical convergence guarantees for non-convex unconstrained problems, it remains a challenge to design provably efficient algorithms for problems with non-convex functional constraints. This paper proposes a class of subgradient methods for constrained optimization where the objective function and the constraint functions are weakly convex and nonsmooth. Our methods solve a sequence of strongly convex subproblems, where a quadratic regularization term is added to both the objective function and each constraint function. Each subproblem can be solved by various algorithms for strongly convex optimization. Under a uniform Slater‚Äôs condition, we establish the computation complexities of our methods for finding a nearly stationary point.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v119-ma20d,
  title = 	 {Quadratically Regularized Subgradient Methods for Weakly Convex Optimization with Weakly Convex Constraints},
  author =       {Ma, Runchao and Lin, Qihang and Yang, Tianbao},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {6554--6564},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/ma20d/ma20d.pdf},
  url = 	 {https://proceedings.mlr.press/v119/ma20d.html},
  abstract = 	 {Optimization models with non-convex constraints arise in many tasks in machine learning, e.g., learning with fairness constraints or Neyman-Pearson classification with non-convex loss. Although many efficient methods have been developed with theoretical convergence guarantees for non-convex unconstrained problems, it remains a challenge to design provably efficient algorithms for problems with non-convex functional constraints. This paper proposes a class of subgradient methods for constrained optimization where the objective function and the constraint functions are weakly convex and nonsmooth. Our methods solve a sequence of strongly convex subproblems, where a quadratic regularization term is added to both the objective function and each constraint function. Each subproblem can be solved by various algorithms for strongly convex optimization. Under a uniform Slater’s condition, we establish the computation complexities of our methods for finding a nearly stationary point.}
}
"
ICML,2020,Individual Fairness for k Clustering,https://proceedings.mlr.press/v119/mahabadi20a.html,"['Mahabadi, Sepideh', 'Vakilian, Ali']",37,"We give a local search based algorithm for $k$-median and $k$-means (and more generally for any $k$-clustering with $\ell_p$ norm cost function) from the perspective of individual fairness. More precisely, for a point $x$ in a point set $P$ of size $n$, let $r(x)$ be the minimum radius such that the ball of radius $r(x)$ centered at $x$ has at least $n/k$ points from $P$. Intuitively, if a set of $k$ random points are chosen from $P$ as centers, every point $x\in P$ expects to have a center within radius $r(x)$. In this work, we show how to get an approximately optimal such fair $k$-clustering: The $k$-median ($k$-means) cost of our solution is within a constant factor of the cost of an optimal fair $k$-clustering, and our solution approximately satisfies the fairness condition (also within a constant factor).",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v119-mahabadi20a,
  title = 	 {Individual Fairness for k-Clustering},
  author =       {Mahabadi, Sepideh and Vakilian, Ali},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {6586--6596},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/mahabadi20a/mahabadi20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/mahabadi20a.html},
  abstract = 	 {We give a local search based algorithm for $k$-median and $k$-means (and more generally for any $k$-clustering with $\ell_p$ norm cost function) from the perspective of individual fairness. More precisely, for a point $x$ in a point set $P$ of size $n$, let $r(x)$ be the minimum radius such that the ball of radius $r(x)$ centered at $x$ has at least $n/k$ points from $P$. Intuitively, if a set of $k$ random points are chosen from $P$ as centers, every point $x\in P$ expects to have a center within radius $r(x)$. In this work, we show how to get an approximately optimal such fair $k$-clustering: The $k$-median ($k$-means) cost of our solution is within a constant factor of the cost of an optimal fair $k$-clustering, and our solution approximately satisfies the fairness condition (also within a constant factor).}
}
"
ICML,2020,Minimax Pareto Fairness: A Multi Objective Perspective,https://proceedings.mlr.press/v119/martinez20a.html,"['Martinez, Natalia', 'Bertran, Martin', 'Sapiro, Guillermo']",105,"In this work we formulate and formally characterize group fairness as a multi-objective optimization problem, where each sensitive group risk is a separate objective. We propose a fairness criterion where a classifier achieves minimax risk and is Pareto-efficient w.r.t. all groups, avoiding unnecessary harm, and can lead to the best zero-gap model if policy dictates so. We provide a simple optimization algorithm compatible with deep neural networks to satisfy these constraints. Since our method does not require test-time access to sensitive attributes, it can be applied to reduce worst-case classification errors between outcomes in unbalanced classification problems. We test the proposed methodology on real case-studies of predicting income, ICU patient mortality, skin lesions classification, and assessing credit risk, demonstrating how our framework compares favorably to other approaches.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v119-martinez20a,
  title = 	 {Minimax Pareto Fairness: A Multi Objective Perspective},
  author =       {Martinez, Natalia and Bertran, Martin and Sapiro, Guillermo},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {6755--6764},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/martinez20a/martinez20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/martinez20a.html},
  abstract = 	 {In this work we formulate and formally characterize group fairness as a multi-objective optimization problem, where each sensitive group risk is a separate objective. We propose a fairness criterion where a classifier achieves minimax risk and is Pareto-efficient w.r.t. all groups, avoiding unnecessary harm, and can lead to the best zero-gap model if policy dictates so. We provide a simple optimization algorithm compatible with deep neural networks to satisfy these constraints. Since our method does not require test-time access to sensitive attributes, it can be applied to reduce worst-case classification errors between outcomes in unbalanced classification problems. We test the proposed methodology on real case-studies of predicting income, ICU patient mortality, skin lesions classification, and assessing credit risk, demonstrating how our framework compares favorably to other approaches.}
}
"
ICML,2020,Optimizing Long term Social Welfare in Recommender Systems: A Constrained Matching Approach,https://proceedings.mlr.press/v119/mladenov20a.html,"['Mladenov, Martin', 'Creager, Elliot', 'Ben-Porat, Omer', 'Swersky, Kevin', 'Zemel, Richard', 'Boutilier, Craig']",31,"Most recommender systems (RS) research assumes that a user‚Äôs utility can be maximized independently of the utility of the other agents (e.g., other users, content providers). In realistic settings, this is often not true ‚Äì the dynamics of an RS ecosystem couple the long-term utility of all agents. In this work, we explore settings in which content providers cannot remain viable unless they receive a certain level of user engagement. We formulate this problem as one of equilibrium selection in the induced dynamical system, and show that it can be solved as an optimal constrained matching problem. Our model ensures the system reaches an equilibrium with maximal social welfare supported by a sufficiently diverse set of viable providers. We demonstrate that even in a simple, stylized dynamical RS model, the standard myopic approach to recommendation - always matching a user to the best provider - performs poorly. We develop several scalable techniques to solve the matching problem, and also draw connections to various notions of user regret and fairness, arguing that these outcomes are fairer in a utilitarian sense.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v119-mladenov20a,
  title = 	 {Optimizing Long-term Social Welfare in Recommender Systems: A Constrained Matching Approach},
  author =       {Mladenov, Martin and Creager, Elliot and Ben-Porat, Omer and Swersky, Kevin and Zemel, Richard and Boutilier, Craig},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {6987--6998},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/mladenov20a/mladenov20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/mladenov20a.html},
  abstract = 	 {Most recommender systems (RS) research assumes that a user’s utility can be maximized independently of the utility of the other agents (e.g., other users, content providers). In realistic settings, this is often not true – the dynamics of an RS ecosystem couple the long-term utility of all agents. In this work, we explore settings in which content providers cannot remain viable unless they receive a certain level of user engagement. We formulate this problem as one of equilibrium selection in the induced dynamical system, and show that it can be solved as an optimal constrained matching problem. Our model ensures the system reaches an equilibrium with maximal social welfare supported by a sufficiently diverse set of viable providers. We demonstrate that even in a simple, stylized dynamical RS model, the standard myopic approach to recommendation - always matching a user to the best provider - performs poorly. We develop several scalable techniques to solve the matching problem, and also draw connections to various notions of user regret and fairness, arguing that these outcomes are fairer in a utilitarian sense.}
}
"
ICML,2020,Fair Learning with Private Demographic Data,https://proceedings.mlr.press/v119/mozannar20a.html,"['Mozannar, Hussein', 'Ohannessian, Mesrob', 'Srebro, Nathan']",36,"Sensitive attributes such as race are rarely available to learners in real world settings as their collection is often restricted by laws and regulations. We give a scheme that allows individuals to release their sensitive information privately while still allowing any downstream entity to learn non-discriminatory predictors. We show how to adapt non-discriminatory learners to work with privatized protected attributes giving theoretical guarantees on performance. Finally, we highlight how the methodology could apply to learning fair predictors in settings where protected attributes are only available for a subset of the data.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v119-mozannar20a,
  title = 	 {Fair Learning with Private Demographic Data},
  author =       {Mozannar, Hussein and Ohannessian, Mesrob and Srebro, Nathan},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {7066--7075},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/mozannar20a/mozannar20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/mozannar20a.html},
  abstract = 	 {Sensitive attributes such as race are rarely available to learners in real world settings as their collection is often restricted by laws and regulations. We give a scheme that allows individuals to release their sensitive information privately while still allowing any downstream entity to learn non-discriminatory predictors. We show how to adapt non-discriminatory learners to work with privatized protected attributes giving theoretical guarantees on performance. Finally, we highlight how the methodology could apply to learning fair predictors in settings where protected attributes are only available for a subset of the data.}
}
"
ICML,2020,Two Simple Ways to Learn Individual Fairness Metrics from Data,https://proceedings.mlr.press/v119/mukherjee20a.html,"['Mukherjee, Debarghya', 'Yurochkin, Mikhail', 'Banerjee, Moulinath', 'Sun, Yuekai']",52,"Individual fairness is an intuitive definition of algorithmic fairness that addresses some of the drawbacks of group fairness. Despite its benefits, it depends on a task specific fair metric that encodes our intuition of what is fair and unfair for the ML task at hand, and the lack of a widely accepted fair metric for many ML tasks is the main barrier to broader adoption of individual fairness. In this paper, we present two simple ways to learn fair metrics from a variety of data types. We show empirically that fair training with the learned metrics leads to improved fairness on three machine learning tasks susceptible to gender and racial biases. We also provide theoretical guarantees on the statistical performance of both approaches.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v119-mukherjee20a,
  title = 	 {Two Simple Ways to Learn Individual Fairness Metrics from Data},
  author =       {Mukherjee, Debarghya and Yurochkin, Mikhail and Banerjee, Moulinath and Sun, Yuekai},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {7097--7107},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/mukherjee20a/mukherjee20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/mukherjee20a.html},
  abstract = 	 {Individual fairness is an intuitive definition of algorithmic fairness that addresses some of the drawbacks of group fairness. Despite its benefits, it depends on a task specific fair metric that encodes our intuition of what is fair and unfair for the ML task at hand, and the lack of a widely accepted fair metric for many ML tasks is the main barrier to broader adoption of individual fairness. In this paper, we present two simple ways to learn fair metrics from a variety of data types. We show empirically that fair training with the learned metrics leads to improved fairness on three machine learning tasks susceptible to gender and racial biases. We also provide theoretical guarantees on the statistical performance of both approaches.}
}
"
ICML,2020,{FR} Train: A Mutual Information Based Approach to Fair and Robust Training,https://proceedings.mlr.press/v119/roh20a.html,"['Roh, Yuji', 'Lee, Kangwook', 'Whang, Steven', 'Suh, Changho']",46,"Trustworthy AI is a critical issue in machine learning where, in addition to training a model that is accurate, one must consider both fair and robust training in the presence of data bias and poisoning. However, the existing model fairness techniques mistakenly view poisoned data as an additional bias to be fixed, resulting in severe performance degradation. To address this problem, we propose FR-Train, which holistically performs fair and robust model training. We provide a mutual information-based interpretation of an existing adversarial training-based fairness-only method, and apply this idea to architect an additional discriminator that can identify poisoned data using a clean validation set and reduce its influence. In our experiments, FR-Train shows almost no decrease in fairness and accuracy in the presence of data poisoning by both mitigating the bias and defending against poisoning. We also demonstrate how to construct clean validation sets using crowdsourcing, and release new benchmark datasets.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v119-roh20a,
  title = 	 {{FR}-Train: A Mutual Information-Based Approach to Fair and Robust Training},
  author =       {Roh, Yuji and Lee, Kangwook and Whang, Steven and Suh, Changho},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {8147--8157},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/roh20a/roh20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/roh20a.html},
  abstract = 	 {Trustworthy AI is a critical issue in machine learning where, in addition to training a model that is accurate, one must consider both fair and robust training in the presence of data bias and poisoning. However, the existing model fairness techniques mistakenly view poisoned data as an additional bias to be fixed, resulting in severe performance degradation. To address this problem, we propose FR-Train, which holistically performs fair and robust model training. We provide a mutual information-based interpretation of an existing adversarial training-based fairness-only method, and apply this idea to architect an additional discriminator that can identify poisoned data using a clean validation set and reduce its influence. In our experiments, FR-Train shows almost no decrease in fairness and accuracy in the presence of data poisoning by both mitigating the bias and defending against poisoning. We also demonstrate how to construct clean validation sets using crowdsourcing, and release new benchmark datasets.}
}
"
ICML,2020,Balancing Competing Objectives with Noisy Data: Score Based Classifiers for Welfare Aware Machine Learning,https://proceedings.mlr.press/v119/rolf20a.html,"['Rolf, Esther', 'Simchowitz, Max', 'Dean, Sarah', 'Liu, Lydia T.', 'Bjorkegren, Daniel', 'Hardt, Moritz', 'Blumenstock, Joshua']",18,"While real-world decisions involve many competing objectives, algorithmic decisions are often evaluated with a single objective function. In this paper, we study algorithmic policies which explicitly trade off between a private objective (such as profit) and a public objective (such as social welfare). We analyze a natural class of policies which trace an empirical Pareto frontier based on learned scores, and focus on how such decisions can be made in noisy or data-limited regimes. Our theoretical results characterize the optimal strategies in this class, bound the Pareto errors due to inaccuracies in the scores, and show an equivalence between optimal strategies and a rich class of fairness-constrained profit-maximizing policies. We then present empirical results in two different contexts ‚Äî online content recommendation and sustainable abalone fisheries ‚Äî to underscore the generality of our approach to a wide range of practical decisions. Taken together, these results shed light on inherent trade-offs in using machine learning for decisions that impact social welfare.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v119-rolf20a,
  title = 	 {Balancing Competing Objectives with Noisy Data: Score-Based Classifiers for Welfare-Aware Machine Learning},
  author =       {Rolf, Esther and Simchowitz, Max and Dean, Sarah and Liu, Lydia T. and Bjorkegren, Daniel and Hardt, Moritz and Blumenstock, Joshua},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {8158--8168},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/rolf20a/rolf20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/rolf20a.html},
  abstract = 	 {While real-world decisions involve many competing objectives, algorithmic decisions are often evaluated with a single objective function. In this paper, we study algorithmic policies which explicitly trade off between a private objective (such as profit) and a public objective (such as social welfare). We analyze a natural class of policies which trace an empirical Pareto frontier based on learned scores, and focus on how such decisions can be made in noisy or data-limited regimes. Our theoretical results characterize the optimal strategies in this class, bound the Pareto errors due to inaccuracies in the scores, and show an equivalence between optimal strategies and a rich class of fairness-constrained profit-maximizing policies. We then present empirical results in two different contexts — online content recommendation and sustainable abalone fisheries — to underscore the generality of our approach to a wide range of practical decisions. Taken together, these results shed light on inherent trade-offs in using machine learning for decisions that impact social welfare.}
}
"
ICML,2020,Bounding the fairness and accuracy of classifiers from population statistics,https://proceedings.mlr.press/v119/sabato20a.html,"['Sabato, Sivan', 'Yom-Tov, Elad']",9,"We consider the study of a classification model whose properties are impossible to estimate using a validation set, either due to the absence of such a set or because access to the classifier, even as a black-box, is impossible. Instead, only aggregate statistics on the rate of positive predictions in each of several sub-populations are available, as well as the true rates of positive labels in each of these sub-populations. We show that these aggregate statistics can be used to lower-bound the discrepancy of a classifier, which is a measure that balances inaccuracy and unfairness. To this end, we define a new measure of unfairness, equal to the fraction of the population on which the classifier behaves differently, compared to its global, ideally fair behavior, as defined by the measure of equalized odds. We propose an efficient and practical procedure for finding the best possible lower bound on the discrepancy of the classifier, given the aggregate statistics, and demonstrate in experiments the empirical tightness of this lower bound, as well as its possible uses on various types of problems, ranging from estimating the quality of voting polls to measuring the effectiveness of patient identification from internet search queries. The code and data are available at https://github.com/sivansabato/bfa.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v119-sabato20a,
  title = 	 {Bounding the fairness and accuracy of classifiers from population statistics},
  author =       {Sabato, Sivan and Yom-Tov, Elad},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {8316--8325},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/sabato20a/sabato20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/sabato20a.html},
  abstract = 	 {We consider the study of a classification model whose properties are impossible to estimate using a validation set, either due to the absence of such a set or because access to the classifier, even as a black-box, is impossible. Instead, only aggregate statistics on the rate of positive predictions in each of several sub-populations are available, as well as the true rates of positive labels in each of these sub-populations. We show that these aggregate statistics can be used to lower-bound the discrepancy of a classifier, which is a measure that balances inaccuracy and unfairness. To this end, we define a new measure of unfairness, equal to the fraction of the population on which the classifier behaves differently, compared to its global, ideally fair behavior, as defined by the measure of equalized odds. We propose an efficient and practical procedure for finding the best possible lower bound on the discrepancy of the classifier, given the aggregate statistics, and demonstrate in experiments the empirical tightness of this lower bound, as well as its possible uses on various types of problems, ranging from estimating the quality of voting polls to measuring the effectiveness of patient identification from internet search queries. The code and data are available at https://github.com/sivansabato/bfa.}
}
"
ICML,2020,Measuring Non Expert Comprehension of Machine Learning Fairness Metrics,https://proceedings.mlr.press/v119/saha20c.html,"['Saha, Debjani', 'Schumann, Candice', 'Mcelfresh, Duncan', 'Dickerson, John', 'Mazurek, Michelle', 'Tschantz, Michael']",44,"Bias in machine learning has manifested injustice in several areas, such as medicine, hiring, and criminal justice. In response, computer scientists have developed myriad definitions of fairness to correct this bias in fielded algorithms. While some definitions are based on established legal and ethical norms, others are largely mathematical. It is unclear whether the general public agrees with these fairness definitions, and perhaps more importantly, whether they understand these definitions. We take initial steps toward bridging this gap between ML researchers and the public, by addressing the question: does a lay audience understand a basic definition of ML fairness? We develop a metric to measure comprehension of three such definitions‚Äìdemographic parity, equal opportunity, and equalized odds. We evaluate this metric using an online survey, and investigate the relationship between comprehension and sentiment, demographics, and the definition itself.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v119-saha20c,
  title = 	 {Measuring Non-Expert Comprehension of Machine Learning Fairness Metrics},
  author =       {Saha, Debjani and Schumann, Candice and Mcelfresh, Duncan and Dickerson, John and Mazurek, Michelle and Tschantz, Michael},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {8377--8387},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/saha20c/saha20c.pdf},
  url = 	 {https://proceedings.mlr.press/v119/saha20c.html},
  abstract = 	 {Bias in machine learning has manifested injustice in several areas, such as medicine, hiring, and criminal justice. In response, computer scientists have developed myriad definitions of fairness to correct this bias in fielded algorithms. While some definitions are based on established legal and ethical norms, others are largely mathematical. It is unclear whether the general public agrees with these fairness definitions, and perhaps more importantly, whether they understand these definitions. We take initial steps toward bridging this gap between ML researchers and the public, by addressing the question: does a lay audience understand a basic definition of ML fairness? We develop a metric to measure comprehension of three such definitions–demographic parity, equal opportunity, and equalized odds. We evaluate this metric using an online survey, and investigate the relationship between comprehension and sentiment, demographics, and the definition itself.}
}
"
ICML,2020,Learning Fair Policies in Multi Objective ({D}eep) Reinforcement Learning with Average and Discounted Rewards,https://proceedings.mlr.press/v119/siddique20a.html,"['Siddique, Umer', 'Weng, Paul', 'Zimmer, Matthieu']",30,"As the operations of autonomous systems generally affect simultaneously several users, it is crucial that their designs account for fairness considerations. In contrast to standard (deep) reinforcement learning (RL), we investigate the problem of learning a policy that treats its users equitably. In this paper, we formulate this novel RL problem, in which an objective function, which encodes a notion of fairness that we formally define, is optimized. For this problem, we provide a theoretical discussion where we examine the case of discounted rewards and that of average rewards. During this analysis, we notably derive a new result in the standard RL setting, which is of independent interest: it states a novel bound on the approximation error with respect to the optimal average reward of that of a policy optimal for the discounted reward. Since learning with discounted rewards is generally easier, this discussion further justifies finding a fair policy for the average reward by learning a fair policy for the discounted reward. Thus, we describe how several classic deep RL algorithms can be adapted to our fair optimization problem, and we validate our approach with extensive experiments in three different domains.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v119-siddique20a,
  title = 	 {Learning Fair Policies in Multi-Objective ({D}eep) Reinforcement Learning with Average and Discounted Rewards},
  author =       {Siddique, Umer and Weng, Paul and Zimmer, Matthieu},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {8905--8915},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/siddique20a/siddique20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/siddique20a.html},
  abstract = 	 {As the operations of autonomous systems generally affect simultaneously several users, it is crucial that their designs account for fairness considerations. In contrast to standard (deep) reinforcement learning (RL), we investigate the problem of learning a policy that treats its users equitably. In this paper, we formulate this novel RL problem, in which an objective function, which encodes a notion of fairness that we formally define, is optimized. For this problem, we provide a theoretical discussion where we examine the case of discounted rewards and that of average rewards. During this analysis, we notably derive a new result in the standard RL setting, which is of independent interest: it states a novel bound on the approximation error with respect to the optimal average reward of that of a policy optimal for the discounted reward. Since learning with discounted rewards is generally easier, this discussion further justifies finding a fair policy for the average reward by learning a fair policy for the discounted reward. Thus, we describe how several classic deep RL algorithms can be adapted to our fair optimization problem, and we validate our approach with extensive experiments in three different domains.}
}
"
ICML,2020,Collaborative Machine Learning with Incentive Aware Model Rewards,https://proceedings.mlr.press/v119/sim20a.html,"['Sim, Rachael Hwee Ling', 'Zhang, Yehong', 'Chan, Mun Choon', 'Low, Bryan Kian Hsiang']",55,"Collaborative machine learning (ML) is an appealing paradigm to build high-quality ML models by training on the aggregated data from many parties. However, these parties are only willing to share their data when given enough incentives, such as a guaranteed fair reward based on their contributions. This motivates the need for measuring a party‚Äôs contribution and designing an incentive-aware reward scheme accordingly. This paper proposes to value a party‚Äôs reward based on Shapley value and information gain on model parameters given its data. Subsequently, we give each party a model as a reward. To formally incentivize the collaboration, we define some desirable properties (e.g., fairness and stability) which are inspired by cooperative game theory but adapted for our model reward that is uniquely freely replicable. Then, we propose a novel model reward scheme to satisfy fairness and trade off between the desirable properties via an adjustable parameter. The value of each party‚Äôs model reward determined by our scheme is attained by injecting Gaussian noise to the aggregated training data with an optimized noise variance. We empirically demonstrate interesting properties of our scheme and evaluate its performance using synthetic and real-world datasets.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v119-sim20a,
  title = 	 {Collaborative Machine Learning with Incentive-Aware Model Rewards},
  author =       {Sim, Rachael Hwee Ling and Zhang, Yehong and Chan, Mun Choon and Low, Bryan Kian Hsiang},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {8927--8936},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/sim20a/sim20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/sim20a.html},
  abstract = 	 {Collaborative machine learning (ML) is an appealing paradigm to build high-quality ML models by training on the aggregated data from many parties. However, these parties are only willing to share their data when given enough incentives, such as a guaranteed fair reward based on their contributions. This motivates the need for measuring a party’s contribution and designing an incentive-aware reward scheme accordingly. This paper proposes to value a party’s reward based on Shapley value and information gain on model parameters given its data. Subsequently, we give each party a model as a reward. To formally incentivize the collaboration, we define some desirable properties (e.g., fairness and stability) which are inspired by cooperative game theory but adapted for our model reward that is uniquely freely replicable. Then, we propose a novel model reward scheme to satisfy fairness and trade off between the desirable properties via an adjustable parameter. The value of each party’s model reward determined by our scheme is attained by injecting Gaussian noise to the aggregated training data with an optimized noise variance. We empirically demonstrate interesting properties of our scheme and evaluate its performance using synthetic and real-world datasets.}
}
"
ICML,2020,Optimizer Benchmarking Needs to Account for Hyperparameter Tuning,https://proceedings.mlr.press/v119/sivaprasad20a.html,"['Sivaprasad, Prabhu Teja', 'Mai, Florian', 'Vogels, Thijs', 'Jaggi, Martin', 'Fleuret, Fran{\\c{c}}ois']",23,"The performance of optimizers, particularly in deep learning, depends considerably on their chosen hyperparameter configuration. The efficacy of optimizers is often studied under near-optimal problem-specific hyperparameters, and finding these settings may be prohibitively costly for practitioners. In this work, we argue that a fair assessment of optimizers‚Äô performance must take the computational cost of hyperparameter tuning into account, i.e., how easy it is to find good hyperparameter configurations using an automatic hyperparameter search. Evaluating a variety of optimizers on an extensive set of standard datasets and architectures, our results indicate that Adam is the most practical solution, particularly in low-budget scenarios.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v119-sivaprasad20a,
  title = 	 {Optimizer Benchmarking Needs to Account for Hyperparameter Tuning},
  author =       {Sivaprasad, Prabhu Teja and Mai, Florian and Vogels, Thijs and Jaggi, Martin and Fleuret, Fran{\c{c}}ois},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {9036--9045},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/sivaprasad20a/sivaprasad20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/sivaprasad20a.html},
  abstract = 	 {The performance of optimizers, particularly in deep learning, depends considerably on their chosen hyperparameter configuration. The efficacy of optimizers is often studied under near-optimal problem-specific hyperparameters, and finding these settings may be prohibitively costly for practitioners. In this work, we argue that a fair assessment of optimizers’ performance must take the computational cost of hyperparameter tuning into account, i.e., how easy it is to find good hyperparameter configurations using an automatic hyperparameter search. Evaluating a variety of optimizers on an extensive set of standard datasets and architectures, our results indicate that Adam is the most practical solution, particularly in low-budget scenarios.}
}
"
ICML,2020,Loss Function Search for Face Recognition,https://proceedings.mlr.press/v119/wang20t.html,"['Wang, Xiaobo', 'Wang, Shuo', 'Chi, Cheng', 'Zhang, Shifeng', 'Mei, Tao']",32,"In face recognition, designing margin-based (\emph{e.g.}, angular, additive, additive angular margins) softmax loss functions plays an important role to learn discriminative features. However, these hand-crafted heuristic methods may be sub-optimal because they require much effort to explore the large design space. Recently, an AutoML for loss function search method AM-LFS has been derived, which leverages reinforcement learning to search loss functions during the training process. But its search space is complex and unstable that hindering its superiority. In this paper, we first analyze that the key to enhance the feature discrimination is actually \textbf{how to reduce the softmax probability}. We then design a unified formulation for the current margin-based softmax losses. Accordingly, we define a novel search space and develop a reward-guided search method to automatically obtain the best candidate. Experimental results on a variety of face recognition benchmarks have demonstrated the effectiveness of our method over the state-of-the-art alternatives.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v119-wang20t,
  title = 	 {Loss Function Search for Face Recognition},
  author =       {Wang, Xiaobo and Wang, Shuo and Chi, Cheng and Zhang, Shifeng and Mei, Tao},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {10029--10038},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/wang20t/wang20t.pdf},
  url = 	 {https://proceedings.mlr.press/v119/wang20t.html},
  abstract = 	 {In face recognition, designing margin-based (\emph{e.g.}, angular, additive, additive angular margins) softmax loss functions plays an important role to learn discriminative features. However, these hand-crafted heuristic methods may be sub-optimal because they require much effort to explore the large design space. Recently, an AutoML for loss function search method AM-LFS has been derived, which leverages reinforcement learning to search loss functions during the training process. But its search space is complex and unstable that hindering its superiority. In this paper, we first analyze that the key to enhance the feature discrimination is actually \textbf{how to reduce the softmax probability}. We then design a unified formulation for the current margin-based softmax losses. Accordingly, we define a novel search space and develop a reward-guided search method to automatically obtain the best candidate. Experimental results on a variety of face recognition benchmarks have demonstrated the effectiveness of our method over the state-of-the-art alternatives.}
}
"
ICML,2020,Learning Structured Latent Factors from Dependent {D}ata:{A} Generative Model Framework from Information Theoretic Perspective,https://proceedings.mlr.press/v119/zhang20m.html,"['Zhang, Ruixiang', 'Koyama, Masanori', 'Ishiguro, Katsuhiko']",2,"Learning controllable and generalizable representation of multivariate data with desired structural properties remains a fundamental problem in machine learning. In this paper, we present a novel framework for learning generative models with various underlying structures in the latent space. Learning controllable and generalizable representation of multivariate data with desired structural properties remains a fundamental problem in machine learning. In this paper, we present a novel framework for learning generative models with various underlying structures in the latent space. We represent the inductive bias in the form of mask variables to model the dependency structure in the graphical model and extend the theory of multivariate information bottleneck (Friedman et al., 2001) to enforce it. Our model provides a principled approach to learn a set of semantically meaningful latent factors that reflect various types of desired structures like capturing correlation or encoding invariance, while also offering the flexibility to automatically estimate the dependency structure from data. We show that our framework unifies many existing generative models and can be applied to a variety of tasks, including multimodal data modeling, algorithmic fairness, and out-of-distribution generalization.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v119-zhang20m,
  title = 	 {Learning Structured Latent Factors from Dependent {D}ata:{A} Generative Model Framework from Information-Theoretic Perspective},
  author =       {Zhang, Ruixiang and Koyama, Masanori and Ishiguro, Katsuhiko},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {11141--11152},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/zhang20m/zhang20m.pdf},
  url = 	 {https://proceedings.mlr.press/v119/zhang20m.html},
  abstract = 	 {Learning controllable and generalizable representation of multivariate data with desired structural properties remains a fundamental problem in machine learning. In this paper, we present a novel framework for learning generative models with various underlying structures in the latent space. Learning controllable and generalizable representation of multivariate data with desired structural properties remains a fundamental problem in machine learning. In this paper, we present a novel framework for learning generative models with various underlying structures in the latent space. We represent the inductive bias in the form of mask variables to model the dependency structure in the graphical model and extend the theory of multivariate information bottleneck (Friedman et al., 2001) to enforce it. Our model provides a principled approach to learn a set of semantically meaningful latent factors that reflect various types of desired structures like capturing correlation or encoding invariance, while also offering the flexibility to automatically estimate the dependency structure from data. We show that our framework unifies many existing generative models and can be applied to a variety of tasks, including multimodal data modeling, algorithmic fairness, and out-of-distribution generalization.}
}
"
ICML,2020,Individual Calibration with Randomized Forecasting,https://proceedings.mlr.press/v119/zhao20e.html,"['Zhao, Shengjia', 'Ma, Tengyu', 'Ermon, Stefano']",39,"Machine learning applications often require calibrated predictions, e.g. a 90% credible interval should contain the true outcome 90% of the times. However, typical definitions of calibration only require this to hold on average, and offer no guarantees on predictions made on individual samples. Thus, predictions can be systematically over or under confident on certain subgroups, leading to issues of fairness and potential vulnerabilities. We show that calibration for individual samples is possible in the regression setup if and only if the predictions are randomized, i.e. outputting randomized credible intervals. Randomization removes systematic bias by trading off bias with variance. We design a training objective to enforce individual calibration and use it to train randomized regression functions. The resulting models are more calibrated for arbitrarily chosen subgroups of the data, and can achieve higher utility in decision making against adversaries that exploit miscalibrated predictions.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v119-zhao20e,
  title = 	 {Individual Calibration with Randomized Forecasting},
  author =       {Zhao, Shengjia and Ma, Tengyu and Ermon, Stefano},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {11387--11397},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/zhao20e/zhao20e.pdf},
  url = 	 {https://proceedings.mlr.press/v119/zhao20e.html},
  abstract = 	 {Machine learning applications often require calibrated predictions, e.g. a 90% credible interval should contain the true outcome 90% of the times. However, typical definitions of calibration only require this to hold on average, and offer no guarantees on predictions made on individual samples. Thus, predictions can be systematically over or under confident on certain subgroups, leading to issues of fairness and potential vulnerabilities. We show that calibration for individual samples is possible in the regression setup if and only if the predictions are randomized, i.e. outputting randomized credible intervals. Randomization removes systematic bias by trading off bias with variance. We design a training objective to enforce individual calibration and use it to train randomized regression functions. The resulting models are more calibrated for arbitrarily chosen subgroups of the data, and can achieve higher utility in decision making against adversaries that exploit miscalibrated predictions.}
}
"
ICML,2021,Regularized Online Allocation Problems: Fairness and Beyond,https://proceedings.mlr.press/v139/balseiro21a.html,"['Balseiro, Santiago', 'Lu, Haihao', 'Mirrokni, Vahab']",20,"Online allocation problems with resource constraints have a rich history in computer science and operations research. In this paper, we introduce the regularized online allocation problem, a variant that includes a non-linear regularizer acting on the total resource consumption. In this problem, requests repeatedly arrive over time and, for each request, a decision maker needs to take an action that generates a reward and consumes resources. The objective is to simultaneously maximize total rewards and the value of the regularizer subject to the resource constraints. Our primary motivation is the online allocation of internet advertisements wherein firms seek to maximize additive objectives such as the revenue or efficiency of the allocation. By introducing a regularizer, firms can account for the fairness of the allocation or, alternatively, punish under-delivery of advertisements‚Äîtwo common desiderata in internet advertising markets. We design an algorithm when arrivals are drawn independently from a distribution that is unknown to the decision maker. Our algorithm is simple, fast, and attains the optimal order of sub-linear regret compared to the optimal allocation with the benefit of hindsight. Numerical experiments confirm the effectiveness of the proposed algorithm and of the regularizers in an internet advertising application.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v139-balseiro21a,
  title = 	 {Regularized Online Allocation Problems: Fairness and Beyond},
  author =       {Balseiro, Santiago and Lu, Haihao and Mirrokni, Vahab},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {630--639},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/balseiro21a/balseiro21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/balseiro21a.html},
  abstract = 	 {Online allocation problems with resource constraints have a rich history in computer science and operations research. In this paper, we introduce the regularized online allocation problem, a variant that includes a non-linear regularizer acting on the total resource consumption. In this problem, requests repeatedly arrive over time and, for each request, a decision maker needs to take an action that generates a reward and consumes resources. The objective is to simultaneously maximize total rewards and the value of the regularizer subject to the resource constraints. Our primary motivation is the online allocation of internet advertisements wherein firms seek to maximize additive objectives such as the revenue or efficiency of the allocation. By introducing a regularizer, firms can account for the fairness of the allocation or, alternatively, punish under-delivery of advertisements—two common desiderata in internet advertising markets. We design an algorithm when arrivals are drawn independently from a distribution that is unknown to the decision maker. Our algorithm is simple, fast, and attains the optimal order of sub-linear regret compared to the optimal allocation with the benefit of hindsight. Numerical experiments confirm the effectiveness of the proposed algorithm and of the regularizers in an internet advertising application.}
}
"
ICML,2021,Model Distillation for Revenue Optimization: Interpretable Personalized Pricing,https://proceedings.mlr.press/v139/biggs21a.html,"['Biggs, Max', 'Sun, Wei', 'Ettl, Markus']",15,"Data-driven pricing strategies are becoming increasingly common, where customers are offered a personalized price based on features that are predictive of their valuation of a product. It is desirable for this pricing policy to be simple and interpretable, so it can be verified, checked for fairness, and easily implemented. However, efforts to incorporate machine learning into a pricing framework often lead to complex pricing policies that are not interpretable, resulting in slow adoption in practice. We present a novel, customized, prescriptive tree-based algorithm that distills knowledge from a complex black-box machine learning algorithm, segments customers with similar valuations and prescribes prices in such a way that maximizes revenue while maintaining interpretability. We quantify the regret of a resulting policy and demonstrate its efficacy in applications with both synthetic and real-world datasets.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v139-biggs21a,
  title = 	 {Model Distillation for Revenue Optimization: Interpretable Personalized Pricing},
  author =       {Biggs, Max and Sun, Wei and Ettl, Markus},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {946--956},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/biggs21a/biggs21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/biggs21a.html},
  abstract = 	 {Data-driven pricing strategies are becoming increasingly common, where customers are offered a personalized price based on features that are predictive of their valuation of a product. It is desirable for this pricing policy to be simple and interpretable, so it can be verified, checked for fairness, and easily implemented. However, efforts to incorporate machine learning into a pricing framework often lead to complex pricing policies that are not interpretable, resulting in slow adoption in practice. We present a novel, customized, prescriptive tree-based algorithm that distills knowledge from a complex black-box machine learning algorithm, segments customers with similar valuations and prescribes prices in such a way that maximizes revenue while maintaining interpretability. We quantify the regret of a resulting policy and demonstrate its efficacy in applications with both synthetic and real-world datasets.}
}
"
ICML,2021,Fair Classification with Noisy Protected Attributes: A Framework with Provable Guarantees,https://proceedings.mlr.press/v139/celis21a.html,"['Celis, L. Elisa', 'Huang, Lingxiao', 'Keswani, Vijay', 'Vishnoi, Nisheeth K.']",28,"We present an optimization framework for learning a fair classifier in the presence of noisy perturbations in the protected attributes. Compared to prior work, our framework can be employed with a very general class of linear and linear-fractional fairness constraints, can handle multiple, non-binary protected attributes, and outputs a classifier that comes with provable guarantees on both accuracy and fairness. Empirically, we show that our framework can be used to attain either statistical rate or false positive rate fairness guarantees with a minimal loss in accuracy, even when the noise is large, in two real-world datasets.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v139-celis21a,
  title = 	 {Fair Classification with Noisy Protected Attributes: A Framework with Provable Guarantees},
  author =       {Celis, L. Elisa and Huang, Lingxiao and Keswani, Vijay and Vishnoi, Nisheeth K.},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {1349--1361},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/celis21a/celis21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/celis21a.html},
  abstract = 	 {We present an optimization framework for learning a fair classifier in the presence of noisy perturbations in the protected attributes. Compared to prior work, our framework can be employed with a very general class of linear and linear-fractional fairness constraints, can handle multiple, non-binary protected attributes, and outputs a classifier that comes with provable guarantees on both accuracy and fairness. Empirically, we show that our framework can be used to attain either statistical rate or false positive rate fairness guarantees with a minimal loss in accuracy, even when the noise is large, in two real-world datasets.}
}
"
ICML,2021,Understanding and Mitigating Accuracy Disparity in Regression,https://proceedings.mlr.press/v139/chi21a.html,"['Chi, Jianfeng', 'Tian, Yuan', 'Gordon, Geoffrey J.', 'Zhao, Han']",10,"With the widespread deployment of large-scale prediction systems in high-stakes domains, e.g., face recognition, criminal justice, etc., disparity on prediction accuracy between different demographic subgroups has called for fundamental understanding on the source of such disparity and algorithmic intervention to mitigate it. In this paper, we study the accuracy disparity problem in regression. To begin with, we first propose an error decomposition theorem, which decomposes the accuracy disparity into the distance between marginal label distributions and the distance between conditional representations, to help explain why such accuracy disparity appears in practice. Motivated by this error decomposition and the general idea of distribution alignment with statistical distances, we then propose an algorithm to reduce this disparity, and analyze its game-theoretic optima of the proposed objective functions. To corroborate our theoretical findings, we also conduct experiments on five benchmark datasets. The experimental results suggest that our proposed algorithms can effectively mitigate accuracy disparity while maintaining the predictive power of the regression models.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v139-chi21a,
  title = 	 {Understanding and Mitigating Accuracy Disparity in Regression},
  author =       {Chi, Jianfeng and Tian, Yuan and Gordon, Geoffrey J. and Zhao, Han},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {1866--1876},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/chi21a/chi21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/chi21a.html},
  abstract = 	 {With the widespread deployment of large-scale prediction systems in high-stakes domains, e.g., face recognition, criminal justice, etc., disparity on prediction accuracy between different demographic subgroups has called for fundamental understanding on the source of such disparity and algorithmic intervention to mitigate it. In this paper, we study the accuracy disparity problem in regression. To begin with, we first propose an error decomposition theorem, which decomposes the accuracy disparity into the distance between marginal label distributions and the distance between conditional representations, to help explain why such accuracy disparity appears in practice. Motivated by this error decomposition and the general idea of distribution alignment with statistical distances, we then propose an algorithm to reduce this disparity, and analyze its game-theoretic optima of the proposed objective functions. To corroborate our theoretical findings, we also conduct experiments on five benchmark datasets. The experimental results suggest that our proposed algorithms can effectively mitigate accuracy disparity while maintaining the predictive power of the regression models.}
}
"
ICML,2021,Fairness and Bias in Online Selection,https://proceedings.mlr.press/v139/correa21a.html,"['Correa, Jose', 'Cristi, Andres', 'Duetting, Paul', 'Norouzi-Fard, Ashkan']",3,"There is growing awareness and concern about fairness in machine learning and algorithm design. This is particularly true in online selection problems where decisions are often biased, for example, when assessing credit risks or hiring staff. We address the issues of fairness and bias in online selection by introducing multi-color versions of the classic secretary and prophet problem. Interestingly, existing algorithms for these problems are either very unfair or very inefficient, so we develop optimal fair algorithms for these new problems and provide tight bounds on their competitiveness. We validate our theoretical findings on real-world data.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v139-correa21a,
  title = 	 {Fairness and Bias in Online Selection},
  author =       {Correa, Jose and Cristi, Andres and Duetting, Paul and Norouzi-Fard, Ashkan},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {2112--2121},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/correa21a/correa21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/correa21a.html},
  abstract = 	 {There is growing awareness and concern about fairness in machine learning and algorithm design. This is particularly true in online selection problems where decisions are often biased, for example, when assessing credit risks or hiring staff. We address the issues of fairness and bias in online selection by introducing multi-color versions of the classic secretary and prophet problem. Interestingly, existing algorithms for these problems are either very unfair or very inefficient, so we develop optimal fair algorithms for these new problems and provide tight bounds on their competitiveness. We validate our theoretical findings on real-world data.}
}
"
ICML,2021,Characterizing Fairness Over the Set of Good Models Under Selective Labels,https://proceedings.mlr.press/v139/coston21a.html,"['Coston, Amanda', 'Rambachan, Ashesh', 'Chouldechova, Alexandra']",30,"Algorithmic risk assessments are used to inform decisions in a wide variety of high-stakes settings. Often multiple predictive models deliver similar overall performance but differ markedly in their predictions for individual cases, an empirical phenomenon known as the ‚ÄúRashomon Effect.‚Äù These models may have different properties over various groups, and therefore have different predictive fairness properties. We develop a framework for characterizing predictive fairness properties over the set of models that deliver similar overall performance, or ‚Äúthe set of good models.‚Äù Our framework addresses the empirically relevant challenge of selectively labelled data in the setting where the selection decision and outcome are unconfounded given the observed data features. Our framework can be used to 1) audit for predictive bias; or 2) replace an existing model with one that has better fairness properties. We illustrate these use cases on a recidivism prediction task and a real-world credit-scoring task.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v139-coston21a,
  title =          {Characterizing Fairness Over the Set of Good Models Under Selective Labels},
  author =       {Coston, Amanda and Rambachan, Ashesh and Chouldechova, Alexandra},
  booktitle =          {Proceedings of the 38th International Conference on Machine Learning},
  pages =          {2144--2155},
  year =          {2021},
  editor =          {Meila, Marina and Zhang, Tong},
  volume =          {139},
  series =          {Proceedings of Machine Learning Research},
  month =          {18--24 Jul},
  publisher =    {PMLR},
  pdf =          {http://proceedings.mlr.press/v139/coston21a/coston21a.pdf},
  url =          {https://proceedings.mlr.press/v139/coston21a.html},
  abstract =          {Algorithmic risk assessments are used to inform decisions in a wide variety of high-stakes settings. Often multiple predictive models deliver similar overall performance but differ markedly in their predictions for individual cases, an empirical phenomenon known as the “Rashomon Effect.” These models may have different properties over various groups, and therefore have different predictive fairness properties. We develop a framework for characterizing predictive fairness properties over the set of models that deliver similar overall performance, or “the set of good models.” Our framework addresses the empirically relevant challenge of selectively labelled data in the setting where the selection decision and outcome are unconfounded given the observed data features. Our framework can be used to 1) audit for predictive bias; or 2) replace an existing model with one that has better fairness properties. We illustrate these use cases on a recidivism prediction task and a real-world credit-scoring task.}
}
"
ICML,2021,Environment Inference for Invariant Learning,https://proceedings.mlr.press/v139/creager21a.html,"['Creager, Elliot', 'Jacobsen, Joern-Henrik', 'Zemel, Richard']",131,"Learning models that gracefully handle distribution shifts is central to research on domain generalization, robust optimization, and fairness. A promising formulation is domain-invariant learning, which identifies the key issue of learning which features are domain-specific versus domain-invariant. An important assumption in this area is that the training examples are partitioned into ‚Äúdomains‚Äù or ‚Äúenvironments‚Äù. Our focus is on the more common setting where such partitions are not provided. We propose EIIL, a general framework for domain-invariant learning that incorporates Environment Inference to directly infer partitions that are maximally informative for downstream Invariant Learning. We show that EIIL outperforms invariant learning methods on the CMNIST benchmark without using environment labels, and significantly outperforms ERM on worst-group performance in the Waterbirds dataset. Finally, we establish connections between EIIL and algorithmic fairness, which enables EIIL to improve accuracy and calibration in a fair prediction problem.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v139-creager21a,
  title = 	 {Environment Inference for Invariant Learning},
  author =       {Creager, Elliot and Jacobsen, Joern-Henrik and Zemel, Richard},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {2189--2200},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/creager21a/creager21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/creager21a.html},
  abstract = 	 {Learning models that gracefully handle distribution shifts is central to research on domain generalization, robust optimization, and fairness. A promising formulation is domain-invariant learning, which identifies the key issue of learning which features are domain-specific versus domain-invariant. An important assumption in this area is that the training examples are partitioned into “domains” or “environments”. Our focus is on the more common setting where such partitions are not provided. We propose EIIL, a general framework for domain-invariant learning that incorporates Environment Inference to directly infer partitions that are maximally informative for downstream Invariant Learning. We show that EIIL outperforms invariant learning methods on the CMNIST benchmark without using environment labels, and significantly outperforms ERM on worst-group performance in the Waterbirds dataset. Finally, we establish connections between EIIL and algorithmic fairness, which enables EIIL to improve accuracy and calibration in a fair prediction problem.}
}
"
ICML,2021,Versatile Verification of Tree Ensembles,https://proceedings.mlr.press/v139/devos21a.html,"['Devos, Laurens', 'Meert, Wannes', 'Davis, Jesse']",7,"Machine learned models often must abide by certain requirements (e.g., fairness or legal). This has spurred interested in developing approaches that can provably verify whether a model satisfies certain properties. This paper introduces a generic algorithm called Veritas that enables tackling multiple different verification tasks for tree ensemble models like random forests (RFs) and gradient boosted decision trees (GBDTs). This generality contrasts with previous work, which has focused exclusively on either adversarial example generation or robustness checking. Veritas formulates the verification task as a generic optimization problem and introduces a novel search space representation. Veritas offers two key advantages. First, it provides anytime lower and upper bounds when the optimization problem cannot be solved exactly. In contrast, many existing methods have focused on exact solutions and are thus limited by the verification problem being NP-complete. Second, Veritas produces full (bounded suboptimal) solutions that can be used to generate concrete examples. We experimentally show that our method produces state-of-the-art robustness estimates, especially when executed with strict time constraints. This is exceedingly important when checking the robustness of large datasets. Additionally, we show that Veritas enables tackling more real-world verification scenarios.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v139-devos21a,
  title = 	 {Versatile Verification of Tree Ensembles},
  author =       {Devos, Laurens and Meert, Wannes and Davis, Jesse},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {2654--2664},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/devos21a/devos21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/devos21a.html},
  abstract = 	 {Machine learned models often must abide by certain requirements (e.g., fairness or legal). This has spurred interested in developing approaches that can provably verify whether a model satisfies certain properties. This paper introduces a generic algorithm called Veritas that enables tackling multiple different verification tasks for tree ensemble models like random forests (RFs) and gradient boosted decision trees (GBDTs). This generality contrasts with previous work, which has focused exclusively on either adversarial example generation or robustness checking. Veritas formulates the verification task as a generic optimization problem and introduces a novel search space representation. Veritas offers two key advantages. First, it provides anytime lower and upper bounds when the optimization problem cannot be solved exactly. In contrast, many existing methods have focused on exact solutions and are thus limited by the verification problem being NP-complete. Second, Veritas produces full (bounded suboptimal) solutions that can be used to generate concrete examples. We experimentally show that our method produces state-of-the-art robustness estimates, especially when executed with strict time constraints. This is exceedingly important when checking the robustness of large datasets. Additionally, we show that Veritas enables tackling more real-world verification scenarios.}
}
"
ICML,2021,Whitening for Self Supervised Representation Learning,https://proceedings.mlr.press/v139/ermolov21a.html,"['Ermolov, Aleksandr', 'Siarohin, Aliaksandr', 'Sangineto, Enver', 'Sebe, Nicu']",127,"Most of the current self-supervised representation learning (SSL) methods are based on the contrastive loss and the instance-discrimination task, where augmented versions of the same image instance (""positives"") are contrasted with instances extracted from other images (""negatives""). For the learning to be effective, many negatives should be compared with a positive pair, which is computationally demanding. In this paper, we propose a different direction and a new loss function for SSL, which is based on the whitening of the latent-space features. The whitening operation has a ""scattering"" effect on the batch samples, avoiding degenerate solutions where all the sample representations collapse to a single point. Our solution does not require asymmetric networks and it is conceptually simple. Moreover, since negatives are not needed, we can extract multiple positive pairs from the same image instance. The source code of the method and of all the experiments is available at: https://github.com/htdt/self-supervised.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v139-ermolov21a,
  title = 	 {Whitening for Self-Supervised Representation Learning},
  author =       {Ermolov, Aleksandr and Siarohin, Aliaksandr and Sangineto, Enver and Sebe, Nicu},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {3015--3024},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/ermolov21a/ermolov21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/ermolov21a.html},
  abstract = 	 {Most of the current self-supervised representation learning (SSL) methods are based on the contrastive loss and the instance-discrimination task, where augmented versions of the same image instance (""positives"") are contrasted with instances extracted from other images (""negatives""). For the learning to be effective, many negatives should be compared with a positive pair, which is computationally demanding. In this paper, we propose a different direction and a new loss function for SSL, which is based on the whitening of the latent-space features. The whitening operation has a ""scattering"" effect on the batch samples, avoiding degenerate solutions where all the sample representations collapse to a single point. Our solution does not require asymmetric networks and it is conceptually simple. Moreover, since negatives are not needed, we can extract multiple positive pairs from the same image instance. The source code of the method and of all the experiments is available at: https://github.com/htdt/self-supervised.}
}
"
ICML,2021,On the Problem of Underranking in Group Fair Ranking,https://proceedings.mlr.press/v139/gorantla21a.html,"['Gorantla, Sruthi', 'Deshpande, Amit', 'Louis, Anand']",7,"Bias in ranking systems, especially among the top ranks, can worsen social and economic inequalities, polarize opinions, and reinforce stereotypes. On the other hand, a bias correction for minority groups can cause more harm if perceived as favoring group-fair outcomes over meritocracy. Most group-fair ranking algorithms post-process a given ranking and output a group-fair ranking. In this paper, we formulate the problem of underranking in group-fair rankings based on how close the group-fair rank of each item is to its original rank, and prove a lower bound on the trade-off achievable for simultaneous underranking and group fairness in ranking. We give a fair ranking algorithm that takes any given ranking and outputs another ranking with simultaneous underranking and group fairness guarantees comparable to the lower bound we prove. Our experimental results confirm the theoretical trade-off between underranking and group fairness, and also show that our algorithm achieves the best of both when compared to the state-of-the-art baselines.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v139-gorantla21a,
  title = 	 {On the Problem of Underranking in Group-Fair Ranking},
  author =       {Gorantla, Sruthi and Deshpande, Amit and Louis, Anand},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {3777--3787},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/gorantla21a/gorantla21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/gorantla21a.html},
  abstract = 	 {Bias in ranking systems, especially among the top ranks, can worsen social and economic inequalities, polarize opinions, and reinforce stereotypes. On the other hand, a bias correction for minority groups can cause more harm if perceived as favoring group-fair outcomes over meritocracy. Most group-fair ranking algorithms post-process a given ranking and output a group-fair ranking. In this paper, we formulate the problem of underranking in group-fair rankings based on how close the group-fair rank of each item is to its original rank, and prove a lower bound on the trade-off achievable for simultaneous underranking and group fairness in ranking. We give a fair ranking algorithm that takes any given ranking and outputs another ranking with simultaneous underranking and group fairness guarantees comparable to the lower bound we prove. Our experimental results confirm the theoretical trade-off between underranking and group fairness, and also show that our algorithm achieves the best of both when compared to the state-of-the-art baselines.}
}
"
ICML,2021,Adversarial Policy Learning in Two player Competitive Games,https://proceedings.mlr.press/v139/guo21b.html,"['Guo, Wenbo', 'Wu, Xian', 'Huang, Sui', 'Xing, Xinyu']",11,"In a two-player deep reinforcement learning task, recent work shows an attacker could learn an adversarial policy that triggers a target agent to perform poorly and even react in an undesired way. However, its efficacy heavily relies upon the zero-sum assumption made in the two-player game. In this work, we propose a new adversarial learning algorithm. It addresses the problem by resetting the optimization goal in the learning process and designing a new surrogate optimization function. Our experiments show that our method significantly improves adversarial agents‚Äô exploitability compared with the state-of-art attack. Besides, we also discover that our method could augment an agent with the ability to abuse the target game‚Äôs unfairness. Finally, we show that agents adversarially re-trained against our adversarial agents could obtain stronger adversary-resistance.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v139-guo21b,
  title = 	 {Adversarial Policy Learning in Two-player Competitive Games},
  author =       {Guo, Wenbo and Wu, Xian and Huang, Sui and Xing, Xinyu},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {3910--3919},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/guo21b/guo21b.pdf},
  url = 	 {https://proceedings.mlr.press/v139/guo21b.html},
  abstract = 	 {In a two-player deep reinforcement learning task, recent work shows an attacker could learn an adversarial policy that triggers a target agent to perform poorly and even react in an undesired way. However, its efficacy heavily relies upon the zero-sum assumption made in the two-player game. In this work, we propose a new adversarial learning algorithm. It addresses the problem by resetting the optimization goal in the learning process and designing a new surrogate optimization function. Our experiments show that our method significantly improves adversarial agents’ exploitability compared with the state-of-art attack. Besides, we also discover that our method could augment an agent with the ability to abuse the target game’s unfairness. Finally, we show that agents adversarially re-trained against our adversarial agents could obtain stronger adversary-resistance.}
}
"
ICML,2021,Optimizing Black box Metrics with Iterative Example Weighting,https://proceedings.mlr.press/v139/hiranandani21a.html,"['Hiranandani, Gaurush', 'Mathur, Jatin', 'Narasimhan, Harikrishna', 'Fard, Mahdi Milani', 'Koyejo, Sanmi']",3,"We consider learning to optimize a classification metric defined by a black-box function of the confusion matrix. Such black-box learning settings are ubiquitous, for example, when the learner only has query access to the metric of interest, or in noisy-label and domain adaptation applications where the learner must evaluate the metric via performance evaluation using a small validation sample. Our approach is to adaptively learn example weights on the training dataset such that the resulting weighted objective best approximates the metric on the validation sample. We show how to model and estimate the example weights and use them to iteratively post-shift a pre-trained class probability estimator to construct a classifier. We also analyze the resulting procedure‚Äôs statistical properties. Experiments on various label noise, domain shift, and fair classification setups confirm that our proposal compares favorably to the state-of-the-art baselines for each application.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v139-hiranandani21a,
  title = 	 {Optimizing Black-box Metrics with Iterative Example Weighting},
  author =       {Hiranandani, Gaurush and Mathur, Jatin and Narasimhan, Harikrishna and Fard, Mahdi Milani and Koyejo, Sanmi},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {4239--4249},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/hiranandani21a/hiranandani21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/hiranandani21a.html},
  abstract = 	 {We consider learning to optimize a classification metric defined by a black-box function of the confusion matrix. Such black-box learning settings are ubiquitous, for example, when the learner only has query access to the metric of interest, or in noisy-label and domain adaptation applications where the learner must evaluate the metric via performance evaluation using a small validation sample. Our approach is to adaptively learn example weights on the training dataset such that the resulting weighted objective best approximates the metric on the validation sample. We show how to model and estimate the example weights and use them to iteratively post-shift a pre-trained class probability estimator to construct a classifier. We also analyze the resulting procedure’s statistical properties. Experiments on various label noise, domain shift, and fair classification setups confirm that our proposal compares favorably to the state-of-the-art baselines for each application.}
}
"
ICML,2021,Fairness for Image Generation with Uncertain Sensitive Attributes,https://proceedings.mlr.press/v139/jalal21b.html,"['Jalal, Ajil', 'Karmalkar, Sushrut', 'Hoffmann, Jessica', 'Dimakis, Alex', 'Price, Eric']",15,"This work tackles the issue of fairness in the context of generative procedures, such as image super-resolution, which entail different definitions from the standard classification setting. Moreover, while traditional group fairness definitions are typically defined with respect to specified protected groups ‚Äì camouflaging the fact that these groupings are artificial and carry historical and political motivations ‚Äì we emphasize that there are no ground truth identities. For instance, should South and East Asians be viewed as a single group or separate groups? Should we consider one race as a whole or further split by gender? Choosing which groups are valid and who belongs in them is an impossible dilemma and being ‚Äúfair‚Äù with respect to Asians may require being ‚Äúunfair‚Äù with respect to South Asians. This motivates the introduction of definitions that allow algorithms to be \emph{oblivious} to the relevant groupings. We define several intuitive notions of group fairness and study their incompatibilities and trade-offs. We show that the natural extension of demographic parity is strongly dependent on the grouping, and \emph{impossible} to achieve obliviously. On the other hand, the conceptually new definition we introduce, Conditional Proportional Representation, can be achieved obliviously through Posterior Sampling. Our experiments validate our theoretical results and achieve fair image reconstruction using state-of-the-art generative models.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v139-jalal21b,
  title = 	 {Fairness for Image Generation with Uncertain Sensitive Attributes},
  author =       {Jalal, Ajil and Karmalkar, Sushrut and Hoffmann, Jessica and Dimakis, Alex and Price, Eric},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {4721--4732},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/jalal21b/jalal21b.pdf},
  url = 	 {https://proceedings.mlr.press/v139/jalal21b.html},
  abstract = 	 {This work tackles the issue of fairness in the context of generative procedures, such as image super-resolution, which entail different definitions from the standard classification setting. Moreover, while traditional group fairness definitions are typically defined with respect to specified protected groups – camouflaging the fact that these groupings are artificial and carry historical and political motivations – we emphasize that there are no ground truth identities. For instance, should South and East Asians be viewed as a single group or separate groups? Should we consider one race as a whole or further split by gender? Choosing which groups are valid and who belongs in them is an impossible dilemma and being “fair” with respect to Asians may require being “unfair” with respect to South Asians. This motivates the introduction of definitions that allow algorithms to be \emph{oblivious} to the relevant groupings. We define several intuitive notions of group fairness and study their incompatibilities and trade-offs. We show that the natural extension of demographic parity is strongly dependent on the grouping, and \emph{impossible} to achieve obliviously. On the other hand, the conceptually new definition we introduce, Conditional Proportional Representation, can be achieved obliviously through Posterior Sampling. Our experiments validate our theoretical results and achieve fair image reconstruction using state-of-the-art generative models.}
}
"
ICML,2021,MorphVAE: Generating Neural Morphologies from 3D Walks using a Variational Autoencoder with Spherical Latent Space,https://proceedings.mlr.press/v139/laturnus21a.html,"['Laturnus, Sophie C.', 'Berens, Philipp']",2,"For the past century, the anatomy of a neuron has been considered one of its defining features: The shape of a neuron‚Äôs dendrites and axon fundamentally determines what other neurons it can connect to. These neurites have been described using mathematical tools e.g. in the context of cell type classification, but generative models of these structures have only rarely been proposed and are often computationally inefficient. Here we propose MorphVAE, a sequence-to-sequence variational autoencoder with spherical latent space as a generative model for neural morphologies. The model operates on walks within the tree structure of a neuron and can incorporate expert annotations on a subset of the data using semi-supervised learning. We develop our model on artificially generated toy data and evaluate its performance on dendrites of excitatory cells and axons of inhibitory cells of mouse motor cortex (M1) and dendrites of retinal ganglion cells. We show that the learned latent feature space allows for better cell type discrimination than other commonly used features. By sampling new walks from the latent space we can easily construct new morphologies with a specified degree of similarity to their reference neuron, providing an efficient generative model for neural morphologies.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v139-laturnus21a,
  title = 	 {MorphVAE: Generating Neural Morphologies from 3D-Walks using a Variational Autoencoder with Spherical Latent Space},
  author =       {Laturnus, Sophie C. and Berens, Philipp},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {6021--6031},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/laturnus21a/laturnus21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/laturnus21a.html},
  abstract = 	 {For the past century, the anatomy of a neuron has been considered one of its defining features: The shape of a neuron’s dendrites and axon fundamentally determines what other neurons it can connect to. These neurites have been described using mathematical tools e.g. in the context of cell type classification, but generative models of these structures have only rarely been proposed and are often computationally inefficient. Here we propose MorphVAE, a sequence-to-sequence variational autoencoder with spherical latent space as a generative model for neural morphologies. The model operates on walks within the tree structure of a neuron and can incorporate expert annotations on a subset of the data using semi-supervised learning. We develop our model on artificially generated toy data and evaluate its performance on dendrites of excitatory cells and axons of inhibitory cells of mouse motor cortex (M1) and dendrites of retinal ganglion cells. We show that the learned latent feature space allows for better cell type discrimination than other commonly used features. By sampling new walks from the latent space we can easily construct new morphologies with a specified degree of similarity to their reference neuron, providing an efficient generative model for neural morphologies.}
}
"
ICML,2021,Fair Selective Classification Via Sufficiency,https://proceedings.mlr.press/v139/lee21b.html,"['Lee, Joshua K', 'Bu, Yuheng', 'Rajan, Deepta', 'Sattigeri, Prasanna', 'Panda, Rameswar', 'Das, Subhro', 'Wornell, Gregory W']",4,"Selective classification is a powerful tool for decision-making in scenarios where mistakes are costly but abstentions are allowed. In general, by allowing a classifier to abstain, one can improve the performance of a model at the cost of reducing coverage and classifying fewer samples. However, recent work has shown, in some cases, that selective classification can magnify disparities between groups, and has illustrated this phenomenon on multiple real-world datasets. We prove that the sufficiency criterion can be used to mitigate these disparities by ensuring that selective classification increases performance on all groups, and introduce a method for mitigating the disparity in precision across the entire coverage scale based on this criterion. We then provide an upper bound on the conditional mutual information between the class label and sensitive attribute, conditioned on the learned features, which can be used as a regularizer to achieve fairer selective classification. The effectiveness of the method is demonstrated on the Adult, CelebA, Civil Comments, and CheXpert datasets.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v139-lee21b,
  title = 	 {Fair Selective Classification Via Sufficiency},
  author =       {Lee, Joshua K and Bu, Yuheng and Rajan, Deepta and Sattigeri, Prasanna and Panda, Rameswar and Das, Subhro and Wornell, Gregory W},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {6076--6086},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/lee21b/lee21b.pdf},
  url = 	 {https://proceedings.mlr.press/v139/lee21b.html},
  abstract = 	 {Selective classification is a powerful tool for decision-making in scenarios where mistakes are costly but abstentions are allowed. In general, by allowing a classifier to abstain, one can improve the performance of a model at the cost of reducing coverage and classifying fewer samples. However, recent work has shown, in some cases, that selective classification can magnify disparities between groups, and has illustrated this phenomenon on multiple real-world datasets. We prove that the sufficiency criterion can be used to mitigate these disparities by ensuring that selective classification increases performance on all groups, and introduce a method for mitigating the disparity in precision across the entire coverage scale based on this criterion. We then provide an upper bound on the conditional mutual information between the class label and sensitive attribute, conditioned on the learned features, which can be used as a regularizer to achieve fairer selective classification. The effectiveness of the method is demonstrated on the Adult, CelebA, Civil Comments, and CheXpert datasets.}
}
"
ICML,2021,Theory of Spectral Method for Union of Subspaces Based Random Geometry Graph,https://proceedings.mlr.press/v139/li21f.html,"['Li, Gen', 'Gu, Yuantao']",2,"Spectral method is a commonly used scheme to cluster data points lying close to Union of Subspaces, a task known as Subspace Clustering. The typical usage is to construct a Random Geometry Graph first and then apply spectral method to the graph to obtain clustering result. The latter step has been coined the name Spectral Clustering. As far as we know, in spite of the significance of both steps in spectral-method-based Subspace Clustering, all existing theoretical results focus on the first step of constructing the graph, but ignore the final step to correct false connections through spectral clustering. This paper establishes a theory to show the power of this method for the first time, in which we demonstrate the mechanism of spectral clustering by analyzing a simplified algorithm under the widely used semi-random model. Based on this theory, we prove the efficiency of Subspace Clustering in fairly broad conditions. The insights and analysis techniques developed in this paper might also have implications for other random graph problems.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v139-li21f,
  title = 	 {Theory of Spectral Method for Union of Subspaces-Based Random Geometry Graph},
  author =       {Li, Gen and Gu, Yuantao},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {6337--6345},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/li21f/li21f.pdf},
  url = 	 {https://proceedings.mlr.press/v139/li21f.html},
  abstract = 	 {Spectral method is a commonly used scheme to cluster data points lying close to Union of Subspaces, a task known as Subspace Clustering. The typical usage is to construct a Random Geometry Graph first and then apply spectral method to the graph to obtain clustering result. The latter step has been coined the name Spectral Clustering. As far as we know, in spite of the significance of both steps in spectral-method-based Subspace Clustering, all existing theoretical results focus on the first step of constructing the graph, but ignore the final step to correct false connections through spectral clustering. This paper establishes a theory to show the power of this method for the first time, in which we demonstrate the mechanism of spectral clustering by analyzing a simplified algorithm under the widely used semi-random model. Based on this theory, we prove the efficiency of Subspace Clustering in fairly broad conditions. The insights and analysis techniques developed in this paper might also have implications for other random graph problems.}
}
"
ICML,2021,Ditto: Fair and Robust Federated Learning Through Personalization,https://proceedings.mlr.press/v139/li21h.html,"['Li, Tian', 'Hu, Shengyuan', 'Beirami, Ahmad', 'Smith, Virginia']",217,"Fairness and robustness are two important concerns for federated learning systems. In this work, we identify that robustness to data and model poisoning attacks and fairness, measured as the uniformity of performance across devices, are competing constraints in statistically heterogeneous networks. To address these constraints, we propose employing a simple, general framework for personalized federated learning, Ditto, that can inherently provide fairness and robustness benefits, and develop a scalable solver for it. Theoretically, we analyze the ability of Ditto to achieve fairness and robustness simultaneously on a class of linear problems. Empirically, across a suite of federated datasets, we show that Ditto not only achieves competitive performance relative to recent personalization methods, but also enables more accurate, robust, and fair models relative to state-of-the-art fair or robust baselines.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v139-li21h,
  title = 	 {Ditto: Fair and Robust Federated Learning Through Personalization},
  author =       {Li, Tian and Hu, Shengyuan and Beirami, Ahmad and Smith, Virginia},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {6357--6368},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/li21h/li21h.pdf},
  url = 	 {https://proceedings.mlr.press/v139/li21h.html},
  abstract = 	 {Fairness and robustness are two important concerns for federated learning systems. In this work, we identify that robustness to data and model poisoning attacks and fairness, measured as the uniformity of performance across devices, are competing constraints in statistically heterogeneous networks. To address these constraints, we propose employing a simple, general framework for personalized federated learning, Ditto, that can inherently provide fairness and robustness benefits, and develop a scalable solver for it. Theoretically, we analyze the ability of Ditto to achieve fairness and robustness simultaneously on a class of linear problems. Empirically, across a suite of federated datasets, we show that Ditto not only achieves competitive performance relative to recent personalization methods, but also enables more accurate, robust, and fair models relative to state-of-the-art fair or robust baselines.}
}
"
ICML,2021,Approximate Group Fairness for Clustering,https://proceedings.mlr.press/v139/li21j.html,"['Li, Bo', 'Li, Lijun', 'Sun, Ankang', 'Wang, Chenhao', 'Wang, Yingfan']",7,"We incorporate group fairness into the algorithmic centroid clustering problem, where $k$ centers are to be located to serve $n$ agents distributed in a metric space. We refine the notion of proportional fairness proposed in [Chen et al., ICML 2019] as {\em core fairness}. A $k$-clustering is in the core if no coalition containing at least $n/k$ agents can strictly decrease their total distance by deviating to a new center together. Our solution concept is motivated by the situation where agents are able to coordinate and utilities are transferable. A string of existence, hardness and approximability results is provided. Particularly, we propose two dimensions to relax core requirements: one is on the degree of distance improvement, and the other is on the size of deviating coalition. For both relaxations and their combination, we study the extent to which relaxed core fairness can be satisfied in metric spaces including line, tree and general metric space, and design approximation algorithms accordingly. We also conduct experiments on synthetic and real-world data to examine the performance of our algorithms.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v139-li21j,
  title = 	 {Approximate Group Fairness for Clustering},
  author =       {Li, Bo and Li, Lijun and Sun, Ankang and Wang, Chenhao and Wang, Yingfan},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {6381--6391},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/li21j/li21j.pdf},
  url = 	 {https://proceedings.mlr.press/v139/li21j.html},
  abstract = 	 {We incorporate group fairness into the algorithmic centroid clustering problem, where $k$ centers are to be located to serve $n$ agents distributed in a metric space. We refine the notion of proportional fairness proposed in [Chen et al., ICML 2019] as {\em core fairness}. A $k$-clustering is in the core if no coalition containing at least $n/k$ agents can strictly decrease their total distance by deviating to a new center together. Our solution concept is motivated by the situation where agents are able to coordinate and utilities are transferable. A string of existence, hardness and approximability results is provided. Particularly, we propose two dimensions to relax core requirements: one is on the degree of distance improvement, and the other is on the size of deviating coalition. For both relaxations and their combination, we study the extent to which relaxed core fairness can be satisfied in metric spaces including line, tree and general metric space, and design approximation algorithms accordingly. We also conduct experiments on synthetic and real-world data to examine the performance of our algorithms.}
}
"
ICML,2021,"A Second look at Exponential and Cosine Step Sizes: Simplicity, Adaptivity, and Performance",https://proceedings.mlr.press/v139/li21z.html,"['Li, Xiaoyu', 'Zhuang, Zhenxun', 'Orabona, Francesco']",7,"Stochastic Gradient Descent (SGD) is a popular tool in training large-scale machine learning models. Its performance, however, is highly variable, depending crucially on the choice of the step sizes. Accordingly, a variety of strategies for tuning the step sizes have been proposed, ranging from coordinate-wise approaches (a.k.a. ‚Äúadaptive‚Äù step sizes) to sophisticated heuristics to change the step size in each iteration. In this paper, we study two step size schedules whose power has been repeatedly confirmed in practice: the exponential and the cosine step sizes. For the first time, we provide theoretical support for them proving convergence rates for smooth non-convex functions, with and without the Polyak-≈Å{}ojasiewicz (PL) condition. Moreover, we show the surprising property that these two strategies are \emph{adaptive} to the noise level in the stochastic gradients of PL functions. That is, contrary to polynomial step sizes, they achieve almost optimal performance without needing to know the noise level nor tuning their hyperparameters based on it. Finally, we conduct a fair and comprehensive empirical evaluation of real-world datasets with deep learning architectures. Results show that, even if only requiring at most two hyperparameters to tune, these two strategies best or match the performance of various finely-tuned state-of-the-art strategies.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v139-li21z,
  title = 	 {A Second look at Exponential and Cosine Step Sizes: Simplicity, Adaptivity, and Performance},
  author =       {Li, Xiaoyu and Zhuang, Zhenxun and Orabona, Francesco},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {6553--6564},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/li21z/li21z.pdf},
  url = 	 {https://proceedings.mlr.press/v139/li21z.html},
  abstract = 	 {Stochastic Gradient Descent (SGD) is a popular tool in training large-scale machine learning models. Its performance, however, is highly variable, depending crucially on the choice of the step sizes. Accordingly, a variety of strategies for tuning the step sizes have been proposed, ranging from coordinate-wise approaches (a.k.a. “adaptive” step sizes) to sophisticated heuristics to change the step size in each iteration. In this paper, we study two step size schedules whose power has been repeatedly confirmed in practice: the exponential and the cosine step sizes. For the first time, we provide theoretical support for them proving convergence rates for smooth non-convex functions, with and without the Polyak-Ł{}ojasiewicz (PL) condition. Moreover, we show the surprising property that these two strategies are \emph{adaptive} to the noise level in the stochastic gradients of PL functions. That is, contrary to polynomial step sizes, they achieve almost optimal performance without needing to know the noise level nor tuning their hyperparameters based on it. Finally, we conduct a fair and comprehensive empirical evaluation of real-world datasets with deep learning architectures. Results show that, even if only requiring at most two hyperparameters to tune, these two strategies best or match the performance of various finely-tuned state-of-the-art strategies.}
}
"
ICML,2021,Towards Understanding and Mitigating Social Biases in Language Models,https://proceedings.mlr.press/v139/liang21a.html,"['Liang, Paul Pu', 'Wu, Chiyu', 'Morency, Louis-Philippe', 'Salakhutdinov, Ruslan']",75,"As machine learning methods are deployed in real-world settings such as healthcare, legal systems, and social science, it is crucial to recognize how they shape social biases and stereotypes in these sensitive decision-making processes. Among such real-world deployments are large-scale pretrained language models (LMs) that can be potentially dangerous in manifesting undesirable representational biases - harmful biases resulting from stereotyping that propagate negative generalizations involving gender, race, religion, and other social constructs. As a step towards improving the fairness of LMs, we carefully define several sources of representational biases before proposing new benchmarks and metrics to measure them. With these tools, we propose steps towards mitigating social biases during text generation. Our empirical results and human evaluation demonstrate effectiveness in mitigating bias while retaining crucial contextual information for high-fidelity text generation, thereby pushing forward the performance-fairness Pareto frontier.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v139-liang21a,
  title =          {Towards Understanding and Mitigating Social Biases in Language Models},
  author =       {Liang, Paul Pu and Wu, Chiyu and Morency, Louis-Philippe and Salakhutdinov, Ruslan},
  booktitle =          {Proceedings of the 38th International Conference on Machine Learning},
  pages =          {6565--6576},
  year =          {2021},
  editor =          {Meila, Marina and Zhang, Tong},
  volume =          {139},
  series =          {Proceedings of Machine Learning Research},
  month =          {18--24 Jul},
  publisher =    {PMLR},
  pdf =          {http://proceedings.mlr.press/v139/liang21a/liang21a.pdf},
  url =          {https://proceedings.mlr.press/v139/liang21a.html},
  abstract =          {As machine learning methods are deployed in real-world settings such as healthcare, legal systems, and social science, it is crucial to recognize how they shape social biases and stereotypes in these sensitive decision-making processes. Among such real-world deployments are large-scale pretrained language models (LMs) that can be potentially dangerous in manifesting undesirable representational biases - harmful biases resulting from stereotyping that propagate negative generalizations involving gender, race, religion, and other social constructs. As a step towards improving the fairness of LMs, we carefully define several sources of representational biases before proposing new benchmarks and metrics to measure them. With these tools, we propose steps towards mitigating social biases during text generation. Our empirical results and human evaluation demonstrate effectiveness in mitigating bias while retaining crucial contextual information for high-fidelity text generation, thereby pushing forward the performance-fairness Pareto frontier.}
}
"
ICML,2021,Blind Pareto Fairness and Subgroup Robustness,https://proceedings.mlr.press/v139/martinez21a.html,"['Martinez, Natalia L', 'Bertran, Martin A', 'Papadaki, Afroditi', 'Rodrigues, Miguel', 'Sapiro, Guillermo']",14,"Much of the work in the field of group fairness addresses disparities between predefined groups based on protected features such as gender, age, and race, which need to be available at train, and often also at test, time. These approaches are static and retrospective, since algorithms designed to protect groups identified a priori cannot anticipate and protect the needs of different at-risk groups in the future. In this work we analyze the space of solutions for worst-case fairness beyond demographics, and propose Blind Pareto Fairness (BPF), a method that leverages no-regret dynamics to recover a fair minimax classifier that reduces worst-case risk of any potential subgroup of sufficient size, and guarantees that the remaining population receives the best possible level of service. BPF addresses fairness beyond demographics, that is, it does not rely on predefined notions of at-risk groups, neither at train nor at test time. Our experimental results show that the proposed framework improves worst-case risk in multiple standard datasets, while simultaneously providing better levels of service for the remaining population. The code is available at github.com/natalialmg/BlindParetoFairness",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v139-martinez21a,
  title = 	 {Blind Pareto Fairness and Subgroup Robustness},
  author =       {Martinez, Natalia L and Bertran, Martin A and Papadaki, Afroditi and Rodrigues, Miguel and Sapiro, Guillermo},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {7492--7501},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/martinez21a/martinez21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/martinez21a.html},
  abstract = 	 {Much of the work in the field of group fairness addresses disparities between predefined groups based on protected features such as gender, age, and race, which need to be available at train, and often also at test, time. These approaches are static and retrospective, since algorithms designed to protect groups identified a priori cannot anticipate and protect the needs of different at-risk groups in the future. In this work we analyze the space of solutions for worst-case fairness beyond demographics, and propose Blind Pareto Fairness (BPF), a method that leverages no-regret dynamics to recover a fair minimax classifier that reduces worst-case risk of any potential subgroup of sufficient size, and guarantees that the remaining population receives the best possible level of service. BPF addresses fairness beyond demographics, that is, it does not rely on predefined notions of at-risk groups, neither at train nor at test time. Our experimental results show that the proposed framework improves worst-case risk in multiple standard datasets, while simultaneously providing better levels of service for the remaining population. The code is available at github.com/natalialmg/BlindParetoFairness}
}
"
ICML,2021,Tight Bounds on the Smallest Eigenvalue of the Neural Tangent Kernel for Deep ReLU Networks,https://proceedings.mlr.press/v139/nguyen21g.html,"['Nguyen, Quynh', 'Mondelli, Marco', 'Montufar, Guido F']",29,"A recent line of work has analyzed the theoretical properties of deep neural networks via the Neural Tangent Kernel (NTK). In particular, the smallest eigenvalue of the NTK has been related to the memorization capacity, the global convergence of gradient descent algorithms and the generalization of deep nets. However, existing results either provide bounds in the two-layer setting or assume that the spectrum of the NTK matrices is bounded away from 0 for multi-layer networks. In this paper, we provide tight bounds on the smallest eigenvalue of NTK matrices for deep ReLU nets, both in the limiting case of infinite widths and for finite widths. In the finite-width setting, the network architectures we consider are fairly general: we require the existence of a wide layer with roughly order of $N$ neurons, $N$ being the number of data samples; and the scaling of the remaining layer widths is arbitrary (up to logarithmic factors). To obtain our results, we analyze various quantities of independent interest: we give lower bounds on the smallest singular value of hidden feature matrices, and upper bounds on the Lipschitz constant of input-output feature maps.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v139-nguyen21g,
  title = 	 {Tight Bounds on the Smallest Eigenvalue of the Neural Tangent Kernel for Deep ReLU Networks},
  author =       {Nguyen, Quynh and Mondelli, Marco and Montufar, Guido F},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {8119--8129},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/nguyen21g/nguyen21g.pdf},
  url = 	 {https://proceedings.mlr.press/v139/nguyen21g.html},
  abstract = 	 {A recent line of work has analyzed the theoretical properties of deep neural networks via the Neural Tangent Kernel (NTK). In particular, the smallest eigenvalue of the NTK has been related to the memorization capacity, the global convergence of gradient descent algorithms and the generalization of deep nets. However, existing results either provide bounds in the two-layer setting or assume that the spectrum of the NTK matrices is bounded away from 0 for multi-layer networks. In this paper, we provide tight bounds on the smallest eigenvalue of NTK matrices for deep ReLU nets, both in the limiting case of infinite widths and for finite widths. In the finite-width setting, the network architectures we consider are fairly general: we require the existence of a wide layer with roughly order of $N$ neurons, $N$ being the number of data samples; and the scaling of the remaining layer widths is arbitrary (up to logarithmic factors). To obtain our results, we analyze various quantities of independent interest: we give lower bounds on the smallest singular value of hidden feature matrices, and upper bounds on the Lipschitz constant of input-output feature maps.}
}
"
ICML,2021,Optimization Planning for 3D ConvNets,https://proceedings.mlr.press/v139/qiu21c.html,"['Qiu, Zhaofan', 'Yao, Ting', 'Ngo, Chong-Wah', 'Mei, Tao']",8,"It is not trivial to optimally learn a 3D Convolutional Neural Networks (3D ConvNets) due to high complexity and various options of the training scheme. The most common hand-tuning process starts from learning 3D ConvNets using short video clips and then is followed by learning long-term temporal dependency using lengthy clips, while gradually decaying the learning rate from high to low as training progresses. The fact that such process comes along with several heuristic settings motivates the study to seek an optimal ""path"" to automate the entire training. In this paper, we decompose the path into a series of training ""states"" and specify the hyper-parameters, e.g., learning rate and the length of input clips, in each state. The estimation of the knee point on the performance-epoch curve triggers the transition from one state to another. We perform dynamic programming over all the candidate states to plan the optimal permutation of states, i.e., optimization path. Furthermore, we devise a new 3D ConvNets with a unique design of dual-head classifier to improve spatial and temporal discrimination. Extensive experiments on seven public video recognition benchmarks demonstrate the advantages of our proposal. With the optimization planning, our 3D ConvNets achieves superior results when comparing to the state-of-the-art recognition methods. More remarkably, we obtain the top-1 accuracy of 80.5% and 82.7% on Kinetics-400 and Kinetics-600 datasets, respectively.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v139-qiu21c,
  title = 	 {Optimization Planning for 3D ConvNets},
  author =       {Qiu, Zhaofan and Yao, Ting and Ngo, Chong-Wah and Mei, Tao},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {8726--8736},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/qiu21c/qiu21c.pdf},
  url = 	 {https://proceedings.mlr.press/v139/qiu21c.html},
  abstract = 	 {It is not trivial to optimally learn a 3D Convolutional Neural Networks (3D ConvNets) due to high complexity and various options of the training scheme. The most common hand-tuning process starts from learning 3D ConvNets using short video clips and then is followed by learning long-term temporal dependency using lengthy clips, while gradually decaying the learning rate from high to low as training progresses. The fact that such process comes along with several heuristic settings motivates the study to seek an optimal ""path"" to automate the entire training. In this paper, we decompose the path into a series of training ""states"" and specify the hyper-parameters, e.g., learning rate and the length of input clips, in each state. The estimation of the knee point on the performance-epoch curve triggers the transition from one state to another. We perform dynamic programming over all the candidate states to plan the optimal permutation of states, i.e., optimization path. Furthermore, we devise a new 3D ConvNets with a unique design of dual-head classifier to improve spatial and temporal discrimination. Extensive experiments on seven public video recognition benchmarks demonstrate the advantages of our proposal. With the optimization planning, our 3D ConvNets achieves superior results when comparing to the state-of-the-art recognition methods. More remarkably, we obtain the top-1 accuracy of 80.5% and 82.7% on Kinetics-400 and Kinetics-600 datasets, respectively.}
}
"
ICML,2021,Multi group Agnostic PAC Learnability,https://proceedings.mlr.press/v139/rothblum21a.html,"['Rothblum, Guy N', 'Yona, Gal']",17,"An agnostic PAC learning algorithm finds a predictor that is competitive with the best predictor in a benchmark hypothesis class, where competitiveness is measured with respect to a given loss function. However, its predictions might be quite sub-optimal for structured subgroups of individuals, such as protected demographic groups. Motivated by such fairness concerns, we study ‚Äúmulti-group agnostic PAC learnability‚Äù: fixing a measure of loss, a benchmark class $\H$ and a (potentially) rich collection of subgroups $\G$, the objective is to learn a single predictor such that the loss experienced by every group $g \in \G$ is not much larger than the best possible loss for this group within $\H$. Under natural conditions, we provide a characterization of the loss functions for which such a predictor is guaranteed to exist. For any such loss function we construct a learning algorithm whose sample complexity is logarithmic in the size of the collection $\G$. Our results unify and extend previous positive and negative results from the multi-group fairness literature, which applied for specific loss functions.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v139-rothblum21a,
  title = 	 {Multi-group Agnostic PAC Learnability},
  author =       {Rothblum, Guy N and Yona, Gal},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {9107--9115},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/rothblum21a/rothblum21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/rothblum21a.html},
  abstract = 	 {An agnostic PAC learning algorithm finds a predictor that is competitive with the best predictor in a benchmark hypothesis class, where competitiveness is measured with respect to a given loss function. However, its predictions might be quite sub-optimal for structured subgroups of individuals, such as protected demographic groups. Motivated by such fairness concerns, we study “multi-group agnostic PAC learnability”: fixing a measure of loss, a benchmark class $\H$ and a (potentially) rich collection of subgroups $\G$, the objective is to learn a single predictor such that the loss experienced by every group $g \in \G$ is not much larger than the best possible loss for this group within $\H$. Under natural conditions, we provide a characterization of the loss functions for which such a predictor is guaranteed to exist. For any such loss function we construct a learning algorithm whose sample complexity is logarithmic in the size of the collection $\G$. Our results unify and extend previous positive and negative results from the multi-group fairness literature, which applied for specific loss functions.}
}
"
ICML,2021,Just How Toxic is Data Poisoning? A Unified Benchmark for Backdoor and Data Poisoning Attacks,https://proceedings.mlr.press/v139/schwarzschild21a.html,"['Schwarzschild, Avi', 'Goldblum, Micah', 'Gupta, Arjun', 'Dickerson, John P', 'Goldstein, Tom']",72,"Data poisoning and backdoor attacks manipulate training data in order to cause models to fail during inference. A recent survey of industry practitioners found that data poisoning is the number one concern among threats ranging from model stealing to adversarial attacks. However, it remains unclear exactly how dangerous poisoning methods are and which ones are more effective considering that these methods, even ones with identical objectives, have not been tested in consistent or realistic settings. We observe that data poisoning and backdoor attacks are highly sensitive to variations in the testing setup. Moreover, we find that existing methods may not generalize to realistic settings. While these existing works serve as valuable prototypes for data poisoning, we apply rigorous tests to determine the extent to which we should fear them. In order to promote fair comparison in future work, we develop standardized benchmarks for data poisoning and backdoor attacks.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v139-schwarzschild21a,
  title = 	 {Just How Toxic is Data Poisoning? A Unified Benchmark for Backdoor and Data Poisoning Attacks},
  author =       {Schwarzschild, Avi and Goldblum, Micah and Gupta, Arjun and Dickerson, John P and Goldstein, Tom},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {9389--9398},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/schwarzschild21a/schwarzschild21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/schwarzschild21a.html},
  abstract = 	 {Data poisoning and backdoor attacks manipulate training data in order to cause models to fail during inference. A recent survey of industry practitioners found that data poisoning is the number one concern among threats ranging from model stealing to adversarial attacks. However, it remains unclear exactly how dangerous poisoning methods are and which ones are more effective considering that these methods, even ones with identical objectives, have not been tested in consistent or realistic settings. We observe that data poisoning and backdoor attacks are highly sensitive to variations in the testing setup. Moreover, we find that existing methods may not generalize to realistic settings. While these existing works serve as valuable prototypes for data poisoning, we apply rigorous tests to determine the extent to which we should fear them. In order to promote fair comparison in future work, we develop standardized benchmarks for data poisoning and backdoor attacks.}
}
"
ICML,2021,Testing Group Fairness via Optimal Transport Projections,https://proceedings.mlr.press/v139/si21a.html,"['Si, Nian', 'Murthy, Karthyek', 'Blanchet, Jose', 'Nguyen, Viet Anh']",7,"We have developed a statistical testing framework to detect if a given machine learning classifier fails to satisfy a wide range of group fairness notions. Our test is a flexible, interpretable, and statistically rigorous tool for auditing whether exhibited biases are intrinsic to the algorithm or simply due to the randomness in the data. The statistical challenges, which may arise from multiple impact criteria that define group fairness and which are discontinuous on model parameters, are conveniently tackled by projecting the empirical measure to the set of group-fair probability models using optimal transport. This statistic is efficiently computed using linear programming, and its asymptotic distribution is explicitly obtained. The proposed framework can also be used to test for composite fairness hypotheses and fairness with multiple sensitive attributes. The optimal transport testing formulation improves interpretability by characterizing the minimal covariate perturbations that eliminate the bias observed in the audit.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v139-si21a,
  title =          {Testing Group Fairness via Optimal Transport Projections},
  author =       {Si, Nian and Murthy, Karthyek and Blanchet, Jose and Nguyen, Viet Anh},
  booktitle =          {Proceedings of the 38th International Conference on Machine Learning},
  pages =          {9649--9659},
  year =          {2021},
  editor =          {Meila, Marina and Zhang, Tong},
  volume =          {139},
  series =          {Proceedings of Machine Learning Research},
  month =          {18--24 Jul},
  publisher =    {PMLR},
  pdf =          {http://proceedings.mlr.press/v139/si21a/si21a.pdf},
  url =          {https://proceedings.mlr.press/v139/si21a.html},
  abstract =          {We have developed a statistical testing framework to detect if a given machine learning classifier fails to satisfy a wide range of group fairness notions. Our test is a flexible, interpretable, and statistically rigorous tool for auditing whether exhibited biases are intrinsic to the algorithm or simply due to the randomness in the data. The statistical challenges, which may arise from multiple impact criteria that define group fairness and which are discontinuous on model parameters, are conveniently tackled by projecting the empirical measure to the set of group-fair probability models using optimal transport. This statistic is efficiently computed using linear programming, and its asymptotic distribution is explicitly obtained. The proposed framework can also be used to test for composite fairness hypotheses and fairness with multiple sensitive attributes. The optimal transport testing formulation improves interpretability by characterizing the minimal covariate perturbations that eliminate the bias observed in the audit.}
}
"
ICML,2021,Collaborative Bayesian Optimization with Fair Regret,https://proceedings.mlr.press/v139/sim21b.html,"['Sim, Rachael Hwee Ling', 'Zhang, Yehong', 'Low, Bryan Kian Hsiang', 'Jaillet, Patrick']",12,"Bayesian optimization (BO) is a popular tool for optimizing complex and costly-to-evaluate black-box objective functions. To further reduce the number of function evaluations, any party performing BO may be interested to collaborate with others to optimize the same objective function concurrently. To do this, existing BO algorithms have considered optimizing a batch of input queries in parallel and provided theoretical bounds on their cumulative regret reflecting inefficiency. However, when the objective function values are correlated with real-world rewards (e.g., money), parties may be hesitant to collaborate if they risk incurring larger cumulative regret (i.e., smaller real-world reward) than others. This paper shows that fairness and efficiency are both necessary for the collaborative BO setting. Inspired by social welfare concepts from economics, we propose a new notion of regret capturing these properties and a collaborative BO algorithm whose convergence rate can be theoretically guaranteed by bounding the new regret, both of which share an adjustable parameter for trading off between fairness vs. efficiency. We empirically demonstrate the benefits (e.g., increased fairness) of our algorithm using synthetic and real-world datasets.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v139-sim21b,
  title = 	 {Collaborative Bayesian Optimization with Fair Regret},
  author =       {Sim, Rachael Hwee Ling and Zhang, Yehong and Low, Bryan Kian Hsiang and Jaillet, Patrick},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {9691--9701},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/sim21b/sim21b.pdf},
  url = 	 {https://proceedings.mlr.press/v139/sim21b.html},
  abstract = 	 {Bayesian optimization (BO) is a popular tool for optimizing complex and costly-to-evaluate black-box objective functions. To further reduce the number of function evaluations, any party performing BO may be interested to collaborate with others to optimize the same objective function concurrently. To do this, existing BO algorithms have considered optimizing a batch of input queries in parallel and provided theoretical bounds on their cumulative regret reflecting inefficiency. However, when the objective function values are correlated with real-world rewards (e.g., money), parties may be hesitant to collaborate if they risk incurring larger cumulative regret (i.e., smaller real-world reward) than others. This paper shows that fairness and efficiency are both necessary for the collaborative BO setting. Inspired by social welfare concepts from economics, we propose a new notion of regret capturing these properties and a collaborative BO algorithm whose convergence rate can be theoretically guaranteed by bounding the new regret, both of which share an adjustable parameter for trading off between fairness vs. efficiency. We empirically demonstrate the benefits (e.g., increased fairness) of our algorithm using synthetic and real-world datasets.}
}
"
ICML,2021,Moreau Yosida $f$ divergences,https://proceedings.mlr.press/v139/terjek21a.html,"[""Terj{\\'e}k, D{\\'a}vid""]",2,"Variational representations of $f$-divergences are central to many machine learning algorithms, with Lipschitz constrained variants recently gaining attention. Inspired by this, we define the Moreau-Yosida approximation of $f$-divergences with respect to the Wasserstein-$1$ metric. The corresponding variational formulas provide a generalization of a number of recent results, novel special cases of interest and a relaxation of the hard Lipschitz constraint. Additionally, we prove that the so-called tight variational representation of $f$-divergences can be to be taken over the quotient space of Lipschitz functions, and give a characterization of functions achieving the supremum in the variational representation. On the practical side, we propose an algorithm to calculate the tight convex conjugate of $f$-divergences compatible with automatic differentiation frameworks. As an application of our results, we propose the Moreau-Yosida $f$-GAN, providing an implementation of the variational formulas for the Kullback-Leibler, reverse Kullback-Leibler, $\chi^2$, reverse $\chi^2$, squared Hellinger, Jensen-Shannon, Jeffreys, triangular discrimination and total variation divergences as GANs trained on CIFAR-10, leading to competitive results and a simple solution to the problem of uniqueness of the optimal critic.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v139-terjek21a,
  title = 	 {Moreau-Yosida $f$-divergences},
  author =       {Terj{\'e}k, D{\'a}vid},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {10214--10224},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/terjek21a/terjek21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/terjek21a.html},
  abstract = 	 {Variational representations of $f$-divergences are central to many machine learning algorithms, with Lipschitz constrained variants recently gaining attention. Inspired by this, we define the Moreau-Yosida approximation of $f$-divergences with respect to the Wasserstein-$1$ metric. The corresponding variational formulas provide a generalization of a number of recent results, novel special cases of interest and a relaxation of the hard Lipschitz constraint. Additionally, we prove that the so-called tight variational representation of $f$-divergences can be to be taken over the quotient space of Lipschitz functions, and give a characterization of functions achieving the supremum in the variational representation. On the practical side, we propose an algorithm to calculate the tight convex conjugate of $f$-divergences compatible with automatic differentiation frameworks. As an application of our results, we propose the Moreau-Yosida $f$-GAN, providing an implementation of the variational formulas for the Kullback-Leibler, reverse Kullback-Leibler, $\chi^2$, reverse $\chi^2$, squared Hellinger, Jensen-Shannon, Jeffreys, triangular discrimination and total variation divergences as GANs trained on CIFAR-10, leading to competitive results and a simple solution to the problem of uniqueness of the optimal critic.}
}
"
ICML,2021,On Disentangled Representations Learned from Correlated Data,https://proceedings.mlr.press/v139/trauble21a.html,"['Tr{\\""a}uble, Frederik', 'Creager, Elliot', 'Kilbertus, Niki', 'Locatello, Francesco', 'Dittadi, Andrea', 'Goyal, Anirudh', 'Sch{\\""o}lkopf, Bernhard', 'Bauer, Stefan']",54,"The focus of disentanglement approaches has been on identifying independent factors of variation in data. However, the causal variables underlying real-world observations are often not statistically independent. In this work, we bridge the gap to real-world scenarios by analyzing the behavior of the most prominent disentanglement approaches on correlated data in a large-scale empirical study (including 4260 models). We show and quantify that systematically induced correlations in the dataset are being learned and reflected in the latent representations, which has implications for downstream applications of disentanglement such as fairness. We also demonstrate how to resolve these latent correlations, either using weak supervision during training or by post-hoc correcting a pre-trained model with a small number of labels.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v139-trauble21a,
  title =          {On Disentangled Representations Learned from Correlated Data},
  author =       {Tr{\""a}uble, Frederik and Creager, Elliot and Kilbertus, Niki and Locatello, Francesco and Dittadi, Andrea and Goyal, Anirudh and Sch{\""o}lkopf, Bernhard and Bauer, Stefan},
  booktitle =          {Proceedings of the 38th International Conference on Machine Learning},
  pages =          {10401--10412},
  year =          {2021},
  editor =          {Meila, Marina and Zhang, Tong},
  volume =          {139},
  series =          {Proceedings of Machine Learning Research},
  month =          {18--24 Jul},
  publisher =    {PMLR},
  pdf =          {http://proceedings.mlr.press/v139/trauble21a/trauble21a.pdf},
  url =          {https://proceedings.mlr.press/v139/trauble21a.html},
  abstract =          {The focus of disentanglement approaches has been on identifying independent factors of variation in data. However, the causal variables underlying real-world observations are often not statistically independent. In this work, we bridge the gap to real-world scenarios by analyzing the behavior of the most prominent disentanglement approaches on correlated data in a large-scale empirical study (including 4260 models). We show and quantify that systematically induced correlations in the dataset are being learned and reflected in the latent representations, which has implications for downstream applications of disentanglement such as fairness. We also demonstrate how to resolve these latent correlations, either using weak supervision during training or by post-hoc correcting a pre-trained model with a small number of labels.}
}
"
ICML,2021,Fairness of Exposure in Stochastic Bandits,https://proceedings.mlr.press/v139/wang21b.html,"['Wang, Lequn', 'Bai, Yiwei', 'Sun, Wen', 'Joachims, Thorsten']",25,"Contextual bandit algorithms have become widely used for recommendation in online systems (e.g. marketplaces, music streaming, news), where they now wield substantial influence on which items get shown to users. This raises questions of fairness to the items ‚Äî and to the sellers, artists, and writers that benefit from this exposure. We argue that the conventional bandit formulation can lead to an undesirable and unfair winner-takes-all allocation of exposure. To remedy this problem, we propose a new bandit objective that guarantees merit-based fairness of exposure to the items while optimizing utility to the users. We formulate fairness regret and reward regret in this setting and present algorithms for both stochastic multi-armed bandits and stochastic linear bandits. We prove that the algorithms achieve sublinear fairness regret and reward regret. Beyond the theoretical analysis, we also provide empirical evidence that these algorithms can allocate exposure to different arms effectively.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v139-wang21b,
  title = 	 {Fairness of Exposure in Stochastic Bandits},
  author =       {Wang, Lequn and Bai, Yiwei and Sun, Wen and Joachims, Thorsten},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {10686--10696},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/wang21b/wang21b.pdf},
  url = 	 {https://proceedings.mlr.press/v139/wang21b.html},
  abstract = 	 {Contextual bandit algorithms have become widely used for recommendation in online systems (e.g. marketplaces, music streaming, news), where they now wield substantial influence on which items get shown to users. This raises questions of fairness to the items — and to the sellers, artists, and writers that benefit from this exposure. We argue that the conventional bandit formulation can lead to an undesirable and unfair winner-takes-all allocation of exposure. To remedy this problem, we propose a new bandit objective that guarantees merit-based fairness of exposure to the items while optimizing utility to the users. We formulate fairness regret and reward regret in this setting and present algorithms for both stochastic multi-armed bandits and stochastic linear bandits. We prove that the algorithms achieve sublinear fairness regret and reward regret. Beyond the theoretical analysis, we also provide empirical evidence that these algorithms can allocate exposure to different arms effectively.}
}
"
ICML,2021,Directional Bias Amplification,https://proceedings.mlr.press/v139/wang21t.html,"['Wang, Angelina', 'Russakovsky, Olga']",21,"Mitigating bias in machine learning systems requires refining our understanding of bias propagation pathways: from societal structures to large-scale data to trained models to impact on society. In this work, we focus on one aspect of the problem, namely bias amplification: the tendency of models to amplify the biases present in the data they are trained on. A metric for measuring bias amplification was introduced in the seminal work by Zhao et al. (2017); however, as we demonstrate, this metric suffers from a number of shortcomings including conflating different types of bias amplification and failing to account for varying base rates of protected attributes. We introduce and analyze a new, decoupled metric for measuring bias amplification, $BiasAmp_{\rightarrow}$ (Directional Bias Amplification). We thoroughly analyze and discuss both the technical assumptions and normative implications of this metric. We provide suggestions about its measurement by cautioning against predicting sensitive attributes, encouraging the use of confidence intervals due to fluctuations in the fairness of models across runs, and discussing the limitations of what this metric captures. Throughout this paper, we work to provide an interrogative look at the technical measurement of bias amplification, guided by our normative ideas of what we want it to encompass. Code is located at https://github.com/princetonvisualai/directional-bias-amp.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v139-wang21t,
  title =          {Directional Bias Amplification},
  author =       {Wang, Angelina and Russakovsky, Olga},
  booktitle =          {Proceedings of the 38th International Conference on Machine Learning},
  pages =          {10882--10893},
  year =          {2021},
  editor =          {Meila, Marina and Zhang, Tong},
  volume =          {139},
  series =          {Proceedings of Machine Learning Research},
  month =          {18--24 Jul},
  publisher =    {PMLR},
  pdf =          {http://proceedings.mlr.press/v139/wang21t/wang21t.pdf},
  url =          {https://proceedings.mlr.press/v139/wang21t.html},
  abstract =          {Mitigating bias in machine learning systems requires refining our understanding of bias propagation pathways: from societal structures to large-scale data to trained models to impact on society. In this work, we focus on one aspect of the problem, namely bias amplification: the tendency of models to amplify the biases present in the data they are trained on. A metric for measuring bias amplification was introduced in the seminal work by Zhao et al. (2017); however, as we demonstrate, this metric suffers from a number of shortcomings including conflating different types of bias amplification and failing to account for varying base rates of protected attributes. We introduce and analyze a new, decoupled metric for measuring bias amplification, $BiasAmp_{\rightarrow}$ (Directional Bias Amplification). We thoroughly analyze and discuss both the technical assumptions and normative implications of this metric. We provide suggestions about its measurement by cautioning against predicting sensitive attributes, encouraging the use of confidence intervals due to fluctuations in the fairness of models across runs, and discussing the limitations of what this metric captures. Throughout this paper, we work to provide an interrogative look at the technical measurement of bias amplification, guided by our normative ideas of what we want it to encompass. Code is located at https://github.com/princetonvisualai/directional-bias-amp.}
}
"
ICML,2021,To be Robust or to be Fair: Towards Fairness in Adversarial Training,https://proceedings.mlr.press/v139/xu21b.html,"['Xu, Han', 'Liu, Xiaorui', 'Li, Yaxin', 'Jain, Anil', 'Tang, Jiliang']",64,"Adversarial training algorithms have been proved to be reliable to improve machine learning models‚Äô robustness against adversarial examples. However, we find that adversarial training algorithms tend to introduce severe disparity of accuracy and robustness between different groups of data. For instance, PGD adversarially trained ResNet18 model on CIFAR-10 has 93% clean accuracy and 67% PGD l_infty-8 adversarial accuracy on the class ‚Äùautomobile‚Äù but only 65% and 17% on class ‚Äùcat‚Äù. This phenomenon happens in balanced datasets and does not exist in naturally trained models when only using clean samples. In this work, we empirically and theoretically show that this phenomenon can generally happen under adversarial training algorithms which minimize DNN models‚Äô robust errors. Motivated by these findings, we propose a Fair-Robust-Learning (FRL) framework to mitigate this unfairness problem when doing adversarial defenses and experimental results validate the effectiveness of FRL.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v139-xu21b,
  title =          {To be Robust or to be Fair: Towards Fairness in Adversarial Training},
  author =       {Xu, Han and Liu, Xiaorui and Li, Yaxin and Jain, Anil and Tang, Jiliang},
  booktitle =          {Proceedings of the 38th International Conference on Machine Learning},
  pages =          {11492--11501},
  year =          {2021},
  editor =          {Meila, Marina and Zhang, Tong},
  volume =          {139},
  series =          {Proceedings of Machine Learning Research},
  month =          {18--24 Jul},
  publisher =    {PMLR},
  pdf =          {http://proceedings.mlr.press/v139/xu21b/xu21b.pdf},
  url =          {https://proceedings.mlr.press/v139/xu21b.html},
  abstract =          {Adversarial training algorithms have been proved to be reliable to improve machine learning models’ robustness against adversarial examples. However, we find that adversarial training algorithms tend to introduce severe disparity of accuracy and robustness between different groups of data. For instance, PGD adversarially trained ResNet18 model on CIFAR-10 has 93% clean accuracy and 67% PGD l_infty-8 adversarial accuracy on the class ”automobile” but only 65% and 17% on class ”cat”. This phenomenon happens in balanced datasets and does not exist in naturally trained models when only using clean samples. In this work, we empirically and theoretically show that this phenomenon can generally happen under adversarial training algorithms which minimize DNN models’ robust errors. Motivated by these findings, we propose a Fair-Robust-Learning (FRL) framework to mitigate this unfairness problem when doing adversarial defenses and experimental results validate the effectiveness of FRL.}
}
"
ICML,2021,Accelerating Safe Reinforcement Learning with Constraint mismatched Baseline Policies,https://proceedings.mlr.press/v139/yang21i.html,"['Yang, Tsung-Yen', 'Rosca, Justinian', 'Narasimhan, Karthik', 'Ramadge, Peter J']",5,"We consider the problem of reinforcement learning when provided with (1) a baseline control policy and (2) a set of constraints that the learner must satisfy. The baseline policy can arise from demonstration data or a teacher agent and may provide useful cues for learning, but it might also be sub-optimal for the task at hand, and is not guaranteed to satisfy the specified constraints, which might encode safety, fairness or other application-specific requirements. In order to safely learn from baseline policies, we propose an iterative policy optimization algorithm that alternates between maximizing expected return on the task, minimizing distance to the baseline policy, and projecting the policy onto the constraint-satisfying set. We analyze our algorithm theoretically and provide a finite-time convergence guarantee. In our experiments on five different control tasks, our algorithm consistently outperforms several state-of-the-art baselines, achieving 10 times fewer constraint violations and 40% higher reward on average.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v139-yang21i,
  title = 	 {Accelerating Safe Reinforcement Learning with Constraint-mismatched Baseline Policies},
  author =       {Yang, Tsung-Yen and Rosca, Justinian and Narasimhan, Karthik and Ramadge, Peter J},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {11795--11807},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/yang21i/yang21i.pdf},
  url = 	 {https://proceedings.mlr.press/v139/yang21i.html},
  abstract = 	 {We consider the problem of reinforcement learning when provided with (1) a baseline control policy and (2) a set of constraints that the learner must satisfy. The baseline policy can arise from demonstration data or a teacher agent and may provide useful cues for learning, but it might also be sub-optimal for the task at hand, and is not guaranteed to satisfy the specified constraints, which might encode safety, fairness or other application-specific requirements. In order to safely learn from baseline policies, we propose an iterative policy optimization algorithm that alternates between maximizing expected return on the task, minimizing distance to the baseline policy, and projecting the policy onto the constraint-satisfying set. We analyze our algorithm theoretically and provide a finite-time convergence guarantee. In our experiments on five different control tasks, our algorithm consistently outperforms several state-of-the-art baselines, achieving 10 times fewer constraint violations and 40% higher reward on average.}
}
"
ICML,2021,Learning Fair Policies in Decentralized Cooperative Multi Agent Reinforcement Learning,https://proceedings.mlr.press/v139/zimmer21a.html,"['Zimmer, Matthieu', 'Glanois, Claire', 'Siddique, Umer', 'Weng, Paul']",20,"We consider the problem of learning fair policies in (deep) cooperative multi-agent reinforcement learning (MARL). We formalize it in a principled way as the problem of optimizing a welfare function that explicitly encodes two important aspects of fairness: efficiency and equity. We provide a theoretical analysis of the convergence of policy gradient for this problem. As a solution method, we propose a novel neural network architecture, which is composed of two sub-networks specifically designed for taking into account these two aspects of fairness. In experiments, we demonstrate the importance of the two sub-networks for fair optimization. Our overall approach is general as it can accommodate any (sub)differentiable welfare function. Therefore, it is compatible with various notions of fairness that have been proposed in the literature (e.g., lexicographic maximin, generalized Gini social welfare function, proportional fairness). Our method is generic and can be implemented in various MARL settings: centralized training and decentralized execution, or fully decentralized. Finally, we experimentally validate our approach in various domains and show that it can perform much better than previous methods, both in terms of efficiency and equity.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v139-zimmer21a,
  title = 	 {Learning Fair Policies in Decentralized Cooperative Multi-Agent Reinforcement Learning},
  author =       {Zimmer, Matthieu and Glanois, Claire and Siddique, Umer and Weng, Paul},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {12967--12978},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/zimmer21a/zimmer21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/zimmer21a.html},
  abstract = 	 {We consider the problem of learning fair policies in (deep) cooperative multi-agent reinforcement learning (MARL). We formalize it in a principled way as the problem of optimizing a welfare function that explicitly encodes two important aspects of fairness: efficiency and equity. We provide a theoretical analysis of the convergence of policy gradient for this problem. As a solution method, we propose a novel neural network architecture, which is composed of two sub-networks specifically designed for taking into account these two aspects of fairness. In experiments, we demonstrate the importance of the two sub-networks for fair optimization. Our overall approach is general as it can accommodate any (sub)differentiable welfare function. Therefore, it is compatible with various notions of fairness that have been proposed in the literature (e.g., lexicographic maximin, generalized Gini social welfare function, proportional fairness). Our method is generic and can be implemented in various MARL settings: centralized training and decentralized execution, or fully decentralized. Finally, we experimentally validate our approach in various domains and show that it can perform much better than previous methods, both in terms of efficiency and equity.}
}
"
ICML,2022,Active Sampling for Min Max Fairness,https://proceedings.mlr.press/v162/abernethy22a.html,"['Abernethy, Jacob D', 'Awasthi, Pranjal', 'Kleindessner, Matth{\\""a}us', 'Morgenstern, Jamie', 'Russell, Chris', 'Zhang, Jie']",9,"We propose simple active sampling and reweighting strategies for optimizing min-max fairness that can be applied to any classification or regression model learned via loss minimization. The key intuition behind our approach is to use at each timestep a datapoint from the group that is worst off under the current model for updating the model. The ease of implementation and the generality of our robust formulation make it an attractive option for improving model performance on disadvantaged groups. For convex learning problems, such as linear or logistic regression, we provide a fine-grained analysis, proving the rate of convergence to a min-max fair solution.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v162-abernethy22a,
  title = 	 {Active Sampling for Min-Max Fairness},
  author =       {Abernethy, Jacob D and Awasthi, Pranjal and Kleindessner, Matth{\""a}us and Morgenstern, Jamie and Russell, Chris and Zhang, Jie},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {53--65},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/abernethy22a/abernethy22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/abernethy22a.html},
  abstract = 	 {We propose simple active sampling and reweighting strategies for optimizing min-max fairness that can be applied to any classification or regression model learned via loss minimization. The key intuition behind our approach is to use at each timestep a datapoint from the group that is worst off under the current model for updating the model. The ease of implementation and the generality of our robust formulation make it an attractive option for improving model performance on disadvantaged groups. For convex learning problems, such as linear or logistic regression, we provide a fine-grained analysis, proving the rate of convergence to a min-max fair solution.}
}
"
ICML,2022,On the Convergence of the Shapley Value in Parametric {B}ayesian Learning Games,https://proceedings.mlr.press/v162/agussurja22a.html,"['Agussurja, Lucas', 'Xu, Xinyi', 'Low, Bryan Kian Hsiang']",2,"Measuring contributions is a classical problem in cooperative game theory where the Shapley value is the most well-known solution concept. In this paper, we establish the convergence property of the Shapley value in parametric Bayesian learning games where players perform a Bayesian inference using their combined data, and the posterior-prior KL divergence is used as the characteristic function. We show that for any two players, under some regularity conditions, their difference in Shapley value converges in probability to the difference in Shapley value of a limiting game whose characteristic function is proportional to the log-determinant of the joint Fisher information. As an application, we present an online collaborative learning framework that is asymptotically Shapley-fair. Our result enables this to be achieved without any costly computations of posterior-prior KL divergences. Only a consistent estimator of the Fisher information is needed. The effectiveness of our framework is demonstrated with experiments using real-world data.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v162-agussurja22a,
  title = 	 {On the Convergence of the Shapley Value in Parametric {B}ayesian Learning Games},
  author =       {Agussurja, Lucas and Xu, Xinyi and Low, Bryan Kian Hsiang},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {180--196},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/agussurja22a/agussurja22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/agussurja22a.html},
  abstract = 	 {Measuring contributions is a classical problem in cooperative game theory where the Shapley value is the most well-known solution concept. In this paper, we establish the convergence property of the Shapley value in parametric Bayesian learning games where players perform a Bayesian inference using their combined data, and the posterior-prior KL divergence is used as the characteristic function. We show that for any two players, under some regularity conditions, their difference in Shapley value converges in probability to the difference in Shapley value of a limiting game whose characteristic function is proportional to the log-determinant of the joint Fisher information. As an application, we present an online collaborative learning framework that is asymptotically Shapley-fair. Our result enables this to be achieved without any costly computations of posterior-prior KL divergences. Only a consistent estimator of the Fisher information is needed. The effectiveness of our framework is demonstrated with experiments using real-world data.}
}
"
ICML,2022,Individual Preference Stability for Clustering,https://proceedings.mlr.press/v162/ahmadi22a.html,"['Ahmadi, Saba', 'Awasthi, Pranjal', 'Khuller, Samir', 'Kleindessner, Matth{\\""a}us', 'Morgenstern, Jamie', 'Sukprasert, Pattara', 'Vakilian, Ali']",6,"In this paper, we propose a natural notion of individual preference (IP) stability for clustering, which asks that every data point, on average, is closer to the points in its own cluster than to the points in any other cluster. Our notion can be motivated from several perspectives, including game theory and algorithmic fairness. We study several questions related to our proposed notion. We first show that deciding whether a given data set allows for an IP-stable clustering in general is NP-hard. As a result, we explore the design of efficient algorithms for finding IP-stable clusterings in some restricted metric spaces. We present a polytime algorithm to find a clustering satisfying exact IP-stability on the real line, and an efficient algorithm to find an IP-stable 2-clustering for a tree metric. We also consider relaxing the stability constraint, i.e., every data point should not be too far from its own cluster compared to any other cluster. For this case, we provide polytime algorithms with different guarantees. We evaluate some of our algorithms and several standard clustering approaches on real data sets.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v162-ahmadi22a,
  title = 	 {Individual Preference Stability for Clustering},
  author =       {Ahmadi, Saba and Awasthi, Pranjal and Khuller, Samir and Kleindessner, Matth{\""a}us and Morgenstern, Jamie and Sukprasert, Pattara and Vakilian, Ali},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {197--246},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/ahmadi22a/ahmadi22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/ahmadi22a.html},
  abstract = 	 {In this paper, we propose a natural notion of individual preference (IP) stability for clustering, which asks that every data point, on average, is closer to the points in its own cluster than to the points in any other cluster. Our notion can be motivated from several perspectives, including game theory and algorithmic fairness. We study several questions related to our proposed notion. We first show that deciding whether a given data set allows for an IP-stable clustering in general is NP-hard. As a result, we explore the design of efficient algorithms for finding IP-stable clusterings in some restricted metric spaces. We present a polytime algorithm to find a clustering satisfying exact IP-stability on the real line, and an efficient algorithm to find an IP-stable 2-clustering for a tree metric. We also consider relaxing the stability constraint, i.e., every data point should not be too far from its own cluster compared to any other cluster. For this case, we provide polytime algorithms with different guarantees. We evaluate some of our algorithms and several standard clustering approaches on real data sets.}
}
"
ICML,2022,{RUM}s from Head to Head Contests,https://proceedings.mlr.press/v162/almanza22a.html,"['Almanza, Matteo', 'Chierichetti, Flavio', 'Kumar, Ravi', 'Panconesi, Alessandro', 'Tomkins, Andrew']",3,"Random utility models (RUMs) encode the likelihood that a particular item will be selected from a slate of competing items. RUMs are well-studied objects in both discrete choice theory and, more recently, in the machine learning community, as they encode a fairly broad notion of rational user behavior. In this paper, we focus on slates of size two representing head-to-head contests. Given a tournament matrix $M$ such that $M_{i,j}$ is the probability that item $j$ will be selected from $\{i, j\}$, we consider the problem of finding the RUM that most closely reproduces $M$. For this problem we obtain a polynomial-time algorithm returning a RUM that approximately minimizes the average error over the pairs. Our experiments show that RUMs can perfectly represent many of the tournament matrices that have been considered in the literature; in fact, the maximum average error induced by RUMs on the matrices we considered is negligible ($\approx 0.001$). We also show that RUMs are competitive, on prediction tasks, with previous approaches.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v162-almanza22a,
  title = 	 {{RUM}s from Head-to-Head Contests},
  author =       {Almanza, Matteo and Chierichetti, Flavio and Kumar, Ravi and Panconesi, Alessandro and Tomkins, Andrew},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {452--467},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/almanza22a/almanza22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/almanza22a.html},
  abstract = 	 {Random utility models (RUMs) encode the likelihood that a particular item will be selected from a slate of competing items. RUMs are well-studied objects in both discrete choice theory and, more recently, in the machine learning community, as they encode a fairly broad notion of rational user behavior. In this paper, we focus on slates of size two representing head-to-head contests. Given a tournament matrix $M$ such that $M_{i,j}$ is the probability that item $j$ will be selected from $\{i, j\}$, we consider the problem of finding the RUM that most closely reproduces $M$. For this problem we obtain a polynomial-time algorithm returning a RUM that approximately minimizes the average error over the pairs. Our experiments show that RUMs can <em>perfectly</em> represent many of the tournament matrices that have been considered in the literature; in fact, the maximum average error induced by RUMs on the matrices we considered is negligible ($\approx 0.001$). We also show that RUMs are competitive, on prediction tasks, with previous approaches.}
}
"
ICML,2022,Fair and Fast k Center Clustering for Data Summarization,https://proceedings.mlr.press/v162/angelidakis22a.html,"['Angelidakis, Haris', 'Kurpisz, Adam', 'Sering, Leon', 'Zenklusen, Rico']",3,"We consider two key issues faced by many clustering methods when used for data summarization, namely (a) an unfair representation of ""demographic groups‚Äù and (b) distorted summarizations, where data points in the summary represent subsets of the original data of vastly different sizes. Previous work made important steps towards handling separately each of these two issues in the context of the fundamental k-Center clustering objective through the study of fast algorithms for natural models that address them. We show that it is possible to effectively address both (a) and (b) simultaneously by presenting a clustering procedure that works for a canonical combined model and (i) is fast, both in theory and practice, (ii) exhibits a worst-case constant-factor guarantee, and (iii) gives promising computational results showing that there can be significant benefits in addressing both issues together instead of sequentially.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v162-angelidakis22a,
  title = 	 {Fair and Fast k-Center Clustering for Data Summarization},
  author =       {Angelidakis, Haris and Kurpisz, Adam and Sering, Leon and Zenklusen, Rico},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {669--702},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/angelidakis22a/angelidakis22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/angelidakis22a.html},
  abstract = 	 {We consider two key issues faced by many clustering methods when used for data summarization, namely (a) an unfair representation of ""demographic groups” and (b) distorted summarizations, where data points in the summary represent subsets of the original data of vastly different sizes. Previous work made important steps towards handling separately each of these two issues in the context of the fundamental k-Center clustering objective through the study of fast algorithms for natural models that address them. We show that it is possible to effectively address both (a) and (b) simultaneously by presenting a clustering procedure that works for a canonical combined model and (i) is fast, both in theory and practice, (ii) exhibits a worst-case constant-factor guarantee, and (iii) gives promising computational results showing that there can be significant benefits in addressing both issues together instead of sequentially.}
}
"
ICML,2022,Neural {F}isher Discriminant Analysis: Optimal Neural Network Embeddings in Polynomial Time,https://proceedings.mlr.press/v162/bartan22a.html,"['Bartan, Burak', 'Pilanci, Mert']",1,"Fisher‚Äôs Linear Discriminant Analysis (FLDA) is a statistical analysis method that linearly embeds data points to a lower dimensional space to maximize a discrimination criterion such that the variance between classes is maximized while the variance within classes is minimized. We introduce a natural extension of FLDA that employs neural networks, called Neural Fisher Discriminant Analysis (NFDA). This method finds the optimal two-layer neural network that embeds data points to optimize the same discrimination criterion. We use tools from convex optimization to transform the optimal neural network embedding problem into a convex problem. The resulting problem is easy to interpret and solve to global optimality. We evaluate the method‚Äôs performance on synthetic and real datasets.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v162-bartan22a,
  title = 	 {Neural {F}isher Discriminant Analysis: Optimal Neural Network Embeddings in Polynomial Time},
  author =       {Bartan, Burak and Pilanci, Mert},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {1647--1663},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/bartan22a/bartan22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/bartan22a.html},
  abstract = 	 {Fisher’s Linear Discriminant Analysis (FLDA) is a statistical analysis method that linearly embeds data points to a lower dimensional space to maximize a discrimination criterion such that the variance between classes is maximized while the variance within classes is minimized. We introduce a natural extension of FLDA that employs neural networks, called Neural Fisher Discriminant Analysis (NFDA). This method finds the optimal two-layer neural network that embeds data points to optimize the same discrimination criterion. We use tools from convex optimization to transform the optimal neural network embedding problem into a convex problem. The resulting problem is easy to interpret and solve to global optimality. We evaluate the method’s performance on synthetic and real datasets.}
}
"
ICML,2022,Information Discrepancy in Strategic Learning,https://proceedings.mlr.press/v162/bechavod22a.html,"['Bechavod, Yahav', 'Podimata, Chara', 'Wu, Steven', 'Ziani, Juba']",11,"We initiate the study of the effects of non-transparency in decision rules on individuals‚Äô ability to improve in strategic learning settings. Inspired by real-life settings, such as loan approvals and college admissions, we remove the assumption typically made in the strategic learning literature, that the decision rule is fully known to individuals, and focus instead on settings where it is inaccessible. In their lack of knowledge, individuals try to infer this rule by learning from their peers (e.g., friends and acquaintances who previously applied for a loan), naturally forming groups in the population, each with possibly different type and level of information regarding the decision rule. We show that, in equilibrium, the principal‚Äôs decision rule optimizing welfare across sub-populations may cause a strong negative externality: the true quality of some of the groups can actually deteriorate. On the positive side, we show that, in many natural cases, optimal improvement can be guaranteed simultaneously for all sub-populations. We further introduce a measure we term information overlap proxy, and demonstrate its usefulness in characterizing the disparity in improvements across sub-populations. Finally, we identify a natural condition under which improvement can be guaranteed for all sub-populations while maintaining high predictive accuracy. We complement our theoretical analysis with experiments on real-world datasets.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v162-bechavod22a,
  title = 	 {Information Discrepancy in Strategic Learning},
  author =       {Bechavod, Yahav and Podimata, Chara and Wu, Steven and Ziani, Juba},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {1691--1715},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/bechavod22a/bechavod22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/bechavod22a.html},
  abstract = 	 {We initiate the study of the effects of non-transparency in decision rules on individuals’ ability to improve in strategic learning settings. Inspired by real-life settings, such as loan approvals and college admissions, we remove the assumption typically made in the strategic learning literature, that the decision rule is fully known to individuals, and focus instead on settings where it is inaccessible. In their lack of knowledge, individuals try to infer this rule by learning from their peers (e.g., friends and acquaintances who previously applied for a loan), naturally forming groups in the population, each with possibly different type and level of information regarding the decision rule. We show that, in equilibrium, the principal’s decision rule optimizing welfare across sub-populations may cause a strong negative externality: the true quality of some of the groups can actually deteriorate. On the positive side, we show that, in many natural cases, optimal improvement can be guaranteed simultaneously for all sub-populations. We further introduce a measure we term information overlap proxy, and demonstrate its usefulness in characterizing the disparity in improvements across sub-populations. Finally, we identify a natural condition under which improvement can be guaranteed for all sub-populations while maintaining high predictive accuracy. We complement our theoretical analysis with experiments on real-world datasets.}
}
"
ICML,2022,Breaking Down Out of Distribution Detection: Many Methods Based on {OOD} Training Data Estimate a Combination of the Same Core Quantities,https://proceedings.mlr.press/v162/bitterwolf22a.html,"['Bitterwolf, Julian', 'Meinke, Alexander', 'Augustin, Maximilian', 'Hein, Matthias']",3,"It is an important problem in trustworthy machine learning to recognize out-of-distribution (OOD) inputs which are inputs unrelated to the in-distribution task. Many out-of-distribution detection methods have been suggested in recent years. The goal of this paper is to recognize common objectives as well as to identify the implicit scoring functions of different OOD detection methods. We focus on the sub-class of methods that use surrogate OOD data during training in order to learn an OOD detection score that generalizes to new unseen out-distributions at test time. We show that binary discrimination between in- and (different) out-distributions is equivalent to several distinct formulations of the OOD detection problem. When trained in a shared fashion with a standard classifier, this binary discriminator reaches an OOD detection performance similar to that of Outlier Exposure. Moreover, we show that the confidence loss which is used by Outlier Exposure has an implicit scoring function which differs in a non-trivial fashion from the theoretically optimal scoring function in the case where training and test out-distribution are the same, which again is similar to the one used when training an Energy-Based OOD detector or when adding a background class. In practice, when trained in exactly the same way, all these methods perform similarly.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v162-bitterwolf22a,
  title = 	 {Breaking Down Out-of-Distribution Detection: Many Methods Based on {OOD} Training Data Estimate a Combination of the Same Core Quantities},
  author =       {Bitterwolf, Julian and Meinke, Alexander and Augustin, Maximilian and Hein, Matthias},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {2041--2074},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/bitterwolf22a/bitterwolf22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/bitterwolf22a.html},
  abstract = 	 {It is an important problem in trustworthy machine learning to recognize out-of-distribution (OOD) inputs which are inputs unrelated to the in-distribution task. Many out-of-distribution detection methods have been suggested in recent years. The goal of this paper is to recognize common objectives as well as to identify the implicit scoring functions of different OOD detection methods. We focus on the sub-class of methods that use surrogate OOD data during training in order to learn an OOD detection score that generalizes to new unseen out-distributions at test time. We show that binary discrimination between in- and (different) out-distributions is equivalent to several distinct formulations of the OOD detection problem. When trained in a shared fashion with a standard classifier, this binary discriminator reaches an OOD detection performance similar to that of Outlier Exposure. Moreover, we show that the confidence loss which is used by Outlier Exposure has an implicit scoring function which differs in a non-trivial fashion from the theoretically optimal scoring function in the case where training and test out-distribution are the same, which again is similar to the one used when training an Energy-Based OOD detector or when adding a background class. In practice, when trained in exactly the same way, all these methods perform similarly.}
}
"
ICML,2022,Fairness with Adaptive Weights,https://proceedings.mlr.press/v162/chai22a.html,"['Chai, Junyi', 'Wang, Xiaoqian']",2,"Fairness is now an important issue in machine learning. There are arising concerns that automated decision-making systems reflect real-world biases. Although a wide range of fairness-related methods have been proposed in recent years, the under-representation problem has been less studied. Due to the uneven distribution of samples from different populations, machine learning models tend to be biased against minority groups when trained by minimizing the average empirical risk across all samples. In this paper, we propose a novel adaptive reweighing method to address representation bias. The goal of our method is to achieve group-level balance among different demographic groups by learning adaptive weights for each sample. Our approach emphasizes more on error-prone samples in prediction and enhances adequate representation of minority groups for fairness. We derive a closed-form solution for adaptive weight assignment and propose an efficient algorithm with theoretical convergence guarantees. We theoretically analyze the fairness of our model and empirically verify that our method strikes a balance between fairness and accuracy. In experiments, our method achieves comparable or better performance than state-of-the-art methods in both classification and regression tasks. Furthermore, our method exhibits robustness to label noise on various benchmark datasets.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v162-chai22a,
  title = 	 {Fairness with Adaptive Weights},
  author =       {Chai, Junyi and Wang, Xiaoqian},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {2853--2866},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/chai22a/chai22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/chai22a.html},
  abstract = 	 {Fairness is now an important issue in machine learning. There are arising concerns that automated decision-making systems reflect real-world biases. Although a wide range of fairness-related methods have been proposed in recent years, the under-representation problem has been less studied. Due to the uneven distribution of samples from different populations, machine learning models tend to be biased against minority groups when trained by minimizing the average empirical risk across all samples. In this paper, we propose a novel adaptive reweighing method to address representation bias. The goal of our method is to achieve group-level balance among different demographic groups by learning adaptive weights for each sample. Our approach emphasizes more on error-prone samples in prediction and enhances adequate representation of minority groups for fairness. We derive a closed-form solution for adaptive weight assignment and propose an efficient algorithm with theoretical convergence guarantees. We theoretically analyze the fairness of our model and empirically verify that our method strikes a balance between fairness and accuracy. In experiments, our method achieves comparable or better performance than state-of-the-art methods in both classification and regression tasks. Furthermore, our method exhibits robustness to label noise on various benchmark datasets.}
}
"
ICML,2022,Mitigating Gender Bias in Face Recognition using the von Mises {F}isher Mixture Model,https://proceedings.mlr.press/v162/conti22a.html,"[""Conti, Jean-R{\\'e}my"", 'Noiry, Nathan', 'Clemencon, Stephan', 'Despiegel, Vincent', ""Gentric, St{\\'e}phane""]",2,"In spite of the high performance and reliability of deep learning algorithms in a wide range of everyday applications, many investigations tend to show that a lot of models exhibit biases, discriminating against specific subgroups of the population (e.g. gender, ethnicity). This urges the practitioner to develop fair systems with a uniform/comparable performance across sensitive groups. In this work, we investigate the gender bias of deep Face Recognition networks. In order to measure this bias, we introduce two new metrics, BFAR and BFRR, that better reflect the inherent deployment needs of Face Recognition systems. Motivated by geometric considerations, we mitigate gender bias through a new post-processing methodology which transforms the deep embeddings of a pre-trained model to give more representation power to discriminated subgroups. It consists in training a shallow neural network by minimizing a Fair von Mises-Fisher loss whose hyperparameters account for the intra-class variance of each gender. Interestingly, we empirically observe that these hyperparameters are correlated with our fairness metrics. In fact, extensive numerical experiments on a variety of datasets show that a careful selection significantly reduces gender bias.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v162-conti22a,
  title = 	 {Mitigating Gender Bias in Face Recognition using the von Mises-{F}isher Mixture Model},
  author =       {Conti, Jean-R{\'e}my and Noiry, Nathan and Clemencon, Stephan and Despiegel, Vincent and Gentric, St{\'e}phane},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {4344--4369},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/conti22a/conti22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/conti22a.html},
  abstract = 	 {In spite of the high performance and reliability of deep learning algorithms in a wide range of everyday applications, many investigations tend to show that a lot of models exhibit biases, discriminating against specific subgroups of the population (e.g. gender, ethnicity). This urges the practitioner to develop fair systems with a uniform/comparable performance across sensitive groups. In this work, we investigate the gender bias of deep Face Recognition networks. In order to measure this bias, we introduce two new metrics, BFAR and BFRR, that better reflect the inherent deployment needs of Face Recognition systems. Motivated by geometric considerations, we mitigate gender bias through a new post-processing methodology which transforms the deep embeddings of a pre-trained model to give more representation power to discriminated subgroups. It consists in training a shallow neural network by minimizing a Fair von Mises-Fisher loss whose hyperparameters account for the intra-class variance of each gender. Interestingly, we empirically observe that these hyperparameters are correlated with our fairness metrics. In fact, extensive numerical experiments on a variety of datasets show that a careful selection significantly reduces gender bias.}
}
"
ICML,2022,Distinguishing rule and exemplar based generalization in learning systems,https://proceedings.mlr.press/v162/dasgupta22b.html,"['Dasgupta, Ishita', 'Grant, Erin', 'Griffiths, Tom']",2,"Machine learning systems often do not share the same inductive biases as humans and, as a result, extrapolate or generalize in ways that are inconsistent with our expectations. The trade-off between exemplar- and rule-based generalization has been studied extensively in cognitive psychology; in this work, we present a protocol inspired by these experimental approaches to probe the inductive biases that control this trade-off in category-learning systems such as artificial neural networks. We isolate two such inductive biases: feature-level bias (differences in which features are more readily learned) and exemplar-vs-rule bias (differences in how these learned features are used for generalization of category labels). We find that standard neural network models are feature-biased and have a propensity towards exemplar-based extrapolation; we discuss the implications of these findings for machine-learning research on data augmentation, fairness, and systematic generalization.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v162-dasgupta22b,
  title =          {Distinguishing rule and exemplar-based generalization in learning systems},
  author =       {Dasgupta, Ishita and Grant, Erin and Griffiths, Tom},
  booktitle =          {Proceedings of the 39th International Conference on Machine Learning},
  pages =          {4816--4830},
  year =          {2022},
  editor =          {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume =          {162},
  series =          {Proceedings of Machine Learning Research},
  month =          {17--23 Jul},
  publisher =    {PMLR},
  pdf =          {https://proceedings.mlr.press/v162/dasgupta22b/dasgupta22b.pdf},
  url =          {https://proceedings.mlr.press/v162/dasgupta22b.html},
  abstract =          {Machine learning systems often do not share the same inductive biases as humans and, as a result, extrapolate or generalize in ways that are inconsistent with our expectations. The trade-off between exemplar- and rule-based generalization has been studied extensively in cognitive psychology; in this work, we present a protocol inspired by these experimental approaches to probe the inductive biases that control this trade-off in category-learning systems such as artificial neural networks. We isolate two such inductive biases: feature-level bias (differences in which features are more readily learned) and exemplar-vs-rule bias (differences in how these learned features are used for generalization of category labels). We find that standard neural network models are feature-biased and have a propensity towards exemplar-based extrapolation; we discuss the implications of these findings for machine-learning research on data augmentation, fairness, and systematic generalization.}
}
"
ICML,2022,Fair Generalized Linear Models with a Convex Penalty,https://proceedings.mlr.press/v162/do22a.html,"['Do, Hyungrok', 'Putzel, Preston', 'Martin, Axel S', 'Smyth, Padhraic', 'Zhong, Judy']",5,"Despite recent advances in algorithmic fairness, methodologies for achieving fairness with generalized linear models (GLMs) have yet to be explored in general, despite GLMs being widely used in practice. In this paper we introduce two fairness criteria for GLMs based on equalizing expected outcomes or log-likelihoods. We prove that for GLMs both criteria can be achieved via a convex penalty term based solely on the linear components of the GLM, thus permitting efficient optimization. We also derive theoretical properties for the resulting fair GLM estimator. To empirically demonstrate the efficacy of the proposed fair GLM, we compare it with other well-known fair prediction methods on an extensive set of benchmark datasets for binary classification and regression. In addition, we demonstrate that the fair GLM can generate fair predictions for a range of response variables, other than binary and continuous outcomes.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v162-do22a,
  title = 	 {Fair Generalized Linear Models with a Convex Penalty},
  author =       {Do, Hyungrok and Putzel, Preston and Martin, Axel S and Smyth, Padhraic and Zhong, Judy},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {5286--5308},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/do22a/do22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/do22a.html},
  abstract = 	 {Despite recent advances in algorithmic fairness, methodologies for achieving fairness with generalized linear models (GLMs) have yet to be explored in general, despite GLMs being widely used in practice. In this paper we introduce two fairness criteria for GLMs based on equalizing expected outcomes or log-likelihoods. We prove that for GLMs both criteria can be achieved via a convex penalty term based solely on the linear components of the GLM, thus permitting efficient optimization. We also derive theoretical properties for the resulting fair GLM estimator. To empirically demonstrate the efficacy of the proposed fair GLM, we compare it with other well-known fair prediction methods on an extensive set of benchmark datasets for binary classification and regression. In addition, we demonstrate that the fair GLM can generate fair predictions for a range of response variables, other than binary and continuous outcomes.}
}
"
ICML,2022,"Contrastive Mixture of Posteriors for Counterfactual Inference, Data Integration and Fairness",https://proceedings.mlr.press/v162/foster22a.html,"['Foster, Adam', 'Vezer, Arpi', 'Glastonbury, Craig A.', 'Creed, Paidi', 'Abujudeh, Samer', 'Sim, Aaron']",1,"Learning meaningful representations of data that can address challenges such as batch effect correction and counterfactual inference is a central problem in many domains including computational biology. Adopting a Conditional VAE framework, we show that marginal independence between the representation and a condition variable plays a key role in both of these challenges. We propose the Contrastive Mixture of Posteriors (CoMP) method that uses a novel misalignment penalty defined in terms of mixtures of the variational posteriors to enforce this independence in latent space. We show that CoMP has attractive theoretical properties compared to previous approaches, and we prove counterfactual identifiability of CoMP under additional assumptions. We demonstrate state-of-the-art performance on a set of challenging tasks including aligning human tumour samples with cancer cell-lines, predicting transcriptome-level perturbation responses, and batch correction on single-cell RNA sequencing data. We also find parallels to fair representation learning and demonstrate that CoMP is competitive on a common task in the field.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v162-foster22a,
  title =          {Contrastive Mixture of Posteriors for Counterfactual Inference, Data Integration and Fairness},
  author =       {Foster, Adam and Vezer, Arpi and Glastonbury, Craig A. and Creed, Paidi and Abujudeh, Samer and Sim, Aaron},
  booktitle =          {Proceedings of the 39th International Conference on Machine Learning},
  pages =          {6578--6621},
  year =          {2022},
  editor =          {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume =          {162},
  series =          {Proceedings of Machine Learning Research},
  month =          {17--23 Jul},
  publisher =    {PMLR},
  pdf =          {https://proceedings.mlr.press/v162/foster22a/foster22a.pdf},
  url =          {https://proceedings.mlr.press/v162/foster22a.html},
  abstract =          {Learning meaningful representations of data that can address challenges such as batch effect correction and counterfactual inference is a central problem in many domains including computational biology. Adopting a Conditional VAE framework, we show that marginal independence between the representation and a condition variable plays a key role in both of these challenges. We propose the Contrastive Mixture of Posteriors (CoMP) method that uses a novel misalignment penalty defined in terms of mixtures of the variational posteriors to enforce this independence in latent space. We show that CoMP has attractive theoretical properties compared to previous approaches, and we prove counterfactual identifiability of CoMP under additional assumptions. We demonstrate state-of-the-art performance on a set of challenging tasks including aligning human tumour samples with cancer cell-lines, predicting transcriptome-level perturbation responses, and batch correction on single-cell RNA sequencing data. We also find parallels to fair representation learning and demonstrate that CoMP is competitive on a common task in the field.}
}
"
ICML,2022,Robin Hood and Matthew Effects: Differential Privacy Has Disparate Impact on Synthetic Data,https://proceedings.mlr.press/v162/ganev22a.html,"['Ganev, Georgi', 'Oprisanu, Bristena', 'De Cristofaro, Emiliano']",9,"Generative models trained with Differential Privacy (DP) can be used to generate synthetic data while minimizing privacy risks. We analyze the impact of DP on these models vis-a-vis underrepresented classes/subgroups of data, specifically, studying: 1) the size of classes/subgroups in the synthetic data, and 2) the accuracy of classification tasks run on them. We also evaluate the effect of various levels of imbalance and privacy budgets. Our analysis uses three state-of-the-art DP models (PrivBayes, DP-WGAN, and PATE-GAN) and shows that DP yields opposite size distributions in the generated synthetic data. It affects the gap between the majority and minority classes/subgroups; in some cases by reducing it (a ""Robin Hood"" effect) and, in others, by increasing it (a ""Matthew"" effect). Either way, this leads to (similar) disparate impacts on the accuracy of classification tasks on the synthetic data, affecting disproportionately more the underrepresented subparts of the data. Consequently, when training models on synthetic data, one might incur the risk of treating different subpopulations unevenly, leading to unreliable or unfair conclusions.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v162-ganev22a,
  title =          {Robin Hood and Matthew Effects: Differential Privacy Has Disparate Impact on Synthetic Data},
  author =       {Ganev, Georgi and Oprisanu, Bristena and De Cristofaro, Emiliano},
  booktitle =          {Proceedings of the 39th International Conference on Machine Learning},
  pages =          {6944--6959},
  year =          {2022},
  editor =          {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume =          {162},
  series =          {Proceedings of Machine Learning Research},
  month =          {17--23 Jul},
  publisher =    {PMLR},
  pdf =          {https://proceedings.mlr.press/v162/ganev22a/ganev22a.pdf},
  url =          {https://proceedings.mlr.press/v162/ganev22a.html},
  abstract =          {Generative models trained with Differential Privacy (DP) can be used to generate synthetic data while minimizing privacy risks. We analyze the impact of DP on these models vis-a-vis underrepresented classes/subgroups of data, specifically, studying: 1) the size of classes/subgroups in the synthetic data, and 2) the accuracy of classification tasks run on them. We also evaluate the effect of various levels of imbalance and privacy budgets. Our analysis uses three state-of-the-art DP models (PrivBayes, DP-WGAN, and PATE-GAN) and shows that DP yields opposite size distributions in the generated synthetic data. It affects the gap between the majority and minority classes/subgroups; in some cases by reducing it (a ""Robin Hood"" effect) and, in others, by increasing it (a ""Matthew"" effect). Either way, this leads to (similar) disparate impacts on the accuracy of classification tasks on the synthetic data, affecting disproportionately more the underrepresented subparts of the data. Consequently, when training models on synthetic data, one might incur the risk of treating different subpopulations unevenly, leading to unreliable or unfair conclusions.}
}
"
ICML,2022,Dual Perspective of Label Specific Feature Learning for Multi Label Classification,https://proceedings.mlr.press/v162/hang22a.html,"['Hang, Jun-Yi', 'Zhang, Min-Ling']",0,"Label-specific features serve as an effective strategy to facilitate multi-label classification, which account for the distinct discriminative properties of each class label via tailoring its own features. Existing approaches implement this strategy in a quite straightforward way, i.e. finding the most pertinent and discriminative features for each class label and directly inducing classifiers on constructed label-specific features. In this paper, we propose a dual perspective for label-specific feature learning, where label-specific discriminative properties are considered by identifying each label‚Äôs own non-informative features and making the discrimination process immutable to variations of these features. To instantiate it, we present a perturbation-based approach DELA to provide classifiers with label-specific immutability on simultaneously identified non-informative features, which is optimized towards a probabilistically-relaxed expected risk minimization problem. Comprehensive experiments on 10 benchmark data sets show that our approach outperforms the state-of-the-art counterparts.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v162-hang22a,
  title = 	 {Dual Perspective of Label-Specific Feature Learning for Multi-Label Classification},
  author =       {Hang, Jun-Yi and Zhang, Min-Ling},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {8375--8386},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/hang22a/hang22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/hang22a.html},
  abstract = 	 {Label-specific features serve as an effective strategy to facilitate multi-label classification, which account for the distinct discriminative properties of each class label via tailoring its own features. Existing approaches implement this strategy in a quite straightforward way, i.e. finding the most pertinent and discriminative features for each class label and directly inducing classifiers on constructed label-specific features. In this paper, we propose a dual perspective for label-specific feature learning, where label-specific discriminative properties are considered by identifying each label’s own non-informative features and making the discrimination process immutable to variations of these features. To instantiate it, we present a perturbation-based approach DELA to provide classifiers with label-specific immutability on simultaneously identified non-informative features, which is optimized towards a probabilistically-relaxed expected risk minimization problem. Comprehensive experiments on 10 benchmark data sets show that our approach outperforms the state-of-the-art counterparts.}
}
"
ICML,2022,Strategic Instrumental Variable Regression: Recovering Causal Relationships From Strategic Responses,https://proceedings.mlr.press/v162/harris22a.html,"['Harris, Keegan', 'Ngo, Dung Daniel T', 'Stapleton, Logan', 'Heidari, Hoda', 'Wu, Steven']",9,"In settings where Machine Learning (ML) algorithms automate or inform consequential decisions about people, individual decision subjects are often incentivized to strategically modify their observable attributes to receive more favorable predictions. As a result, the distribution the assessment rule is trained on may differ from the one it operates on in deployment. While such distribution shifts, in general, can hinder accurate predictions, our work identifies a unique opportunity associated with shifts due to strategic responses: We show that we can use strategic responses effectively to recover causal relationships between the observable features and outcomes we wish to predict, even under the presence of unobserved confounding variables. Specifically, our work establishes a novel connection between strategic responses to ML models and instrumental variable (IV) regression by observing that the sequence of deployed models can be viewed as an instrument that affects agents‚Äô observable features but does not directly influence their outcomes. We show that our causal recovery method can be utilized to improve decision-making across several important criteria: individual fairness, agent outcomes, and predictive risk. In particular, we show that if decision subjects differ in their ability to modify non-causal attributes, any decision rule deviating from the causal coefficients can lead to (potentially unbounded) individual-level unfairness. .",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v162-harris22a,
  title = 	 {Strategic Instrumental Variable Regression: Recovering Causal Relationships From Strategic Responses},
  author =       {Harris, Keegan and Ngo, Dung Daniel T and Stapleton, Logan and Heidari, Hoda and Wu, Steven},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {8502--8522},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/harris22a/harris22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/harris22a.html},
  abstract = 	 {In settings where Machine Learning (ML) algorithms automate or inform consequential decisions about people, individual decision subjects are often incentivized to strategically modify their observable attributes to receive more favorable predictions. As a result, the distribution the assessment rule is trained on may differ from the one it operates on in deployment. While such distribution shifts, in general, can hinder accurate predictions, our work identifies a unique opportunity associated with shifts due to strategic responses: We show that we can use strategic responses effectively to recover causal relationships between the observable features and outcomes we wish to predict, even under the presence of unobserved confounding variables. Specifically, our work establishes a novel connection between strategic responses to ML models and instrumental variable (IV) regression by observing that the sequence of deployed models can be viewed as an instrument that affects agents’ observable features but does not directly influence their outcomes. We show that our causal recovery method can be utilized to improve decision-making across several important criteria: individual fairness, agent outcomes, and predictive risk. In particular, we show that if decision subjects differ in their ability to modify non-causal attributes, any decision rule deviating from the causal coefficients can lead to (potentially unbounded) individual-level unfairness. .}
}
"
ICML,2022,Input agnostic Certified Group Fairness via {G}aussian Parameter Smoothing,https://proceedings.mlr.press/v162/jin22g.html,"['Jin, Jiayin', 'Zhang, Zeru', 'Zhou, Yang', 'Wu, Lingfei']",2,"Only recently, researchers attempt to provide classification algorithms with provable group fairness guarantees. Most of these algorithms suffer from harassment caused by the requirement that the training and deployment data follow the same distribution. This paper proposes an input-agnostic certified group fairness algorithm, FairSmooth, for improving the fairness of classification models while maintaining the remarkable prediction accuracy. A Gaussian parameter smoothing method is developed to transform base classifiers into their smooth versions. An optimal individual smooth classifier is learnt for each group with only the data regarding the group and an overall smooth classifier for all groups is generated by averaging the parameters of all the individual smooth ones. By leveraging the theory of nonlinear functional analysis, the smooth classifiers are reformulated as output functions of a Nemytskii operator. Theoretical analysis is conducted to derive that the Nemytskii operator is smooth and induces a Frechet differentiable smooth manifold. We theoretically demonstrate that the smooth manifold has a global Lipschitz constant that is independent of the domain of the input data, which derives the input-agnostic certified group fairness.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v162-jin22g,
  title = 	 {Input-agnostic Certified Group Fairness via {G}aussian Parameter Smoothing},
  author =       {Jin, Jiayin and Zhang, Zeru and Zhou, Yang and Wu, Lingfei},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {10340--10361},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/jin22g/jin22g.pdf},
  url = 	 {https://proceedings.mlr.press/v162/jin22g.html},
  abstract = 	 {Only recently, researchers attempt to provide classification algorithms with provable group fairness guarantees. Most of these algorithms suffer from harassment caused by the requirement that the training and deployment data follow the same distribution. This paper proposes an input-agnostic certified group fairness algorithm, FairSmooth, for improving the fairness of classification models while maintaining the remarkable prediction accuracy. A Gaussian parameter smoothing method is developed to transform base classifiers into their smooth versions. An optimal individual smooth classifier is learnt for each group with only the data regarding the group and an overall smooth classifier for all groups is generated by averaging the parameters of all the individual smooth ones. By leveraging the theory of nonlinear functional analysis, the smooth classifiers are reformulated as output functions of a Nemytskii operator. Theoretical analysis is conducted to derive that the Nemytskii operator is smooth and induces a Frechet differentiable smooth manifold. We theoretically demonstrate that the smooth manifold has a global Lipschitz constant that is independent of the domain of the input data, which derives the input-agnostic certified group fairness.}
}
"
ICML,2022,Matching Learned Causal Effects of Neural Networks with Domain Priors,https://proceedings.mlr.press/v162/kancheti22a.html,"['Kancheti, Sai Srinivas', 'Reddy, Abbavaram Gowtham', 'Balasubramanian, Vineeth N', 'Sharma, Amit']",399,"A trained neural network can be interpreted as a structural causal model (SCM) that provides the effect of changing input variables on the model‚Äôs output. However, if training data contains both causal and correlational relationships, a model that optimizes prediction accuracy may not necessarily learn the true causal relationships between input and output variables. On the other hand, expert users often have prior knowledge of the causal relationship between certain input variables and output from domain knowledge. Therefore, we propose a regularization method that aligns the learned causal effects of a neural network with domain priors, including both direct and total causal effects. We show that this approach can generalize to different kinds of domain priors, including monotonicity of causal effect of an input variable on output or zero causal effect of a variable on output for purposes of fairness. Our experiments on twelve benchmark datasets show its utility in regularizing a neural network model to maintain desired causal effects, without compromising on accuracy. Importantly, we also show that a model thus trained is robust and gets improved accuracy on noisy inputs.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v162-kancheti22a,
  title =          {Matching Learned Causal Effects of Neural Networks with Domain Priors},
  author =       {Kancheti, Sai Srinivas and Reddy, Abbavaram Gowtham and Balasubramanian, Vineeth N and Sharma, Amit},
  booktitle =          {Proceedings of the 39th International Conference on Machine Learning},
  pages =          {10676--10696},
  year =          {2022},
  editor =          {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume =          {162},
  series =          {Proceedings of Machine Learning Research},
  month =          {17--23 Jul},
  publisher =    {PMLR},
  pdf =          {https://proceedings.mlr.press/v162/kancheti22a/kancheti22a.pdf},
  url =          {https://proceedings.mlr.press/v162/kancheti22a.html},
  abstract =          {A trained neural network can be interpreted as a structural causal model (SCM) that provides the effect of changing input variables on the model’s output. However, if training data contains both causal and correlational relationships, a model that optimizes prediction accuracy may not necessarily learn the true causal relationships between input and output variables. On the other hand, expert users often have prior knowledge of the causal relationship between certain input variables and output from domain knowledge. Therefore, we propose a regularization method that aligns the learned causal effects of a neural network with domain priors, including both direct and total causal effects. We show that this approach can generalize to different kinds of domain priors, including monotonicity of causal effect of an input variable on output or zero causal effect of a variable on output for purposes of fairness. Our experiments on twelve benchmark datasets show its utility in regularizing a neural network model to maintain desired causal effects, without compromising on accuracy. Importantly, we also show that a model thus trained is robust and gets improved accuracy on noisy inputs.}
}
"
ICML,2022,Neural Network Poisson Models for Behavioural and Neural Spike Train Data,https://proceedings.mlr.press/v162/khajehnejad22a.html,"['Khajehnejad, Moein', 'Habibollahi, Forough', 'Nock, Richard', 'Arabzadeh, Ehsan', 'Dayan, Peter', 'Dezfouli, Amir']",1,"One of the most important and challenging application areas for complex machine learning methods is to predict, characterize and model rich, multi-dimensional, neural data. Recent advances in neural recording techniques have made it possible to monitor the activity of a large number of neurons across different brain regions as animals perform behavioural tasks. This poses the critical challenge of establishing links between neural activity at a microscopic scale, which might for instance represent sensory input, and at a macroscopic scale, which then generates behaviour. Predominant modeling methods apply rather disjoint techniques to these scales; by contrast, we suggest an end-to-end model which exploits recent developments of flexible, but tractable, neural network point-process models to characterize dependencies between stimuli, actions, and neural data. We apply this model to a public dataset collected using Neuropixel probes in mice performing a visually-guided behavioural task as well as a synthetic dataset produced from a hierarchical network model with reciprocally connected sensory and integration circuits intended to characterize animal behaviour in a fixed-duration motion discrimination task. We show that our model outperforms previous approaches and contributes novel insights into the relationships between neural activity and behaviour.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v162-khajehnejad22a,
  title = 	 {Neural Network Poisson Models for Behavioural and Neural Spike Train Data},
  author =       {Khajehnejad, Moein and Habibollahi, Forough and Nock, Richard and Arabzadeh, Ehsan and Dayan, Peter and Dezfouli, Amir},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {10974--10996},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/khajehnejad22a/khajehnejad22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/khajehnejad22a.html},
  abstract = 	 {One of the most important and challenging application areas for complex machine learning methods is to predict, characterize and model rich, multi-dimensional, neural data. Recent advances in neural recording techniques have made it possible to monitor the activity of a large number of neurons across different brain regions as animals perform behavioural tasks. This poses the critical challenge of establishing links between neural activity at a microscopic scale, which might for instance represent sensory input, and at a macroscopic scale, which then generates behaviour. Predominant modeling methods apply rather disjoint techniques to these scales; by contrast, we suggest an end-to-end model which exploits recent developments of flexible, but tractable, neural network point-process models to characterize dependencies between stimuli, actions, and neural data. We apply this model to a public dataset collected using Neuropixel probes in mice performing a visually-guided behavioural task as well as a synthetic dataset produced from a hierarchical network model with reciprocally connected sensory and integration circuits intended to characterize animal behaviour in a fixed-duration motion discrimination task. We show that our model outperforms previous approaches and contributes novel insights into the relationships between neural activity and behaviour.}
}
"
ICML,2022,Learning fair representation with a parametric integral probability metric,https://proceedings.mlr.press/v162/kim22b.html,"['Kim, Dongha', 'Kim, Kunwoong', 'Kong, Insung', 'Ohn, Ilsang', 'Kim, Yongdai']",4,"As they have a vital effect on social decision-making, AI algorithms should be not only accurate but also fair. Among various algorithms for fairness AI, learning fair representation (LFR), whose goal is to find a fair representation with respect to sensitive variables such as gender and race, has received much attention. For LFR, the adversarial training scheme is popularly employed as is done in the generative adversarial network type algorithms. The choice of a discriminator, however, is done heuristically without justification. In this paper, we propose a new adversarial training scheme for LFR, where the integral probability metric (IPM) with a specific parametric family of discriminators is used. The most notable result of the proposed LFR algorithm is its theoretical guarantee about the fairness of the final prediction model, which has not been considered yet. That is, we derive theoretical relations between the fairness of representation and the fairness of the prediction model built on the top of the representation (i.e., using the representation as the input). Moreover, by numerical experiments, we show that our proposed LFR algorithm is computationally lighter and more stable, and the final prediction model is competitive or superior to other LFR algorithms using more complex discriminators.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v162-kim22b,
  title =          {Learning fair representation with a parametric integral probability metric},
  author =       {Kim, Dongha and Kim, Kunwoong and Kong, Insung and Ohn, Ilsang and Kim, Yongdai},
  booktitle =          {Proceedings of the 39th International Conference on Machine Learning},
  pages =          {11074--11101},
  year =          {2022},
  editor =          {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume =          {162},
  series =          {Proceedings of Machine Learning Research},
  month =          {17--23 Jul},
  publisher =    {PMLR},
  pdf =          {https://proceedings.mlr.press/v162/kim22b/kim22b.pdf},
  url =          {https://proceedings.mlr.press/v162/kim22b.html},
  abstract =          {As they have a vital effect on social decision-making, AI algorithms should be not only accurate but also fair. Among various algorithms for fairness AI, learning fair representation (LFR), whose goal is to find a fair representation with respect to sensitive variables such as gender and race, has received much attention. For LFR, the adversarial training scheme is popularly employed as is done in the generative adversarial network type algorithms. The choice of a discriminator, however, is done heuristically without justification. In this paper, we propose a new adversarial training scheme for LFR, where the integral probability metric (IPM) with a specific parametric family of discriminators is used. The most notable result of the proposed LFR algorithm is its theoretical guarantee about the fairness of the final prediction model, which has not been considered yet. That is, we derive theoretical relations between the fairness of representation and the fairness of the prediction model built on the top of the representation (i.e., using the representation as the input). Moreover, by numerical experiments, we show that our proposed LFR algorithm is computationally lighter and more stable, and the final prediction model is competitive or superior to other LFR algorithms using more complex discriminators.}
}
"
ICML,2022,Generalized Strategic Classification and the Case of Aligned Incentives,https://proceedings.mlr.press/v162/levanon22a.html,"['Levanon, Sagi', 'Rosenfeld, Nir']",3,"Strategic classification studies learning in settings where self-interested users can strategically modify their features to obtain favorable predictive outcomes. A key working assumption, however, is that ‚Äúfavorable‚Äù always means ‚Äúpositive‚Äù; this may be appropriate in some applications (e.g., loan approval), but reduces to a fairly narrow view of what user interests can be. In this work we argue for a broader perspective on what accounts for strategic user behavior, and propose and study a flexible model of generalized strategic classification. Our generalized model subsumes most current models but includes other novel settings; among these, we identify and target one intriguing sub-class of problems in which the interests of users and the system are aligned. This setting reveals a surprising fact: that standard max-margin losses are ill-suited for strategic inputs. Returning to our fully generalized model, we propose a novel max-margin framework for strategic learning that is practical and effective, and which we analyze theoretically. We conclude with a set of experiments that empirically demonstrate the utility of our approach.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v162-levanon22a,
  title = 	 {Generalized Strategic Classification and the Case of Aligned Incentives},
  author =       {Levanon, Sagi and Rosenfeld, Nir},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {12593--12618},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/levanon22a/levanon22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/levanon22a.html},
  abstract = 	 {Strategic classification studies learning in settings where self-interested users can strategically modify their features to obtain favorable predictive outcomes. A key working assumption, however, is that “favorable” always means “positive”; this may be appropriate in some applications (e.g., loan approval), but reduces to a fairly narrow view of what user interests can be. In this work we argue for a broader perspective on what accounts for strategic user behavior, and propose and study a flexible model of generalized strategic classification. Our generalized model subsumes most current models but includes other novel settings; among these, we identify and target one intriguing sub-class of problems in which the interests of users and the system are aligned. This setting reveals a surprising fact: that standard max-margin losses are ill-suited for strategic inputs. Returning to our fully generalized model, we propose a novel max-margin framework for strategic learning that is practical and effective, and which we analyze theoretically. We conclude with a set of experiments that empirically demonstrate the utility of our approach.}
}
"
ICML,2022,Achieving Fairness at No Utility Cost via Data Reweighing with Influence,https://proceedings.mlr.press/v162/li22p.html,"['Li, Peizhao', 'Liu, Hongfu']",2,"With the fast development of algorithmic governance, fairness has become a compulsory property for machine learning models to suppress unintentional discrimination. In this paper, we focus on the pre-processing aspect for achieving fairness, and propose a data reweighing approach that only adjusts the weight for samples in the training phase. Different from most previous reweighing methods which usually assign a uniform weight for each (sub)group, we granularly model the influence of each training sample with regard to fairness-related quantity and predictive utility, and compute individual weights based on influence under the constraints from both fairness and utility. Experimental results reveal that previous methods achieve fairness at a non-negligible cost of utility, while as a significant advantage, our approach can empirically release the tradeoff and obtain cost-free fairness for equal opportunity. We demonstrate the cost-free fairness through vanilla classifiers and standard training processes, compared to baseline methods on multiple real-world tabular datasets. Code available at https://github.com/brandeis-machine-learning/influence-fairness.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v162-li22p,
  title = 	 {Achieving Fairness at No Utility Cost via Data Reweighing with Influence},
  author =       {Li, Peizhao and Liu, Hongfu},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {12917--12930},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/li22p/li22p.pdf},
  url = 	 {https://proceedings.mlr.press/v162/li22p.html},
  abstract = 	 {With the fast development of algorithmic governance, fairness has become a compulsory property for machine learning models to suppress unintentional discrimination. In this paper, we focus on the pre-processing aspect for achieving fairness, and propose a data reweighing approach that only adjusts the weight for samples in the training phase. Different from most previous reweighing methods which usually assign a uniform weight for each (sub)group, we granularly model the influence of each training sample with regard to fairness-related quantity and predictive utility, and compute individual weights based on influence under the constraints from both fairness and utility. Experimental results reveal that previous methods achieve fairness at a non-negligible cost of utility, while as a significant advantage, our approach can empirically release the tradeoff and obtain cost-free fairness for equal opportunity. We demonstrate the cost-free fairness through vanilla classifiers and standard training processes, compared to baseline methods on multiple real-world tabular datasets. Code available at https://github.com/brandeis-machine-learning/influence-fairness.}
}
"
ICML,2022,Let Invariant Rationale Discovery Inspire Graph Contrastive Learning,https://proceedings.mlr.press/v162/li22v.html,"['Li, Sihang', 'Wang, Xiang', 'Zhang, An', 'Wu, Yingxin', 'He, Xiangnan', 'Chua, Tat-Seng']",8,"Leading graph contrastive learning (GCL) methods perform graph augmentations in two fashions: (1) randomly corrupting the anchor graph, which could cause the loss of semantic information, or (2) using domain knowledge to maintain salient features, which undermines the generalization to other domains. Taking an invariance look at GCL, we argue that a high-performing augmentation should preserve the salient semantics of anchor graphs regarding instance-discrimination. To this end, we relate GCL with invariant rationale discovery, and propose a new framework, Rationale-aware Graph Contrastive Learning (RGCL). Specifically, without supervision signals, RGCL uses a rationale generator to reveal salient features about graph instance-discrimination as the rationale, and then creates rationale-aware views for contrastive learning. This rationale-aware pre-training scheme endows the backbone model with the powerful representation ability, further facilitating the fine-tuning on downstream tasks. On MNIST-Superpixel and MUTAG datasets, visual inspections on the discovered rationales showcase that the rationale generator successfully captures the salient features (\ie distinguishing semantic nodes in graphs). On biochemical molecule and social network benchmark datasets, the state-of-the-art performance of RGCL demonstrates the effectiveness of rationale-aware views for contrastive learning. Our codes are available at https://github.com/lsh0520/RGCL.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v162-li22v,
  title = 	 {Let Invariant Rationale Discovery Inspire Graph Contrastive Learning},
  author =       {Li, Sihang and Wang, Xiang and Zhang, An and Wu, Yingxin and He, Xiangnan and Chua, Tat-Seng},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {13052--13065},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/li22v/li22v.pdf},
  url = 	 {https://proceedings.mlr.press/v162/li22v.html},
  abstract = 	 {Leading graph contrastive learning (GCL) methods perform graph augmentations in two fashions: (1) randomly corrupting the anchor graph, which could cause the loss of semantic information, or (2) using domain knowledge to maintain salient features, which undermines the generalization to other domains. Taking an invariance look at GCL, we argue that a high-performing augmentation should preserve the salient semantics of anchor graphs regarding instance-discrimination. To this end, we relate GCL with invariant rationale discovery, and propose a new framework, Rationale-aware Graph Contrastive Learning (RGCL). Specifically, without supervision signals, RGCL uses a rationale generator to reveal salient features about graph instance-discrimination as the rationale, and then creates rationale-aware views for contrastive learning. This rationale-aware pre-training scheme endows the backbone model with the powerful representation ability, further facilitating the fine-tuning on downstream tasks. On MNIST-Superpixel and MUTAG datasets, visual inspections on the discovered rationales showcase that the rationale generator successfully captures the salient features (\ie distinguishing semantic nodes in graphs). On biochemical molecule and social network benchmark datasets, the state-of-the-art performance of RGCL demonstrates the effectiveness of rationale-aware views for contrastive learning. Our codes are available at https://github.com/lsh0520/RGCL.}
}
"
ICML,2022,Multi slots Online Matching with High Entropy,https://proceedings.mlr.press/v162/lu22e.html,"['Lu, Xingyu', 'Wu, Qintong', 'Zhong, Wenliang']",0,"Online matching with diversity and fairness pursuit, a common building block in the recommendation and advertising, can be modeled as constrained convex programming with high entropy. While most existing approaches are based on the ‚Äúsingle slot‚Äù assumption (i.e., assigning one item per iteration), they cannot be directly applied to cases with multiple slots, e.g., stock-aware top-N recommendation and advertising at multiple places. Particularly, the gradient computation and resource allocation are both challenging under this setting due to the absence of a closed-form solution. To overcome these obstacles, we develop a novel algorithm named Online subGradient descent for Multi-slots Allocation (OG-MA). It uses an efficient pooling algorithm to compute closed-form of the gradient then performs a roulette swapping for allocation, yielding a sub-linear regret with linear cost per iteration. Extensive experiments on synthetic and industrial data sets demonstrate that OG-MA is a fast and promising method for multi-slots online matching.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v162-lu22e,
  title = 	 {Multi-slots Online Matching with High Entropy},
  author =       {Lu, Xingyu and Wu, Qintong and Zhong, Wenliang},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {14412--14428},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/lu22e/lu22e.pdf},
  url = 	 {https://proceedings.mlr.press/v162/lu22e.html},
  abstract = 	 {Online matching with diversity and fairness pursuit, a common building block in the recommendation and advertising, can be modeled as constrained convex programming with high entropy. While most existing approaches are based on the “single slot” assumption (i.e., assigning one item per iteration), they cannot be directly applied to cases with multiple slots, e.g., stock-aware top-N recommendation and advertising at multiple places. Particularly, the gradient computation and resource allocation are both challenging under this setting due to the absence of a closed-form solution. To overcome these obstacles, we develop a novel algorithm named Online subGradient descent for Multi-slots Allocation (OG-MA). It uses an efficient pooling algorithm to compute closed-form of the gradient then performs a roulette swapping for allocation, yielding a sub-linear regret with linear cost per iteration. Extensive experiments on synthetic and industrial data sets demonstrate that OG-MA is a fast and promising method for multi-slots online matching.}
}
"
ICML,2022,Quantification and Analysis of Layer wise and Pixel wise Information Discarding,https://proceedings.mlr.press/v162/ma22b.html,"['Ma, Haotian', 'Zhang, Hao', 'Zhou, Fan', 'Zhang, Yinqing', 'Zhang, Quanshi']",0,"This paper presents a method to explain how the information of each input variable is gradually discarded during the forward propagation in a deep neural network (DNN), which provides new perspectives to explain DNNs. We define two types of entropy-based metrics, i.e. (1) the discarding of pixel-wise information used in the forward propagation, and (2) the uncertainty of the input reconstruction, to measure input information contained by a specific layer from two perspectives. Unlike previous attribution metrics, the proposed metrics ensure the fairness of comparisons between different layers of different DNNs. We can use these metrics to analyze the efficiency of information processing in DNNs, which exhibits strong connections to the performance of DNNs. We analyze information discarding in a pixel-wise manner, which is different from the information bottleneck theory measuring feature information w.r.t. the sample distribution. Experiments have shown the effectiveness of our metrics in analyzing classic DNNs and explaining existing deep-learning techniques. The code is available at https://github.com/haotianSustc/deepinfo.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v162-ma22b,
  title = 	 {Quantification and Analysis of Layer-wise and Pixel-wise Information Discarding},
  author =       {Ma, Haotian and Zhang, Hao and Zhou, Fan and Zhang, Yinqing and Zhang, Quanshi},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {14664--14698},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/ma22b/ma22b.pdf},
  url = 	 {https://proceedings.mlr.press/v162/ma22b.html},
  abstract = 	 {This paper presents a method to explain how the information of each input variable is gradually discarded during the forward propagation in a deep neural network (DNN), which provides new perspectives to explain DNNs. We define two types of entropy-based metrics, i.e. (1) the discarding of pixel-wise information used in the forward propagation, and (2) the uncertainty of the input reconstruction, to measure input information contained by a specific layer from two perspectives. Unlike previous attribution metrics, the proposed metrics ensure the fairness of comparisons between different layers of different DNNs. We can use these metrics to analyze the efficiency of information processing in DNNs, which exhibits strong connections to the performance of DNNs. We analyze information discarding in a pixel-wise manner, which is different from the information bottleneck theory measuring feature information w.r.t. the sample distribution. Experiments have shown the effectiveness of our metrics in analyzing classic DNNs and explaining existing deep-learning techniques. The code is available at https://github.com/haotianSustc/deepinfo.}
}
"
ICML,2022,On the Effects of Artificial Data Modification,https://proceedings.mlr.press/v162/marcu22a.html,"['Marcu, Antonia', 'Prugel-Bennett, Adam']",0,"Data distortion is commonly applied in vision models during both training (e.g methods like MixUp and CutMix) and evaluation (e.g. shape-texture bias and robustness). This data modification can introduce artificial information. It is often assumed that the resulting artefacts are detrimental to training, whilst being negligible when analysing models. We investigate these assumptions and conclude that in some cases they are unfounded and lead to incorrect results. Specifically, we show current shape bias identification methods and occlusion robustness measures are biased and propose a fairer alternative for the latter. Subsequently, through a series of experiments we seek to correct and strengthen the community‚Äôs perception of how augmenting affects learning of vision models. Based on our empirical results we argue that the impact of the artefacts must be understood and exploited rather than eliminated.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v162-marcu22a,
  title = 	 {On the Effects of Artificial Data Modification},
  author =       {Marcu, Antonia and Prugel-Bennett, Adam},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {15050--15069},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/marcu22a/marcu22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/marcu22a.html},
  abstract = 	 {Data distortion is commonly applied in vision models during both training (e.g methods like MixUp and CutMix) and evaluation (e.g. shape-texture bias and robustness). This data modification can introduce artificial information. It is often assumed that the resulting artefacts are detrimental to training, whilst being negligible when analysing models. We investigate these assumptions and conclude that in some cases they are unfounded and lead to incorrect results. Specifically, we show current shape bias identification methods and occlusion robustness measures are biased and propose a fairer alternative for the latter. Subsequently, through a series of experiments we seek to correct and strengthen the community’s perception of how augmenting affects learning of vision models. Based on our empirical results we argue that the impact of the artefacts must be understood and exploited rather than eliminated.}
}
"
ICML,2022,Personalized Federated Learning through Local Memorization,https://proceedings.mlr.press/v162/marfoq22a.html,"['Marfoq, Othmane', 'Neglia, Giovanni', 'Vidal, Richard', 'Kameni, Laetitia']",6,"Federated learning allows clients to collaboratively learn statistical models while keeping their data local. Federated learning was originally used to train a unique global model to be served to all clients, but this approach might be sub-optimal when clients‚Äô local data distributions are heterogeneous. In order to tackle this limitation, recent personalized federated learning methods train a separate model for each client while still leveraging the knowledge available at other clients. In this work, we exploit the ability of deep neural networks to extract high quality vectorial representations (embeddings) from non-tabular data, e.g., images and text, to propose a personalization mechanism based on local memorization. Personalization is obtained by interpolating a collectively trained global model with a local $k$-nearest neighbors (kNN) model based on the shared representation provided by the global model. We provide generalization bounds for the proposed approach in the case of binary classification, and we show on a suite of federated datasets that this approach achieves significantly higher accuracy and fairness than state-of-the-art methods.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v162-marfoq22a,
  title = 	 {Personalized Federated Learning through Local Memorization},
  author =       {Marfoq, Othmane and Neglia, Giovanni and Vidal, Richard and Kameni, Laetitia},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {15070--15092},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/marfoq22a/marfoq22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/marfoq22a.html},
  abstract = 	 {Federated learning allows clients to collaboratively learn statistical models while keeping their data local. Federated learning was originally used to train a unique global model to be served to all clients, but this approach might be sub-optimal when clients’ local data distributions are heterogeneous. In order to tackle this limitation, recent personalized federated learning methods train a separate model for each client while still leveraging the knowledge available at other clients. In this work, we exploit the ability of deep neural networks to extract high quality vectorial representations (embeddings) from non-tabular data, e.g., images and text, to propose a personalization mechanism based on local memorization. Personalization is obtained by interpolating a collectively trained global model with a local $k$-nearest neighbors (kNN) model based on the shared representation provided by the global model. We provide generalization bounds for the proposed approach in the case of binary classification, and we show on a suite of federated datasets that this approach achieves significantly higher accuracy and fairness than state-of-the-art methods.}
}
"
ICML,2022,Causal Conceptions of Fairness and their Consequences,https://proceedings.mlr.press/v162/nilforoshan22a.html,"['Nilforoshan, Hamed', 'Gaebler, Johann D', 'Shroff, Ravi', 'Goel, Sharad']",8,"Recent work highlights the role of causality in designing equitable decision-making algorithms. It is not immediately clear, however, how existing causal conceptions of fairness relate to one another, or what the consequences are of using these definitions as design principles. Here, we first assemble and categorize popular causal definitions of algorithmic fairness into two broad families: (1) those that constrain the effects of decisions on counterfactual disparities; and (2) those that constrain the effects of legally protected characteristics, like race and gender, on decisions. We then show, analytically and empirically, that both families of definitions almost always‚Äîin a measure theoretic sense‚Äîresult in strongly Pareto dominated decision policies, meaning there is an alternative, unconstrained policy favored by every stakeholder with preferences drawn from a large, natural class. For example, in the case of college admissions decisions, policies constrained to satisfy causal fairness definitions would be disfavored by every stakeholder with neutral or positive preferences for both academic preparedness and diversity. Indeed, under a prominent definition of causal fairness, we prove the resulting policies require admitting all students with the same probability, regardless of academic qualifications or group membership. Our results highlight formal limitations and potential adverse consequences of common mathematical notions of causal fairness.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v162-nilforoshan22a,
  title = 	 {Causal Conceptions of Fairness and their Consequences},
  author =       {Nilforoshan, Hamed and Gaebler, Johann D and Shroff, Ravi and Goel, Sharad},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {16848--16887},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/nilforoshan22a/nilforoshan22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/nilforoshan22a.html},
  abstract = 	 {Recent work highlights the role of causality in designing equitable decision-making algorithms. It is not immediately clear, however, how existing causal conceptions of fairness relate to one another, or what the consequences are of using these definitions as design principles. Here, we first assemble and categorize popular causal definitions of algorithmic fairness into two broad families: (1) those that constrain the effects of decisions on counterfactual disparities; and (2) those that constrain the effects of legally protected characteristics, like race and gender, on decisions. We then show, analytically and empirically, that both families of definitions <em>almost always</em>—in a measure theoretic sense—result in strongly Pareto dominated decision policies, meaning there is an alternative, unconstrained policy favored by every stakeholder with preferences drawn from a large, natural class. For example, in the case of college admissions decisions, policies constrained to satisfy causal fairness definitions would be disfavored by every stakeholder with neutral or positive preferences for both academic preparedness and diversity. Indeed, under a prominent definition of causal fairness, we prove the resulting policies require admitting all students with the same probability, regardless of academic qualifications or group membership. Our results highlight formal limitations and potential adverse consequences of common mathematical notions of causal fairness.}
}
"
ICML,2022,A Differential Entropy Estimator for Training Neural Networks,https://proceedings.mlr.press/v162/pichler22a.html,"['Pichler, Georg', 'Colombo, Pierre Jean A.', 'Boudiaf, Malik', 'Koliander, G{\\""u}nther', 'Piantanida, Pablo']",4,"Mutual Information (MI) has been widely used as a loss regularizer for training neural networks. This has been particularly effective when learn disentangled or compressed representations of high dimensional data. However, differential entropy (DE), another fundamental measure of information, has not found widespread use in neural network training. Although DE offers a potentially wider range of applications than MI, off-the-shelf DE estimators are either non differentiable, computationally intractable or fail to adapt to changes in the underlying distribution. These drawbacks prevent them from being used as regularizers in neural networks training. To address shortcomings in previously proposed estimators for DE, here we introduce KNIFE, a fully parameterized, differentiable kernel-based estimator of DE. The flexibility of our approach also allows us to construct KNIFE-based estimators for conditional (on either discrete or continuous variables) DE, as well as MI. We empirically validate our method on high-dimensional synthetic data and further apply it to guide the training of neural networks for real-world tasks. Our experiments on a large variety of tasks, including visual domain adaptation, textual fair classification, and textual fine-tuning demonstrate the effectiveness of KNIFE-based estimation. Code can be found at https://github.com/g-pichler/knife.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v162-pichler22a,
  title = 	 {A Differential Entropy Estimator for Training Neural Networks},
  author =       {Pichler, Georg and Colombo, Pierre Jean A. and Boudiaf, Malik and Koliander, G{\""u}nther and Piantanida, Pablo},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {17691--17715},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/pichler22a/pichler22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/pichler22a.html},
  abstract = 	 {Mutual Information (MI) has been widely used as a loss regularizer for training neural networks. This has been particularly effective when learn disentangled or compressed representations of high dimensional data. However, differential entropy (DE), another fundamental measure of information, has not found widespread use in neural network training. Although DE offers a potentially wider range of applications than MI, off-the-shelf DE estimators are either non differentiable, computationally intractable or fail to adapt to changes in the underlying distribution. These drawbacks prevent them from being used as regularizers in neural networks training. To address shortcomings in previously proposed estimators for DE, here we introduce KNIFE, a fully parameterized, differentiable kernel-based estimator of DE. The flexibility of our approach also allows us to construct KNIFE-based estimators for conditional (on either discrete or continuous variables) DE, as well as MI. We empirically validate our method on high-dimensional synthetic data and further apply it to guide the training of neural networks for real-world tasks. Our experiments on a large variety of tasks, including visual domain adaptation, textual fair classification, and textual fine-tuning demonstrate the effectiveness of KNIFE-based estimation. Code can be found at https://github.com/g-pichler/knife.}
}
"
ICML,2022,Selective Regression under Fairness Criteria,https://proceedings.mlr.press/v162/shah22a.html,"['Shah, Abhin', 'Bu, Yuheng', 'Lee, Joshua K', 'Das, Subhro', 'Panda, Rameswar', 'Sattigeri, Prasanna', 'Wornell, Gregory W']",1,"Selective regression allows abstention from prediction if the confidence to make an accurate prediction is not sufficient. In general, by allowing a reject option, one expects the performance of a regression model to increase at the cost of reducing coverage (i.e., by predicting on fewer samples). However, as we show, in some cases, the performance of a minority subgroup can decrease while we reduce the coverage, and thus selective regression can magnify disparities between different sensitive subgroups. Motivated by these disparities, we propose new fairness criteria for selective regression requiring the performance of every subgroup to improve with a decrease in coverage. We prove that if a feature representation satisfies the sufficiency criterion or is calibrated for mean and variance, then the proposed fairness criteria is met. Further, we introduce two approaches to mitigate the performance disparity across subgroups: (a) by regularizing an upper bound of conditional mutual information under a Gaussian assumption and (b) by regularizing a contrastive loss for conditional mean and conditional variance prediction. The effectiveness of these approaches is demonstrated on synthetic and real-world datasets.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v162-shah22a,
  title =          {Selective Regression under Fairness Criteria},
  author =       {Shah, Abhin and Bu, Yuheng and Lee, Joshua K and Das, Subhro and Panda, Rameswar and Sattigeri, Prasanna and Wornell, Gregory W},
  booktitle =          {Proceedings of the 39th International Conference on Machine Learning},
  pages =          {19598--19615},
  year =          {2022},
  editor =          {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume =          {162},
  series =          {Proceedings of Machine Learning Research},
  month =          {17--23 Jul},
  publisher =    {PMLR},
  pdf =          {https://proceedings.mlr.press/v162/shah22a/shah22a.pdf},
  url =          {https://proceedings.mlr.press/v162/shah22a.html},
  abstract =          {Selective regression allows abstention from prediction if the confidence to make an accurate prediction is not sufficient. In general, by allowing a reject option, one expects the performance of a regression model to increase at the cost of reducing coverage (i.e., by predicting on fewer samples). However, as we show, in some cases, the performance of a minority subgroup can decrease while we reduce the coverage, and thus selective regression can magnify disparities between different sensitive subgroups. Motivated by these disparities, we propose new fairness criteria for selective regression requiring the performance of every subgroup to improve with a decrease in coverage. We prove that if a feature representation satisfies the <em>sufficiency</em> criterion or is <em>calibrated for mean and variance</em>, then the proposed fairness criteria is met. Further, we introduce two approaches to mitigate the performance disparity across subgroups: (a) by regularizing an upper bound of conditional mutual information under a Gaussian assumption and (b) by regularizing a contrastive loss for conditional mean and conditional variance prediction. The effectiveness of these approaches is demonstrated on synthetic and real-world datasets.}
}
"
ICML,2022,Metric Fair Active Learning,https://proceedings.mlr.press/v162/shen22b.html,"['Shen, Jie', 'Cui, Nan', 'Wang, Jing']",0,"Active learning has become a prevalent technique for designing label-efficient algorithms, where the central principle is to only query and fit ‚Äúinformative‚Äù labeled instances. It is, however, known that an active learning algorithm may incur unfairness due to such instance selection procedure. In this paper, we henceforth study metric-fair active learning of homogeneous halfspaces, and show that under the distribution-dependent PAC learning model, fairness and label efficiency can be achieved simultaneously. We further propose two extensions of our main results: 1) we show that it is possible to make the algorithm robust to the adversarial noise¬†‚Äì¬†one of the most challenging noise models in learning theory; and 2) it is possible to significantly improve the label complexity when the underlying halfspace is sparse.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v162-shen22b,
  title = 	 {Metric-Fair Active Learning},
  author =       {Shen, Jie and Cui, Nan and Wang, Jing},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {19809--19826},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/shen22b/shen22b.pdf},
  url = 	 {https://proceedings.mlr.press/v162/shen22b.html},
  abstract = 	 {Active learning has become a prevalent technique for designing label-efficient algorithms, where the central principle is to only query and fit “informative” labeled instances. It is, however, known that an active learning algorithm may incur unfairness due to such instance selection procedure. In this paper, we henceforth study metric-fair active learning of homogeneous halfspaces, and show that under the distribution-dependent PAC learning model, fairness and label efficiency can be achieved simultaneously. We further propose two extensions of our main results: 1) we show that it is possible to make the algorithm robust to the adversarial noise&nbsp;–&nbsp;one of the most challenging noise models in learning theory; and 2) it is possible to significantly improve the label complexity when the underlying halfspace is sparse.}
}
"
ICML,2022,Fair Representation Learning through Implicit Path Alignment,https://proceedings.mlr.press/v162/shui22a.html,"['Shui, Changjian', 'Chen, Qi', 'Li, Jiaqi', 'Wang, Boyu', ""Gagn{\\'e}, Christian""]",2,"We consider a fair representation learning perspective, where optimal predictors, on top of the data representation, are ensured to be invariant with respect to different sub-groups. Specifically, we formulate this intuition as a bi-level optimization, where the representation is learned in the outer-loop, and invariant optimal group predictors are updated in the inner-loop. Moreover, the proposed bi-level objective is demonstrated to fulfill the sufficiency rule, which is desirable in various practical scenarios but was not commonly studied in the fair learning. Besides, to avoid the high computational and memory cost of differentiating in the inner-loop of bi-level objective, we propose an implicit path alignment algorithm, which only relies on the solution of inner optimization and the implicit differentiation rather than the exact optimization path. We further analyze the error gap of the implicit approach and empirically validate the proposed method in both classification and regression settings. Experimental results show the consistently better trade-off in prediction performance and fairness measurement.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v162-shui22a,
  title =          {Fair Representation Learning through Implicit Path Alignment},
  author =       {Shui, Changjian and Chen, Qi and Li, Jiaqi and Wang, Boyu and Gagn{\'e}, Christian},
  booktitle =          {Proceedings of the 39th International Conference on Machine Learning},
  pages =          {20156--20175},
  year =          {2022},
  editor =          {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume =          {162},
  series =          {Proceedings of Machine Learning Research},
  month =          {17--23 Jul},
  publisher =    {PMLR},
  pdf =          {https://proceedings.mlr.press/v162/shui22a/shui22a.pdf},
  url =          {https://proceedings.mlr.press/v162/shui22a.html},
  abstract =          {We consider a fair representation learning perspective, where optimal predictors, on top of the data representation, are ensured to be invariant with respect to different sub-groups. Specifically, we formulate this intuition as a bi-level optimization, where the representation is learned in the outer-loop, and invariant optimal group predictors are updated in the inner-loop. Moreover, the proposed bi-level objective is demonstrated to fulfill the sufficiency rule, which is desirable in various practical scenarios but was not commonly studied in the fair learning. Besides, to avoid the high computational and memory cost of differentiating in the inner-loop of bi-level objective, we propose an implicit path alignment algorithm, which only relies on the solution of inner optimization and the implicit differentiation rather than the exact optimization path. We further analyze the error gap of the implicit approach and empirically validate the proposed method in both classification and regression settings. Experimental results show the consistently better trade-off in prediction performance and fairness measurement.}
}
"
ICML,2022,Intriguing Properties of Input Dependent Randomized Smoothing,https://proceedings.mlr.press/v162/sukeni-k22a.html,"[""S{\\'u}ken\\'{\\i}k, Peter"", 'Kuvshinov, Aleksei', 'G{\\""u}nnemann, Stephan']",5,"Randomized smoothing is currently considered the state-of-the-art method to obtain certifiably robust classifiers. Despite its remarkable performance, the method is associated with various serious problems such as ‚Äúcertified accuracy waterfalls‚Äù, certification vs. accuracy trade-off, or even fairness issues. Input-dependent smoothing approaches have been proposed with intention of overcoming these flaws. However, we demonstrate that these methods lack formal guarantees and so the resulting certificates are not justified. We show that in general, the input-dependent smoothing suffers from the curse of dimensionality, forcing the variance function to have low semi-elasticity. On the other hand, we provide a theoretical and practical framework that enables the usage of input-dependent smoothing even in the presence of the curse of dimensionality, under strict restrictions. We present one concrete design of the smoothing variance function and test it on CIFAR10 and MNIST. Our design mitigates some of the problems of classical smoothing and is formally underlined, yet further improvement of the design is still necessary.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v162-sukeni-k22a,
  title = 	 {Intriguing Properties of Input-Dependent Randomized Smoothing},
  author =       {S{\'u}ken\'{\i}k, Peter and Kuvshinov, Aleksei and G{\""u}nnemann, Stephan},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {20697--20743},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/sukeni-k22a/sukeni-k22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/sukeni-k22a.html},
  abstract = 	 {Randomized smoothing is currently considered the state-of-the-art method to obtain certifiably robust classifiers. Despite its remarkable performance, the method is associated with various serious problems such as “certified accuracy waterfalls”, certification vs. accuracy trade-off, or even fairness issues. Input-dependent smoothing approaches have been proposed with intention of overcoming these flaws. However, we demonstrate that these methods lack formal guarantees and so the resulting certificates are not justified. We show that in general, the input-dependent smoothing suffers from the curse of dimensionality, forcing the variance function to have low semi-elasticity. On the other hand, we provide a theoretical and practical framework that enables the usage of input-dependent smoothing even in the presence of the curse of dimensionality, under strict restrictions. We present one concrete design of the smoothing variance function and test it on CIFAR10 and MNIST. Our design mitigates some of the problems of classical smoothing and is formally underlined, yet further improvement of the design is still necessary.}
}
"
ICML,2022,Simple and near optimal algorithms for hidden stratification and multi group learning,https://proceedings.mlr.press/v162/tosh22a.html,"['Tosh, Christopher J', 'Hsu, Daniel']",5,"Multi-group agnostic learning is a formal learning criterion that is concerned with the conditional risks of predictors within subgroups of a population. The criterion addresses recent practical concerns such as subgroup fairness and hidden stratification. This paper studies the structure of solutions to the multi-group learning problem, and provides simple and near-optimal algorithms for the learning problem.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v162-tosh22a,
  title = 	 {Simple and near-optimal algorithms for hidden stratification and multi-group learning},
  author =       {Tosh, Christopher J and Hsu, Daniel},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {21633--21657},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/tosh22a/tosh22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/tosh22a.html},
  abstract = 	 {Multi-group agnostic learning is a formal learning criterion that is concerned with the conditional risks of predictors within subgroups of a population. The criterion addresses recent practical concerns such as subgroup fairness and hidden stratification. This paper studies the structure of solutions to the multi-group learning problem, and provides simple and near-optimal algorithms for the learning problem.}
}
"
ICML,2022,Prototype Based Classification from Hierarchy to Fairness,https://proceedings.mlr.press/v162/tucker22a.html,"['Tucker, Mycal', 'Shah, Julie A.']",0,"Artificial neural nets can represent and classify many types of high-dimensional data but are often tailored to particular applications ‚Äì e.g., for ‚Äúfair‚Äù or ‚Äúhierarchical‚Äù classification. Once an architecture has been selected, it is often difficult for humans to adjust models for a new task; for example, a hierarchical classifier cannot be easily transformed into a fair classifier that shields a protected field. Our contribution in this work is a new neural network architecture, the concept subspace network (CSN), which generalizes existing specialized classifiers to produce a unified model capable of learning a spectrum of multi-concept relationships. We demonstrate that CSNs reproduce state-of-the-art results in fair classification when enforcing concept independence, may be transformed into hierarchical classifiers, or may even reconcile fairness and hierarchy within a single classifier. The CSN is inspired by and matches the performance of existing prototype-based classifiers that promote interpretability.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v162-tucker22a,
  title = 	 {Prototype Based Classification from Hierarchy to Fairness},
  author =       {Tucker, Mycal and Shah, Julie A.},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {21884--21900},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/tucker22a/tucker22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/tucker22a.html},
  abstract = 	 {Artificial neural nets can represent and classify many types of high-dimensional data but are often tailored to particular applications – e.g., for “fair” or “hierarchical” classification. Once an architecture has been selected, it is often difficult for humans to adjust models for a new task; for example, a hierarchical classifier cannot be easily transformed into a fair classifier that shields a protected field. Our contribution in this work is a new neural network architecture, the concept subspace network (CSN), which generalizes existing specialized classifiers to produce a unified model capable of learning a spectrum of multi-concept relationships. We demonstrate that CSNs reproduce state-of-the-art results in fair classification when enforcing concept independence, may be transformed into hierarchical classifiers, or may even reconcile fairness and hierarchy within a single classifier. The CSN is inspired by and matches the performance of existing prototype-based classifiers that promote interpretability.}
}
"
ICML,2022,Understanding Instance Level Impact of Fairness Constraints,https://proceedings.mlr.press/v162/wang22ac.html,"['Wang, Jialu', 'Wang, Xin Eric', 'Liu, Yang']",1,"A variety of fairness constraints have been proposed in the literature to mitigate group-level statistical bias. Their impacts have been largely evaluated for different groups of populations corresponding to a set of sensitive attributes, such as race or gender. Nonetheless, the community has not observed sufficient explorations for how imposing fairness constraints fare at an instance level. Building on the concept of influence function, a measure that characterizes the impact of a training example on the target model and its predictive performance, this work studies the influence of training examples when fairness constraints are imposed. We find out that under certain assumptions, the influence function with respect to fairness constraints can be decomposed into a kernelized combination of training examples. One promising application of the proposed fairness influence function is to identify suspicious training examples that may cause model discrimination by ranking their influence scores. We demonstrate with extensive experiments that training on a subset of weighty data examples leads to lower fairness violations with a trade-off of accuracy.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v162-wang22ac,
  title =          {Understanding Instance-Level Impact of Fairness Constraints},
  author =       {Wang, Jialu and Wang, Xin Eric and Liu, Yang},
  booktitle =          {Proceedings of the 39th International Conference on Machine Learning},
  pages =          {23114--23130},
  year =          {2022},
  editor =          {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume =          {162},
  series =          {Proceedings of Machine Learning Research},
  month =          {17--23 Jul},
  publisher =    {PMLR},
  pdf =          {https://proceedings.mlr.press/v162/wang22ac/wang22ac.pdf},
  url =          {https://proceedings.mlr.press/v162/wang22ac.html},
  abstract =          {A variety of fairness constraints have been proposed in the literature to mitigate group-level statistical bias. Their impacts have been largely evaluated for different groups of populations corresponding to a set of sensitive attributes, such as race or gender. Nonetheless, the community has not observed sufficient explorations for how imposing fairness constraints fare at an instance level. Building on the concept of influence function, a measure that characterizes the impact of a training example on the target model and its predictive performance, this work studies the influence of training examples when fairness constraints are imposed. We find out that under certain assumptions, the influence function with respect to fairness constraints can be decomposed into a kernelized combination of training examples. One promising application of the proposed fairness influence function is to identify suspicious training examples that may cause model discrimination by ranking their influence scores. We demonstrate with extensive experiments that training on a subset of weighty data examples leads to lower fairness violations with a trade-off of accuracy.}
}
"
ICML,2022,Metric Fair Classifier Derandomization,https://proceedings.mlr.press/v162/wu22a.html,"['Wu, Jimmy', 'Chen, Yatong', 'Liu, Yang']",2,"We study the problem of classifier derandomization in machine learning: given a stochastic binary classifier $f: X \to [0,1]$, sample a deterministic classifier $\hat{f}: X \to \{0,1\}$ that approximates the output of $f$ in aggregate over any data distribution. Recent work revealed how to efficiently derandomize a stochastic classifier with strong output approximation guarantees, but at the cost of individual fairness ‚Äî that is, if $f$ treated similar inputs similarly, $\hat{f}$ did not. In this paper, we initiate a systematic study of classifier derandomization with metric fairness guarantees. We show that the prior derandomization approach is almost maximally metric-unfair, and that a simple ‚Äúrandom threshold‚Äù derandomization achieves optimal fairness preservation but with weaker output approximation. We then devise a derandomization procedure that provides an appealing tradeoff between these two: if $f$ is $\alpha$-metric fair according to a metric $d$ with a locality-sensitive hash (LSH) family, then our derandomized $\hat{f}$ is, with high probability, $O(\alpha)$-metric fair and a close approximation of $f$. We also prove generic results applicable to all (fair and unfair) classifier derandomization procedures, including a bias-variance decomposition and reductions between various notions of metric fairness.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v162-wu22a,
  title = 	 {Metric-Fair Classifier Derandomization},
  author =       {Wu, Jimmy and Chen, Yatong and Liu, Yang},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {23999--24016},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/wu22a/wu22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/wu22a.html},
  abstract = 	 {We study the problem of <em>classifier derandomization</em> in machine learning: given a stochastic binary classifier $f: X \to [0,1]$, sample a deterministic classifier $\hat{f}: X \to \{0,1\}$ that approximates the output of $f$ in aggregate over any data distribution. Recent work revealed how to efficiently derandomize a stochastic classifier with strong output approximation guarantees, but at the cost of individual fairness — that is, if $f$ treated similar inputs similarly, $\hat{f}$ did not. In this paper, we initiate a systematic study of classifier derandomization with metric fairness guarantees. We show that the prior derandomization approach is almost maximally metric-unfair, and that a simple “random threshold” derandomization achieves optimal fairness preservation but with weaker output approximation. We then devise a derandomization procedure that provides an appealing tradeoff between these two: if $f$ is $\alpha$-metric fair according to a metric $d$ with a locality-sensitive hash (LSH) family, then our derandomized $\hat{f}$ is, with high probability, $O(\alpha)$-metric fair and a close approximation of $f$. We also prove generic results applicable to all (fair and unfair) classifier derandomization procedures, including a bias-variance decomposition and reductions between various notions of metric fairness.}
}
"
ICML,2022,Active fairness auditing,https://proceedings.mlr.press/v162/yan22c.html,"['Yan, Tom', 'Zhang, Chicheng']",0,"The fast spreading adoption of machine learning (ML) by companies across industries poses significant regulatory challenges. One such challenge is scalability: how can regulatory bodies efficiently audit these ML models, ensuring that they are fair? In this paper, we initiate the study of query-based auditing algorithms that can estimate the demographic parity of ML models in a query-efficient manner. We propose an optimal deterministic algorithm, as well as a practical randomized, oracle-efficient algorithm with comparable guarantees. Furthermore, we make inroads into understanding the optimal query complexity of randomized active fairness estimation algorithms. Our first exploration of active fairness estimation aims to put AI governance on firmer theoretical foundations.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v162-yan22c,
  title =          {Active fairness auditing},
  author =       {Yan, Tom and Zhang, Chicheng},
  booktitle =          {Proceedings of the 39th International Conference on Machine Learning},
  pages =          {24929--24962},
  year =          {2022},
  editor =          {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume =          {162},
  series =          {Proceedings of Machine Learning Research},
  month =          {17--23 Jul},
  publisher =    {PMLR},
  pdf =          {https://proceedings.mlr.press/v162/yan22c/yan22c.pdf},
  url =          {https://proceedings.mlr.press/v162/yan22c.html},
  abstract =          {The fast spreading adoption of machine learning (ML) by companies across industries poses significant regulatory challenges. One such challenge is scalability: how can regulatory bodies efficiently <em>audit</em> these ML models, ensuring that they are fair? In this paper, we initiate the study of query-based auditing algorithms that can estimate the demographic parity of ML models in a query-efficient manner. We propose an optimal deterministic algorithm, as well as a practical randomized, oracle-efficient algorithm with comparable guarantees. Furthermore, we make inroads into understanding the optimal query complexity of randomized active fairness estimation algorithms. Our first exploration of active fairness estimation aims to put AI governance on firmer theoretical foundations.}
}
"
ICML,2022,Adaptive Conformal Predictions for Time Series,https://proceedings.mlr.press/v162/zaffran22a.html,"['Zaffran, Margaux', 'Feron, Olivier', 'Goude, Yannig', 'Josse, Julie', 'Dieuleveut, Aymeric']",11,"Uncertainty quantification of predictive models is crucial in decision-making problems. Conformal prediction is a general and theoretically sound answer. However, it requires exchangeable data, excluding time series. While recent works tackled this issue, we argue that Adaptive Conformal Inference (ACI, Gibbs & Cand{√®}s, 2021), developed for distribution-shift time series, is a good procedure for time series with general dependency. We theoretically analyse the impact of the learning rate on its efficiency in the exchangeable and auto-regressive case. We propose a parameter-free method, AgACI, that adaptively builds upon ACI based on online expert aggregation. We lead extensive fair simulations against competing methods that advocate for ACI‚Äôs use in time series. We conduct a real case study: electricity price forecasting. The proposed aggregation algorithm provides efficient prediction intervals for day-ahead forecasting. All the code and data to reproduce the experiments are made available on GitHub.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v162-zaffran22a,
  title = 	 {Adaptive Conformal Predictions for Time Series},
  author =       {Zaffran, Margaux and Feron, Olivier and Goude, Yannig and Josse, Julie and Dieuleveut, Aymeric},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {25834--25866},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/zaffran22a/zaffran22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/zaffran22a.html},
  abstract = 	 {Uncertainty quantification of predictive models is crucial in decision-making problems. Conformal prediction is a general and theoretically sound answer. However, it requires exchangeable data, excluding time series. While recent works tackled this issue, we argue that Adaptive Conformal Inference (ACI, Gibbs &amp; Cand{è}s, 2021), developed for distribution-shift time series, is a good procedure for time series with general dependency. We theoretically analyse the impact of the learning rate on its efficiency in the exchangeable and auto-regressive case. We propose a parameter-free method, AgACI, that adaptively builds upon ACI based on online expert aggregation. We lead extensive fair simulations against competing methods that advocate for ACI’s use in time series. We conduct a real case study: electricity price forecasting. The proposed aggregation algorithm provides efficient prediction intervals for day-ahead forecasting. All the code and data to reproduce the experiments are made available on GitHub.}
}
"
ICML,2022,Fairness Interventions as ({D}is){I}ncentives for Strategic Manipulation,https://proceedings.mlr.press/v162/zhang22l.html,"['Zhang, Xueru', 'Khalili, Mohammad Mahdi', 'Jin, Kun', 'Naghizadeh, Parinaz', 'Liu, Mingyan']",1,"Although machine learning (ML) algorithms are widely used to make decisions about individuals in various domains, concerns have arisen that (1) these algorithms are vulnerable to strategic manipulation and ""gaming the algorithm""; and (2) ML decisions may exhibit bias against certain social groups. Existing works have largely examined these as two separate issues, e.g., by focusing on building ML algorithms robust to strategic manipulation, or on training a fair ML algorithm. In this study, we set out to understand the impact they each have on the other, and examine how to characterize fair policies in the presence of strategic behavior. The strategic interaction between a decision maker and individuals (as decision takers) is modeled as a two-stage (Stackelberg) game; when designing an algorithm, the former anticipates the latter may manipulate their features in order to receive more favorable decisions. We analytically characterize the equilibrium strategies of both, and examine how the algorithms and their resulting fairness properties are affected when the decision maker is strategic (anticipates manipulation), as well as the impact of fairness interventions on equilibrium strategies. In particular, we identify conditions under which anticipation of strategic behavior may mitigate/exacerbate unfairness, and conditions under which fairness interventions can serve as (dis)incentives for strategic manipulation.",,,,,,,,,,,,,,,,,"
@InProceedings{pmlr-v162-zhang22l,
  title = 	 {Fairness Interventions as ({D}is){I}ncentives for Strategic Manipulation},
  author =       {Zhang, Xueru and Khalili, Mohammad Mahdi and Jin, Kun and Naghizadeh, Parinaz and Liu, Mingyan},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {26239--26264},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/zhang22l/zhang22l.pdf},
  url = 	 {https://proceedings.mlr.press/v162/zhang22l.html},
  abstract = 	 {Although machine learning (ML) algorithms are widely used to make decisions about individuals in various domains, concerns have arisen that (1) these algorithms are vulnerable to strategic manipulation and ""gaming the algorithm""; and (2) ML decisions may exhibit bias against certain social groups. Existing works have largely examined these as two separate issues, e.g., by focusing on building ML algorithms robust to strategic manipulation, or on training a fair ML algorithm. In this study, we set out to understand the impact they each have on the other, and examine how to characterize fair policies in the presence of strategic behavior. The strategic interaction between a decision maker and individuals (as decision takers) is modeled as a two-stage (Stackelberg) game; when designing an algorithm, the former anticipates the latter may manipulate their features in order to receive more favorable decisions. We analytically characterize the equilibrium strategies of both, and examine how the algorithms and their resulting fairness properties are affected when the decision maker is strategic (anticipates manipulation), as well as the impact of fairness interventions on equilibrium strategies. In particular, we identify conditions under which anticipation of strategic behavior may mitigate/exacerbate unfairness, and conditions under which fairness interventions can serve as (dis)incentives for strategic manipulation.}
}
"