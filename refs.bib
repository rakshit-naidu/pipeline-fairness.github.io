@inproceedings{black2022algorithmic,
	title        = {Algorithmic fairness and vertical equity: Income fairness with IRS tax audit models},
	author       = {Black, Emily and Elzayn, Hadi and Chouldechova, Alexandra and Goldin, Jacob and Ho, Daniel},
	booktitle    = {2022 ACM Conference on Fairness, Accountability, and Transparency},
	pages        = {1479--1503},
	year         = 2022,
}
@inproceedings{barocas2020not,
	title        = {When not to design, build, or deploy},
	author       = {Barocas, Solon and Biega, Asia J and Fish, Benjamin and Niklas, J{\k{e}}drzej and Stark, Luke},
	booktitle    = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
	pages        = {695--695},
	year         = 2020,
}
@inproceedings{biswas2021ensuring,
	title        = {Ensuring fairness under prior probability shifts},
	author       = {Biswas, Arpita and Mukherjee, Suvam},
	booktitle    = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
	pages        = {414--424},
	year         = 2021,
}
@inproceedings{albarghouthi2019fairness,
	title        = {Fairness-aware programming},
	author       = {Albarghouthi, Aws and Vinitsky, Samuel},
	booktitle    = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
	pages        = {211--219},
	year         = 2019,
}
@inproceedings{ghosh2022faircanary,
	title        = {Faircanary: Rapid continuous explainable fairness},
	author       = {Ghosh, Avijit and Shanbhag, Aalok and Wilson, Christo},
	booktitle    = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
	pages        = {307--316},
	year         = 2022,
}
@inproceedings{mattei2018fairness,
	title        = {Fairness in deceased organ matching},
	author       = {Mattei, Nicholas and Saffidine, Abdallah and Walsh, Toby},
	booktitle    = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
	pages        = {236--242},
	year         = 2018,
}
@article{ding2021retiring,
	title        = {Retiring adult: New datasets for fair machine learning},
	author       = {Ding, Frances and Hardt, Moritz and Miller, John and Schmidt, Ludwig},
	journal      = {Advances in neural information processing systems},
	volume       = 34,
	pages        = {6478--6490},
	year         = 2021,
}
@inproceedings{Schmucker2020,
	author       = {Robin Schmucker and Michele Donini and Valerio Perrone and Cédric Archambeau},
	title        = {Multi-objective multi-fidelity hyperparameter optimization with application to fairness},
	year         = 2020,
	url          = {https://www.amazon.science/publications/multi-objective-multi-fidelity-hyperparameter-optimization-with-application-to-fairness},
	booktitle    = {NeurIPS 2020 Workshop on Meta-learning},
}
@inproceedings{benami2021distributive,
	title        = {The distributive effects of risk prediction in environmental compliance: Algorithmic design, environmental justice, and public policy},
	author       = {Benami, Elinor and Whitaker, Reid and La, Vincent and Lin, Hongjin and Anderson, Brandon R and Ho, Daniel E},
	booktitle    = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
	pages        = {90--105},
	year         = 2021,
}
@misc{human_rights_watch_2020,
	title        = {UK: Automated Benefits System Failing People in Need},
	url          = {https://www.hrw.org/news/2020/09/29/uk-automated-benefits-system-failing-people-need},
	journal      = {Human Rights Watch},
	year         = 2020,
	month        = {Oct},
}
@inproceedings{leino2018featurewise,
	title        = {Feature-Wise Bias Amplification},
	author       = {Klas Leino and Matt Fredrikson and Emily Black and Shayak Sen and Anupam Datta},
	year         = 2019,
	booktitle    = {International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=S1ecm2C9K7},
}
@inproceedings{Baharlouei2020Rényi,
	title        = {Rényi Fair Inference},
	author       = {Sina Baharlouei and Maher Nouiehed and Ahmad Beirami and Meisam Razaviyayn},
	booktitle    = {International Conference on Learning Representations},
	year         = 2020,
	url          = {https://openreview.net/forum?id=HkgsUJrtDB},
}
@misc{ecoa_comments,
	title        = {Comment for 1002.6 - Rules Concerning Evaluation of Applications},
	year         = 2011,
	howpublished = {\url{https://www.consumerfinance.gov/rules-policy/regulations/1002/interp-6/}},
}
@misc{USDOJ2021,
	title        = {Section VII- Proving discrimination- Disparate impact},
	journal      = {The United States Department of Justice},
	howpublished = {\url{https://www.justice.gov/crt/fcs/T6Manual7}},
	year         = 2021,
}
@misc{naacp_statement,
	title        = {NAACP Legal Defense and Educational Fund and Student Borrower Protection Center Announce Fair Lending Testing Agreement with Upstart Network},
	year         = 2020,
	howpublished = {\url{https://protectborrowers.org/naacpldf-sbpc-upstart-agreement/}},
}
@misc{colfax2022report3,
	title        = {Fair Lending Monitorship of Upstart Network’s Lending Model: Third Report of the Independent Monitor},
	author       = {Relman Colfax},
	year         = 2022,
	howpublished = {\url{https://www.relmanlaw.com/media/cases/1333_PUBLIC\%20Upstart\%20Monitorship\%203rd\%20Report\%20FINAL.pdf}},
}

@article{chouldechova2017fair,
  title={Fair prediction with disparate impact: A study of bias in recidivism prediction instruments},
  author={Chouldechova, Alexandra},
  journal={Big data},
  volume={5},
  number={2},
  pages={153--163},
  year={2017},
  publisher={Mary Ann Liebert, Inc. 140 Huguenot Street, 3rd Floor New Rochelle, NY 10801 USA}
}
@inproceedings{suresh2022towards,
	title        = {Towards Intersectional Feminist and Participatory ML: A Case Study in Supporting Feminicide Counterdata Collection},
	author       = {Suresh, Harini and Movva, Rajiv and Dogan, Amelia Lee and Bhargava, Rahul and Cruxen, Isadora and Cuba, {\'A}ngeles Martinez and Taurino, Guilia and So, Wonyoung and D'Ignazio, Catherine},
	booktitle    = {2022 ACM Conference on Fairness, Accountability, and Transparency},
	pages        = {667--678},
	year         = 2022,
}
@inproceedings{katell2020toward,
	title        = {Toward situated interventions for algorithmic equity: lessons from the field},
	author       = {Katell, Michael and Young, Meg and Dailey, Dharma and Herman, Bernease and Guetler, Vivian and Tam, Aaron and Bintz, Corinne and Raz, Daniella and Krafft, PM},
	booktitle    = {Proceedings of the 2020 conference on fairness, accountability, and transparency},
	pages        = {45--55},
	year         = 2020,
}
@inproceedings{pmlr-v139-coston21a,
	title        = {Characterizing Fairness Over the Set of Good Models Under Selective Labels},
	author       = {Coston, Amanda and Rambachan, Ashesh and Chouldechova, Alexandra},
	booktitle    = {Proceedings of the 38th International Conference on Machine Learning},
	pages        = {2144--2155},
	year         = 2021,
	editor       = {Meila, Marina and Zhang, Tong},
	volume       = 139,
	series       = {Proceedings of Machine Learning Research},
	month        = {18--24 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v139/coston21a/coston21a.pdf},
	url          = {https://proceedings.mlr.press/v139/coston21a.html},
	abstract     = {Algorithmic risk assessments are used to inform decisions in a wide variety of high-stakes settings. Often multiple predictive models deliver similar overall performance but differ markedly in their predictions for individual cases, an empirical phenomenon known as the “Rashomon Effect.” These models may have different properties over various groups, and therefore have different predictive fairness properties. We develop a framework for characterizing predictive fairness properties over the set of models that deliver similar overall performance, or “the set of good models.” Our framework addresses the empirically relevant challenge of selectively labelled data in the setting where the selection decision and outcome are unconfounded given the observed data features. Our framework can be used to 1) audit for predictive bias; or 2) replace an existing model with one that has better fairness properties. We illustrate these use cases on a recidivism prediction task and a real-world credit-scoring task.},
}
@misc{colfax2021report1,
	title        = {ir Lending Monitorship of Upstart Network’s Lending Model: Initial Report of the Independent Monitor},
	author       = {Relman Colfax},
	year         = 2021,
	howpublished = {\url{https://www.relmanlaw.com/media/cases/1088_Upstart\%20Initial\%20Report\%20-\%20Final.pdf}},
}
@misc{colfax2021report2,
	title        = {ir Lending Monitorship of Upstart Network’s Lending Model: Second Report of the Independent Monitor},
	author       = {Relman Colfax},
	year         = 2021,
	hawpublished = {\url{https://www.relmanlaw.com/media/cases/1180_PUBLIC\%20Upstart\%20Monitorship_2nd\%20Report_FINAL.pdf}},
}
@inproceedings{Yurochkin2020Training,
	title        = {Training individually fair ML models with sensitive subspace robustness},
	author       = {Mikhail Yurochkin and Amanda Bower and Yuekai Sun},
	booktitle    = {International Conference on Learning Representations},
	year         = 2020,
	url          = {https://openreview.net/forum?id=B1gdkxHFDH},
}
@inproceedings{Yang2020Projection-Based,
	title        = {Projection-Based Constrained Policy Optimization},
	author       = {Tsung-Yen Yang and Justinian Rosca and Karthik Narasimhan and Peter J. Ramadge},
	booktitle    = {International Conference on Learning Representations},
	year         = 2020,
	url          = {https://openreview.net/forum?id=rke3TJrtPS},
}
@inproceedings{Zhao2020Conditional,
	title        = {Conditional Learning of Fair Representations},
	author       = {Han Zhao and Amanda Coston and Tameem Adel and Geoffrey J. Gordon},
	booktitle    = {International Conference on Learning Representations},
	year         = 2020,
	url          = {https://openreview.net/forum?id=Hkekl0NFPr},
}
@inproceedings{dittadi2021on,
	title        = {On the Transfer of Disentangled Representations in Realistic Settings},
	author       = {Andrea Dittadi and Frederik Tr{\"a}uble and Francesco Locatello and Manuel Wuthrich and Vaibhav Agrawal and Ole Winther and Stefan Bauer and Bernhard Sch{\"o}lkopf},
	booktitle    = {International Conference on Learning Representations},
	year         = 2021,
	url          = {https://openreview.net/forum?id=8VXvj1QNRl1},
}
@article{boyd2021datasheets,
	title        = {Datasheets for datasets help ml engineers notice and understand ethical issues in training data},
	author       = {Boyd, Karen L},
	journal      = {Proceedings of the ACM on Human-Computer Interaction},
	volume       = 5,
	number       = {CSCW2},
	pages        = {1--27},
	year         = 2021,
	publisher    = {ACM New York, NY, USA},
}
@article{paullada2021data,
	title        = {Data and its (dis) contents: A survey of dataset development and use in machine learning research},
	author       = {Paullada, Amandalynne and Raji, Inioluwa Deborah and Bender, Emily M and Denton, Emily and Hanna, Alex},
	journal      = {Patterns},
	volume       = 2,
	number       = 11,
	pages        = 100336,
	year         = 2021,
	publisher    = {Elsevier},
}
@inproceedings{cheng2021fairfil,
	title        = {FairFil: Contrastive Neural Debiasing Method for Pretrained Text Encoders},
	author       = {Pengyu Cheng and Weituo Hao and Siyang Yuan and Shijing Si and Lawrence Carin},
	booktitle    = {International Conference on Learning Representations},
	year         = 2021,
	url          = {https://openreview.net/forum?id=N6JECD-PI5w},
}
@inproceedings{bower2021individually,
	title        = {Individually Fair Rankings},
	author       = {Amanda Bower and Hamid Eftekhari and Mikhail Yurochkin and Yuekai Sun},
	booktitle    = {International Conference on Learning Representations},
	year         = 2021,
	url          = {https://openreview.net/forum?id=71zCSP\%5FHuBN},
}
@inproceedings{vargo2021individually,
	title        = {Individually Fair Gradient Boosting},
	author       = {Alexander Vargo and Fan Zhang and Mikhail Yurochkin and Yuekai Sun},
	booktitle    = {International Conference on Learning Representations},
	year         = 2021,
	url          = {https://openreview.net/forum?id=JBAa9we1AL},
}
@inproceedings{yurochkin2021sensei,
	title        = {SenSeI: Sensitive Set Invariance for Enforcing Individual Fairness},
	author       = {Mikhail Yurochkin and Yuekai Sun},
	booktitle    = {International Conference on Learning Representations},
	year         = 2021,
	url          = {https://openreview.net/forum?id=DktZb97\%5FFx},
}
@inproceedings{navon2021learning,
	title        = {Learning the Pareto Front with Hypernetworks},
	author       = {Aviv Navon and Aviv Shamsian and Ethan Fetaya and Gal Chechik},
	booktitle    = {International Conference on Learning Representations},
	year         = 2021,
	url          = {https://openreview.net/forum?id=NjF772F4ZZR},
}
@inproceedings{maity2021statistical,
	title        = {Statistical inference for individual fairness},
	author       = {Subha Maity and Songkai Xue and Mikhail Yurochkin and Yuekai Sun},
	booktitle    = {International Conference on Learning Representations},
	year         = 2021,
	url          = {https://openreview.net/forum?id=z9k8BWL-\%5F2u},
}
@inproceedings{roh2021fairbatch,
	title        = {FairBatch: Batch Selection for Model Fairness},
	author       = {Yuji Roh and Kangwook Lee and Steven Euijong Whang and Changho Suh},
	booktitle    = {International Conference on Learning Representations},
	year         = 2021,
	url          = {https://openreview.net/forum?id=YNnpaAKeCfx},
}
@inproceedings{li2021on,
	title        = {On Dyadic Fairness: Exploring and Mitigating Bias in Graph Connections},
	author       = {Peizhao Li and Yifei Wang and Han Zhao and Pengyu Hong and Hongfu Liu},
	booktitle    = {International Conference on Learning Representations},
	year         = 2021,
	url          = {https://openreview.net/forum?id=xgGS6PmzNq6},
}
@inproceedings{chuang2021fair,
	title        = {Fair Mixup: Fairness via Interpolation},
	author       = {Ching-Yao Chuang and Youssef Mroueh},
	booktitle    = {International Conference on Learning Representations},
	year         = 2021,
	url          = {https://openreview.net/forum?id=DNl5s5BXeBn},
}
@inproceedings{li2021tilted,
	title        = {Tilted Empirical Risk Minimization},
	author       = {Tian Li and Ahmad Beirami and Maziar Sanjabi and Virginia Smith},
	booktitle    = {International Conference on Learning Representations},
	year         = 2021,
	url          = {https://openreview.net/forum?id=K5YasWXZT3O},
}
@inproceedings{giguere2022fairness,
	title        = {Fairness Guarantees under Demographic Shift},
	author       = {Stephen Giguere and Blossom Metevier and Yuriy Brun and Philip S. Thomas and Scott Niekum and Bruno Castro da Silva},
	booktitle    = {International Conference on Learning Representations},
	year         = 2022,
	url          = {https://openreview.net/forum?id=wbPObLm6ueA},
}
@inproceedings{xu2022controlling,
	title        = {Controlling Directions Orthogonal to a Classifier},
	author       = {Yilun Xu and Hao He and Tianxiao Shen and Tommi S. Jaakkola},
	booktitle    = {International Conference on Learning Representations},
	year         = 2022,
	url          = {https://openreview.net/forum?id=DIjCrlsu6Z},
}
@inproceedings{humayun2022magnet,
	title        = {Ma{GNET}: Uniform Sampling from Deep Generative Network Manifolds Without Retraining},
	author       = {Ahmed Imtiaz Humayun and Randall Balestriero and Richard Baraniuk},
	booktitle    = {International Conference on Learning Representations},
	year         = 2022,
	url          = {https://openreview.net/forum?id=r5qumLiYwf9},
}
@inproceedings{salvador2022faircal,
	title        = {FairCal: Fairness Calibration for Face Verification},
	author       = {Tiago Salvador and Stephanie Cairns and Vikram Voleti and Noah Marshall and Adam M Oberman},
	booktitle    = {International Conference on Learning Representations},
	year         = 2022,
	url          = {https://openreview.net/forum?id=nRj0NcmSuxb},
}
@inproceedings{dullerud2022is,
	title        = {Is Fairness Only Metric Deep? Evaluating and Addressing Subgroup Gaps in Deep Metric Learning},
	author       = {Natalie Dullerud and Karsten Roth and Kimia Hamidieh and Nicolas Papernot and Marzyeh Ghassemi},
	booktitle    = {International Conference on Learning Representations},
	year         = 2022,
	url          = {https://openreview.net/forum?id=js62\%5FxuLDDv},
}
@inproceedings{tsai2022conditional,
	title        = {Conditional Contrastive Learning with Kernel},
	author       = {Yao-Hung Hubert Tsai and Tianqin Li and Martin Q. Ma and Han Zhao and Kun Zhang and Louis-Philippe Morency and Ruslan Salakhutdinov},
	booktitle    = {International Conference on Learning Representations},
	year         = 2022,
	url          = {https://openreview.net/forum?id=AAJLBoGt0XM},
}
@inproceedings{balunovic2022fair,
	title        = {Fair Normalizing Flows},
	author       = {Mislav Balunovic and Anian Ruoss and Martin Vechev},
	booktitle    = {International Conference on Learning Representations},
	year         = 2022,
	url          = {https://openreview.net/forum?id=BrFIKuxrZE},
}
@inproceedings{balakrishnan2022diverse,
	title        = {Diverse Client Selection for Federated Learning via Submodular Maximization},
	author       = {Ravikumar Balakrishnan and Tian Li and Tianyi Zhou and Nageen Himayat and Virginia Smith and Jeff Bilmes},
	booktitle    = {International Conference on Learning Representations},
	year         = 2022,
	url          = {https://openreview.net/forum?id=nwKXyFvaUm},
}
@inproceedings{zhu2022the,
	title        = {The Rich Get Richer: Disparate Impact of Semi-Supervised Learning},
	author       = {Zhaowei Zhu and Tianyi Luo and Yang Liu},
	booktitle    = {International Conference on Learning Representations},
	year         = 2022,
	url          = {https://openreview.net/forum?id=DXPftn5kjQK},
}
@inproceedings{jiang2022generalized,
	title        = {Generalized Demographic Parity for Group Fairness},
	author       = {Zhimeng Jiang and Xiaotian Han and Chao Fan and Fan Yang and Ali Mostafavi and Xia Hu},
	booktitle    = {International Conference on Learning Representations},
	year         = 2022,
	url          = {https://openreview.net/forum?id=YigKlMJwjye},
}
@inproceedings{wei2022learning,
	title        = {Learning with Noisy Labels Revisited: A Study Using Real-World Human Annotations},
	author       = {Jiaheng Wei and Zhaowei Zhu and Hao Cheng and Tongliang Liu and Gang Niu and Yang Liu},
	booktitle    = {International Conference on Learning Representations},
	year         = 2022,
	url          = {https://openreview.net/forum?id=TBWA6PLJZQm},
}
@inproceedings{vu2022distributionally,
	title        = {Distributionally Robust Fair Principal Components via Geodesic Descents},
	author       = {Hieu Vu and Toan Tran and Man-Chung Yue and Viet Anh Nguyen},
	booktitle    = {International Conference on Learning Representations},
	year         = 2022,
	url          = {https://openreview.net/forum?id=9NVd-DMtThY},
}
@inproceedings{park2021understanding,
	title        = {Understanding the Representation and Representativeness of Age in AI Data Sets},
	author       = {Park, Joon Sung and Bernstein, Michael S and Brewer, Robin N and Kamar, Ece and Morris, Meredith Ringel},
	booktitle    = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
	pages        = {834--842},
	year         = 2021,
}
@article{chawla2002smote,
	title        = {SMOTE: synthetic minority over-sampling technique},
	author       = {Chawla, Nitesh V and Bowyer, Kevin W and Hall, Lawrence O and Kegelmeyer, W Philip},
	journal      = {Journal of artificial intelligence research},
	volume       = 16,
	pages        = {321--357},
	year         = 2002,
}
@article{van2021decaf,
	title        = {Decaf: Generating fair synthetic data using causally-aware generative networks},
	author       = {van Breugel, Boris and Kyono, Trent and Berrevoets, Jeroen and van der Schaar, Mihaela},
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 34,
	pages        = {22221--22233},
	year         = 2021,
}
@inproceedings{suriyakumar2021chasing,
	title        = {Chasing your long tails: Differentially private prediction in health care settings},
	author       = {Suriyakumar, Vinith M and Papernot, Nicolas and Goldenberg, Anna and Ghassemi, Marzyeh},
	booktitle    = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
	pages        = {723--734},
	year         = 2021,
}
@article{bagdasaryan2019differential,
	title        = {Differential privacy has disparate impact on model accuracy},
	author       = {Bagdasaryan, Eugene and Poursaeed, Omid and Shmatikov, Vitaly},
	journal      = {Advances in neural information processing systems},
	volume       = 32,
	year         = 2019,
}
@inproceedings{de2020case,
	title        = {A case for humans-in-the-loop: Decisions in the presence of erroneous algorithmic scores},
	author       = {De-Arteaga, Maria and Fogliato, Riccardo and Chouldechova, Alexandra},
	booktitle    = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
	pages        = {1--12},
	year         = 2020,
}
@article{passi2020making,
	title        = {Making data science systems work},
	author       = {Passi, Samir and Sengers, Phoebe},
	journal      = {Big Data \& Society},
	volume       = 7,
	number       = 2,
	pages        = 2053951720939605,
	year         = 2020,
	publisher    = {SAGE Publications Sage UK: London, England},
}
@inproceedings{green2019disparate,
	title        = {Disparate interactions: An algorithm-in-the-loop analysis of fairness in risk assessments},
	author       = {Green, Ben and Chen, Yiling},
	booktitle    = {Proceedings of the conference on fairness, accountability, and transparency},
	pages        = {90--99},
	year         = 2019,
}
@article{madras2018predict,
	title        = {Predict responsibly: improving fairness and accuracy by learning to defer},
	author       = {Madras, David and Pitassi, Toni and Zemel, Richard},
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 31,
	year         = 2018,
}
@inproceedings{keswani2021towards,
	title        = {Towards unbiased and accurate deferral to multiple experts},
	author       = {Keswani, Vijay and Lease, Matthew and Kenthapadi, Krishnaram},
	booktitle    = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
	pages        = {154--165},
	year         = 2021,
}
@article{green2019principles,
	title        = {The principles and limits of algorithm-in-the-loop decision making},
	author       = {Green, Ben and Chen, Yiling},
	journal      = {Proceedings of the ACM on Human-Computer Interaction},
	volume       = 3,
	number       = {CSCW},
	pages        = {1--24},
	year         = 2019,
	publisher    = {ACM New York, NY, USA},
}
@inproceedings{cheng2022child,
	author       = {Cheng, Hao-Fei and Stapleton, Logan and Kawakami, Anna and Sivaraman, Venkatesh and Cheng, Yanghuidi and Qing, Diana and Perer, Adam and Holstein, Kenneth and Wu, Zhiwei Steven and Zhu, Haiyi},
	title        = {How Child Welfare Workers Reduce Racial Disparities in Algorithmic Decisions},
	year         = 2022,
	isbn         = 9781450391573,
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	url          = {https://doi.org/10.1145/3491102.3501831},
	doi          = {10.1145/3491102.3501831},
	abstract     = {Machine learning tools have been deployed in various contexts to support human decision-making, in the hope that human-algorithm collaboration can improve decision quality. However, the question of whether such collaborations reduce or exacerbate biases in decision-making remains underexplored. In this work, we conducted a mixed-methods study, analyzing child welfare call screen workers’ decision-making over a span of four years, and interviewing them on how they incorporate algorithmic predictions into their decision-making process. Our data analysis shows that, compared to the algorithm alone, workers reduced the disparity in screen-in rate between Black and white children from 20\% to 9\%. Our qualitative data show that workers achieved this by making holistic risk assessments and adjusting for the algorithm’s limitations. Our analyses also show more nuanced results about how human-algorithm collaboration affects prediction accuracy, and how to measure these effects. These results shed light on potential mechanisms for improving human-algorithm collaboration in high-risk decision-making contexts.},
	booktitle    = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
	articleno    = 162,
	numpages     = 22,
	keywords     = {human-centered AI, machine learning, child welfare, algorithm-assisted decision-making, algorithmic biases},
	location     = {New Orleans, LA, USA},
	series       = {CHI '22},
}
@article{jacobs2021machine,
	title        = {How machine-learning recommendations influence clinician treatment selections: the example of antidepressant selection},
	author       = {Jacobs, Maia and Pradier, Melanie F and McCoy Jr, Thomas H and Perlis, Roy H and Doshi-Velez, Finale and Gajos, Krzysztof Z},
	journal      = {Translational psychiatry},
	volume       = 11,
	number       = 1,
	pages        = 108,
	year         = 2021,
	publisher    = {Nature Publishing Group UK London},
}
@inproceedings{lum2020impact,
	title        = {The impact of overbooking on a pre-trial risk assessment tool},
	author       = {Lum, Kristian and Boudin, Chesa and Price, Megan},
	booktitle    = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
	pages        = {482--491},
	year         = 2020,
}
@article{d2020underspecification,
	title        = {Underspecification presents challenges for credibility in modern machine learning},
	author       = {D'Amour, Alexander and Heller, Katherine and Moldovan, Dan and Adlam, Ben and Alipanahi, Babak and Beutel, Alex and Chen, Christina and Deaton, Jonathan and Eisenstein, Jacob and Hoffman, Matthew D and others},
	year         = 2020,
	journal      = {arXiv preprint arXiv:2011.03395},
}
@inproceedings{li2022data,
	title        = {Data-Centric Factors in Algorithmic Fairness},
	author       = {Li, Nianyun and Goel, Naman and Ash, Elliott},
	booktitle    = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
	pages        = {396--410},
	year         = 2022,
}
@article{liu2021can,
	title        = {Can less be more? when increasing-to-balancing label noise rates considered beneficial},
	author       = {Liu, Yang and Wang, Jialu},
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 34,
	pages        = {17467--17479},
	year         = 2021,
}
@inproceedings{jiang2021towards,
	title        = {Towards equity and algorithmic fairness in student grade prediction},
	author       = {Jiang, Weijie and Pardos, Zachary A},
	booktitle    = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
	pages        = {608--617},
	year         = 2021,
}
@inproceedings{mishler2019modeling,
	title        = {Modeling risk and achieving algorithmic fairness using potential outcomes},
	author       = {Mishler, Alan},
	booktitle    = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
	pages        = {555--556},
	year         = 2019,
}
@inproceedings{koesten2019collaborative,
	title        = {Collaborative practices with structured data: Do tools support what users need?},
	author       = {Koesten, Laura and Kacprzak, Emilia and Tennison, Jeni and Simperl, Elena},
	booktitle    = {Proceedings of the 2019 chi conference on human factors in computing systems},
	pages        = {1--14},
	year         = 2019,
}
@inproceedings{hube2019understanding,
	author       = {Hube, Christoph and Fetahu, Besnik and Gadiraju, Ujwal},
	title        = {Understanding and Mitigating Worker Biases in the Crowdsourced Collection of Subjective Judgments},
	year         = 2019,
	series       = {CHI '19},
}
@article{miceli2020subjectivity,
	author       = {Miceli, Milagros and Schuessler, Martin and Yang, Tianling},
	title        = {Between Subjectivity and Imposition: Power Dynamics in Data Annotation for Computer Vision},
	year         = 2020,
	journal      = {CSCW},
}
@article{crone2006impact,
	title        = {The impact of preprocessing on data mining: An evaluation of classifier sensitivity in direct marketing},
	author       = {Crone, Sven F and Lessmann, Stefan and Stahlbock, Robert},
	journal      = {European Journal of Operational Research},
	volume       = 173,
	number       = 3,
	pages        = {781--800},
	year         = 2006,
	publisher    = {Elsevier},
}
@article{garg2020dropping,
	title        = {Dropping Standardized Testing for Admissions Trades Off Information and Access},
	author       = {Garg, Nikhil and Li, Hannah and Monachou, Faidra},
	journal      = {arXiv preprint arXiv:2010.04396},
	year         = 2020,
}
@inproceedings{wang2020towards,
	title        = {Towards fairness in visual recognition: Effective strategies for bias mitigation},
	author       = {Wang, Zeyu and Qinami, Klint and Karakozis, Ioannis Christos and Genova, Kyle and Nair, Prem and Hata, Kenji and Russakovsky, Olga},
	booktitle    = {Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
	pages        = {8919--8928},
	year         = 2020,
}
@inproceedings{belitz2021automating,
	title        = {Automating procedurally fair feature selection in machine learning},
	author       = {Belitz, Clara and Jiang, Lan and Bosch, Nigel},
	booktitle    = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
	pages        = {379--389},
	year         = 2021,
}
@article{frye2020asymmetric,
	title        = {Asymmetric shapley values: incorporating causal knowledge into model-agnostic explainability},
	author       = {Frye, Christopher and Rowat, Colin and Feige, Ilya},
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 33,
	pages        = {1229--1239},
	year         = 2020,
}
@inproceedings{robertson2022bio,
	title        = {A Bio-Inspired Framework for Machine Bias Interpretation},
	author       = {Robertson, Jake and Stinson, Catherine and Hu, Ting},
	booktitle    = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
	pages        = {588--598},
	year         = 2022,
}
@article{emelianov2022fair,
	title        = {On fair selection in the presence of implicit and differential variance},
	author       = {Emelianov, Vitalii and Gast, Nicolas and Gummadi, Krishna P and Loiseau, Patrick},
	journal      = {Artificial Intelligence},
	volume       = 302,
	pages        = 103609,
	year         = 2022,
	publisher    = {Elsevier},
}
@inproceedings{grgic2018beyond,
	title        = {Beyond distributive fairness in algorithmic decision making: Feature selection for procedurally fair learning},
	author       = {Grgi{\'c}-Hla{\v{c}}a, Nina and Zafar, Muhammad Bilal and Gummadi, Krishna P and Weller, Adrian},
	booktitle    = {Proceedings of the AAAI Conference on Artificial Intelligence},
	volume       = 32,
	number       = 1,
	year         = 2018,
}
@inproceedings{sambasivan2021everyone,
	title        = {“Everyone wants to do the model work, not the data work”: Data Cascades in High-Stakes AI},
	author       = {Sambasivan, Nithya and Kapania, Shivani and Highfill, Hannah and Akrong, Diana and Paritosh, Praveen and Aroyo, Lora M},
	booktitle    = {proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
	pages        = {1--15},
	year         = 2021,
}
@article{bartik2016deleting,
	title        = {Deleting a signal: Evidence from pre-employment credit checks},
	author       = {Bartik, Alexander and Nelson, Scott},
	year         = 2016,
	publisher    = {MIT Department of Economics Graduate Student Research Paper},
}
@inproceedings{biswas2021fair,
	title        = {Fair preprocessing: towards understanding compositional fairness of data transformers in machine learning pipeline},
	author       = {Biswas, Sumon and Rajan, Hridesh},
	booktitle    = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
	pages        = {981--993},
	year         = 2021,
}
@inproceedings{yik2022identifying,
	title        = {Identifying Bias in Data Using Two-Distribution Hypothesis Tests},
	author       = {Yik, William and Serafini, Limnanthes and Lindsey, Timothy and Monta{\~n}ez, George D},
	booktitle    = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
	pages        = {831--844},
	year         = 2022,
}
@article{alshdaifat2021effect,
	title        = {The effect of preprocessing techniques, applied to numeric features, on classification algorithms’ performance},
	author       = {Alshdaifat, Esra’a and Alshdaifat, Doa’a and Alsarhan, Ayoub and Hussein, Fairouz and El-Salhi, Subhieh Moh’d Faraj S},
	journal      = {Data},
	volume       = 6,
	number       = 2,
	pages        = 11,
	year         = 2021,
	publisher    = {MDPI},
}
@article{zheng2003existence,
	title        = {On the existence and significance of data preprocessing biases in web-usage mining},
	author       = {Zheng, Zhiqiang and Padmanabhan, Balaji and Kimbrough, Steven O},
	journal      = {INFORMS Journal on Computing},
	volume       = 15,
	number       = 2,
	pages        = {148--170},
	year         = 2003,
	publisher    = {INFORMS},
}
@inproceedings{sen2015turkers,
	author       = {Sen, Shilad and Giesel, Margaret E. and Gold, Rebecca and Hillmann, Benjamin and Lesicko, Matt and Naden, Samuel and Russell, Jesse and Wang, Zixiao (Ken) and Hecht, Brent},
	title        = {Turkers, Scholars, "Arafat" and "Peace": Cultural Communities and Algorithmic Gold Standards},
	year         = 2015,
	series       = {CSCW},
}
@inproceedings{barabas2020studying,
	title        = {Studying up: reorienting the study of algorithmic fairness around issues of power},
	author       = {Barabas, Chelsea and Doyle, Colin and Rubinovitz, JB and Dinakar, Karthik},
	booktitle    = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
	pages        = {167--176},
	year         = 2020,
}
@inproceedings{raji2022fallacy,
	title        = {The fallacy of AI functionality},
	author       = {Raji, Inioluwa Deborah and Kumar, I Elizabeth and Horowitz, Aaron and Selbst, Andrew},
	booktitle    = {2022 ACM Conference on Fairness, Accountability, and Transparency},
	pages        = {959--972},
	year         = 2022,
}
@inproceedings{diaz2022crowdworksheets,
	title        = {Crowdworksheets: Accounting for individual and collective identities underlying crowdsourced dataset annotation},
	author       = {D{\'\i}az, Mark and Kivlichan, Ian and Rosen, Rachel and Baker, Dylan and Amironesei, Razvan and Prabhakaran, Vinodkumar and Denton, Emily},
	booktitle    = {2022 ACM Conference on Fairness, Accountability, and Transparency},
	pages        = {2342--2351},
	year         = 2022,
}
@inproceedings{goel2019crowdsourcing,
	title        = {Crowdsourcing with fairness, diversity and budget constraints},
	author       = {Goel, Naman and Faltings, Boi},
	booktitle    = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
	pages        = {297--304},
	year         = 2019,
}
@inproceedings{wan2022fairness,
	title        = {Fairness in Representation for Multilingual {NLP}: Insights from Controlled Experiments on Conditional Language Modeling},
	author       = {Ada Wan},
	booktitle    = {International Conference on Learning Representations},
	year         = 2022,
	url          = {https://openreview.net/forum?id=-llS6TiOew},
}
@article{gebru2018datasheets,
	title        = {Datasheets for datasets. arXiv},
	author       = {Gebru, Timnit and Morgenstern, Jamie and Vecchione, Briana and Vaughan, Jennifer Wortman and Wallach, Hanna and Daum{\'e}, Hal III and Crawford, Kate},
	journal      = {arXiv preprint arXiv:1803.09010},
	year         = 2018,
}
@inproceedings{coston2019fair,
	title        = {Fair transfer learning with missing protected attributes},
	author       = {Coston, Amanda and Ramamurthy, Karthikeyan Natesan and Wei, Dennis and Varshney, Kush R and Speakman, Skyler and Mustahsan, Zairah and Chakraborty, Supriyo},
	booktitle    = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
	pages        = {91--98},
	year         = 2019,
}
@inproceedings{singh2021fairness,
	title        = {Fairness violations and mitigation under covariate shift},
	author       = {Singh, Harvineet and Singh, Rina and Mhasawade, Vishwali and Chunara, Rumi},
	booktitle    = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
	pages        = {3--13},
	year         = 2021,
}
@inproceedings{foulds2020intersectional,
	title        = {An intersectional definition of fairness},
	author       = {Foulds, James R and Islam, Rashidul and Keya, Kamrun Naher and Pan, Shimei},
	booktitle    = {2020 IEEE 36th International Conference on Data Engineering (ICDE)},
	pages        = {1918--1921},
	year         = 2020,
	organization = {IEEE},
}
@inproceedings{islam2021can,
	title        = {Can we obtain fairness for free?},
	author       = {Islam, Rashidul and Pan, Shimei and Foulds, James R},
	booktitle    = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
	pages        = {586--596},
	year         = 2021,
}
@inproceedings{perrone2021fair,
	title        = {Fair bayesian optimization},
	author       = {Perrone, Valerio and Donini, Michele and Zafar, Muhammad Bilal and Schmucker, Robin and Kenthapadi, Krishnaram and Archambeau, C{\'e}dric},
	booktitle    = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
	pages        = {854--863},
	year         = 2021,
}
@inproceedings{sharma2020data,
	title        = {Data augmentation for discrimination prevention and bias disambiguation},
	author       = {Sharma, Shubham and Zhang, Yunfeng and R{\'\i}os Aliaga, Jes{\'u}s M and Bouneffouf, Djallel and Muthusamy, Vinod and Varshney, Kush R},
	booktitle    = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
	pages        = {358--364},
	year         = 2020,
}
@article{wei2021learning,
	title        = {Learning with noisy labels revisited: A study using real-world human annotations},
	author       = {Wei, Jiaheng and Zhu, Zhaowei and Cheng, Hao and Liu, Tongliang and Niu, Gang and Liu, Yang},
	journal      = {arXiv preprint arXiv:2110.12088},
	year         = 2021,
}
@article{room2021executive,
	title        = {Executive order on advancing racial equity and support for underserved communities through the federal government},
	author       = {ROOM, BRIEFING},
	year         = 2021,
}
@misc{cfpb_2022,
	title        = {CFPB targets unfair discrimination in Consumer Finance},
	url          = {https://www.consumerfinance.gov/about-us/newsroom/cfpb-targets-unfair-discrimination-in-consumer-finance/#:~:text=The\%20CFPB\%20published\%20an\%20updated,benefits\%20to\%20consumers\%20or\%20competition.},
	journal      = {Consumer Financial Protection Bureau},
	year         = 2022,
	month        = {Mar},
}
@article{jobin2019global,
	title        = {The global landscape of AI ethics guidelines},
	author       = {Jobin, Anna and Ienca, Marcello and Vayena, Effy},
	journal      = {Nature Machine Intelligence},
	volume       = 1,
	number       = 9,
	pages        = {389--399},
	year         = 2019,
	publisher    = {Nature Publishing Group UK London},
}
@inproceedings{donahue2022human,
	title        = {Human-algorithm collaboration: Achieving complementarity and avoiding unfairness},
	author       = {Donahue, Kate and Chouldechova, Alexandra and Kenthapadi, Krishnaram},
	booktitle    = {2022 ACM Conference on Fairness, Accountability, and Transparency},
	pages        = {1639--1656},
	year         = 2022,
}
@article{lum2019measures,
	title        = {Measures of fairness for New York City’s Supervised Release Risk Assessment Tool},
	author       = {Lum, Kristian and Shah, Tarak},
	journal      = {Human Rights Data Analytics Group},
	pages        = 21,
	year         = 2019,
}
@article{lum2016predict,
	title        = {To predict and serve?},
	author       = {Lum, Kristian and Isaac, William},
	journal      = {Significance},
	volume       = 13,
	number       = 5,
	pages        = {14--19},
	year         = 2016,
	publisher    = {Wiley Online Library},
}
@inproceedings{friedler2019comparative,
	title        = {A comparative study of fairness-enhancing interventions in machine learning},
	author       = {Friedler, Sorelle A and Scheidegger, Carlos and Venkatasubramanian, Suresh and Choudhary, Sonam and Hamilton, Evan P and Roth, Derek},
	booktitle    = {Proceedings of the conference on fairness, accountability, and transparency},
	pages        = {329--338},
	year         = 2019,
}
@inproceedings{wang2021fair,
	title        = {Fair classification with group-dependent label noise},
	author       = {Wang, Jialu and Liu, Yang and Levy, Caleb},
	booktitle    = {Proceedings of the 2021 ACM conference on fairness, accountability, and transparency},
	pages        = {526--536},
	year         = 2021,
}
@article{akpinar2022sandbox,
	title        = {A Sandbox Tool to Bias (Stress)-Test Fairness Algorithms},
	author       = {Akpinar, Nil-Jana and Nagireddy, Manish and Stapleton, Logan and Cheng, Hao-Fei and Zhu, Haiyi and Wu, Steven and Heidari, Hoda},
	journal      = {arXiv preprint arXiv:2204.10233},
	year         = 2022,
}
@incollection{suresh2021framework,
	title        = {A framework for understanding sources of harm throughout the machine learning life cycle},
	author       = {Suresh, Harini and Guttag, John},
	booktitle    = {Equity and access in algorithms, mechanisms, and optimization},
	pages        = {1--9},
	year         = 2021,
}
@inproceedings{khani2021removing,
	title        = {Removing spurious features can hurt accuracy and affect groups disproportionately},
	author       = {Khani, Fereshte and Liang, Percy},
	booktitle    = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
	pages        = {196--205},
	year         = 2021,
}
@inproceedings{garg2021standardized,
	title        = {Standardized tests and affirmative action: The role of bias and variance},
	author       = {Garg, Nikhil and Li, Hannah and Monachou, Faidra},
	booktitle    = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
	pages        = {261--261},
	year         = 2021,
}
@inproceedings{marda2020data,
	title        = {Data in New Delhi's predictive policing system},
	author       = {Marda, Vidushi and Narayan, Shivangi},
	booktitle    = {Proceedings of the 2020 conference on fairness, accountability, and transparency},
	pages        = {317--324},
	year         = 2020,
}
@article{coston2022validity,
	title        = {A Validity Perspective on Evaluating the Justified Use of Data-driven Decision-making Algorithms},
	author       = {Coston, Amanda and Kawakami, Anna and Zhu, Haiyi and Holstein, Ken and Heidari, Hoda},
	journal      = {arXiv preprint arXiv:2206.14983},
	year         = 2022,
}
@inproceedings{jacobs2021measurement,
	title        = {Measurement and fairness},
	author       = {Jacobs, Abigail Z and Wallach, Hanna},
	booktitle    = {Proceedings of the 2021 ACM conference on fairness, accountability, and transparency},
	pages        = {375--385},
	year         = 2021,
}
@inproceedings{kilby2021algorithmic,
	title        = {Algorithmic fairness in predicting opioid use disorder using machine learning},
	author       = {Kilby, Angela E},
	booktitle    = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
	pages        = {272--272},
	year         = 2021,
}
@inproceedings{hynes2017data,
	title        = {The Data Linter: Lightweight Automated Sanity Checking for ML Data Sets},
	author       = {Nick Hynes and D. Sculley and Michael Terry},
	year         = 2017,
	url          = {http://learningsys.org/nips17/assets/papers/paper\%5F19.pdf},
}

@InProceedings{buolamwini2018gender,
  title = 	 {Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification},
  author = 	 {Buolamwini, Joy and Gebru, Timnit},
  booktitle = 	 {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
  pages = 	 {77--91},
  year = 	 {2018},
  editor = 	 {Friedler, Sorelle A. and Wilson, Christo},
  volume = 	 {81},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--24 Feb},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf},
  url = 	 {https://proceedings.mlr.press/v81/buolamwini18a.html},
  abstract = 	 {Recent studies demonstrate that machine learning algorithms can discriminate based on classes like race and gender. In this work, we present an approach to evaluate bias present in automated facial analysis algorithms and datasets with respect to phenotypic subgroups. Using the dermatologist  approved Fitzpatrick Skin Type classification system, we characterize the gender and skin type distribution of two facial analysis benchmarks, IJB-A and Adience. We find that these datasets are overwhelmingly composed of lighter-skinned subjects (79.6\% for IJB-A and 86.2\% for Adience) and introduce a new facial analysis dataset which is balanced by gender and skin type. We evaluate 3 commercial gender classification systems using our dataset and show that darker-skinned females are the most misclassified group (with error rates of up to 34.7\%). The maximum error rate for lighter-skinned males is 0.8\%. The substantial disparities in the accuracy of classifying darker females, lighter females, darker males, and lighter males in gender classification systems require urgent attention if commercial companies are to build genuinely fair, transparent and accountable facial analysis algorithms.}
}

@inproceedings{kallus2018residual,
	title        = {Residual Unfairness in Fair Machine Learning from Prejudiced Data},
	author       = {Kallus, Nathan and Zhou, Angela},
	booktitle    = {Proceedings of the 35th International Conference on Machine Learning},
	pages        = {2439--2448},
	year         = 2018,
	editor       = {Dy, Jennifer and Krause, Andreas},
	volume       = 80,
	series       = {Proceedings of Machine Learning Research},
	month        = {10--15 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v80/kallus18a/kallus18a.pdf},
	url          = {https://proceedings.mlr.press/v80/kallus18a.html},
	abstract     = {Recent work in fairness in machine learning has proposed adjusting for fairness by equalizing accuracy metrics across groups and has also studied how datasets affected by historical prejudices may lead to unfair decision policies. We connect these lines of work and study the residual unfairness that arises when a fairness-adjusted predictor is not actually fair on the target population due to systematic censoring of training data by existing biased policies. This scenario is particularly common in the same applications where fairness is a concern. We characterize theoretically the impact of such censoring on standard fairness metrics for binary classifiers and provide criteria for when residual unfairness may or may not appear. We prove that, under certain conditions, fairness-adjusted classifiers will in fact induce residual unfairness that perpetuates the same injustices, against the same groups, that biased the data to begin with, thus showing that even state-of-the-art fair machine learning can have a ""bias in, bias out"" property. When certain benchmark data is available, we show how sample reweighting can estimate and adjust fairness metrics while accounting for censoring. We use this to study the case of Stop, Question, and Frisk (SQF) and demonstrate that attempting to adjust for fairness perpetuates the same injustices that the policy is infamous for.},
}
@article{li2021autobalance,
	title        = {Autobalance: Optimized loss functions for imbalanced data},
	author       = {Li, Mingchen and Zhang, Xuechen and Thrampoulidis, Christos and Chen, Jiasi and Oymak, Samet},
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 34,
	pages        = {3163--3177},
	year         = 2021,
}
@article{krishnan2016activeclean,
	title        = {Activeclean: Interactive data cleaning for statistical modeling},
	author       = {Krishnan, Sanjay and Wang, Jiannan and Wu, Eugene and Franklin, Michael J and Goldberg, Ken},
	journal      = {Proceedings of the VLDB Endowment},
	volume       = 9,
	number       = 12,
	pages        = {948--959},
	year         = 2016,
	publisher    = {VLDB Endowment},
}
@inproceedings{breck2019data,
	title        = {Data Validation for Machine Learning.},
	author       = {Breck, Eric and Polyzotis, Neoklis and Roy, Sudip and Whang, Steven and Zinkevich, Martin},
	booktitle    = {MLSys},
	year         = 2019,
}
@inproceedings{kandel2011wrangler,
	title        = {Wrangler: Interactive visual specification of data transformation scripts},
	author       = {Kandel, Sean and Paepcke, Andreas and Hellerstein, Joseph and Heer, Jeffrey},
	booktitle    = {Proceedings of the sigchi conference on human factors in computing systems},
	pages        = {3363--3372},
	year         = 2011,
}
@article{hill2020wrongfully,
	title        = {Wrongfully accused by an algorithm},
	author       = {Hill, Kashmir},
	year         = 2020,
	journal      = {The New York Times, June},
	volume       = 24,
}
@misc{schuppe2019facial,
	title        = {How facial recognition became a routine policing tool in America},
	author       = {Schuppe, J},
	year         = 2019,
	publisher    = {NBC News.},
}

@inproceedings{li2022more,
  title={When More Data Lead Us Astray: Active Data Acquisition in the Presence of Label Bias},
  author={Li, Yunyi and De-Arteaga, Maria and Saar-Tsechansky, Maytal},
  booktitle={Proceedings of the AAAI Conference on Human Computation and Crowdsourcing},
  volume={10},
  number={1},
  pages={133--146},
  year={2022}
}

@article{freeman2021use,
  title={Use of artificial intelligence for image analysis in breast cancer screening programmes: systematic review of test accuracy},
  author={Freeman, Karoline and Geppert, Julia and Stinton, Chris and Todkill, Daniel and Johnson, Samantha and Clarke, Aileen and Taylor-Phillips, Sian},
  journal={bmj},
  volume={374},
  year={2021},
  publisher={British Medical Journal Publishing Group}
}

@article{szalavitz2021pain,
  title={The pain was Unbearable. so why did doctors turn her away},
  author={Szalavitz, Maia},
  journal={Wired},
  year={2021}
}

@misc{oakden2019hidden,
  title={Hidden stratification causes clinically meaningful failures in machine learning for medical imaging. arXiv},
  author={Oakden-Rayner, L and Dunnmon, J and Carniero, G and R{\'e}, C},
  year={2019}
}

@article{mooney1997vertical,
	title        = {Vertical equity: weighting outcomes? or establishing procedures?},
	author       = {Mooney, Gavin and Jan, Stephen},
	year         = 1997,
	journal      = {Health Policy},
	publisher    = {Elsevier},
	volume       = 39,
	number       = 1,
	pages        = {79--87},
}
@misc{legal_information_institute,
	title        = {Affirmative action},
	journal      = {Legal Information Institute},
	publisher    = {Cornell University},
	url          = {https://www.law.cornell.edu/wex/affirmative\%5Faction},
}
@article{horitz_ineq_aff_act,
	title        = {Horizontal inequalities and affirmative action},
	author       = {Arnim Langer and Frances Stewart and Maarten Schroyens},
	year         = 2022,
	month        = {November},
	publisher    = {UNU-WIDER},
	volume       = 2016,
	number       = 18,
}
@article{brown2012affirmative,
	title        = {Affirmative action: Foundations, contexts, and debates},
	author       = {Brown, Graham and Langer, Arnim and Stewart, Frances},
	year         = 2012,
	journal      = {Affirmative action in plural societies. Basingstoke: Palgrave Macmillan},
	pages        = 29,
}
@techreport{blumkin2005affirmative,
	title        = {Affirmative Action and Economic Justice},
	author       = {Blumkin, Tomer and Margalioth, Yoram and Sadka, Efraim and others},
	year         = 2005,
}
@incollection{sep-equality,
	title        = {{Equality}},
	author       = {Gosepath, Stefan},
	year         = 2021,
	booktitle    = {The {Stanford} Encyclopedia of Philosophy},
	publisher    = {Metaphysics Research Lab, Stanford University},
	editor       = {Edward N. Zalta},
	howpublished = {\url{https://plato.stanford.edu/archives/sum2021/entries/equality/}},
	edition      = {{S}ummer 2021},
}
@misc{aff_action_CFR,
	title        = {34 CFR § 100.3(6)(ii)},
	author       = {Federal Register},
	url          = {https://www.ecfr.gov/current/title-34/subtitle-B/chapter-I/part-100/section-100.3},
}
@book{haidt2012righteous,
	title        = {The righteous mind: Why good people are divided by politics and religion},
	author       = {Haidt, Jonathan},
	year         = 2012,
	publisher    = {Vintage},
}
@article{berliant1985horizontal,
	title        = {The horizontal and vertical equity characteristics of the federal individual income tax, 1966-1977},
	author       = {Berliant, Marcus C and Strauss, Robert P and others},
	year         = 1985,
	journal      = {Horizontal Equity, Uncertainty and Economic Well-being},
	publisher    = {Chicago, IL: University of Chicago Press},
	pages        = {179--211},
}
@article{lecun1995learning,
	title        = {Learning algorithms for classification: A comparison on handwritten digit recognition},
	author       = {LeCun, Yann and Jackel, LD and Bottou, L{\'e}on and Cortes, Corinna and Denker, John S and Drucker, Harris and Guyon, Isabelle and Muller, Urs A and Sackinger, Eduard and Simard, Patrice and others},
	year         = 1995,
	journal      = {Neural networks: the statistical mechanics perspective},
	publisher    = {World Scientific Singapore},
	volume       = 261,
	pages        = 276,
}
@inproceedings{zhang19trades,
	title        = {Theoretically Principled Trade-off between Robustness and Accuracy},
	author       = {Zhang, Hongyang and Yu, Yaodong and Jiao, Jiantao and Xing, Eric and Ghaoui, Laurent El and Jordan, Michael},
	year         = 2019,
	booktitle    = {International Conference on Machine Learning (ICML)},
}
@misc{fdic,
	title        = {Equal Credit Opportunity Act (Regulation B)},
	year         = 2011,
	howpublished = {\url{https://www.fdic.gov/regulations/laws/rules/6500-200.html}},
}
@misc{psa,
	title        = {How It Works},
	year         = 2020,
	month        = {Nov},
	journal      = {Advancing Pretrial Policy & Research (APPR)},
	publisher    = {Arnold Ventures},
	url          = {https://advancingpretrial.org/psa/factors/},
}
@misc{psa_justice,
	title        = {Pretrial Justice},
	year         = 2020,
	month        = {Nov},
	journal      = {Advancing Pretrial Policy & Research (APPR)},
	publisher    = {Arnold Ventures},
	url          = {https://advancingpretrial.org/pretrial-justice/pretrial-justice/},
}
@misc{rates_drug_use,
	title        = {Rates of Drug Use and Sales, by Race; Rates of Drug Related Criminal Justice Measures, by Race},
	year         = 2021,
	month        = {Sep},
	journal      = {Rates of Drug Use and Sales, by Race},
	publisher    = {The Hamilton Project (Brookings)},
	url          = {https://www.hamiltonproject.org/charts/rates\%5Fof\%5Fdrug\%5Fuse\%5Fand\%5Fsales\%5Fby\%5Frace\%5Frates\%5Fof\%5Fdrug\%5Frelated\%5Fcriminal\%5Fjustice},
}
@article{latessa2010creation,
	title        = {The creation and validation of the Ohio Risk Assessment System (ORAS)},
	author       = {Latessa, Edward J and Lemke, Richard and Makarios, Matthew and Smith, Paula},
	year         = 2010,
	journal      = {Fed. Probation},
	publisher    = {HeinOnline},
	volume       = 74,
	pages        = 16,
}
@misc{grant,
	title        = {Report to the Governor and the Legislature},
	author       = {Grant, Glenn  A.},
	publisher    = {New Jersey Courts},
	url          = {https://www.njcourts.gov/courts/assets/criminal/cjrannualreport2019.pdf},
}
@misc{general_data_protection_reg,
	title        = {European Parliament and Council of European Union (2016) Regulation (EU) 2016/679.},
	author       = {GDPR},
	year         = 2016,
	howpublished = {\url{https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=CELEX:32016R0679&from=EN}},
}
@inproceedings{babaev2019rnn,
	title        = {ET-RNN: Applying Deep Learning to Credit Loan Applications},
	author       = {Babaev et al., Dmitrii},
	year         = 2019,
	booktitle    = {KDD},
}
@article{gdprinterp,
	title        = {{Meaningful information and the right to explanation}},
	author       = {Selbst, Andrew D and Powles, Julia},
	year         = 2017,
	month        = 12,
	journal      = {International Data Privacy Law},
	volume       = 7,
	number       = 4,
	pages        = {233--242},
	issn         = {2044-3994},
}
@article{balasubramanian2018insurance,
	title        = {Insurance 2030: The impact of AI on the future of insurance},
	author       = {Balasubramanian et al., Ramnath},
	year         = 2018,
	journal      = {McKinsey \& Company},
}
@book{henryvunited,
	title        = {Henry v. United States},
	year         = 1959,
	volume       = {361 U.S. 98},
}
@inproceedings{shinde2018comparative,
	title        = {Comparative Study of Regression Models and Deep Learning Models for Insurance Cost Prediction},
	author       = {Shinde et al., Aditya},
	year         = 2018,
	booktitle    = {ISDA},
}
@article{murgia2019s,
	title        = {Who’s using your face? The ugly truth about facial recognition|},
	author       = {Murgia, Madhumita},
	year         = 2019,
	journal      = {Financial Times},
}
@misc{lopatto2020clearview,
	title        = {Clearview AI CEO says ‘over 2,400 police agencies’ are using its facial recognition software},
	author       = {Lopatto, Elizabeth},
	year         = 2020,
	publisher    = {The Verge, June},
}
@article{vincent2020nypd,
	title        = {NYPD used facial recognition to track down Black Lives Matter activist},
	author       = {Vincent, James},
	year         = 2020,
	journal      = {The Verge, August},
}
@inproceedings{huang2008labeled,
	title        = {Labeled faces in the wild: A database forstudying face recognition in unconstrained environments},
	author       = {Huang, Gary B and Mattar, Marwan and Berg, Tamara and Learned-Miller, Eric},
	year         = 2008,
}
@inproceedings{Mironov17Renyi,
	title        = {Rényi Differential Privacy},
	author       = {Ilya Mironov},
	year         = 2017,
	booktitle    = {Proceedings of 30th IEEE Computer Security Foundations Symposium (CSF)},
}
@inproceedings{resnet,
	title        = {Deep Residual Learning for Image Recognition},
	author       = {K. {He} and X. {Zhang} and S. {Ren} and J. {Sun}},
	year         = 2016,
	booktitle    = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
}
@misc{tf-determinism,
	note         = {Retrieved on 6/5/2020},
	key          = {tensorflow-determinism Python package},
	howpublished = {Available at: https://pypi.org/project/tensorflow-determinism/},
}
@inproceedings{Dwork15Adaptive,
	title        = {Preserving Statistical Validity in Adaptive Data Analysis},
	author       = {Dwork, Cynthia and Feldman, Vitaly and Hardt, Moritz and Pitassi, Toniann and Reingold, Omer and Roth, Aaron Leon},
	year         = 2015,
	booktitle    = {Proceedings of the Forty-Seventh Annual ACM Symposium on Theory of Computing (STOC)},
}
@incollection{Bengio+chapter2007,
	title        = {Scaling Learning Algorithms Towards {AI}},
	author       = {Bengio, Yoshua and LeCun, Yann},
	year         = 2007,
	booktitle    = {Large Scale Kernel Machines},
	publisher    = {MIT Press},
}

@inproceedings{richardson2021towards,
  title={Towards fairness in practice: A practitioner-oriented rubric for evaluating Fair ML Toolkits},
  author={Richardson, Brianna and Garcia-Gathright, Jean and Way, Samuel F and Thom, Jennifer and Cramer, Henriette},
  booktitle={Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
  pages={1--13},
  year={2021}
}

@article{pessach2022review,
  title={A review on fairness in machine learning},
  author={Pessach, Dana and Shmueli, Erez},
  journal={ACM Computing Surveys (CSUR)},
  volume={55},
  number={3},
  pages={1--44},
  year={2022},
  publisher={ACM New York, NY}
}

@inproceedings{mitchell2019model,
  title={Model cards for model reporting},
  author={Mitchell, Margaret and Wu, Simone and Zaldivar, Andrew and Barnes, Parker and Vasserman, Lucy and Hutchinson, Ben and Spitzer, Elena and Raji, Inioluwa Deborah and Gebru, Timnit},
  booktitle={Proceedings of the conference on fairness, accountability, and transparency},
  pages={220--229},
  year={2019}
}

@inproceedings{xu2021robust,
  title={To be robust or to be fair: Towards fairness in adversarial training},
  author={Xu, Han and Liu, Xiaorui and Li, Yaxin and Jain, Anil and Tang, Jiliang},
  booktitle={International Conference on Machine Learning},
  pages={11492--11501},
  year={2021},
  organization={PMLR}
}

@inproceedings{ma2022tradeoff,
  title={On the Tradeoff Between Robustness and Fairness},
  author={Ma, Xinsong and Wang, Zekai and Liu, Weiwei},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022}
}

@inproceedings{benz2021robustness,
  title={Robustness may be at odds with fairness: An empirical study on class-wise accuracy},
  author={Benz, Philipp and Zhang, Chaoning and Karjauv, Adil and Kweon, In So},
  booktitle={NeurIPS 2020 Workshop on Pre-registration in Machine Learning},
  pages={325--342},
  year={2021},
  organization={PMLR}
}


@article{AIF360,
	title        = {{AI} Fairness 360: An Extensible Toolkit for Detecting, Understanding, and Mitigating Unwanted Algorithmic Bias},
	author       = {Rachel K. E. Bellamy and Kuntal Dey and Michael Hind and Samuel C. Hoffman and Stephanie Houde and Kalapriya Kannan and Pranay Lohia and Jacquelyn Martino and Sameep Mehta and Aleksandra Mojsilovic and Seema Nagar and Karthikeyan Natesan Ramamurthy and John T. Richards and Diptikalyan Saha and Prasanna Sattigeri and Moninder Singh and Kush R. Varshney and Yunfeng Zhang},
	year         = 2018,
	journal      = {CoRR},
	volume       = {abs/1810.01943},
	url          = {http://arxiv.bs/1810.0194},
	archiveprefix = {arXiv},
	eprint       = {1810.01943},
}
@article{adler2018auditing,
	title        = {Auditing black-box models for indirect influence},
	author       = {Adler, Philip and Falk, Casey and Friedler, Sorelle A and Nix, Tionney and Rybeck, Gabriel and Scheidegger, Carlos and Smith, Brandon and Venkatasubramanian, Suresh},
	year         = 2018,
	journal      = {Knowledge and Information Systems},
	volume       = 54,
	number       = 1,
	pages        = {95--122},
}
@inproceedings{pmlr-v80-agarwal18a,
	title        = {A Reductions Approach to Fair Classification},
	author       = {Agarwal, Alekh and Beygelzimer, Alina and Dudik, Miroslav and Langford, John and Wallach, Hanna},
	booktitle    = {Proceedings of the 35th International Conference on Machine Learning},
	pages        = {60--69},
	year         = 2018,
	editor       = {Dy, Jennifer and Krause, Andreas},
	volume       = 80,
	series       = {Proceedings of Machine Learning Research},
	month        = {10--15 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v80/agarwal18a/agarwal18a.pdf},
	url          = {https://proceedings.mlr.press/v80/agarwal18a.html},
	abstract     = {We present a systematic approach for achieving fairness in a binary classification setting. While we focus on two well-known quantitative definitions of fairness, our approach encompasses many other previously studied definitions as special cases. The key idea is to reduce fair classification to a sequence of cost-sensitive classification problems, whose solutions yield a randomized classifier with the lowest (empirical) error subject to the desired constraints. We introduce two reductions that work for any representation of the cost-sensitive classifier and compare favorably to prior baselines on a variety of data sets, while overcoming several of their disadvantages.},
}
@inproceedings{altschuler2017near,
	title        = {Near-linear time approximation algorithms for optimal transport via {S}inkhorn iteration},
	author       = {Altschuler, Jason and Weed, Jonathan and Rigollet, Philippe},
	year         = 2017,
	booktitle    = {Advances in Neural Information Processing Systems},
	pages        = {1964--1974},
}
@article{angwin2016machine,
	title        = {Machine bias: There's software used across the country to predict future criminals. And it's biased against blacks.},
	author       = {Angwin, Julia and Larson, Jeff and Mattu, Surya and Kirchner, Lauren},
	year         = 2016,
	journal      = {ProPublica},
}
@article{arjovsky2017wasserstein,
	title        = {Wasserstein {GAN}},
	author       = {Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'e}on},
	year         = 2017,
	journal      = {arXiv preprint arXiv:1701.07875},
}
@article{cheng2017survey,
	title        = {A survey of model compression and acceleration for deep neural networks},
	author       = {Cheng, Yu and Wang, Duo and Zhou, Pan and Zhang, Tao},
	year         = 2017,
	journal      = {arXiv preprint arXiv:1710.09282},
}
@article{asher2017inside,
	title        = {Inside the algorithm that tries to predict gun violence in {C}hicago},
	author       = {Asher, Jeff and Arthur, Rob},
	year         = 2017,
	journal      = {The New York Times},
}
@article{barnes2010estimating,
	title        = {Estimating the Effect of Gang Membership on Nonviolent and Violent Delinquency: A Counterfactual Analysis},
	author       = {Barnes, JC and Beaver, Kevin M and Miller, J Mitchell},
	year         = 2010,
	journal      = {Aggressive behavior},
	volume       = 36,
	number       = 6,
	pages        = {437--451},
}
@article{barocas2016big,
	title        = {Big data's disparate impact},
	author       = {Barocas, Solon and Selbst, Andrew D},
	year         = 2016,
	journal      = {California Law Review},
	volume       = 104,
	pages        = {671--732},
}
@book{boyd2004convex,
	title        = {Convex Optimization},
	author       = {Boyd, Stephen and Vandenberghe, Lieven},
	year         = 2004,
	publisher    = {Cambridge {U}niversity {P}ress},
}
@article{fliptest,
	title        = {FlipTest: Fairness Auditing via Optimal Transport},
	author       = {Emily Black and Samuel Yeom and Matt Fredrikson},
	year         = 2019,
	journal      = {CoRR},
	volume       = {abs/1906.09218},
	url          = {http://arxiv.bs/1906.0921},
	archiveprefix = {arXiv},
	eprint       = {1906.09218},
	timestamp    = {Mon, 24 Jun 2019 17:28:45 +0200},
	biburl       = {https://dblp.ec/bib/journals/corr/abs-1906-0921},
	bibsource    = {dblp computer science bibliography, https://dblp.org},
}

@article{dastin2018amazon,
	title        = {Amazon scraps secret {AI} recruiting tool that showed bias against women},
	author       = {Dastin, Jeffrey},
	year         = 2018,
	journal      = {Reuters},
}
@article{datta2015automated,
	title        = {Automated experiments on ad privacy settings},
	author       = {Datta, Amit and Tschantz, Michael Carl and Datta, Anupam},
	year         = 2015,
	journal      = {Privacy Enhancing Technologies},
	volume       = 2015,
	number       = 1,
	pages        = {92--112},
}
@inproceedings{datta2016algorithmic,
	title        = {Algorithmic transparency via quantitative input influence: Theory and experiments with learning systems},
	author       = {Datta, Anupam and Sen, Shayak and Zick, Yair},
	year         = 2016,
	booktitle    = {IEEE Symposium on Security and Privacy},
	pages        = {598--617},
}
@article{datta2017proxy,
	title        = {Proxy Discrimination in Data-Driven Systems},
	author       = {Datta, Anupam and Fredrikson, Matt and Ko, Gihyuk and Mardziel, Piotr and Sen, Shayak},
	year         = 2017,
	journal      = {arXiv preprint arXiv:1707.08120},
}
@inproceedings{datta2017use,
	title        = {Use Privacy in Data-Driven Systems: Theory and Experiments with Machine Learnt Programs},
	author       = {Datta, Anupam and Fredrikson, Matt and Ko, Gihyuk and Mardziel, Piotr and Sen, Shayak},
	year         = 2017,
	booktitle    = {ACM SIGSAC Conference on Computer and Communications Security},
}
@article{delbarrio2018obtaining,
	title        = {Obtaining fairness using optimal transport theory},
	author       = {del Barrio, Eustasio and Gamboa, Fabrice and Gordaliza, Paula and Loubes, Jean-Michel},
	year         = 2018,
	journal      = {arXiv preprint arXiv:1806.03195},
}
@article{delayed,
	title        = {Delayed Impact of Fair Machine Learning},
	author       = {Lydia T. Liu and Sarah Dean and Esther Rolf and Max Simchowitz and Moritz Hardt},
	year         = 2018,
	journal      = {CoRR},
	volume       = {abs/1803.04383},
	url          = {http://arxiv.bs/1803.0438},
	archiveprefix = {arXiv},
	eprint       = {1803.04383},
	timestamp    = {Mon, 13 Aug 2018 16:47:11 +0200},
	biburl       = {https://dblp.ec/bib/journals/corr/abs-1803-0438},
	bibsource    = {dblp computer science bibliography, https://dblp.org},
}
@inproceedings{dwork2012fairness,
	title        = {Fairness through awareness},
	author       = {Dwork, Cynthia and Hardt, Moritz and Pitassi, Toniann and Reingold, Omer and Zemel, Richard},
	year         = 2012,
	booktitle    = {Innovations in Theoretical Computer Science},
}
@article{dwork2018fairness,
	title        = {Fairness Under Composition},
	author       = {Cynthia Dwork and Christina Ilvento},
	year         = 2018,
	journal      = {CoRR},
	volume       = {abs/1806.06122},
}
@inproceedings{feldman2015certifying,
	title        = {Certifying and removing disparate impact},
	author       = {Feldman, Michael and Friedler, Sorelle A and Moeller, John and Scheidegger, Carlos and Venkatasubramanian, Suresh},
	year         = 2015,
	booktitle    = {ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
}
@inproceedings{goodfellow2014generative,
	title        = {Generative adversarial nets},
	author       = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	year         = 2014,
	booktitle    = {Advances in Neural Information Processing Systems},
	pages        = {2672--2680},
}
@article{hajian2012methodology,
	title        = {A methodology for direct and indirect discrimination prevention in data mining},
	author       = {Hajian, Sara and Domingo-Ferrer, Josep},
	year         = 2012,
	journal      = {IEEE transactions on knowledge and data engineering},
	publisher    = {IEEE},
	volume       = 25,
	number       = 7,
	pages        = {1445--1459},
}
@inproceedings{hardt2016equality,
	title        = {Equality of opportunity in supervised learning},
	author       = {Hardt, Moritz and Price, Eric and Srebro, Nati},
	year         = 2016,
	booktitle    = {Advances in Neural Information Processing Systems},
}
@inproceedings{pmlr-v80-hebert-johnson18a,
	title        = {Multicalibration: Calibration for the ({C}omputationally-Identifiable) Masses},
	author       = {Hebert-Johnson, Ursula and Kim, Michael and Reingold, Omer and Rothblum, Guy},
	booktitle    = {Proceedings of the 35th International Conference on Machine Learning},
	pages        = {1939--1948},
	year         = 2018,
	editor       = {Dy, Jennifer and Krause, Andreas},
	volume       = 80,
	series       = {Proceedings of Machine Learning Research},
	month        = {10--15 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v80/hebert-johnson18a/hebert-johnson18a.pdf},
	url          = {https://proceedings.mlr.press/v80/hebert-johnson18a.html},
	abstract     = {We develop and study multicalibration as a new measure of fairness in machine learning that aims to mitigate inadvertent or malicious discrimination that is introduced at training time (even from ground truth data). Multicalibration guarantees meaningful (calibrated) predictions for every subpopulation that can be identified within a specified class of computations. The specified class can be quite rich; in particular, it can contain many overlapping subgroups of a protected group. We demonstrate that in many settings this strong notion of protection from discrimination is provably attainable and aligned with the goal of obtaining accurate predictions. Along the way, we present algorithms for learning a multicalibrated predictor, study the computational complexity of this task, and illustrate tight connections to the agnostic learning model.},
}
@article{ingold2016amazon,
	title        = {{A}mazon doesn't consider the race of its customers. {S}hould it?},
	author       = {Ingold, David and Soper, Spencer},
	year         = 2016,
	journal      = {Bloomberg},
}
@inproceedings{joseph2016fairness,
	title        = {Fairness in learning: Classic and contextual bandits},
	author       = {Joseph, Matthew and Kearns, Michael and Morgenstern, Jamie H and Roth, Aaron},
	year         = 2016,
	booktitle    = {Advances in Neural Information Processing Systems},
}
@article{kamiran2012data,
	title        = {Data preprocessing techniques for classification without discrimination},
	author       = {Kamiran, Faisal and Calders, Toon},
	year         = 2012,
	journal      = {Knowledge and Information Systems},
	publisher    = {Springer},
	volume       = 33,
	number       = 1,
	pages        = {1--33},
}
@inproceedings{pmlr-v80-kearns18a,
	title        = {Preventing Fairness Gerrymandering: Auditing and Learning for Subgroup Fairness},
	author       = {Kearns, Michael and Neel, Seth and Roth, Aaron and Wu, Zhiwei Steven},
	booktitle    = {Proceedings of the 35th International Conference on Machine Learning},
	pages        = {2564--2572},
	year         = 2018,
	editor       = {Dy, Jennifer and Krause, Andreas},
	volume       = 80,
	series       = {Proceedings of Machine Learning Research},
	month        = {10--15 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v80/kearns18a/kearns18a.pdf},
	url          = {https://proceedings.mlr.press/v80/kearns18a.html},
	abstract     = {The most prevalent notions of fairness in machine learning fix a small collection of pre-defined groups (such as race or gender), and then ask for approximate parity of some statistic of the classifier (such as false positive rate) across these groups. Constraints of this form are susceptible to fairness gerrymandering, in which a classifier is fair on each individual group, but badly violates the fairness constraint on structured subgroups, such as certain combinations of protected attribute values. We thus consider fairness across exponentially or infinitely many subgroups, defined by a structured class of functions over the protected attributes. We first prove that the problem of auditing subgroup fairness for both equality of false positive rates and statistical parity is computationally equivalent to the problem of weak agnostic learning — which means it is hard in the worst case, even for simple structured subclasses. However, it also suggests that common heuristics for learning can be applied to successfully solve the auditing problem in practice. We then derive an algorithm that provably converges in a polynomial number of steps to the best subgroup-fair distribution over classifiers, given access to an oracle which can solve the agnostic learning problem. The algorithm is based on a formulation of subgroup fairness as a zero-sum game between a Learner (the primal player) and an Auditor (the dual player). We implement a variant of this algorithm using heuristic oracles, and show that we can effectively both audit and learn fair classifiers on a real dataset.},
}
@inproceedings{kearns2019empirical,
	title        = {An empirical study of rich subgroup fairness for machine learning},
	author       = {Kearns, Michael and Neel, Seth and Roth, Aaron and Wu, Zhiwei Steven},
	year         = 2019,
	booktitle    = {Conference on Fairness, Accountability, and Transparency},
	pages        = {100--109},
}
@inproceedings{kleinberg2017inherent,
	title        = {Inherent trade-offs in the fair determination of risk scores},
	author       = {Kleinberg, Jon and Mullainathan, Sendhil and Raghavan, Manish},
	year         = 2017,
	booktitle    = {Innovations in Theoretical Computer Science},
	pages        = {43:1--43:23},
}
@inproceedings{kilbertusneurips17,
	author       = {Kilbertus, Niki and Rojas Carulla, Mateo and Parascandolo, Giambattista and Hardt, Moritz and Janzing, Dominik and Sch\"{o}lkopf, Bernhard},
	booktitle    = {Advances in Neural Information Processing Systems},
	editor       = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
	pages        = {},
	publisher    = {Curran Associates, Inc.},
	title        = {Avoiding Discrimination through Causal Reasoning},
	url          = {https://proceedings.neurips.cc/paper\%5Ffiles/paper/2017/file/f5f8590cd58a54e94377e6ae2eded4d9-Paper.pdf},
	volume       = 30,
	year         = 2017,
}
@inproceedings{pmlr-v81-buolamwini18a,
	title        = {Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification},
	author       = {Buolamwini, Joy and Gebru, Timnit},
	booktitle    = {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
	pages        = {77--91},
	year         = 2018,
	editor       = {Friedler, Sorelle A. and Wilson, Christo},
	volume       = 81,
	series       = {Proceedings of Machine Learning Research},
	month        = {23--24 Feb},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf},
	url          = {https://proceedings.mlr.press/v81/buolamwini18a.html},
	abstract     = {Recent studies demonstrate that machine learning algorithms can discriminate based on classes like race and gender. In this work, we present an approach to evaluate bias present in automated facial analysis algorithms and datasets with respect to phenotypic subgroups. Using the dermatologist  approved Fitzpatrick Skin Type classification system, we characterize the gender and skin type distribution of two facial analysis benchmarks, IJB-A and Adience. We find that these datasets are overwhelmingly composed of lighter-skinned subjects (79.6% for IJB-A and 86.2% for Adience) and introduce a new facial analysis dataset which is balanced by gender and skin type. We evaluate 3 commercial gender classification systems using our dataset and show that darker-skinned females are the most misclassified group (with error rates of up to 34.7%). The maximum error rate for lighter-skinned males is 0.8%. The substantial disparities in the accuracy of classifying darker females, lighter females, darker males, and lighter males in gender classification systems require urgent attention if commercial companies are to build genuinely fair, transparent and accountable facial analysis algorithms.},
}
@article{kuhn1955hungarian,
	title        = {The {H}ungarian method for the assignment problem},
	author       = {H. W. Kuhn and Bryn Yaw},
	year         = 1955,
	journal      = {Naval Research Logistics Quarterly},
	volume       = 2,
	pages        = {83--97},
}
@inproceedings{kusnerneurips17,
	author       = {Kusner, Matt J and Loftus, Joshua and Russell, Chris and Silva, Ricardo},
	booktitle    = {Advances in Neural Information Processing Systems},
	editor       = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
	pages        = {},
	publisher    = {Curran Associates, Inc.},
	title        = {Counterfactual Fairness},
	url          = {https://proceedings.neurips.cc/paper\%5Ffiles/paper/2017/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf},
	volume       = 30,
	year         = 2017,
}
@article{leygonie2019adversarial,
	title        = {Adversarial Computation of Optimal Transport Maps},
	author       = {Leygonie, Jacob and She, Jennifer and Almahairi, Amjad and Rajeswar, Sai and Courville, Aaron},
	year         = 2019,
	journal      = {arXiv preprint arXiv:1906.09691},
}
@inproceedings{lipton2018does,
	title        = {Does mitigating {ML}'s impact disparity require treatment disparity?},
	author       = {Lipton, Zachary and McAuley, Julian and Chouldechova, Alexandra},
	year         = 2018,
	booktitle    = {Advances in Neural Information Processing Systems},
	pages        = {8125--8135},
}
@inproceedings{ma2018implicit,
	title        = {Implicit Regularization in Nonconvex Statistical Estimation: Gradient Descent Converges Linearly for Phase Retrieval and Matrix Completion},
	author       = {Ma, Cong and Wang, Kaizheng and Chi, Yuejie and Chen, Yuxin},
	year         = 2018,
	booktitle    = {International Conference on Machine Learning},
	pages        = {3351--3360},
}
@article{marr2017how,
	title        = {How {AI} And Machine Learning Are Used To Transform The Insurance Industry},
	author       = {Marr, Bernard},
	year         = 2017,
	journal      = {Forbes},
}
@inproceedings{perrot2016mapping,
	title        = {Mapping estimation for discrete optimal transport},
	author       = {Perrot, Micha{\"e}l and Courty, Nicolas and Flamary, R{\'e}mi and Habrard, Amaury},
	year         = 2016,
	booktitle    = {Advances in Neural Information Processing Systems},
	pages        = {4197--4205},
}
@article{peyre2019computational,
	title        = {Computational Optimal Transport},
	author       = {Peyr{\'{e}}, Gabriel and Cuturi, Marco},
	year         = 2019,
	journal      = {Foundations and Trends{\textregistered} in Machine Learning},
	volume       = 11,
	number       = {5--6},
	pages        = {355--607},
}
@article{quanrud2018approximating,
	title        = {Approximating optimal transport with linear programs},
	author       = {Quanrud, Kent},
	year         = 2018,
	journal      = {arXiv preprint arXiv:1810.05957},
}
@article{redmond2002data,
	title        = {A data-driven software tool for enabling cooperative information sharing among police departments},
	author       = {Redmond, Michael and Baveja, Alok},
	year         = 2002,
	journal      = {European Journal of Operational Research},
	volume       = 141,
	number       = 3,
	pages        = {660--678},
}
@article{seguy2017large,
	title        = {Large-scale optimal transport and mapping estimation},
	author       = {Seguy, Vivien and Damodaran, Bharath Bhushan and Flamary, R{\'e}mi and Courty, Nicolas and Rolet, Antoine and Blondel, Mathieu},
	year         = 2017,
	journal      = {arXiv preprint arXiv:1711.02283},
}
@inproceedings{tramer2017fairtest,
	title        = {FairTest: Discovering unwarranted associations in data-driven applications},
	author       = {Tram{\`e}r, Florian and Atlidakis, Vaggelis and Geambasu, Roxana and Hsu, Daniel and Hubaux, Jean-Pierre and Humbert, Mathias and Juels, Ari and Lin, Huang},
	year         = 2017,
	booktitle    = {IEEE European Symposium on Security and Privacy},
	pages        = {401--416},
}
@inproceedings{ustun2019actionable,
	title        = {Actionable Recourse in Linear Classification},
	author       = {Ustun, Berk and Spangher, Alexander and Liu, Yang},
	year         = 2019,
	booktitle    = {Conference on Fairness, Accountability, and Transparency},
	pages        = {10--19},
}
@book{villani2008optimal,
	title        = {Optimal transport: Old and new},
	author       = {Villani, C{\'e}dric},
	year         = 2008,
	publisher    = {Springer Science \& Business Media},
	volume       = 338,
}
@article{wachter2018counterfactual,
	title        = {Counterfactual explanations without opening the black box: Automated decisions and the GDPR},
	author       = {Wachter, Sandra and Mittelstadt, Brent and Russell, Chris},
	year         = 2018,
	journal      = {Harvard Journal of Law \& Technology},
	volume       = 31,
	number       = 2,
	pages        = {841--887},
}
@book{wightman1998lsac,
	title        = {{LSAC} national longitudinal bar passage study},
	author       = {Wightman, Linda F and Ramsey, Henry},
	year         = 1998,
	publisher    = {Law School Admission Council},
}
@inproceedings{woodworth2017learning,
	title        = {Learning Non-Discriminatory Predictors},
	author       = {Woodworth, Blake and Gunasekar, Suriya and Ohannessian, Mesrob I and Srebro, Nathan},
	year         = 2017,
	booktitle    = {Conference on Learning Theory},
	pages        = {1920--1953},
}
@article{yang2019scalable,
	title        = {Scalable Unbalanced Optimal Transport using Generative Adversarial Networks},
	author       = {Karren D. Yang and Caroline Uhler},
	year         = 2018,
	journal      = {arXiv preprint arXiv:1810.11447},
}
@inproceedings{yeom2018hunting,
	title        = {Hunting for discriminatory proxies in linear regression models},
	author       = {Yeom, Samuel and Datta, Anupam and Fredrikson, Matt},
	year         = 2018,
	booktitle    = {Advances in Neural Information Processing Systems},
	pages        = {4568--4578},
}
@inproceedings{zafar2017fairness-aistats,
	title        = {Fairness Constraints: Mechanisms for Fair Classification},
	author       = {Zafar, Muhammad Bilal and Valera, Isabel and Rogriguez, Manuel Gomez and Gummadi, Krishna P},
	year         = 2017,
	booktitle    = {Artificial Intelligence and Statistics},
	pages        = {962--970},
}
@inproceedings{zemel2013learning,
	title        = {Learning fair representations},
	author       = {Zemel, Rich and Wu, Yu and Swersky, Kevin and Pitassi, Toni and Dwork, Cynthia},
	year         = 2013,
	booktitle    = {International Conference on Machine Learning},
	pages        = {325--333},
}
@article{zhang2016identifying,
	title        = {Identifying significant predictive bias in classifiers},
	author       = {Zhang, Zhe and Neill, Daniel B},
	year         = 2016,
	journal      = {arXiv preprint arXiv:1611.08292},
}
%end Publications

%Supreme Court cases sorted by key
@misc{connecticut1982,
	title        = {\textit{Connecticut v.\ Teal}},
	author       = {{Supreme Court of the United States}},
	year         = 1982,
	howpublished = {457 U.S. 440},
}
@misc{gratz2003,
	title        = {\textit{Gratz v.\ Bolinger}},
	author       = {{Supreme Court of the United States}},
	year         = 2003,
	howpublished = {539 U.S. 244},
}
@misc{griggs1971,
	title        = {\textit{Griggs v.\ Duke Power Co.}},
	author       = {{Supreme Court of the United States}},
	year         = 1971,
	howpublished = {401 U.S. 424},
}
@misc{ricci2009,
	title        = {\textit{Ricci v.\ DeStefano}},
	author       = {{Supreme Court of the United States}},
	year         = 2009,
	howpublished = {557 U.S. 557},
}
%end Supreme Court cases

%Miscellaneous sorted by key
@misc{allegheny,
	title        = {Developing Predictive Models to Support Child Maltreatment Hotline Screening Decisions: {A}llegheny {C}ounty Methodology and Implementation},
	author       = {Vaithianathan, Rhema and Putnam-Hornstein, Emily and Jiang, Nan and Nand, Parma and Maloney, Tim},
	year         = 2017,
	howpublished = {https://wleghenycountyanalytics.us/wp-contet/uploads/2018/02/DevelopingPredictiveRiskModels-package_011618.pdf},
}
@misc{chicago,
	title        = {{S}trategic {S}ubject {L}ist},
	author       = {{City of Chicago}},
	year         = 2017,
	howpublished = {https:/.cityofchicago.org/Public-Safety/Strategic-Subject-List/4aki-r3np},
}
@misc{compas,
	title        = {Practitioner's Guide to {COMPAS} Core},
	author       = {Equivant},
	year         = 2019,
	howpublished = {http://quivant.com/wp-content/uploas/Practitioners-Guide-to-COMPAS-Core-040419.pdf},
}
@misc{compasrebuttal,
	title        = {{COMPAS} risk scales: Demonstrating accuracy equity and predictive parity},
	author       = {Dieterich, William and Mendoza, Christina and Brennan, Tim},
	year         = 2016,
	howpublished = {http://larisgroup.com/rs/430-MBX-989/images/ProPublica_Commentary_Final_070616.df},
}
@misc{alex2016fair,
	title        = {Fair prediction with disparate impact: A study of bias in recidivism prediction instruments},
	author       = {Alexandra Chouldechova},
	year         = 2016,
	eprint       = {1610.07524},
	archiveprefix = {arXiv},
	primaryclass = {stat.AP},
}
@misc{han2015deep,
	title        = {Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding},
	author       = {Song Han and Huizi Mao and William J. Dally},
	year         = 2015,
	eprint       = {1510.00149},
	archiveprefix = {arXiv},
	primaryclass = {cs.CV},
}
@article{healthcare,
	title        = {What Happens When an Algorithm Cuts Your Health Care},
	author       = {Lecher, Colin},
	year         = 2018,
	journal      = {The Verge},
}
@misc{cvxopt,
	title        = {{CVXOPT}: {P}ython software for convex optimization},
	author       = {Andersen, Martin S and Dahl, Joachim and Vandenberghe, Lieven},
	howpublished = {http://t.or},
}
@misc{eucheckresponse,
	title        = {Shortcut or Sleight of Hand{?} Why The Checklist Approach In The {EU} Guidelines Does Not Work},
	author       = {Davola, Antonio and Black, Emily and Gulson, Kalervo and Rockwell, Geoffrey and Selinger, Evan and Zeide, Elana},
	howpublished = {https://ae.og/from-shortcut-to-sleight-of-hand-why-the-checklist-approach-in-the-eu-guidelines-does-not-work/},
}
@misc{eeoc,
	title        = {Uniform Guidelines on Employee Selection Procedures},
	author       = {{Equal Employment Opportunities Commission}},
	year         = 1978,
	howpublished = {29 CFR Part 1607},
}
@misc{gurobi,
	title        = {Gurobi Optimizer Reference Manual},
	author       = {{Gurobi Optimization, LLC}},
	year         = 2019,
	howpublished = {httpsw.gurobi.com/documentation/8.1/refman.pd},
}
@misc{keras,
	title        = {Keras},
	author       = {Chollet, Fran\c{c}ois and others},
	year         = 2015,
	howpublished = {httpsras.i},
}
@misc{tensorflow,
	title        = {{TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
	author       = {Mart\'{\i}n~Abadi and Ashish~Agarwal and Paul~Barham and Eugene~Brevdo and Zhifeng~Chen and Craig~Citro and Greg~S.~Corrado and Andy~Davis and Jeffrey~Dean and Matthieu~Devin and Sanjay~Ghemawat and Ian~Goodfellow and Andrew~Harp and Geoffrey~Irving and Michael~Isard and Yangqing Jia and Rafal~Jozefowicz and Lukasz~Kaiser and Manjunath~Kudlur and Josh~Levenberg and Dandelion~Man\'{e} and Rajat~Monga and Sherry~Moore and Derek~Murray and Chris~Olah and Mike~Schuster and Jonathon~Shlens and Benoit~Steiner and Ilya~Sutskever and Kunal~Talwar and Paul~Tucker and Vincent~Vanhoucke and Vijay~Vasudevan and Fernanda~Vi\'{e}gas and Oriol~Vinyals and Pete~Warden and Martin~Wattenberg and Martin~Wicke and Yuan~Yu and Xiaoqiang~Zheng},
	year         = 2015,
	howpublished = {https:/tensorflow.org/},
}
@article{theano,
	title        = {Theano: A {P}ython framework for fast computation of mathematical expressions},
	author       = {{Theano Development Team}},
	year         = 2016,
	journal      = {arXiv preprint arXiv:1605.02688},
}
@misc{uci,
	title        = {{UCI} Machine Learning Repository},
	author       = {Dua, Dheeru and Karra Taniskidou, Efi},
	year         = 2017,
	howpublished = {https:/ive.ics.uci.edu/ml},
}
@misc{uk-equality,
	title        = {{Equality Act 2010}},
	author       = {{Parliament of the United Kingdom}},
	year         = 2010,
	howpublished = {https:/legislation.gov.uk/ukpga/2010/15/contents},
}
@misc{hashimoto2018fairness,
	title        = {Fairness Without Demographics in Repeated Loss Minimization},
	author       = {Tatsunori B. Hashimoto and Megha Srivastava and Hongseok Namkoong and Percy Liang},
	year         = 2018,
	eprint       = {1806.08010},
	archiveprefix = {arXiv},
	primaryclass = {stat.ML},
}
@inproceedings{blackmodel2022,
	title        = {Model Multiplicity: Opportunities, Concerns, and Solutions},
	author       = {Emily Black and Manish Raghavan and Solon Barocas},
	year         = 2022,
	booktitle    = {ACM FAccT 2022},
}
@article{bertrand2004emily,
	title        = {Are Emily and Greg more employable than Lakisha and Jamal? A field experiment on labor market discrimination},
	author       = {Bertrand, Marianne and Mullainathan, Sendhil},
	year         = 2004,
	journal      = {American economic review},
	volume       = 94,
	number       = 4,
	pages        = {991--1013},
}
@inproceedings{blackIRS2022,
	title        = {Algorithmic Fairness and Vertical Equity: Income Fairness with IRS Tax Audit Models},
	author       = {Emily Black and Hadi Elzayn and Alexandra Chouldechova and Jacob Goldin and Daniel Ho},
	year         = 2022,
	booktitle    = {ACM FAccT 2022},
}
@online{dan_speech,
	title        = {AI for Government},
	author       = {Daniel Ho},
	url          = {https://www.youtube.com/watch?v=SnGUWHgLP-Q&ab\%5Fchannel=ICMEStudio},
	date         = 2020,
	organization = {Stanford Institute for Computational and Electrical Engineering},
}
@inproceedings{elzaynIRS2022,
	title        = {Horizontal Equity in IRS Audits},
	author       = {Hadi Elzayn and Evelyn Smith and Jacob Goldin and Daniel Ho},
	year         = 2022,
	booktitle    = {Forthcoming},
}
@inproceedings{menalso,
	title        = {Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraints},
	author       = {Zhao, Jieyu  and Wang, Tianlu  and Yatskar, Mark  and Ordonez, Vicente  and Chang, Kai-Wei},
	month        = sep,
	booktitle    = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
}
@inproceedings{feat,
	title        = {Feature-Wise Bias Amplification},
	author       = {Leino, Klas and Fredrikson, Matt and Black, Emily and Sen, Shayak and Datta, Anupam},
	year         = 2019,
	booktitle    = {International Conference on Learning Representations (ICLR)},
}
@article{eubanks2018child,
	title        = {A child abuse prediction model fails poor families},
	author       = {Eubanks, Virginia},
	year         = 2018,
	journal      = {Wired Magazine},
}
@inproceedings{han2015learning,
	title        = {Learning both weights and connections for efficient neural network},
	author       = {Han, Song and Pool, Jeff and Tran, John and Dally, William},
	year         = 2015,
	booktitle    = {Advances in neural information processing systems},
	pages        = {1135--1143},
}
@inproceedings{krizhevsky_imagenet_2012,
	title        = {Imagenet classification with deep convolutional neural networks},
	author       = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
	year         = 2012,
	pages        = {1097--1105},
	url          = {http://papers.nips.cer/4824-imagenet-classification-},
	urldate      = {2016-08-19},
	bole         = {Advances in neural information processing system},
	file         = {Krizhevsky et al. - 2012 - Imagenet classification with deep convolutional neural networks.pdf:/Users/filiped/Zotero/storage/3XF97UGU/Krizhevsky et al. - 2012 - Imagenet classification with deep convolutional neural networks.pdf:application/pdf},
}
@article{kingma_adam:_2014,
	title        = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle   = {Adam},
	author       = {Kingma, Diederik P. and Ba, Jimmy},
	year         = 2014,
	month        = dec,
	url          = {http://arxiv.org/abs.698},
	urldate      = {2018-02-09},
	note         = {arXiv: 1412.6980},
	abstract     = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	jou          = {arXiv:1412.6980 [cs},
	keywords     = {Computer Science - Learning},
	file         = {Kingma and Ba - 2014 - Adam - A Method for Stochastic Optimization.pdf:/Users/filiped/Zotero/storage/HWS7BF6K/Kingma and Ba - 2014 - Adam - A Method for Stochastic Optimization.pdf:application/pdf},
}
%end Miscellaneous
@article{Hinton06,
	title        = {A Fast Learning Algorithm for Deep Belief Nets},
	author       = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
	year         = 2006,
	journal      = {Neural Computation},
	volume       = 18,
	pages        = {1527--1554},
}
@book{goodfellow2016deep,
	title        = {Deep learning},
	author       = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
	year         = 2016,
	publisher    = {MIT Press},
	volume       = 1,
}
@article{feldman2019shorttale,
	title        = {Does Learning Require Memorization? {A} Short Tale about a Long Tail},
	author       = {Vitaly Feldman},
	year         = 2019,
	journal      = {CoRR},
	volume       = {abs/1906.05271},
}
@article{wang2016learningwithdp,
	title        = {Learning with Differential Privacy: Stability, Learnability and the Sufficiency and Necessity of ERM Principle},
	author       = {Yu-Xiang Wang and Jing Lei and Stephen E. Fienberg},
	year         = 2016,
	journal      = {Journal of Machine Learning Research},
	volume       = 17,
	number       = 183,
	pages        = {1--40},
	url          = {http://jmlr.orgrs/v17/15-313.htl},
}
@inproceedings{hardt2016trainfaster,
	title        = {Train Faster, Generalize Better: Stability of Stochastic Gradient Descent},
	author       = {Hardt, Moritz and Recht, Benjamin and Singer, Yoram},
	year         = 2016,
	booktitle    = {Proceedings of the 33rd International Conference on International Conference on Machine Learning},
	series       = {ICML’16},
	numpages     = 10,
}
@incollection{lei2018sgdover-param,
	title        = {How SGD Selects the Global Minima in Over-parameterized Learning: A Dynamical Stability Perspective},
	author       = {Wu, Lei and Ma, Chao and E, Weinan},
	year         = 2018,
	booktitle    = {Advances in Neural Information Processing Systems 31},
	publisher    = {Curran Associates, Inc.},
	pages        = {8279--8288},
	url          = {http://papers.nips.ccr/8049-how-sgd-selects-the-global-minima-in-over-parameterized-learning--dynamical-stability-perspective.pdf},
	editor       = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
}
@inproceedings{Kuzborskij2018DataDependentSO,
	title        = {Data-Dependent Stability of Stochastic Gradient Descent},
	author       = {Ilja Kuzborskij and Christoph H. Lampert},
	year         = 2018,
	booktitle    = {ICML},
}
@misc{szegedy2013intriguing,
	title        = {Intriguing properties of neural networks},
	author       = {Christian Szegedy and Wojciech Zaremba and Ilya Sutskever and Joan Bruna and Dumitru Erhan and Ian Goodfellow and Rob Fergus},
	year         = 2013,
	eprint       = {1312.6199},
	archiveprefix = {arXiv},
	primaryclass = {cs.CV},
}
@inproceedings{madry2018towards,
	title        = {Towards Deep Learning Models Resistant to Adversarial Attacks},
	author       = {Aleksander Madry and Aleksandar Makelov and Ludwig Schmidt and Dimitris Tsipras and Adrian Vladu},
	year         = 2018,
	booktitle    = {International Conference on Learning Representations},
}
@inproceedings{simonyan2014deep,
	title        = {Deep inside convolutional networks: Visualising image classification models and saliency maps},
	author       = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
	year         = 2014,
	booktitle    = {International Conference on Learning Representations (ICLR)},
}
@article{advexamplesgoodfellow,
	title        = {Explaining and Harnessing Adversarial Examples},
	author       = {Goodfellow, Ian and Shlens, Jonathon and Szegedy, Christian},
	year         = 2014,
	month        = 12,
	journal      = {arXiv 1412.6572},
	pages        = {},
}
@incollection{ilyas2019adversarialnotbugs,
	title        = {Adversarial Examples Are Not Bugs, They Are Features},
	author       = {Ilyas, Andrew and Santurkar, Shibani and Tsipras, Dimitris and Engstrom, Logan and Tran, Brandon and Madry, Aleksander},
	year         = 2019,
	booktitle    = {Advances in Neural Information Processing Systems 32},
}
@inproceedings{tsipras2018robustness,
	title        = {Robustness May Be at Odds with Accuracy},
	author       = {Dimitris Tsipras and Shibani Santurkar and Logan Engstrom and Alexander Turner and Aleksander Madry},
	year         = 2019,
	booktitle    = {International Conference on Learning Representations},
}
@article{noack2019does,
	title        = {Does Interpretability of Neural Networks Imply Adversarial Robustness?},
	author       = {Noack, Adam and Ahern, Isaac and Dou, Dejing and Li, Boyang},
	year         = 2019,
	journal      = {CoRR},
	volume       = {abs/1912.03430},
}
@inproceedings{Etmann2019Ontheconnection,
	title        = {On the Connection Between Adversarial Robustness and Saliency Map Interpretability},
	author       = {Christian Etmann and Sebastian Lunz and Peter Maass and Carola-Bibiane Sch{\"o}nlieb},
	year         = 2019,
	booktitle    = {ICML},
}
@article{leino2019memorizationprivacy,
	title        = {Stolen Memories: Leveraging Model Memorization for Calibrated White-Box Membership Inference},
	author       = {Klas Leino and Matt Fredrikson},
	year         = 2020,
	booktitle    = {{USENIX} Security Symposium},
}
@article{zhang2016understanding,
	title        = {Understanding deep learning requires rethinking generalization},
	author       = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
	year         = 2016,
	journal      = {CoRR},
	volume       = {abs/1611.03530},
}
@article{carlini2018secretsharer,
	title        = {The Secret Sharer: Measuring Unintended Neural Network Memorization {\&} Extracting Secrets},
	author       = {Nicholas Carlini and Chang Liu and Jernej Kos and {\'{U}}lfar Erlingsson and Dawn Song},
	year         = 2018,
	journal      = {CoRR},
	volume       = {abs/1802.08232},
	url          = {http://arxiv.bs/1802.0823},
	archiveprefix = {arXiv},
	eprint       = {1802.08232},
	timestamp    = {Mon, 22 Jul 2019 13:37:30 +0200},
	biburl       = {https://dblp.ec/journals/corr/abs-1802-08232.bi},
	bibsource    = {dblp computer science bibliography, https://dblp.org},
}
@inproceedings{song2017remembertoomuch,
	title        = {Machine Learning Models That Remember Too Much},
	author       = {Song, Congzheng and Ristenpart, Thomas and Shmatikov, Vitaly},
	year         = 2017,
	booktitle    = {Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security},
}
@inproceedings{arpit2017closer,
	title        = {A closer look at memorization in deep networks},
	author       = {Arpit, Devansh and Jastrz{\k{e}}bski, Stanis{\l}aw and Ballas, Nicolas and Krueger, David and Bengio, Emmanuel and Kanwal, Maxinder S and Maharaj, Tegan and Fischer, Asja and Courville, Aaron and Bengio, Yoshua and others},
	year         = 2017,
	booktitle    = {Proceedings of the 34th International Conference on Machine Learning-Volume 70},
}
@inproceedings{yeom2018privacy,
	title        = {Privacy Risk in Machine Learning: Analyzing the Connection to Overfitting},
	author       = {S. {Yeom} and I. {Giacomelli} and M. {Fredrikson} and S. {Jha}},
	year         = 2018,
	booktitle    = {2018 IEEE 31st Computer Security Foundations Symposium (CSF)},
}
@article{antun2020instabilities,
	title        = {On instabilities of deep learning in image reconstruction and the potential costs of AI},
	author       = {Antun, Vegard and Renna, Francesco and Poon, Clarice and Adcock, Ben and Hansen, Anders C.},
	year         = 2020,
	journal      = {Proceedings of the National Academy of Sciences},
	publisher    = {National Academy of Sciences},
	doi          = {10.1073/pnas.1907377117},
	issn         = {0027-8424},
	url          = {https://www.pnas.ortent/early/2020/05/08/190737711},
	elocation-id = 201907377,
	eprint       = {https://www.pnas.org/content/early/2020/05/08/1907377117.full.pdf},
}
@misc{dwork2006differential,
	title        = {Differential Privacy},
	author       = {Dwork, Cynthia},
	year         = 2006,
	journal      = {ICALP 2006},
}
@misc{georgetown,
	title        = {The Perpetual Lineup},
	author       = {Clare Garvie and Alvaro Bedoya and Jonathan Frankle},
	year         = 2016,
	publisher    = {Center on Privacy & Technology at Georgetown Law},
}
@article{elisseeff2003leave,
	title        = {Leave-one-out error and stability of learning algorithms with applications},
	author       = {Elisseeff, Andr{\'e} and Pontil, Massimiliano and others},
	year         = 2003,
	journal      = {NATO science series sub series iii computer and systems sciences},
	publisher    = {IOS press},
}
@inproceedings{covington2016deep,
	title        = {Deep neural networks for youtube recommendations},
	author       = {Covington, Paul and Adams, Jay and Sargin, Emre},
	year         = 2016,
	booktitle    = {Proceedings of the 10th ACM conference on recommender systems},
}
@inproceedings{acien2018measuring,
	title        = {Measuring the gender and ethnicity bias in deep models for face recognition},
	author       = {Acien, Alejandro and Morales, Aythami and Vera-Rodriguez, Ruben and Bartolome, Ivan and Fierrez, Julian},
	year         = 2018,
	booktitle    = {Iberoamerican Congress on Pattern Recognition},
	organization = {Springer},
}
@inproceedings{bias_in_word_embeddings,
	title        = {Bias in Word Embeddings},
	author       = {Papakyriakopoulos, Orestis and Hegelich, Simon and Serrano, Juan Carlos Medina and Marco, Fabienne},
	year         = 2020,
	booktitle    = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
}
@inproceedings{bolukbasi2016man,
	title        = {Man is to computer programmer as woman is to homemaker? debiasing word embeddings},
	author       = {Bolukbasi, Tolga and Chang, Kai-Wei and Zou, James Y and Saligrama, Venkatesh and Kalai, Adam T},
	year         = 2016,
	booktitle    = {Advances in neural information processing systems},
}
@article{litjens2017medical,
	title        = {A survey on deep learning in medical image analysis},
	author       = {Geert Litjens et. al},
	year         = 2017,
	journal      = {Medical Image Analysis},
}
@article{Bakator_2018,
	title        = {Deep Learning and Medical Diagnosis: A Review of Literature},
	author       = {Bakator, Mihalj and Radosav, Dragica},
	year         = 2018,
	month        = {Aug},
	journal      = {Multimodal Technologies and Interaction},
	publisher    = {MDPI AG},
}
@article{Addo_2018,
	title        = {Credit Risk Analysis Using Machine and Deep Learning Models},
	author       = {Addo, Peter and Guegan, Dominique and Hassani, Bertrand},
	year         = 2018,
	month        = {Apr},
	journal      = {Risks},
}
@article{sirignano2016deep,
	title        = {Deep learning for mortgage risk},
	author       = {Sirignano, Justin and Sadhwani, Apaar and Giesecke, Kay},
	year         = 2016,
	journal      = {CoRR},
	volume       = {abs/1607.02470},
}
@article{lipton2018mythos,
	title        = {The mythos of model interpretability},
	author       = {Lipton, Zachary C},
	year         = 2018,
	journal      = {Queue},
	publisher    = {ACM New York, NY, USA},
	volume       = 16,
	number       = 3,
	pages        = {31--57},
}
@inproceedings{10.1145/3351095.3372828,
	title        = {Mitigating Bias in Algorithmic Hiring: Evaluating Claims and Practices},
	author       = {Raghavan, Manish and Barocas, Solon and Kleinberg, Jon and Levy, Karen},
	year         = 2020,
	location     = {Barcelona, Spain},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {FAT* ’20},
	numpages     = 13,
	keywords     = {algorithmic bias, discrimination law, algorithmic hiring},
}
@article{kaminski2019right,
	title        = {The right to explanation, explained},
	author       = {Kaminski, Margot E},
	year         = 2019,
	journal      = {Berkeley Tech. LJ},
	publisher    = {HeinOnline},
	volume       = 34,
	pages        = 189,
}
@online{xiao2017/online,
	title        = {Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms},
	author       = {Han Xiao and Kashif Rasul and Roland Vollgraf},
	year         = 2017,
	date         = {2017-08-28},
	eprintclass  = {cs.LG},
	eprinttype   = {arXiv},
	eprint       = {cs.LG/1708.07747},
}
@inproceedings{yeom2020individual,
	title        = {Individual Fairness Revisited: Transferring Techniques from Adversarial Robustness},
	author       = {Yeom, Samuel and Fredrikson, Matt},
	year         = 2020,
	booktitle    = {IJCAI},
}
@inproceedings{liu2017algorithmic,
	title        = {Algorithmic stability and hypothesis complexity},
	author       = {Liu, Tongliang and Lugosi, G{\'a}bor and Neu, Gergely and Tao, Dacheng},
	year         = 2017,
	booktitle    = {Proceedings of the 34th International Conference on Machine Learning-Volume 70},
	pages        = {2159--2167},
	organization = {JMLR. org},
}
@article{bousquet2002stability,
	title        = {Stability and generalization},
	author       = {Bousquet, Olivier and Elisseeff, Andr{\'e}},
	year         = 2002,
	journal      = {Journal of machine learning research},
	volume       = 2,
	number       = {Mar},
	pages        = {499--526},
}
@article{shalev2010learnability,
	title        = {Learnability, stability and uniform convergence},
	author       = {Shalev-Shwartz, Shai and Shamir, Ohad and Srebro, Nathan and Sridharan, Karthik},
	year         = 2010,
	journal      = {Journal of Machine Learning Research},
	volume       = 11,
}
@article{yeom2020overfitting,
	title        = {Overfitting, robustness, and malicious algorithms: {A} study of potential causes of privacy risk in machine learning},
	author       = {Samuel Yeom and Irene Giacomelli and Alan Menaged and Matt Fredrikson and Somesh Jha},
	year         = 2020,
	journal      = {J. Comput. Secur.},
	volume       = 28,
	number       = 1,
}
@article{nagarajan2018deterministic,
	title        = {Deterministic implementations for reproducibility in deep reinforcement learning},
	author       = {Nagarajan, Prabhat and Warnell, Garrett and Stone, Peter},
	year         = 2018,
	journal      = {CoRR},
	volume       = {abs/1809.05676},
}
@inproceedings{song2019membership,
	title        = {Membership inference attacks against adversarially robust deep learning models},
	author       = {Song, Liwei and Shokri, Reza and Mittal, Prateek},
	year         = 2019,
	booktitle    = {2019 IEEE Security and Privacy Workshops (SPW)},
	organization = {IEEE},
}
a service of Schloss Dagstuhl - Leibniz Center for Informatics  homebrowsesearchabout
@inproceedings{cohen19certified,
	title        = {Certified Adversarial Robustness via Randomized Smoothing},
	author       = {Cohen, Jeremy and Rosenfeld, Elan and Kolter, Zico},
	year         = 2019,
	booktitle    = {Proceedings of the 36th International Conference on Machine Learning (ICML)},
}
@inproceedings{wong18provable,
	title        = {Provable Defenses against Adversarial Examples via the Convex Outer Adversarial Polytope},
	author       = {Wong, Eric and Kolter, Zico},
	year         = 2018,
	booktitle    = {Proceedings of the 35th International Conference on Machine Learning (ICML)},
}
@inproceedings{papernot16limitations,
	title        = {The Limitations of Deep Learning in Adversarial Settings},
	author       = {N. {Papernot} and P. {McDaniel} and S. {Jha} and M. {Fredrikson} and Z. B. {Celik} and A. {Swami}},
	year         = 2016,
	booktitle    = {2016 IEEE European Symposium on Security and Privacy (EuroS P)},
}
@book{bonnans2013perturbation,
	title        = {Perturbation analysis of optimization problems},
	author       = {Bonnans, J Fr{\'e}d{\'e}ric and Shapiro, Alexander},
	year         = 2013,
	publisher    = {Springer Science \& Business Media},
}
@article{laugel2017inverse,
	title        = {Comparison-based inverse classification for interpretability in machine learning},
	author       = {Laugel, Thibault and Lesot, Marie-Jeanne and Marsala, Christophe and Renard, Xavier and Detyniecki, Marcin},
	year         = 2018,
	journal      = {IPMU},
}
@article{grath2018interpretable,
	title        = {Interpretable credit application predictions with counterfactual explanations},
	author       = {Grath, Rory Mc and Costabello, Luca and Van, Chan Le and Sweeney, Paul and Kamiab, Farbod and Shen, Zhao and Lecue, Freddy},
	year         = 2018,
	journal      = {arXiv preprint arXiv:1811.05245},
}
@article{van2019interpretable,
	title        = {Interpretable counterfactual explanations guided by prototypes},
	author       = {Van Looveren, Arnaud and Klaise, Janis},
	year         = 2019,
	journal      = {arXiv preprint arXiv:1907.02584},
}
@inproceedings{10.1145/3097983.3098039,
	title        = {Interpretable Predictions of Tree-Based Ensembles via Actionable Feature Tweaking},
	author       = {Tolomei, Gabriele and Silvestri, Fabrizio and Haines, Andrew and Lalmas, Mounia},
	year         = 2017,
	series       = {KDD},
}
@article{wang2018leveraging,
	title        = {Leveraging deep learning with LDA-based text analytics to detect automobile insurance fraud},
	author       = {Wang, Yibo and Xu, Wei},
	year         = 2018,
	journal      = {Decision Support Systems},
	publisher    = {Elsevier},
	volume       = 105,
	pages        = {87--95},
}
@inproceedings{liu2014early,
	title        = {Early diagnosis of Alzheimer's disease with deep learning},
	author       = {Liu, Siqi and Liu, Sidong and Cai, Weidong and Pujol, Sonia and Kikinis, Ron and Feng, Dagan},
	year         = 2014,
	booktitle    = {2014 IEEE 11th international symposium on biomedical imaging (ISBI)},
	pages        = {1015--1018},
	organization = {IEEE},
}
@inproceedings{sun2016computer,
	title        = {Computer aided lung cancer diagnosis with deep learning algorithms},
	author       = {Sun, Wenqing and Zheng, Bin and Qian, Wei},
	year         = 2016,
	booktitle    = {Medical imaging 2016: computer-aided diagnosis},
	volume       = 9785,
	pages        = {97850Z},
	organization = {International Society for Optics and Photonics},
}
@article{de2018clinically,
	title        = {Clinically applicable deep learning for diagnosis and referral in retinal disease},
	author       = {De Fauw, Jeffrey and Ledsam, Joseph R and Romera-Paredes, Bernardino and Nikolov, Stanislav and Tomasev, Nenad and Blackwell, Sam and Askham, Harry and Glorot, Xavier and O’Donoghue, Brendan and Visentin, Daniel and others},
	year         = 2018,
	journal      = {Nature medicine},
	publisher    = {Nature Publishing Group},
	volume       = 24,
	number       = 9,
	pages        = {1342--1350},
}
@article{adebayo2018sanity,
	title        = {Sanity Checks for Saliency Maps},
	author       = {Adebayo, Julius and Gilmer, Justin and Muelly, Michael and Goodfellow, Ian and Hardt, Moritz and Kim, Been},
	year         = 2018,
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 31,
	pages        = {9505--9515},
}
%now the slew of CF algos to cite
@article{dhurandhar2018explanations,
	title        = {Explanations based on the missing: Towards contrastive explanations with pertinent negatives},
	author       = {Dhurandhar, Amit and Chen, Pin-Yu and Luss, Ronny and Tu, Chun-Chen and Ting, Paishun and Shanmugam, Karthikeyan and Das, Payel},
	year         = 2018,
	journal      = {arXiv preprint arXiv:1802.07623},
}
@inproceedings{dandl2020multi,
	title        = {Multi-objective counterfactual explanations},
	author       = {Dandl, Susanne and Molnar, Christoph and Binder, Martin and Bischl, Bernd},
	year         = 2020,
	booktitle    = {International Conference on Parallel Problem Solving from Nature},
	pages        = {448--469},
	organization = {Springer},
}
@inproceedings{keane2020good,
	title        = {Good counterfactuals and where to find them: A case-based technique for generating counterfactuals for explainable ai (xai)},
	author       = {Keane, Mark T and Smyth, Barry},
	year         = 2020,
	booktitle    = {International Conference on Case-Based Reasoning},
	pages        = {163--178},
	organization = {Springer},
}
@inproceedings{mothilal2020explaining,
	title        = {Explaining machine learning classifiers through diverse counterfactual explanations},
	author       = {Mothilal, Ramaravind K and Sharma, Amit and Tan, Chenhao},
	year         = 2020,
	booktitle    = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
	pages        = {607--617},
}
@article{sharma2019certifai,
	title        = {Certifai: Counterfactual explanations for robustness, transparency, interpretability, and fairness of artificial intelligence models},
	author       = {Sharma, Shubham and Henderson, Jette and Ghosh, Joydeep},
	year         = 2019,
	journal      = {arXiv preprint arXiv:1905.07857},
}
@article{verma2020counterfactual,
	title        = {Counterfactual Explanations for Machine Learning: A Review},
	author       = {Verma, Sahil and Dickerson, John and Hines, Keegan},
	year         = 2020,
	journal      = {arXiv preprint arXiv:2010.10596},
}
@article{karimi2020algorithmic,
	title        = {Algorithmic recourse under imperfect causal knowledge: a probabilistic approach},
	author       = {Karimi, Amir-Hossein and von K{\"u}gelgen, Julius and Sch{\"o}lkopf, Bernhard and Valera, Isabel},
	year         = 2020,
	journal      = {arXiv preprint arXiv:2006.06831},
}
@article{mehrer2020individual,
	title        = {Individual differences among deep neural network models},
	author       = {Mehrer, Johannes and Spoerer, Courtney J and Kriegeskorte, Nikolaus and Kietzmann, Tim C},
	year         = 2020,
	journal      = {Nature communications},
	publisher    = {Nature Publishing Group},
	volume       = 11,
	number       = 1,
	pages        = {1--12},
}
@inproceedings{poyiadzi2020face,
	title        = {FACE: feasible and actionable counterfactual explanations},
	author       = {Poyiadzi, Rafael and Sokol, Kacper and Santos-Rodriguez, Raul and De Bie, Tijl and Flach, Peter},
	year         = 2020,
	booktitle    = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
	pages        = {344--350},
}
@article{mahajan2019preserving,
	title        = {Preserving causal constraints in counterfactual explanations for machine learning classifiers},
	author       = {Mahajan, Divyat and Tan, Chenhao and Sharma, Amit},
	year         = 2019,
	journal      = {arXiv preprint arXiv:1912.03277},
}
@inproceedings{pawelczyk2020learning,
	title        = {Learning model-agnostic counterfactual explanations for tabular data},
	author       = {Pawelczyk, Martin and Broelemann, Klaus and Kasneci, Gjergji},
	year         = 2020,
	booktitle    = {Proceedings of The Web Conference 2020},
	pages        = {3126--3132},
}
@inproceedings{lash2017generalized,
	title        = {Generalized inverse classification},
	author       = {Lash, Michael T and Lin, Qihang and Street, Nick and Robinson, Jennifer G and Ohlmann, Jeffrey},
	year         = 2017,
	booktitle    = {Proceedings of the 2017 SIAM International Conference on Data Mining},
	pages        = {162--170},
	organization = {SIAM},
}
@article{guidotti2018local,
	title        = {Local rule-based explanations of black box decision systems},
	author       = {Guidotti, Riccardo and Monreale, Anna and Ruggieri, Salvatore and Pedreschi, Dino and Turini, Franco and Giannotti, Fosca},
	year         = 2018,
	journal      = {arXiv preprint arXiv:1805.10820},
}
@article{joshi2019towards,
	title        = {Towards realistic individual recourse and actionable explanations in black-box decision making systems},
	author       = {Joshi, Shalmali and Koyejo, Oluwasanmi and Vijitbenjaronk, Warut and Kim, Been and Ghosh, Joydeep},
	year         = 2019,
	journal      = {arXiv preprint arXiv:1907.09615},
}
%specifically for credit
@inproceedings{mc2018interpretable,
	title        = {Interpretable Credit Application Predictions With Counterfactual Explanations},
	author       = {Mc Grath, Rory and Costabello, Luca and Le Van, Chan and Sweeney, Paul and Kamiab, Farbod and Shen, Zhao and Lecue, Freddy},
	year         = 2018,
	booktitle    = {NIPS 2018-Workshop on Challenges and Opportunities for AI in Financial Services: the Impact of Fairness, Explainability, Accuracy, and Privacy},
}
@article{yang2020generating,
	title        = {Generating Plausible Counterfactual Explanations for Deep Transformers in Financial Text Classification},
	author       = {Yang, Linyi and Kenny, Eoin M and Ng, Tin Lok James and Yang, Yi and Smyth, Barry and Dong, Ruihai},
	year         = 2020,
	journal      = {arXiv preprint arXiv:2010.12512},
}
@inproceedings{sundararajan2017axiomatic,
	title        = {Axiomatic attribution for deep networks},
	author       = {Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
	year         = 2017,
	booktitle    = {International Conference on Machine Learning},
	pages        = {3319--3328},
	organization = {PMLR},
}
%CF and AE
@article{freiesleben2020counterfactual,
	title        = {Counterfactual Explanations \& Adversarial Examples--Common Grounds, Essential Differences, and Potential Transfers},
	author       = {Freiesleben, Timo},
	year         = 2020,
	journal      = {arXiv preprint arXiv:2009.05487},
}
@article{browne2020semantics,
	title        = {Semantics and explanation: why counterfactual explanations produce adversarial examples in deep neural networks},
	author       = {Browne, Kieran and Swift, Ben},
	year         = 2020,
	journal      = {arXiv preprint arXiv:2012.10076},
}
@inproceedings{sokol2019counterfactual,
	title        = {Counterfactual explanations of machine learning predictions: opportunities and challenges for AI safety},
	author       = {Sokol, Kacper and Flach, Peter A},
	year         = 2019,
	booktitle    = {SafeAI at AAAI},
}
@article{wang20explanations,
	title        = {Smoothed Geometry for Robust Attribution},
	author       = {Wang, Zifan and Wang, Haofan and Ramkumar, Shakul and Mardziel, Piotr and Fredrikson, Matt and Datta, Anupam},
	year         = 2020,
	booktitle    = {Advances in Neural Information Processing Systems},
}
@article{dombrowski2019explanations,
	title        = {Explanations can be manipulated and geometry is to blame},
	author       = {Dombrowski, Ann-Kathrin and Alber, Maximilian and Anders, Christopher J and Ackermann, Marcel and M{\"u}ller, Klaus-Robert and Kessel, Pan},
	year         = 2019,
	journal      = {arXiv preprint arXiv:1906.07983},
}
@article{croce2019provable,
	title        = {Provable Robustness of ReLU networks via Maximization of Linear Regions},
	author       = {Croce, Francesco and Andriushchenko, Maksym and Hein, Matthias},
	year         = 2019,
	journal      = {AISTATS 2019},
}
@inproceedings{ghorbani2019interpretation,
	title        = {Interpretation of neural networks is fragile},
	author       = {Ghorbani, Amirata and Abid, Abubakar and Zou, James},
	year         = 2019,
	booktitle    = {Proceedings of the AAAI Conference on Artificial Intelligence},
	volume       = 33,
	number       = {01},
	pages        = {3681--3688},
}
@misc{pwc,
	title        = {Managing the risks of machine learning and artificial intelligence models in the financial services industry},
	author       = {PwC},
	year         = 2020,
	publisher    = {PwC},
}
@misc{merchant_2020,
	title        = {Model lifecycle transformation: How banks are unlocking efficiencies: Accenture},
	author       = {Merchant, Gordon et al.},
	year         = 2020,
	month        = {Dec},
	journal      = {Financial Services Blog},
	publisher    = {Accenture},
}
%problems
@inproceedings{barocas2020hidden,
	title        = {The hidden assumptions behind counterfactual explanations and principal reasons},
	author       = {Barocas, Solon and Selbst, Andrew D and Raghavan, Manish},
	year         = 2020,
	booktitle    = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
	pages        = {80--89},
}
@article{laugel2019issues,
	title        = {Issues with post-hoc counterfactual explanations: a discussion},
	author       = {Laugel, Thibault and Lesot, Marie-Jeanne and Marsala, Christophe and Detyniecki, Marcin},
	year         = 2019,
	journal      = {arXiv preprint arXiv:1906.04774},
}
@article{marx2019,
	title        = {Predictive Multiplicity in Classification.},
	author       = {Charles T. Marx and Flávio du Pin Calmon and Berk Ustun},
	year         = 2019,
	journal      = {CoRR},
	volume       = {abs/1909.06677},
	url          = {http://arxiv.org/abs/1909.06677},
	publtype     = {informal},
	cdate        = 1546300800000,
}
@article{heo2019fooling,
	title        = {Fooling Neural Network Interpretations via Adversarial Model Manipulation},
	author       = {Heo, Juyeon and Joo, Sunghwan and Moon, Taesup},
	year         = 2019,
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 32,
	pages        = {2925--2936},
}
@inproceedings{dong2018boosting,
	title        = {Boosting adversarial attacks with momentum},
	author       = {Dong, Yinpeng and Liao, Fangzhou and Pang, Tianyu and Su, Hang and Zhu, Jun and Hu, Xiaolin and Li, Jianguo},
	year         = 2018,
	booktitle    = {Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages        = {9185--9193},
}
@inproceedings{demontis2019adversarial,
	title        = {Why do adversarial attacks transfer? explaining transferability of evasion and poisoning attacks},
	author       = {Demontis, Ambra and Melis, Marco and Pintor, Maura and Jagielski, Matthew and Biggio, Battista and Oprea, Alina and Nita-Rotaru, Cristina and Roli, Fabio},
	year         = 2019,
	booktitle    = {28th $\{$USENIX$\}$ Security Symposium ($\{$USENIX$\}$ Security 19)},
	pages        = {321--338},
}
@article{art2018,
	title        = {Adversarial Robustness Toolbox v1.2.0},
	author       = {Nicolae, Maria-Irina and Sinn, Mathieu and Tran, Minh~Ngoc and Buesser, Beat and Rawat, Ambrish and Wistuba, Martin and Zantedeschi, Valentina and Baracaldo, Nathalie and Chen, Bryant and Ludwig, Heiko and Molloy, Ian and Edwards, Ben},
	year         = 2018,
	journal      = {CoRR},
	volume       = {1807.01069},
	url          = {https://arxiv.org/pdf/1807.01069},
}
@article{Paul,
	title        = {Analysis of different norms and corresponding Lipschitz constants for global optimization},
	author       = {Paulavičius, Remigijus and Žilinskas, Julius},
	year         = 2006,
	month        = {01},
	journal      = {Technological and Economic Development of Economy},
	volume       = 12,
	pages        = {301--306},
	doi          = {10.1080/13928619.2006.9637758},
}
@inproceedings{Xie_2019_CVPR,
	title        = {Improving Transferability of Adversarial Examples With Input Diversity},
	author       = {Xie, Cihang and Zhang, Zhishuai and Zhou, Yuyin and Bai, Song and Wang, Jianyu and Ren, Zhou and Yuille, Alan L.},
	year         = 2019,
	month        = {June},
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
}
@inproceedings{pmlr-v124-pawelczyk20a,
	title        = {On Counterfactual Explanations under Predictive Multiplicity},
	author       = {Pawelczyk, Martin and Broelemann, Klaus and Kasneci, Gjergji.},
	year         = 2020,
	booktitle    = {Proceedings of the 36th Conference on Uncertainty in Artificial Intelligence (UAI)},
	series       = {Proceedings of Machine Learning Research},
}
@inproceedings{blackleave2021,
	title        = {Leave-one-out Unfairness},
	author       = {Black, Emily and Fredrikson, Matt},
	year         = 2021,
	booktitle    = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
	pages        = {285--295},
}
@misc{rawal2021i,
	title        = {Can I Still Trust You?: Understanding the Impact of Distribution Shifts on Algorithmic Recourses},
	author       = {Kaivalya Rawal and Ece Kamar and Himabindu Lakkaraju},
	year         = 2021,
	eprint       = {2012.11788},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG},
}
@misc{FCRA,
	title        = {Fair Credit Reporting Act},
	author       = {Bureau of Consumer Financial Protection},
	year         = 2020,
	publisher    = {Bureau of Consumer Financial Protection},
	howpublished = {\url{https://www.ftc.gov/enforcement/statutes/fair-credit-reporting-act}},
	key          = {Fair Credit Reporting Act, Public Law 91-508},
}
%end Publications

%Supreme Court cases sorted by key
%end Supreme Court cases

%Miscellaneous sorted by key
@article{international2009estimation,
	title        = {Estimation of the warfarin dose with clinical and pharmacogenetic data},
	author       = {International Warfarin Pharmacogenetics Consortium},
	year         = 2009,
	journal      = {New England Journal of Medicine},
	publisher    = {Mass Medical Soc},
	volume       = 360,
	number       = 8,
	pages        = {753--764},
}
@misc{fico,
	title        = {FICO xML Challenge},
	author       = {FICO},
	year         = 2018,
	howpublished = {https://community.fico.com/s/explainable-machine-learning-challenge},
}
@misc{fico_license,
	title        = {Dataset Usage License, FICO xML Challenge},
	author       = {FICO},
	year         = 2018,
	howpublished = {https://community.fico.com/s/explainable-machine-learning-challenge?tabset-3158a=a4c37},
}
@inproceedings{chen2018ead,
	title        = {Ead: elastic-net attacks to deep neural networks via adversarial examples},
	author       = {Chen, Pin-Yu and Sharma, Yash and Zhang, Huan and Yi, Jinfeng and Hsieh, Cho-Jui},
	year         = 2018,
	booktitle    = {Proceedings of the AAAI Conference on Artificial Intelligence},
	volume       = 32,
	number       = 1,
}
@inproceedings{Wong2020Fast,
	title        = {Fast is better than free: Revisiting adversarial training},
	author       = {Eric Wong and Leslie Rice and J. Zico Kolter},
	year         = 2020,
	booktitle    = {International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=BJx040EFvH},
}
@article{Goodfellow2015ExplainingAH,
	title        = {Explaining and Harnessing Adversarial Examples},
	author       = {I. Goodfellow and Jonathon Shlens and Christian Szegedy},
	year         = 2015,
	journal      = {CoRR},
	volume       = {abs/1412.6572},
}
%end Miscellaneous
@inproceedings{carlini2017towards,
	title        = {Towards evaluating the robustness of neural networks},
	author       = {Carlini, Nicholas and Wagner, David},
	year         = 2017,
	booktitle    = {2017 ieee symposium on security and privacy (sp)},
	pages        = {39--57},
	organization = {IEEE},
}
@book{hastie2017generalized,
	title        = {Generalized additive models},
	author       = {Hastie, Trevor J and Tibshirani, Robert J},
	year         = 2017,
	publisher    = {Routledge},
}
@inproceedings{lou2012intelligible,
	title        = {Intelligible models for classification and regression},
	author       = {Lou, Yin and Caruana, Rich and Gehrke, Johannes},
	year         = 2012,
	booktitle    = {Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining},
	pages        = {150--158},
}
@article{dressel2018accuracy,
	title        = {The accuracy, fairness, and limits of predicting recidivism},
	author       = {Dressel, Julia and Farid, Hany},
	year         = 2018,
	journal      = {Science advances},
	publisher    = {American Association for the Advancement of Science},
	volume       = 4,
	number       = 1,
	pages        = {eaao5580},
}
@article{ben1998robust,
	title        = {Robust convex optimization},
	author       = {Ben-Tal, Aharon and Nemirovski, Arkadi},
	year         = 1998,
	journal      = {Mathematics of operations research},
	publisher    = {INFORMS},
	volume       = 23,
	number       = 4,
	pages        = {769--805},
}
@inproceedings{dalvi2004adversarial,
	title        = {Adversarial classification},
	author       = {Dalvi, Nilesh and Domingos, Pedro and Sanghai, Sumit and Verma, Deepak},
	year         = 2004,
	booktitle    = {Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining},
	pages        = {99--108},
}
@inproceedings{wang2019gaining,
	title        = {Gaining free or low-cost interpretability with interpretable partial substitute},
	author       = {Wang, Tong},
	year         = 2019,
	booktitle    = {International Conference on Machine Learning},
	pages        = {6505--6514},
	organization = {PMLR},
}
@article{bertsimas2019price,
	title        = {The price of interpretability},
	author       = {Bertsimas, Dimitris and Delarue, Arthur and Jaillet, Patrick and Martin, Sebastien},
	year         = 2019,
	journal      = {arXiv preprint arXiv:1907.03419},
}
@inproceedings{tsipras2019robustness,
	title        = {Robustness may be at odds with accuracy},
	author       = {Tsipras, Dimitris and Santurkar, Shibani and Engstrom, Logan and Turner, Alexander and Madry, Aleksander},
	year         = 2019,
	booktitle    = {International Conference on Learning Representations},
}
@inproceedings{cooper2021emergent,
	title        = {Emergent Unfairness in Algorithmic Fairness-Accuracy Trade-Off Research},
	author       = {Cooper, A Feder and Abrams, Ellen},
	year         = 2021,
	booktitle    = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
	pages        = {46--54},
}
@article{chen2018my,
	title        = {Why is my classifier discriminatory?},
	author       = {Chen, Irene and Johansson, Fredrik D and Sontag, David},
	year         = 2018,
	journal      = {arXiv preprint arXiv:1805.12002},
}
@inproceedings{pmlr-v119-dutta20a,
	title        = {Is There a Trade-Off Between Fairness and Accuracy? {A} Perspective Using Mismatched Hypothesis Testing},
	author       = {Dutta, Sanghamitra and Wei, Dennis and Yueksel, Hazar and Chen, Pin-Yu and Liu, Sijia and Varshney, Kush},
	booktitle    = {Proceedings of the 37th International Conference on Machine Learning},
	pages        = {2803--2813},
	year         = 2020,
	editor       = {III, Hal Daumé and Singh, Aarti},
	volume       = 119,
	series       = {Proceedings of Machine Learning Research},
	month        = {13--18 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v119/dutta20a/dutta20a.pdf},
	url          = {https://proceedings.mlr.press/v119/dutta20a.html},
	abstract     = {A trade-off between accuracy and fairness is almost taken as a given in the existing literature on fairness in machine learning. Yet, it is not preordained that accuracy should decrease with increased fairness. Novel to this work, we examine fair classification through the lens of mismatched hypothesis testing: trying to find a classifier that distinguishes between two ideal distributions when given two mismatched distributions that are biased. Using Chernoff information, a tool in information theory, we theoretically demonstrate that, contrary to popular belief, there always exist ideal distributions such that optimal fairness and accuracy (with respect to the ideal distributions) are achieved simultaneously: there is no trade-off. Moreover, the same classifier yields the lack of a trade-off with respect to ideal distributions while yielding a trade-off when accuracy is measured with respect to the given (possibly biased) dataset. To complement our main result, we formulate an optimization to find ideal distributions and derive fundamental limits to explain why a trade-off exists on the given biased dataset. We also derive conditions under which active data collection can alleviate the fairness-accuracy trade-off in the real world. Our results lead us to contend that it is problematic to measure accuracy with respect to data that reflects bias, and instead, we should be considering accuracy with respect to ideal, unbiased data.},
}
@inproceedings{pmlr-v81-menon18a,
	title        = {The cost of fairness in binary classification},
	author       = {Menon, Aditya Krishna and Williamson, Robert C},
	booktitle    = {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
	pages        = {107--118},
	year         = 2018,
	editor       = {Friedler, Sorelle A. and Wilson, Christo},
	volume       = 81,
	series       = {Proceedings of Machine Learning Research},
	month        = {23--24 Feb},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v81/menon18a/menon18a.pdf},
	url          = {https://proceedings.mlr.press/v81/menon18a.html},
	abstract     = {Binary classifiers are often required to possess fairness in the sense of not overly discriminating with respect to a feature deemed sensitive e.g. race. We study the inherent tradeoffs in learning classifiers with a fairness constraint in the form of two questions: what is the best accuracy we can expect for a given level of fairness?, and what is the nature of these optimal fairness-aware classifiers? To answer these questions, we provide three main contributions. First, we relate two existing fairness measures to cost-sensitive risks. Second, we show that for such cost-sensitive fairness measures, the optimal classifier is an instance-dependent thresholding of the class-probability function. Third, we relate the tradeoff between accuracy and fairness to the alignment between the target and sensitive features’ class-probabilities. A practical implication of our analysis is a simple approach to the fairness-aware problem which involves suitably thresholding class-probability estimates.},
}
@article{kleinberg2018selection,
	title        = {Selection problems in the presence of implicit bias},
	author       = {Kleinberg, Jon and Raghavan, Manish},
	year         = 2018,
	journal      = {arXiv preprint arXiv:1801.03533},
}
@inproceedings{heidari2021allocating,
	title        = {Allocating Opportunities in a Dynamic Model of Intergenerational Mobility},
	author       = {Heidari, Hoda and Kleinberg, Jon},
	year         = 2021,
	booktitle    = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
	pages        = {15--25},
}
@inproceedings{blum2020recovering,
	title        = {Recovering from Biased Data: Can Fairness Constraints Improve Accuracy?},
	author       = {Avrim Blum and Kevin Stangl},
	year         = 2020,
	booktitle    = {1st Symposium on Foundations of Responsible Computing, {FORC} 2020, June 1-3, 2020, Harvard University, Cambridge, MA, {USA} (virtual conference)},
	publisher    = {Schloss Dagstuhl - Leibniz-Zentrum f{\"{u}}r Informatik},
	series       = {LIPIcs},
	volume       = 156,
	pages        = {3:1--3:20},
	doi          = {10.4230/LIPIcs.FORC.2020.3},
	url          = {https://doi.org/10.4230/LIPIcs.FORC.2020.3},
	editor       = {Aaron Roth},
	timestamp    = {Fri, 22 May 2020 21:56:39 +0200},
	biburl       = {https://dblp.org/rec/conf/forc/BlumS20.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org},
}
@book{miettinen2012nonlinear,
	title        = {Nonlinear multiobjective optimization},
	author       = {Miettinen, Kaisa},
	year         = 2012,
	publisher    = {Springer Science \& Business Media},
	volume       = 12,
}
@article{rodolfa2021empirical,
	title        = {Empirical observation of negligible fairness--accuracy trade-offs in machine learning for public policy},
	author       = {Rodolfa, Kit T and Lamba, Hemank and Ghani, Rayid},
	year         = 2021,
	journal      = {Nature Machine Intelligence},
	publisher    = {Nature Publishing Group},
	volume       = 3,
	number       = 10,
	pages        = {896--904},
}
@article{dong2019variable,
	title        = {Variable importance clouds: A way to explore variable importance for the set of good models},
	author       = {Dong, Jiayun and Rudin, Cynthia},
	year         = 2019,
	journal      = {arXiv preprint arXiv:1901.03209},
}
@inproceedings{Rodolfa2020CaseInterventions,
	title        = {{Case study: Predictive fairness to reduce misdemeanor recidivism through social service interventions}},
	year         = 2020,
	booktitle    = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
	author       = {Rodolfa, Kit T. and Salomon, Erika and Haynes, Lauren and Mendieta, Iván Higuera and Larson, Jamie and Ghani, Rayid},
	month        = 1,
	pages        = {142--153},
	publisher    = {ACM},
	url          = {https://dl.acm.org/doi/10.1145/3351095.3372863},
	address      = {New York, NY, USA},
	isbn         = 9781450369367,
	doi          = {10.1145/3351095.3372863},
	keywords     = {Algorithmic Fairness, Criminal Justice, Machine Learning Disparities, Racial Bias},
}
@article{skeem2020using,
	title        = {Using algorithms to address trade-offs inherent in predicting recidivism},
	author       = {Skeem, Jennifer and Lowenkamp, Christopher},
	year         = 2020,
	journal      = {Behavioral sciences \& the law},
	publisher    = {Wiley Online Library},
	volume       = 38,
	number       = 3,
	pages        = {259--278},
}
@article{raghavan2019challenges,
	title        = {Challenges for mitigating bias in algorithmic hiring},
	author       = {Raghavan, Manish and Barocas, Solon},
	year         = 2019,
	journal      = {Brookings. Retrieved February},
	volume       = 25,
	pages        = 2020,
}
@book{hellman2008discrimination,
	title        = {When Is Discrimination Wrong?},
	author       = {Hellman, Deborah},
	year         = 2008,
	publisher    = {Harvard University Press},
}
@article{karimi2020survey,
	title        = {A survey of algorithmic recourse: definitions, formulations, solutions, and prospects},
	author       = {Karimi, Amir-Hossein and Barthe, Gilles and Sch{\"o}lkopf, Bernhard and Valera, Isabel},
	year         = 2020,
	journal      = {arXiv preprint arXiv:2010.04050},
}
@inproceedings{kohavi1996bias,
	title        = {Bias plus variance decomposition for zero-one loss functions},
	author       = {Kohavi, Ron and Wolpert, David H and others},
	year         = 1996,
	booktitle    = {ICML},
	volume       = 96,
	pages        = {275--83},
}
@inproceedings{domingos2000unified,
	title        = {A unified bias-variance decomposition},
	author       = {Domingos, Pedro},
	year         = 2000,
	booktitle    = {Proceedings of 17th International Conference on Machine Learning},
	pages        = {231--238},
}
@article{rudin2019stop,
	title        = {Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead},
	author       = {Rudin, Cynthia},
	year         = 2019,
	journal      = {Nature Machine Intelligence},
	publisher    = {Nature Publishing Group},
	volume       = 1,
	number       = 5,
	pages        = {206--215},
}
@article{el2010foundations,
	title        = {On the Foundations of Noise-free Selective Classification.},
	author       = {El-Yaniv, Ran and others},
	year         = 2010,
	journal      = {Journal of Machine Learning Research},
	volume       = 11,
	number       = 5,
}
@inproceedings{Opitz96,
	title        = {Generating Accurate and Diverse Members of a Neural-Network Ensemble},
	author       = {Opitz, David and Shavlik, Jude},
	year         = 1996,
	booktitle    = {Advances in Neural Information Processing Systems},
}
@article{linusson2020efficient,
	title        = {Efficient conformal predictor ensembles},
	author       = {Linusson, Henrik and Johansson, Ulf and Bostr{\"o}m, Henrik},
	year         = 2020,
	journal      = {Neurocomputing},
	publisher    = {Elsevier},
	volume       = 397,
	pages        = {266--278},
}
@article{gupta2019nested,
	title        = {Nested conformal prediction and quantile out-of-bag ensemble methods},
	author       = {Gupta, Chirag and Kuchibhotla, Arun K and Ramdas, Aaditya K},
	year         = 2019,
	journal      = {arXiv preprint arXiv:1910.10562},
}
@inproceedings{lofstrom2013effective,
	title        = {Effective utilization of data in inductive conformal prediction using ensembles of neural networks},
	author       = {L{\"o}fstr{\"o}m, Tuve and Johansson, Ulf and Bostr{\"o}m, Henrik},
	year         = 2013,
	booktitle    = {The 2013 International Joint Conference on Neural Networks (IJCNN)},
	pages        = {1--8},
	organization = {IEEE},
}

@misc{disp_impact_doctrine,
title={Title VI Legal Manual, SECTION VII: PROVING DISCRIMINATION – DISPARATE IMPACT},
author={United States Dept. Of Justice},
year={Accessed 2023.},
url={https://www.justice.gov/crt/fcs/T6Manual7#:~:text=)\%3B\%20Gaston\%20Cty.-,v.,results\%20in\%20racial\%20discrimination.\%E2\%80\%9D\%20H.R.}
}

@article{bent2019algorithmic,
  title={Is algorithmic affirmative action legal},
  author={Bent, Jason R},
  journal={Geo. LJ},
  volume={108},
  pages={803},
  year={2019},
  publisher={HeinOnline}
}

@article{ho2020affirmative,
  title={Affirmative algorithms: The legal grounds for fairness as awareness},
  author={Ho, Daniel E and Xiang, Alice},
  journal={U. Chi. L. Rev. Online},
  pages={134},
  year={2020},
  publisher={HeinOnline}
}

@article{lakshminarayanan2016simple,
	title        = {Simple and scalable predictive uncertainty estimation using deep ensembles},
	author       = {Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
	year         = 2016,
	journal      = {arXiv preprint arXiv:1612.01474},
}
@article{ovadia2019can,
	title        = {Can you trust your model's uncertainty? Evaluating predictive uncertainty under dataset shift},
	author       = {Ovadia, Yaniv and Fertig, Emily and Ren, Jie and Nado, Zachary and Sculley, David and Nowozin, Sebastian and Dillon, Joshua V and Lakshminarayanan, Balaji and Snoek, Jasper},
	year         = 2019,
	journal      = {arXiv preprint arXiv:1906.02530},
}
@inproceedings{guo2017calibration,
	title        = {On calibration of modern neural networks},
	author       = {Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q},
	year         = 2017,
	booktitle    = {International Conference on Machine Learning},
	pages        = {1321--1330},
	organization = {PMLR},
}
@inproceedings{maclin1995combining,
	title        = {Combining the predictions of multiple classifiers: Using competitive learning to initialize neural networks},
	author       = {Maclin, Richard and Shavlik, Jude W and others},
	year         = 1995,
	booktitle    = {International Joint Conference on Artificial Intelligence (IJCAI)},
}
@article{krogh1995validation,
	title        = {Neural Network Ensembles, Cross Validation, and Active Learning},
	author       = {Krogh, Anders and Vedelsby, Jesper},
	year         = 1995,
	journal      = {Neural Information Processing Systems (NeurIPS)},
}
@article{hansen1990neural,
	title        = {Neural network ensembles},
	author       = {Hansen, Lars Kai and Salamon, Peter},
	year         = 1990,
	journal      = {IEEE transactions on pattern analysis and machine intelligence},
	publisher    = {IEEE},
	volume       = 12,
	number       = 10,
	pages        = {993--1001},
}
@article{meir1995bias,
	title        = {Bias, variance and the combination of least squares estimators},
	author       = {Meir, Ronny and Tesauro, G and Touretzky, DS and Leen, TK},
	year         = 1995,
	journal      = {Advances in neural information processing systems},
	publisher    = {MORGAN KAUFMANN PUBLISHERS},
	pages        = {295--302},
}
@article{hung2019rank,
	title        = {Rank verification for exponential families},
	author       = {Hung, Kenneth and Fithian, William and others},
	year         = 2019,
	journal      = {Annals of Statistics},
	publisher    = {Institute of Mathematical Statistics},
	volume       = 47,
	number       = 2,
	pages        = {758--782},
}
@inproceedings{kolen91,
	title        = {Back Propagation is Sensitive to Initial Conditions},
	author       = {Kolen, John and Pollack, Jordan},
	year         = 1991,
	booktitle    = {Neural Information Processing Systems (NeurIPS)},
}
@article{geman1992neural,
	title        = {Neural networks and the bias/variance dilemma},
	author       = {Geman, Stuart and Bienenstock, Elie and Doursat, Ren{\'e}},
	year         = 1992,
	journal      = {Neural computation},
	publisher    = {MIT Press},
	volume       = 4,
	number       = 1,
	pages        = {1--58},
}
@article{zhou2002ensembling,
	title        = {Ensembling neural networks: many could be better than all},
	author       = {Zhou, Zhi-Hua and Wu, Jianxin and Tang, Wei},
	year         = 2002,
	journal      = {Artificial intelligence},
	publisher    = {Elsevier},
	volume       = 137,
	number       = {1-2},
	pages        = {239--263},
}
@article{zhang2018three,
	title        = {A three-way selective ensemble model for multi-label classification},
	author       = {Zhang, Yuanjian and Miao, Duoqian and Zhang, Zhifei and Xu, Jianfeng and Luo, Sheng},
	year         = 2018,
	journal      = {International Journal of Approximate Reasoning},
}
@article{WEI201782,
	title        = {A novel hierarchical selective ensemble classifier with bioinformatics application},
	author       = {Leyi Wei and Shixiang Wan and Jiasheng Guo and Kelvin KL Wong},
	year         = 2017,
	journal      = {Artificial Intelligence in Medicine},
}
@inproceedings{NEURIPS2020_075b051e,
	title        = {Debugging Tests for Model Explanations},
	author       = {Adebayo, Julius and Muelly, Michael and Liccardi, Ilaria and Kim, Been},
	year         = 2020,
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = 33,
	pages        = {700--712},
	url          = {https://proceedings.neurips.cc/paper/2020/file/075b051ec3d22dac7b33f788da631fd4-Paper.pdf},
	editor       = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
}
@article{kather2016multi,
	title        = {Multi-class texture analysis in colorectal cancer histology},
	author       = {Kather, Jakob Nikolas and Weis, Cleo-Aron and Bianconi, Francesco and Melchers, Susanne M and Schad, Lothar R and Gaiser, Timo and Marx, Alexander and Z{"o}llner, Frank Gerrit},
	year         = 2016,
	journal      = {Scientific reports},
	publisher    = {Nature Publishing Group},
	volume       = 6,
	pages        = 27988,
}
@article{XIAO2018534,
	title        = {A hybrid model based on selective ensemble for energy consumption forecasting in China},
	author       = {Jin Xiao and Yuxi Li and Ling Xie and Dunhu Liu and Jing Huang},
	year         = 2018,
	journal      = {Energy},
}
@inproceedings{fumera2005dynamics,
	title        = {Dynamics of variance reduction in bagging and other techniques based on randomisation},
	author       = {Fumera, Giorgio and Roli, Fabio and Serrau, Alessandra},
	year         = 2005,
	booktitle    = {International Workshop on Multiple Classifier Systems},
}
@article{fisher2019all,
	title        = {All Models are Wrong, but Many are Useful: Learning a Variable's Importance by Studying an Entire Class of Prediction Models Simultaneously.},
	author       = {Fisher, Aaron and Rudin, Cynthia and Dominici, Francesca},
	year         = 2019,
	journal      = {J. Mach. Learn. Res.},
	volume       = 20,
	number       = 177,
	pages        = {1--81},
}
@article{liu2020intrinsic,
	title        = {On the Intrinsic Differential Privacy of Bagging},
	author       = {Liu, Hongbin and Jia, Jinyuan and Gong, Neil Zhenqiang},
	year         = 2020,
	journal      = {arXiv preprint arXiv:2008.09845},
}
@article{papernot2016semi,
	title        = {Semi-supervised knowledge transfer for deep learning from private training data},
	author       = {Papernot, Nicolas and Abadi, Mart{\'\i}n and Erlingsson, Ulfar and Goodfellow, Ian and Talwar, Kunal},
	year         = 2016,
	journal      = {arXiv preprint arXiv:1610.05755},
}
@inproceedings{lincoln90,
	title        = {Synergy of Clustering Multiple Back Propagation Networks},
	author       = {Lincoln, William and Skrzypek, Josef},
	year         = 1990,
	booktitle    = {Neural Information Processing Systems (NeurIPS)},
}
@inproceedings{papernot2018scalable,
	title        = {Scalable Private Learning with PATE},
	author       = {Papernot, Nicolas and Song, Shuang and Mironov, Ilya and Raghunathan, Ananth and Talwar, Kunal and Erlingsson, Ulfar},
	year         = 2018,
	booktitle    = {International Conference on Learning Representations},
}
@article{naftaly1997optimal,
	title        = {Optimal ensemble averaging of neural networks},
	author       = {Naftaly, Ury and Intrator, Nathan and Horn, David},
	year         = 1997,
	journal      = {Network: Computation in Neural Systems},
	volume       = 8,
	number       = 3,
	pages        = {283--296},
}
@inproceedings{leino18influence,
	title        = {Influence-Directed Explanations for Deep Convolutional Networks},
	author       = {Klas Leino and Shayak Sen and Anupam Datta and Matt Fredrikson and Linyi Li},
	year         = 2018,
	booktitle    = {IEEE International Test Conference (ITC)},
}
@article{wang2004image,
	title        = {Image quality assessment: from error visibility to structural similarity},
	author       = {Wang, Zhou and Bovik, Alan C and Sheikh, Hamid R and Simoncelli, Eero P},
	year         = 2004,
	journal      = {IEEE transactions on image processing},
	publisher    = {IEEE},
	volume       = 13,
	number       = 4,
	pages        = {600--612},
}
@inproceedings{he2016deep,
	title        = {Deep residual learning for image recognition},
	author       = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year         = 2016,
	booktitle    = {Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages        = {770--778},
}
@inproceedings{deng2009imagenet,
	title        = {Imagenet: A large-scale hierarchical image database},
	author       = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
	year         = 2009,
	booktitle    = {2009 IEEE conference on computer vision and pattern recognition},
	pages        = {248--255},
	organization = {Ieee},
}
@misc{structural_similarity_index,
	title        = {Structural similarity index},
	journal      = {Structural similarity index - skimage v0.19.0.dev0 docs},
	url          = {https://scikit-image.org/docs/dev/auto\%5Fexamples/transform/plot\%5Fssim.html},
}
@article{nejm-warfarin,
	title        = {Estimation of the Warfarin Dose with Clinical and Pharmacogenetic Data},
	author       = {{International Warfarin Pharmacogenetic Consortium}},
	year         = 2009,
	journal      = {New England Journal of Medicine},
	volume       = 360,
	number       = 8,
	pages        = {753--764},
}
@article{berk2017convex,
	title        = {A convex framework for fair regression},
	author       = {Berk, Richard and Heidari, Hoda and Jabbari, Shahin and Joseph, Matthew and Kearns, Michael and Morgenstern, Jamie and Neel, Seth and Roth, Aaron},
	year         = 2017,
	journal      = {arXiv preprint arXiv:1706.02409},
}
@article{creel2021algorithmic,
	title        = {The Algorithmic Leviathan: Arbitrariness, Fairness, and Opportunity in Algorithmic Decision Making Systems},
	author       = {Creel, Kathleen and Hellman, Deborah},
	year         = 2021,
	journal      = {Virginia Public Law and Legal Theory Research Paper},
	number       = {2021-13},
}
@inproceedings{black2022forthcoming,
	title        = {Reducing Racial Disparity Through Procedural Interventions: Conceptions and Outcomes},
	author       = {Black, Emily and Barocas, Solon and Chouldechova, Alexandra and Koepke, Logan and Lum, Kristian and Madaio, Michael and Riley, Sarah},
	year         = 2022,
}
@article{black2021consistent,
	title        = {Consistent Counterfactuals for Deep Models},
	author       = {Emily Black and Wang, Zifan and Fredrikson, Matt and Datta, Anupam},
	year         = 2021,
	journal      = {arXiv preprint arXiv:2110.03109},
}
@article{black2021selective,
	title        = {Selective Ensembles for Consistent Predictions},
	author       = {Black, Emily and Leino, Klas and Fredrikson, Matt},
	year         = 2021,
	journal      = {arXiv preprint arXiv:2111.08230},
}
@inproceedings{pmlr-v119-anders20a,
	title        = {Fairwashing explanations with off-manifold detergent},
	author       = {Anders, Christopher and Pasliev, Plamen and Dombrowski, Ann-Kathrin and M{\""u}ller, Klaus-Robert and Kessel, Pan},
	booktitle    = {Proceedings of the 37th International Conference on Machine Learning},
	pages        = {314--323},
	year         = 2020,
	editor       = {III, Hal Daumé and Singh, Aarti},
	volume       = 119,
	series       = {Proceedings of Machine Learning Research},
	month        = {13--18 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v119/anders20a/anders20a.pdf},
	url          = {https://proceedings.mlr.press/v119/anders20a.html},
	abstract     = {Explanation methods promise to make black-box classifiers more transparent. As a result, it is hoped that they can act as proof for a sensible, fair and trustworthy decision-making process of the algorithm and thereby increase its acceptance by the end-users. In this paper, we show both theoretically and experimentally that these hopes are presently unfounded. Specifically, we show that, for any classifier $g$, one can always construct another classifier $\tilde{g}$ which has the same behavior on the data (same train, validation, and test error) but has arbitrarily manipulated explanation maps. We derive this statement theoretically using differential geometry and demonstrate it experimentally for various explanation methods, architectures, and datasets. Motivated by our theoretical insights, we then propose a modification of existing explanation methods which makes them significantly more robust.},
}
@article{opitz1999popular,
	title        = {Popular ensemble methods: An empirical study},
	author       = {Opitz, David and Maclin, Richard},
	year         = 1999,
	journal      = {Journal of artificial intelligence research},
	volume       = 11,
	pages        = {169--198},
}
@article{jones2020selective,
	title        = {Selective classification can magnify disparities across groups},
	author       = {Jones, Erik and Sagawa, Shiori and Koh, Pang Wei and Kumar, Ananya and Liang, Percy},
	year         = 2020,
	journal      = {arXiv preprint arXiv:2010.14134},
}
@incollection{polikar2012ensemble,
	title        = {Ensemble learning},
	author       = {Polikar, Robi},
	year         = 2012,
	booktitle    = {Ensemble machine learning},
	publisher    = {Springer},
	pages        = {1--34},
}
@article{freund1997decision,
	title        = {A decision-theoretic generalization of on-line learning and an application to boosting},
	author       = {Freund, Yoav and Schapire, Robert E},
	year         = 1997,
	journal      = {Journal of computer and system sciences},
	publisher    = {Elsevier},
	volume       = 55,
	number       = 1,
	pages        = {119--139},
}
@inproceedings{dvornik2019diversity,
	title        = {Diversity with cooperation: Ensemble methods for few-shot classification},
	author       = {Dvornik, Nikita and Schmid, Cordelia and Mairal, Julien},
	year         = 2019,
	booktitle    = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
	pages        = {3723--3731},
}
@article{hasan2020diabetes,
	title        = {Diabetes prediction using ensembling of different machine learning classifiers},
	author       = {Hasan, Md Kamrul and Alam, Md Ashraful and Das, Dola and Hossain, Eklas and Hasan, Mahmudul},
	year         = 2020,
	journal      = {IEEE Access},
	publisher    = {IEEE},
	volume       = 8,
	pages        = {76516--76531},
}
@article{tumer1996error,
	title        = {Error correlation and error reduction in ensemble classifiers},
	author       = {Tumer, Kagan and Ghosh, Joydeep},
	year         = 1996,
	journal      = {Connection science},
	publisher    = {Taylor \& Francis},
	volume       = 8,
	number       = {3-4},
	pages        = {385--404},
}
@article{citron2014scored,
	title        = {The scored society: Due process for automated predictions},
	author       = {Citron, Danielle Keats and Pasquale, Frank},
	year         = 2014,
	journal      = {Wash. L. Rev.},
	publisher    = {HeinOnline},
	volume       = 89,
	pages        = 1,
}
@article{citron2007technological,
	title        = {Technological due process},
	author       = {Citron, Danielle Keats},
	year         = 2007,
	journal      = {Wash. L Rev.},
	publisher    = {HeinOnline},
	volume       = 85,
	pages        = 1249,
}
@misc{florida_law,
	title        = {2011 Florida Statues},
	year         = 2011,
	howpublished = {\url{https://www.flsenate.gov/laws/statutes/2011/120.57}},
}
@article{kim2022race,
	title        = {Race-aware algorithms: Fairness, nondiscrimination and affirmative action.},
	author       = {Kim, Pauline T.},
	year         = 2022,
	journal      = {California Law Review},
	volume       = 110,
}
@article{selbst2018intuitive,
	title        = {The intuitive appeal of explainable machines},
	author       = {Selbst, Andrew D and Barocas, Solon},
	year         = 2018,
	journal      = {Fordham L. Rev.},
	publisher    = {HeinOnline},
	volume       = 87,
	pages        = 1085,
}
@misc{APA,
	title        = {Codified at 5 U.S.C. {\S~ 706}},
	year         = 1946,
	key          = {Administrative Procedure Act},
}
@misc{ECOA,
	title        = {Codified at 15 U.S.C. {\S~1691}, et seq.},
	year         = 1974,
	key          = {Equal Credit Opportunities Act, Public Law 93-495},
}
@misc{cong_record,
	title        = {116 {Cong. Reg.} 36572},
	year         = 1970,
}
@misc{RegulationB,
	title        = {12 C.F.R. {\S~1002} et seq.},
	year         = 2003,
	key          = {Regulation B},
}
@article{kaminski2021right,
	title        = {The right to contest AI},
	author       = {Kaminski, Margot E and Urban, Jennifer M},
	year         = 2021,
	journal      = {Columbia Law Review},
	publisher    = {JSTOR},
	volume       = 121,
	number       = 7,
	pages        = {1957--2048},
}
@article{perry2014may,
	title        = {May the Odds Be Ever in Your Favor: Lotteries in Law},
	author       = {Perry, Ronen and Zarsky, Tal Z},
	year         = 2014,
	journal      = {Ala. L. Rev.},
	publisher    = {HeinOnline},
	volume       = 66,
	pages        = 1035,
}
@article{kim2016data,
	title        = {Data-driven discrimination at work},
	author       = {Kim, Pauline T},
	year         = 2016,
	journal      = {Wm. \& Mary L. Rev.},
	publisher    = {HeinOnline},
	volume       = 58,
	pages        = 857,
}
@article{crawford2014big,
	title        = {Big data and due process: Toward a framework to redress predictive privacy harms},
	author       = {Crawford, Kate and Schultz, Jason},
	year         = 2014,
	journal      = {BCL Rev.},
	publisher    = {HeinOnline},
	volume       = 55,
	pages        = 93,
}
@article{richards2013three,
	title        = {Three paradoxes of big data},
	author       = {Richards, Neil M and King, Jonathan H},
	year         = 2013,
	journal      = {Stan. L. Rev. Online},
	publisher    = {HeinOnline},
	volume       = 66,
	pages        = 41,
}
@inproceedings{wick2019unlocking,
	title        = {Unlocking Fairness: a Trade-off Revisited},
	author       = {Wick, Michael and panda, swetasudha and Tristan, Jean-Baptiste},
	year         = 2019,
	booktitle    = {Advances in Neural Information Processing Systems},
	volume       = 32,
}
@article{Obermeyer447,
	title        = {Dissecting racial bias in an algorithm used to manage the health of populations},
	author       = {Obermeyer, Ziad and Powers, Brian and Vogeli, Christine and Mullainathan, Sendhil},
	year         = 2019,
	journal      = {Science},
	publisher    = {American Association for the Advancement of Science},
	volume       = 366,
	number       = 6464,
	pages        = {447--453},
}
@article{rich2016machine,
	title        = {Machine learning, automated suspicion algorithms, and the fourth amendment},
	author       = {Rich, Michael L},
	year         = 2016,
	journal      = {University of Pennsylvania Law Review},
	publisher    = {JSTOR},
	pages        = {871--929},
}
@article{lehr2017playing,
	title        = {Playing with the data: what legal scholars should learn about machine learning},
	author       = {Lehr, David and Ohm, Paul},
	year         = 2017,
	journal      = {UCDL Rev.},
	publisher    = {HeinOnline},
	volume       = 51,
	pages        = 653,
}
@inproceedings{corbett2017algorithmic,
	title        = {Algorithmic decision making and the cost of fairness},
	author       = {Corbett-Davies, Sam and Pierson, Emma and Feller, Avi and Goel, Sharad and Huq, Aziz},
	year         = 2017,
	booktitle    = {Proceedings of the 23rd acm sigkdd international conference on knowledge discovery and data mining},
	pages        = {797--806},
}
@misc{colon_license,
	title        = {Collection of textures in colorectal cancer histology},
	author       = {Kather, Jakob Nikolas and Zöllner, Frank Gerrit and Bianconi, Francesco and Melchers, Susanne M and Schad, Lothar R and Gaiser, Timo and Marx, Alexander and Weis, Cleo-Aron},
	year         = 2016,
	month        = {May},
	journal      = {Zenodo},
	url          = {https://zenodo.org/record/53169#.YLkg2WdKiqD},
}
@article{bressman2003beyond,
	title        = {Beyond accountability: Arbitrariness and legitimacy in the administrative state},
	author       = {Bressman, Lisa Schultz},
	year         = 2003,
	journal      = {NYUL Rev.},
	publisher    = {HeinOnline},
	volume       = 78,
	pages        = 461,
}
@article{strandburg2019rulemaking,
	title        = {Rulemaking and inscrutable automated decision tools},
	author       = {Strandburg, Katherine J},
	year         = 2019,
	journal      = {Columbia Law Review},
	publisher    = {JSTOR},
	volume       = 119,
	number       = 7,
	pages        = {1851--1886},
}
@inproceedings{passi2019problem,
	title        = {Problem formulation and fairness},
	author       = {Passi, Samir and Barocas, Solon},
	year         = 2019,
	booktitle    = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
	pages        = {39--48},
}
@article{bloomberg,
	title        = {Equifax AI Innovation Opens Doors to Millions Seeking Credit},
	author       = {Bloomberg},
	year         = 2020,
}
@inproceedings{zhangaies18,
	author       = {Zhang, Brian Hu and Lemoine, Blake and Mitchell, Margaret},
	title        = {Mitigating Unwanted Biases with Adversarial Learning},
	year         = 2018,
	isbn         = 9781450360128,
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	url          = {https://doi.org/10.1145/3278721.3278779},
	doi          = {10.1145/3278721.3278779},
	abstract     = {Machine learning is a tool for building models that accurately represent input training data. When undesired biases concerning demographic groups are in the training data, well-trained models will reflect those biases. We present a framework for mitigating such biases by including a variable for the group of interest and simultaneously learning a predictor and an adversary. The input to the network X, here text or census data, produces a prediction Y, such as an analogy completion or income bracket, while the adversary tries to model a protected variable Z, here gender or zip code. The objective is to maximize the predictor's ability to predict Y while minimizing the adversary's ability to predict Z. Applied to analogy completion, this method results in accurate predictions that exhibit less evidence of stereotyping Z. When applied to a classification task using the UCI Adult (Census) Dataset, it results in a predictive model that does not lose much accuracy while achieving very close to equality of odds (Hardt, et al., 2016). The method is flexible and applicable to multiple definitions of fairness as well as a wide range of gradient-based learning models, including both regression and classification tasks.},
	booktitle    = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
	pages        = {335–340},
	numpages     = 6,
	keywords     = {unbiasing, debiasing, multi-task learning, adversarial learning},
	location     = {New Orleans, LA, USA},
	series       = {AIES '18},
}
%end Publications

%Supreme Court cases sorted by key
%end Supreme Court cases
@article{valentini2004cancer,
	title        = {Cancer recognition with bagged ensembles of support vector machines},
	author       = {Valentini, Giorgio and Muselli, Marco and Ruffino, Francesca},
	year         = 2004,
	journal      = {Neurocomputing},
	publisher    = {Elsevier},
	volume       = 56,
	pages        = {461--466},
}
@techreport{perrone1992networks,
	title        = {When networks disagree: Ensemble methods for hybrid neural networks},
	author       = {Perrone, Michael P and Cooper, Leon N},
	year         = 1992,
	institution  = {BROWN UNIV PROVIDENCE RI INST FOR BRAIN AND NEURAL SYSTEMS},
}
@article{che2011decision,
	title        = {Decision tree and ensemble learning algorithms with their applications in bioinformatics},
	author       = {Che, Dongsheng and Liu, Qi and Rasheed, Khaled and Tao, Xiuping},
	year         = 2011,
	journal      = {Software tools and algorithms for biological systems},
	publisher    = {Springer},
	pages        = {191--199},
}
@article{sagi2018ensemble,
	title        = {Ensemble learning: A survey},
	author       = {Sagi, Omer and Rokach, Lior},
	year         = 2018,
	journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
	publisher    = {Wiley Online Library},
	volume       = 8,
	number       = 4,
	pages        = {e1249},
}
@article{kleinberg2021algorithmic,
	title        = {Algorithmic monoculture and social welfare},
	author       = {Kleinberg, Jon and Raghavan, Manish},
	year         = 2021,
	journal      = {Proceedings of the National Academy of Sciences},
	publisher    = {National Acad Sciences},
	volume       = 118,
	number       = 22,
}
@book{shalev2014understanding,
	title        = {Understanding machine learning: From theory to algorithms},
	author       = {Shalev-Shwartz, Shai and Ben-David, Shai},
	year         = 2014,
	publisher    = {Cambridge university press},
}
@article{dawid1985calibration,
	title        = {Calibration-based empirical probability},
	author       = {Dawid, A Philip},
	year         = 1985,
	journal      = {The Annals of Statistics},
	publisher    = {JSTOR},
	pages        = {1251--1274},
}
@article{donoho201750,
	title        = {50 years of data science},
	author       = {Donoho, David},
	year         = 2017,
	journal      = {Journal of Computational and Graphical Statistics},
	publisher    = {Taylor \& Francis},
	volume       = 26,
	number       = 4,
	pages        = {745--766},
}
@article{birhane2021values,
	title        = {The values encoded in machine learning research},
	author       = {Birhane, Abeba and Kalluri, Pratyusha and Card, Dallas and Agnew, William and Dotan, Ravit and Bao, Michelle},
	year         = 2021,
	journal      = {arXiv preprint arXiv:2106.15590},
}
@article{breiman2001statistical,
	title        = {Statistical modeling: The two cultures (with comments and a rejoinder by the author)},
	author       = {Breiman, Leo},
	year         = 2001,
	journal      = {Statistical science},
	publisher    = {Institute of Mathematical Statistics},
	volume       = 16,
	number       = 3,
	pages        = {199--231},
}
@article{renard2021understanding,
	title        = {Understanding Prediction Discrepancies in Machine Learning Classifiers},
	author       = {Renard, Xavier and Laugel, Thibault and Detyniecki, Marcin},
	year         = 2021,
	journal      = {arXiv preprint arXiv:2104.05467},
}
@misc{ftcproposed,
	title        = {Trade Regulation Rule on Commercial Surveillance and Data Security},
	author       = {Federal Trade Commission},
	journal      = {87 FR 51273},
	year         = {Proposed 08/22/2022},
}
@incollection{singh2022fair,
	title        = {Fair decision-making for food inspections},
	author       = {Singh, Shubham and Shah, Bhuvni and Kanich, Chris and Kash, Ian A},
	booktitle    = {Equity and Access in Algorithms, Mechanisms, and Optimization},
	pages        = {1--11},
	year         = 2022,
}
@inproceedings{madaio2020co,
	title        = {Co-designing checklists to understand organizational challenges and opportunities around fairness in AI},
	author       = {Madaio, Michael A and Stark, Luke and Wortman Vaughan, Jennifer and Wallach, Hanna},
	booktitle    = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
	pages        = {1--14},
	year         = 2020,
}
@inproceedings{kleinberg2019simplicity,
	title        = {Simplicity creates inequity: implications for fairness, stereotypes, and interpretability},
	author       = {Kleinberg, Jon and Mullainathan, Sendhil},
	year         = 2019,
	booktitle    = {Proceedings of the 2019 ACM Conference on Economics and Computation},
	pages        = {807--808},
}
@article{chen2018interpretable,
	title        = {An interpretable model with globally consistent explanations for credit risk},
	author       = {Chen, Chaofan and Lin, Kangcheng and Rudin, Cynthia and Shaposhnik, Yaron and Wang, Sijia and Wang, Tong},
	year         = 2018,
	journal      = {arXiv preprint arXiv:1811.12615},
}
@article{hardt2021amazon,
	title        = {Amazon sagemaker clarify: Machine learning bias detection and explainability in the cloud},
	author       = {Hardt, Michaela and Chen, Xiaoguang and Cheng, Xiaoyi and Donini, Michele and Gelman, Jason and Gollaprolu, Satish and He, John and Larroy, Pedro and Liu, Xinyu and McCarthy, Nick and others},
	journal      = {arXiv preprint arXiv:2109.03285},
	year         = 2021,
}
@article{semenova2019study,
	title        = {A study in Rashomon curves and volumes: A new perspective on generalization and model simplicity in machine learning},
	author       = {Semenova, Lesia and Rudin, Cynthia and Parr, Ronald},
	year         = 2019,
	journal      = {arXiv preprint arXiv:1908.01755},
}
@book{Larsen_2019,
	title        = {Resumes, Robots, and Racism: The Truth about AI in Hiring},
	author       = {Larsen, Loren},
	year         = 2019,
	month        = {Mar},
	publisher    = {HireVue},
}
@article{engstrom2020government,
	title        = {Government by algorithm: Artificial intelligence in federal administrative agencies},
	author       = {Engstrom, David Freeman and Ho, Daniel E and Sharkey, Catherine M and Cu{\'e}llar, Mariano-Florentino},
	year         = 2020,
	journal      = {Administrative Conference of the United States},
}
@article{mcdaniel1992horizontal,
	title        = {Horizontal and vertical equity: the Musgrave/Kaplow exchange},
	author       = {McDaniel, Paul R and Repetti, James R},
	year         = 1992,
	journal      = {Fla. Tax Rev.},
	publisher    = {HeinOnline},
	volume       = 1,
	pages        = 607,
}
@article{corbett2018measure,
	title        = {The measure and mismeasure of fairness: A critical review of fair machine learning},
	author       = {Corbett-Davies, Sam and Goel, Sharad},
	year         = 2018,
	journal      = {arXiv preprint arXiv:1808.00023},
}
@misc{data_collection_taxes,
	title        = {How Financial Reporting Helps American Workers and Ensures that Top Earners Pay Their Fair Share},
	author       = {U.S. Department of the Treasury},
	year         = 2021,
	month        = {Oct},
	url          = {https://home.treasury.gov/news/featured-stories/how-financial-reporting-helps-american-workers-and-ensures-that-top-earners-pay-their-fair-share},
}
@book{edelhertz1970nature,
	title        = {The nature, impact, and prosecution of white-collar crime},
	author       = {Edelhertz, Herbert},
	year         = 1970,
	publisher    = {National Institute of Law Enforcement and Criminal Justice},
	volume       = 2,
}
@article{wall2019measuring,
	title        = {Measuring white collar crime},
	author       = {Wall-Parker, April},
	year         = 2019,
	journal      = {The Handbook of White-Collar Crime},
	publisher    = {Wiley Online Library},
	pages        = {32--44},
}
@misc{national_taxpayer_advocate,
	title        = {The IRS is Significantly Underfunded to Serve Taxpayers and Collect Tax},
	author       = {National Taxpayer Advocate},
	year         = 2020,
	publisher    = {National Taxpayer Advocate},
	url          = {https://www.taxpayeradvocate.irs.gov/wp-content/uploads/2020/08/Most-Serious-Problems-IRS-Significantly-Underfunded.pdf},
}
@book{parrillo2013against,
	title        = {Against the Profit Motive: The Salary Revolution in American Government, 1780-1940},
	author       = {Parrillo, Nicholas R},
	year         = 2013,
	publisher    = {Yale University Press},
}
@article{hunter1996irs,
	title        = {An IRS production function},
	author       = {Hunter, William J and Nelson, Michael A},
	year         = 1996,
	journal      = {National Tax Journal},
	publisher    = {The University of Chicago Press},
	volume       = 49,
	number       = 1,
	pages        = {105--115},
}
@misc{kiel,
	title        = {It's Getting Worse: The IRS Now Audits Poor Americans at About the Same Rate as the Top 1\%},
	author       = {Kiel, Paul},
	journal      = {ProPublica},
}
@article{zafar2019fairness,
	title        = {Fairness constraints: A flexible approach for fair classification},
	author       = {Zafar, Muhammad Bilal and Valera, Isabel and Gomez-Rodriguez, Manuel and Gummadi, Krishna P},
	year         = 2019,
	journal      = {The Journal of Machine Learning Research},
	publisher    = {JMLR. org},
	volume       = 20,
	number       = 1,
	pages        = {2737--2778},
}
@article{yan2020fairness,
	title        = {Fairness in practice: a survey on equity in urban mobility},
	author       = {Yan, An and Howe, Bill},
	year         = 2020,
	journal      = {A Quarterly bulletin of the Computer Society of the IEEE Technical Committee on Data Engineering},
	volume       = 42,
	number       = 3,
}
@article{elkins2006horizontal,
	title        = {Horizontal equity as a principle of tax theory},
	author       = {Elkins, David},
	year         = 2006,
	journal      = {Yale L. \& Pol'y Rev.},
	publisher    = {HeinOnline},
	volume       = 24,
	pages        = 43,
}
@article{celis2017ranking,
	title        = {Ranking with fairness constraints},
	author       = {Celis, L Elisa and Straszak, Damian and Vishnoi, Nisheeth K},
	year         = 2017,
	journal      = {arXiv preprint arXiv:1704.06840},
}
@inproceedings{singh2018fairness,
	title        = {Fairness of exposure in rankings},
	author       = {Singh, Ashudeep and Joachims, Thorsten},
	year         = 2018,
	booktitle    = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
	pages        = {2219--2228},
}
@inproceedings{celis2019classification,
	title        = {Classification with fairness constraints: A meta-algorithm with provable guarantees},
	author       = {Celis, L Elisa and Huang, Lingxiao and Keswani, Vijay and Vishnoi, Nisheeth K},
	year         = 2019,
	booktitle    = {Proceedings of the conference on fairness, accountability, and transparency},
	pages        = {319--328},
}
@article{doninineurips18,
	author       = {Donini, Michele and Oneto, Luca and Ben-David, Shai and Shawe-Taylor, John S and Pontil, Massimiliano},
	booktitle    = {Advances in Neural Information Processing Systems},
	editor       = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
	pages        = {},
	publisher    = {Curran Associates, Inc.},
	title        = {Empirical Risk Minimization Under Fairness Constraints},
	url          = {https://proceedings.neurips.cc/paper\%5Ffiles/paper/2018/file/83cdcec08fbf90370fcf53bdd56604ff-Paper.pdf},
	volume       = 31,
	year         = 2018,
}
@article{saleiro2018aequitas,
	title        = {Aequitas: A bias and fairness audit toolkit},
	author       = {Saleiro, Pedro and Kuester, Benedict and Hinkson, Loren and London, Jesse and Stevens, Abby and Anisfeld, Ari and Rodolfa, Kit T and Ghani, Rayid},
	year         = 2018,
	journal      = {arXiv preprint arXiv:1811.05577},
}
@inproceedings{verma2018fairness,
	title        = {Fairness definitions explained},
	author       = {Verma, Sahil and Rubin, Julia},
	year         = 2018,
	booktitle    = {2018 ieee/acm international workshop on software fairness (fairware)},
	pages        = {1--7},
	organization = {IEEE},
}
@misc{compliance_presence,
	title        = {Compliance Presence},
	author       = {Internal Revenue Service (IRS)},
	journal      = {Internal Revenue Service},
	url          = {https://www.irs.gov/statistics/compliance-presence},
}
@misc{NRP,
	title        = {National Research Program Overview},
	author       = {Internal Revenue Service (IRS)},
	journal      = {Internal Revenue Service},
	url          = {https://www.irs.gov/irm/part4/irm\%5F04-022-001},
}
@misc{num_audits,
	title        = {IRS Data Book 2020},
	year         = 2020,
	journal      = {Internal Revenue Service},
	url          = {https://www.irs.gov/pub/irs-pdf/p55b.pdf},
}
@misc{taxpayer_adv,
	title        = {Annual Report to Congress},
	author       = {National Taxpayer Advocate},
	year         = 2019,
	journal      = {National Taxpayer Advocate},
	url          = {https://www.taxpayeradvocate.irs.gov/reports/2019-annual-report-to-congress/full-report/},
}
@misc{budget_dropoff,
	title        = {IRS Update on Audits},
	author       = {Internal Revenue Service (IRS)},
	journal      = {Internal Revenue Service},
	url          = {https://www.irs.gov/newsroom/irs-update-on-audits},
}
@misc{sample_weights,
	title        = {NRP Examining Process},
	author       = {Internal Revenue Service (IRS)},
	journal      = {Internal Revenue Service},
	url          = {https://www.irs.gov/irm/part4/irm\%5F04-022-004r},
}
@article{franzoni1998tax,
	title        = {Tax evasion and tax compliance},
	author       = {Franzoni, Luigi A},
	year         = 1998,
	journal      = {Available at SSRN 137430},
}
@article{kaplow1990optimal,
	title        = {Optimal taxation with costly enforcement and evasion},
	author       = {Kaplow, Louis},
	year         = 1990,
	journal      = {Journal of Public Economics},
	publisher    = {Elsevier},
	volume       = 43,
	number       = 2,
	pages        = {221--236},
}
@article{andreoni1998tax,
	title        = {Tax compliance},
	author       = {Andreoni, James and Erard, Brian and Feinstein, Jonathan},
	year         = 1998,
	journal      = {Journal of economic literature},
	publisher    = {JSTOR},
	volume       = 36,
	number       = 2,
	pages        = {818--860},
}
@article{cremer1990evading,
	title        = {Evading, auditing and taxing: The equity-compliance tradeoff},
	author       = {Cremer, Helmuth and Marchand, Maurice and Pestieau, Pierre},
	year         = 1990,
	journal      = {Journal of Public Economics},
	publisher    = {Elsevier},
	volume       = 43,
	number       = 1,
	pages        = {67--92},
}
@article{ray2014government,
	title        = {A government success story: How data analysis by the Social Security Appeals Council (with a push from the Administrative Conference of the United States) is transforming social security disability adjudication},
	author       = {Ray, Gerald K and Lubbers, Jeffrey S},
	year         = 2014,
	journal      = {Geo. Wash. L. Rev.},
	publisher    = {HeinOnline},
	volume       = 83,
	pages        = 1575,
}
@inproceedings{lakkaraju2017selective,
	title        = {The selective labels problem: Evaluating algorithmic predictions in the presence of unobservables},
	author       = {Lakkaraju, Himabindu and Kleinberg, Jon and Leskovec, Jure and Ludwig, Jens and Mullainathan, Sendhil},
	year         = 2017,
	booktitle    = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	pages        = {275--284},
}
@misc{kiel2018s,
	title        = {Who’s more likely to be audited: A person making \$20,000 or \$400, 000},
	author       = {Kiel, Paul and Eisinger, Jesse},
	year         = 2018,
	publisher    = {ProPublica},
}
@misc{macnabb,
	title        = {Study of Tax Court Cases In Which the IRS Conceded the Taxpayer was Entitled to Earned Income Tax Credit (EITC)},
	author       = {MacNabb, Jill},
	publisher    = {IRS Taxpayer Advocate},
	url          = {https://www.taxpayeradvocate.irs.gov/wp-content/uploads/2020/08/Research-Studies-Study-of-Tax-Court-Cases-in-Which-the-IRS-Conceded-the-Taxpayer-was-Entitled-to-Earned-Income-Tax-Credit-EITC.pdf},
}
@techreport{guyton2018effects,
	title        = {The effects of EITC correspondence audits on low-income earners},
	author       = {Guyton, John and Leibel, Kara and Manoli, Dayanand S and Patel, Ankur and Payne, Mark and Schafer, Brenda},
	year         = 2018,
	institution  = {National Bureau of Economic Research},
}
@misc{tax_gap,
	title        = {IRS Newsroom},
	author       = {Internal Revenue Service (IRS)},
	journal      = {Internal Revenue Service},
	url          = {https://www.irs.gov/newsroom/the-tax-gap},
}
@misc{internal_revenue,
	title        = {IRS releases new Tax Gap estimates},
	journal      = {Internal Revenue Service},
	url          = {https://www.irs.gov/newsroom/irs-releases-new-tax-gap-estimates-compliance-rates-remain-substantially-unchanged-from-prior-study},
}
@inproceedings{zehlike2017fa,
	title        = {Fa* ir: A fair top-k ranking algorithm},
	author       = {Zehlike, Meike and Bonchi, Francesco and Castillo, Carlos and Hajian, Sara and Megahed, Mohamed and Baeza-Yates, Ricardo},
	year         = 2017,
	booktitle    = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
	pages        = {1569--1578},
}
@inproceedings{pmlr-v97-mary19a,
	title        = {Fairness-Aware Learning for Continuous Attributes and Treatments},
	author       = {Mary, Jeremie and Calauz{\`e}nes, Cl{\'e}ment and Karoui, Noureddine El},
	booktitle    = {Proceedings of the 36th International Conference on Machine Learning},
	pages        = {4382--4391},
	year         = 2019,
	editor       = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	volume       = 97,
	series       = {Proceedings of Machine Learning Research},
	month        = {09--15 Jun},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v97/mary19a/mary19a.pdf},
	url          = {https://proceedings.mlr.press/v97/mary19a.html},
	abstract     = {We address the problem of algorithmic fairness: ensuring that the outcome of a classifier is not biased towards certain values of sensitive variables such as age, race or gender. As common fairness metrics can be expressed as measures of (conditional) independence between variables, we propose to use the Rényi maximum correlation coefficient to generalize fairness measurement to continuous variables. We exploit Witsenhausen’s characterization of the Rényi correlation coefficient to propose a differentiable implementation linked to $f$-divergences. This allows us to generalize fairness-aware learning to continuous variables by using a penalty that upper bounds this coefficient. Theses allows fairness to be extented to variables such as mixed ethnic groups or financial status without thresholds effects. This penalty can be estimated on mini-batches allowing to use deep nets. Experiments show favorable comparisons to state of the art on binary variables and prove the ability to protect continuous ones},
}
@inproceedings{holstein2019improving,
	title        = {Improving fairness in machine learning systems: What do industry practitioners need?},
	author       = {Holstein, Kenneth and Wortman Vaughan, Jennifer and Daum{\'e} III, Hal and Dudik, Miro and Wallach, Hanna},
	year         = 2019,
	booktitle    = {Proceedings of the 2019 CHI conference on human factors in computing systems},
	pages        = {1--16},
}
@inproceedings{niculescu2005predicting,
	title        = {Predicting good probabilities with supervised learning},
	author       = {Niculescu-Mizil, Alexandru and Caruana, Rich},
	year         = 2005,
	booktitle    = {Proceedings of the 22nd international conference on Machine learning},
	pages        = {625--632},
}
@article{aliaies21,
	author       = {Ali, Junaid and Lahoti, Preethi and Gummadi, Krishna P.},
	title        = {Accounting for Model Uncertainty in Algorithmic Discrimination},
	year         = 2021,
	isbn         = 9781450384735,
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	url          = {https://doi.org/10.1145/3461702.3462630},
	doi          = {10.1145/3461702.3462630},
	abstract     = {Traditional approaches to ensure group fairness in algorithmic decision making aim to equalize "total" error rates for different subgroups in the population. In contrast, we argue that the fairness approaches should instead focus only on equalizing errors arising due to model uncertainty (a.k.a epistemic uncertainty), caused due to lack of knowledge about the best model or due to lack of data. In other words, our proposal calls for ignoring the errors that occur due to uncertainty inherent in the data, i.e., aleatoric uncertainty. We draw a connection between predictive multiplicity and model uncertainty and argue that the techniques from predictive multiplicity could be used to identify errors made due to model uncertainty. We propose scalable convex proxies to come up with classifiers that exhibit predictive multiplicity and empirically show that our methods are comparable in performance and up to four orders of magnitude faster than the current state-of-the-art. We further pro- pose methods to achieve our goal of equalizing group error rates arising due to model uncertainty in algorithmic decision making and demonstrate the effectiveness of these methods using synthetic and real-world datasets},
	booktitle    = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
	pages        = {336–345},
	numpages     = 10,
	keywords     = {algorithmic fairness, predictive multiplicity, classification, model uncertainty},
	location     = {Virtual Event, USA},
	series       = {AIES '21},
}
@article{fox2011distinguishing,
	title        = {Distinguishing two dimensions of uncertainty},
	author       = {Fox, Craig R and {\"U}lk{\"u}men, G{\"u}lden},
	year         = 2011,
	journal      = {Fox, Craig R. and G{\"u}lden {\"U}lk{\"u}men (2011),“Distinguishing Two Dimensions of Uncertainty,” in Essays in Judgment and Decision Making, Brun, W., Kirkeb{\o}en, G. and Montgomery, H., eds. Oslo: Universitetsforlaget},
}
@inproceedings{heidari2019moral,
	title        = {A moral framework for understanding fair ml through economic models of equality of opportunity},
	author       = {Heidari, Hoda and Loi, Michele and Gummadi, Krishna P and Krause, Andreas},
	year         = 2019,
	booktitle    = {Proceedings of the conference on fairness, accountability, and transparency},
	pages        = {181--190},
}
@article{livingston2020preventing,
	title        = {Preventing Racial Bias in Federal AI},
	author       = {Livingston, Morgan},
	year         = 2020,
	journal      = {JSPG},
	volume       = 16,
}
@article{gao,
	title        = {Artificial Intelligence: An Accountability Framework for Federal Agencies and Other Entities},
	author       = {U.S. Government Accountability Office},
	year         = 2021,
	month        = {June},
}
@misc{bideneo,
	title        = {Exec. Order No. 13985 86 Fed. Reg. 7009, Advancing Racial Equity and Support for Underserved Communities Through the Federal Government},
	author       = {U.S. President},
	date         = {Jan. 20, 2021},
}
@misc{gaopreparers,
	title        = {In a Limited Study, Preparers Made Significant Errors},
	author       = {McTigue, Jamse R. Jr.},
	url          = {https://www.gao.gov/assets/gao-14-467t.pdf},
	date         = {Apr. 08, 2014},
}
@article{ewall-wice_21AD,
	title        = {Rich people and corporations need to pay up, says IRS head as agency looks to collect \$1 trillion in unpaid taxes},
	author       = {EWALL-WICE, SARAH},
	year         = {21AD},
	month        = {Apr},
	journal      = {CBS News},
	url          = {https://www.cbsnews.com/news/irs-wealthy-people-corporations-1-trillion/},
}
@article{saez2016generalized,
	title        = {Generalized social marginal welfare weights for optimal tax theory},
	author       = {Saez, Emmanuel and Stantcheva, Stefanie},
	year         = 2016,
	journal      = {American Economic Review},
	volume       = 106,
	number       = 1,
	pages        = {24--45},
}
@inbook{musgrave_slemrod_1994,
	title        = {Progressive taxation, equity, and tax design},
	author       = {Musgrave, Richard A. and Slemrod, Joel},
	year         = 1994,
	booktitle    = {Tax Progressivity and Income Inequality},
	publisher    = {Cambridge University Press},
	pages        = {341–356},
	doi          = {10.1017/CBO9780511571824.019},
	place        = {Cambridge},
}
@inproceedings{mullainathan2021inequity,
	title        = {On the Inequity of Predicting A While Hoping for B},
	author       = {Mullainathan, Sendhil and Obermeyer, Ziad},
	year         = 2021,
	booktitle    = {AEA Papers and Proceedings},
	volume       = 111,
	pages        = {37--42},
}
@inproceedings{geyik2019fairness,
	title        = {Fairness-aware ranking in search \& recommendation systems with application to linkedin talent search},
	author       = {Geyik, Sahin Cem and Ambler, Stuart and Kenthapadi, Krishnaram},
	year         = 2019,
	booktitle    = {Proceedings of the 25th acm sigkdd international conference on knowledge discovery \& data mining},
	pages        = {2221--2231},
}
@misc{treasury_report,
	title        = {The Earned Income Tax Credit Examination Compliance Strategy Can Be Improved},
	author       = {TREASURY INSPECTOR GENERAL FOR TAX ADMINISTRATION},
	journal      = {https://www.treasury.gov/tigta/auditreports/2021reports/202130051fr.pdf},
	publisher    = {US Treasury},
	url          = {https://www.treasury.gov/tigta/auditreports/2021reports/202130051fr.pdf},
}
@article{lamba2021empirical,
	title        = {An Empirical Comparison of Bias Reduction Methods on Real-World Problems in High-Stakes Policy Settings},
	author       = {Lamba, Hemank and Rodolfa, Kit T and Ghani, Rayid},
	year         = 2021,
	journal      = {ACM SIGKDD Explorations Newsletter},
	publisher    = {ACM New York, NY, USA},
	volume       = 23,
	number       = 1,
	pages        = {69--85},
}
@inproceedings{brown2019toward,
	title        = {Toward algorithmic accountability in public services: A qualitative study of affected community perspectives on algorithmic decision-making in child welfare services},
	author       = {Brown, Anna and Chouldechova, Alexandra and Putnam-Hornstein, Emily and Tobin, Andrew and Vaithianathan, Rhema},
	year         = 2019,
	booktitle    = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
	pages        = {1--12},
}
@inproceedings{pmlr-v81-binns18a,
	title        = {Fairness in Machine Learning: Lessons from Political Philosophy},
	author       = {Binns, Reuben},
	booktitle    = {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
	pages        = {149--159},
	year         = 2018,
	editor       = {Friedler, Sorelle A. and Wilson, Christo},
	volume       = 81,
	series       = {Proceedings of Machine Learning Research},
	month        = {23--24 Feb},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v81/binns18a/binns18a.pdf},
	url          = {https://proceedings.mlr.press/v81/binns18a.html},
	abstract     = {What does it mean for a machine learning model to be ‘fair’, in terms which can be operationalised? Should fairness consist of ensuring everyone has an equal probability of obtaining some benefit, or should we aim instead to minimise the harms to the least advantaged? Can the relevant ideal be determined by reference to some alternative state of affairs in which a particular social pattern of discrimination does not exist? Various definitions proposed in recent literature make different assumptions about what terms like discrimination and fairness mean and how they can be defined in mathematical terms. Questions of discrimination, egalitarianism and justice are of significant interest to moral and political philosophers, who have expended significant efforts in formalising and defending these central concepts. It is therefore unsurprising that attempts to formalise ‘fairness’ in machine learning contain echoes of these old philosophical debates. This paper draws on existing work in moral and political philosophy in order to elucidate emerging debates about fair machine learning.},
}
@inproceedings{hutchinson201950,
	title        = {50 years of test (un) fairness: Lessons for machine learning},
	author       = {Hutchinson, Ben and Mitchell, Margaret},
	year         = 2019,
	booktitle    = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
	pages        = {49--58},
}
@article{scikit-learn,
	title        = {Scikit-learn: Machine Learning in {P}ython},
	author       = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
	year         = 2011,
	journal      = {Journal of Machine Learning Research},
	volume       = 12,
	pages        = {2825--2830},
}
%end Publications

%Supreme Court cases sorted by key
%end Supreme Court cases

%Miscellaneous sorted by key
@techreport{bird2020fairlearn,
	title        = {Fairlearn: A toolkit for assessing and improving fairness in {AI}},
	author       = {Bird, Sarah and Dud{\'i}k, Miro and Edgar, Richard and Horn, Brandon and Lutz, Roman and Milan, Vanessa and Sameki, Mehrnoosh and Wallach, Hanna and Walker, Kathleen},
	year         = 2020,
	month        = {May},
	number       = {MSR-TR-2020-32},
	url          = {https://www.microsoft.com/en-us/research/publication/fairlearn-a-toolkit-for-assessing-and-improving-fairness-in-ai/},
	institution  = {Microsoft},
}
a service of Schloss Dagstuhl - Leibniz Center for Informatics  homebrowsesearchabout
%now the slew of CF algos to cite
%specifically for credit
%CF and AE
%problems
%end Publications

%Supreme Court cases sorted by key
%end Supreme Court cases


a service of Schloss Dagstuhl - Leibniz Center for Informatics  homebrowsesearchabout
%now the slew of CF algos to cite
%specifically for credit
%CF and AE
@inproceedings{Devlin2019BERTPO,
	title        = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
	author       = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
	year         = 2019,
	booktitle    = {NAACL},
}
@inproceedings{influence-directed,
	title        = {Influence-Directed Explanations for Deep Convolutional Networks},
	author       = {Leino, Klas and Sen, Shayak and Datta, Anupam and Fredrikson, Matt and Li, Linyi},
	year         = 2018,
	booktitle    = {2018 IEEE International Test Conference (ITC)},
	volume       = {},
	number       = {},
	pages        = {1--8},
	doi          = {10.1109/TEST.2018.8624792},
}
@inproceedings{fromherz20projections,
	title        = {Fast Geometric Projections for Local Robustness Certification},
	author       = {Aymeric Fromherz and Klas Leino and Matt Fredrikson and Bryan Parno and Corina Păsăreanu},
	year         = 2021,
	booktitle    = {International Conference on Learning Representations (ICLR)},
}
@inproceedings{Jordan2019ProvableCF,
	title        = {Provable Certificates for Adversarial Examples: Fitting a Ball in the Union of Polytopes},
	author       = {Matt Jordan and Justin Lewis and A. Dimakis},
	year         = 2019,
	booktitle    = {NeurIPS},
}
@inproceedings{DBLP:conf/nips/HaninR19,
	title        = {Deep ReLU Networks Have Surprisingly Few Activation Patterns},
	author       = {Boris Hanin and David Rolnick},
	year         = 2019,
	booktitle    = {Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada},
}
@article{Wang2021BoundaryAP,
	title        = {Boundary Attributions Provide Normal (Vector) Explanations},
	author       = {Zifan Wang and Matt Fredrikson and Anupam Datta},
	year         = 2021,
	journal      = {ArXiv},
	volume       = {abs/2103.11257},
}
@article{Kong2020PANNsLP,
	title        = {PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition},
	author       = {Qiuqiang Kong and Yin Cao and Turab Iqbal and Yuxuan Wang and Wenwu Wang and Mark D. Plumbley},
	year         = 2020,
	journal      = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	volume       = 28,
	pages        = {2880--2894},
}
@article{Jayram2019TransferLI,
	title        = {Transfer Learning in Visual and Relational Reasoning},
	author       = {T. S. Jayram and Vincent Marois and T. Kornuta and V. Albouy and E. Sevgen and Ahmet S. Ozcan},
	year         = 2019,
	journal      = {ArXiv},
	volume       = {abs/1911.11938},
}
%problems
@book{10.5555/1642718,
	title        = {Causality: Models, Reasoning and Inference},
	author       = {Pearl, Judea},
	year         = 2009,
	publisher    = {Cambridge University Press},
	address      = {USA},
	isbn         = {052189560X},
	edition      = {2nd},
}
@misc{center2019sources,
	title        = {Sources of revenue for the federal government},
	author       = {Center, Tax Policy},
	year         = 2019,
}
@article{mitchell2021algorithmic,
	title        = {Algorithmic fairness: Choices, assumptions, and definitions},
	author       = {Mitchell, Shira and Potash, Eric and Barocas, Solon and D'Amour, Alexander and Lum, Kristian},
	year         = 2021,
	journal      = {Annual Review of Statistics and Its Application},
	publisher    = {Annual Reviews},
	volume       = 8,
	pages        = {141--163},
}
@techreport{guyton2021tax,
	title        = {Tax evasion at the top of the income distribution: theory and evidence},
	author       = {Guyton, John and Langetieg, Patrick and Reck, Daniel and Risch, Max and Zucman, Gabriel},
	year         = 2021,
	institution  = {National Bureau of Economic Research},
}
@inproceedings{jeanselme2022imputation,
	title        = {Imputation Strategies Under Clinical Presence: Impact on Algorithmic Fairness},
	author       = {Jeanselme, Vincent and De-Arteaga, Maria and Zhang, Zhe and Barrett, Jessica and Tom, Brian},
	booktitle    = {Machine Learning for Health},
	pages        = {12--34},
	year         = 2022,
	organization = {PMLR},
}
@misc{MLPolicyclass,
	title        = {Machine Learning for Public Policy Lab},
	author       = {Ghani and Rodolfa},
	howpublished = {\url{https://github.com/dssg/mlforpublicpolicylab}},
	note         = {Accessed: 2022-09-30},
}
@misc{DataScienceScoping,
	title        = {Data Science Project Scoping Guide},
	author       = {Data Science and Public Policy at Carnegie Mellon University},
	howpublished = {\url{http://www.datasciencepublicpolicy.org/our-work/tools-guides/data-science-project-scoping-guide/}},
}
@misc{gdpr,
	title        = {European Parliament and Council of European Union (2016) Regulation (EU) 2016/679.},
	year         = 2016,
	howpublished = {\url{https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=CELEX:32016R0679&from=EN}},
}
%end Publications

%Supreme Court cases sorted by key
%end Supreme Court cases

%Miscellaneous sorted by key
%end Miscellaneous
a service of Schloss Dagstuhl - Leibniz Center for Informatics  homebrowsesearchabout
@article{black2022model,
	title        = {Model Multiplicity: Opportunities, Concerns, and Solutions},
	author       = {BLACK, EMILY and RAGHAVAN, MANISH and BAROCAS, SOLON},
	year         = 2022,
}
@article{Ramachandran2020,
	title        = {predictive Analytics for Retention in care in an Urban HiV clinic},
	author       = {Ramachandran, Arthi and Kumar, Avishek and Koenig, Hannes and De Unanue, Adolfo and Sung, Christina and Walsh, Joe and Schneider, John and Ghani, Rayid and Ridgway, Jessica P},
	journal      = {Scientific reports},
	volume       = 10,
	number       = 1,
	pages        = {1--10},
	year         = 2020,
	publisher    = {Nature Publishing Group},
}
@article{Kumar2018UsingBreaks,
	title        = {{Using machine learning to assess the risk of and prevent water main breaks}},
	year         = 2018,
	journal      = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	author       = {Kumar, Avishek and Ali Vanderveld, R. and Edelstein, Sam and Rizvi, Syed Ali Asad and Wilson, Kevin H. and Finch, Adria and Brooks, Benjamin and Kenney, Chad and Maxwell, Andrew and Zuckerbraun, Joe and Ghani, Rayid},
	month        = 7,
	pages        = {472--480},
	publisher    = {Association for Computing Machinery},
	url          = {https://doi.org/10.1145/3219819.3219835},
	isbn         = 9781450355520,
	doi          = {10.1145/3219819.3219835},
}
@inproceedings{Ye2019UsingCity,
	title        = {{Using machine learning to help vulnerable tenants in New York City}},
	year         = 2019,
	booktitle    = {COMPASS 2019 - Proceedings of the 2019 Conference on Computing and Sustainable Societies},
	author       = {Ye, Teng and Johnson, Rebecca and Fu, Samantha and Copeny, Jerica and Donnelly, Bridgit and Freeman, Alex and Lima, Mirian and Walsh, Joe and Ghani, Rayid},
	month        = 7,
	pages        = {248--258},
	volume       = 11,
	publisher    = {Association for Computing Machinery, Inc},
	url          = {http://dl.acm.org/citation.cfm?doid=3314344.3332484},
	address      = {New York, New York, USA},
	isbn         = 9781450367141,
	doi          = {10.1145/3314344.3332484},
	keywords     = {Machine Learning, Public Policy, Resource Allocation, Social Good, Tenant Harassment},
}
@article{Potash2020,
	title        = {Validation of a Machine Learning Model to Predict Childhood Lead Poisoning},
	author       = {Potash, Eric and Ghani, Rayid and Walsh, Joe and Jorgensen, Emile and Lohff, Cortland and Prachand, Nik and Mansour, Raed},
	journal      = {JAMA Network Open},
	volume       = 3,
	number       = 9,
	pages        = {e2012734--e2012734},
	year         = 2020,
	publisher    = {American Medical Association},
}
@inproceedings{potash2015predictive,
	title        = {Predictive modeling for public health: Preventing childhood lead poisoning},
	author       = {Potash, Eric and Brew, Joe and Loewi, Alexander and Majumdar, Subhabrata and Reece, Andrew and Walsh, Joe and Rozier, Eric and Jorgenson, Emile and Mansour, Raed and Ghani, Rayid},
	booktitle    = {ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)},
	year         = 2015,
}
@inproceedings{somanchi2015early,
	title        = {Early prediction of cardiac arrest (code blue) using electronic medical records},
	author       = {Somanchi, Sriram and Adhikari, Samrachana and Lin, Allen and Eneva, Elena and Ghani, Rayid},
	booktitle    = {ACM SIGKDD international conference on knowledge discovery and data mining (KDD)},
	year         = 2015,
}
@inproceedings{Bauman2018ReducingInterventions,
	title        = {{Reducing Incarceration through Prioritized Interventions}},
	year         = 2018,
	booktitle    = {Proceedings of the Conference on Computing and Sustainable Societies (COMPASS)},
	author       = {Bauman, Matthew J. and Sullivan, Robert and Schneweis, Chris and Ghani, Rayid and Boxer, Kate S. and Lin, Tzu-Yun and Salomon, Erika and Naveed, Hareem and Haynes, Lauren and Walsh, Joe and Helsby, Jen and Yoder, Steve},
	pages        = {1--8},
	publisher    = {ACM},
	url          = {http://dl.acm.org/citation.cfm?doid=3209811.3209869},
	address      = {New York, New York, USA},
	isbn         = 9781450358163,
	doi          = {10.1145/3209811.3209869},
}
@article{pan2017machine,
	title        = {Machine learning for social services: a study of prenatal case management in Illinois},
	author       = {Pan, Ian and Nolan, Laura B and Brown, Rashida R and Khan, Romana and van der Boor, Paul and Harris, Daniel G and Ghani, Rayid},
	journal      = {American journal of public health},
	volume       = 107,
	number       = 6,
	pages        = {938--944},
	year         = 2017,
	publisher    = {American Public Health Association},
}
@inproceedings{lakkaraju2015machine,
	title        = {A machine learning framework to identify students at risk of adverse academic outcomes},
	author       = {Lakkaraju, Himabindu and Aguiar, Everaldo and Shan, Carl and Miller, David and Bhanpuri, Nasir and Ghani, Rayid and Addison, Kecia L},
	booktitle    = {ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)},
	year         = 2015,
}
@misc{mvescblog,
	title        = {MVESC: Finding the Missing Students Who Don’t Graduate On Time},
	url          = {http://www.dssgfellowship.org/2016/08/05/mvesc-finding-the-missing-students-who-dont-graduate-on-time/},
	journal      = {Data Science for Social Good Blog},
	publisher    = {Data Science for Social Good Blog},
	author       = {Cheng, Xiang},
	year         = 2016,
	month        = {Aug},
}
@misc{tulsaposter,
	title        = {Targeting Education Interventions For Young Students},
	url          = {https://notconfusing.com/images/uploads/2016/09/Tulsa-Poster.pdf/},
	journal      = {Data Science for Social Good Poster},
	publisher    = {Data Science for Social Good},
	author       = {Monica Alexander, Charlotte Huang, Maximilian Klein, Ali Vanderveld, Kevin Wilson, Chad Kenney},
	year         = 2016,
	month        = {Aug},
}
"@inproceedings{10.1145/3306618.3314242,
author = {Morgan, Andrew and Pass, Rafael},
title = {Paradoxes in Fair Computer-Aided Decision Making},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314242},
doi = {10.1145/3306618.3314242},
abstract = {Computer-aided decision making--where a human decision-maker is aided by a computational classifier in making a decision--is becoming increasingly prevalent. For instance, judges in at least nine states make use of algorithmic tools meant to determine ""recidivism risk scores"" for criminal defendants in sentencing, parole, or bail decisions. A subject of much recent debate is whether such algorithmic tools are ""fair"" in the sense that they do not discriminate against certain groups (e.g., races) of people. Our main result shows that for ""non-trivial"" computer-aided decision making, either the classifier must be discriminatory, or a rational decision-maker using the output of the classifier is forced to be discriminatory. We further provide a complete characterization of situations where fair computer-aided decision making is possible.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {85–90},
numpages = {6},
keywords = {game theory, impossibility, algorithmic fairness, fairness in classification},
location = {Honolulu, HI, USA},
series = {AIES '19}
}
  
"
"@inproceedings{10.1145/3306618.3314236,
author = {Coston, Amanda and Ramamurthy, Karthikeyan Natesan and Wei, Dennis and Varshney, Kush R. and Speakman, Skyler and Mustahsan, Zairah and Chakraborty, Supriyo},
title = {Fair Transfer Learning with Missing Protected Attributes},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314236},
doi = {10.1145/3306618.3314236},
abstract = {Risk assessment is a growing use for machine learning models. When used in high-stakes applications, especially ones regulated by anti-discrimination laws or governed by societal norms for fairness, it is important to ensure that learned models do not propagate and scale any biases that may exist in training data. In this paper, we add on an additional challenge beyond fairness: unsupervised domain adaptation to covariate shift between a source and target distribution. Motivated by the real-world problem of risk assessment in new markets for health insurance in the United States and mobile money-based loans in East Africa, we provide a precise formulation of the machine learning with covariate shift and score parity problem. Our formulation focuses on situations in which protected attributes are not available in either the source or target domain. We propose two new weighting methods: prevalence-constrained covariate shift (PCCS) which does not require protected attributes in the target domain and target-fair covariate shift (TFCS) which does not require protected attributes in the source domain. We empirically demonstrate their efficacy in two applications.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {91–98},
numpages = {8},
keywords = {fairness, transfer learning, risk assessments},
location = {Honolulu, HI, USA},
series = {AIES '19}
}
  
"
"@inproceedings{10.1145/3306618.3314289,
author = {Whittlestone, Jess and Nyrup, Rune and Alexandrova, Anna and Cave, Stephen},
title = {The Role and Limits of Principles in AI Ethics: Towards a Focus on Tensions},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314289},
doi = {10.1145/3306618.3314289},
abstract = {The last few years have seen a proliferation of principles for AI ethics. There is substantial overlap between different sets of principles, with widespread agreement that AI should be used for the common good, should not be used to harm people or undermine their rights, and should respect widely held values such as fairness, privacy, and autonomy. While articulating and agreeing on principles is important, it is only a starting point. Drawing on comparisons with the field of bioethics, we highlight some of the limitations of principles: in particular, they are often too broad and high-level to guide ethics in practice. We suggest that an important next step for the field of AI ethics is to focus on exploring the tensions that inevitably arise as we try to implement principles in practice. By explicitly recognising these tensions we can begin to make decisions about how they should be resolved in specific cases, and develop frameworks and guidelines for AI ethics that are rigorous and practically relevant. We discuss some different specific ways that tensions arise in AI ethics, and what processes might be needed to resolve them.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {195–200},
numpages = {6},
keywords = {artificial intelligence, ethics, principles},
location = {Honolulu, HI, USA},
series = {AIES '19}
}
  
"
"@inproceedings{10.1145/3306618.3314282,
author = {Goel, Naman and Faltings, Boi},
title = {Crowdsourcing with Fairness, Diversity and Budget Constraints},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314282},
doi = {10.1145/3306618.3314282},
abstract = {Recent studies have shown that the labels collected from crowdworkers can be discriminatory with respect to sensitive attributes such as gender and race. This raises questions about the suitability of using crowdsourced data for further use, such as for training machine learning algorithms. In this work, we address the problem of fair and diverse data collection from a crowd under budget constraints. We propose a novel algorithm which maximizes the expected accuracy of the collected data, while ensuring that the errors satisfy desired notions of fairness. We provide guarantees on the performance of our algorithm and show that the algorithm performs well in practice through experiments on a real dataset.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {297–304},
numpages = {8},
keywords = {data quality, crowdsourcing, bias, fairness},
location = {Honolulu, HI, USA},
series = {AIES '19}
}
  
"
"@inproceedings{10.1145/3306618.3314270,
author = {Swinger, Nathaniel and De-Arteaga, Maria and Heffernan IV, Neil Thomas and Leiserson, Mark DM and Kalai, Adam Tauman},
title = {What Are the Biases in My Word Embedding?},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314270},
doi = {10.1145/3306618.3314270},
abstract = {This paper presents an algorithm for enumerating biases in word embeddings. The algorithm exposes a large number of offensive associations related to sensitive features such as race and gender on publicly available embeddings, including a supposedly ""debiased"" embedding. These biases are concerning in light of the widespread use of word embeddings. The associations are identified by geometric patterns in word embeddings that run parallel between people's names and common lower-case tokens. The algorithm is highly unsupervised: it does not even require the sensitive features to be pre-specified. This is desirable because: (a) many forms of discrimination?such as racial discrimination-are linked to social constructs that may vary depending on the context, rather than to categories with fixed definitions; and (b) it makes it easier to identify biases against intersectional groups, which depend on combinations of sensitive features. The inputs to our algorithm are a list of target tokens, e.g. names, and a word embedding. It outputs a number of Word Embedding Association Tests (WEATs) that capture various biases present in the data. We illustrate the utility of our approach on publicly available word embeddings and lists of names, and evaluate its output using crowdsourcing. We also show how removing names may not remove potential proxy bias.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {305–311},
numpages = {7},
keywords = {fairness, bias, word embeddings},
location = {Honolulu, HI, USA},
series = {AIES '19}
}
  
"
"@inproceedings{10.1145/3306618.3314323,
author = {Mishler, Alan},
title = {Modeling Risk and Achieving Algorithmic Fairness Using Potential Outcomes},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314323},
doi = {10.1145/3306618.3314323},
abstract = {Predictive models and algorithms are increasingly used to support human decision makers, raising concerns about how to ensure that these algorithms are fair. Additionally, these tools are generally designed to predict observable outcomes, but this is problematic when the treatment or exposure is confounded with the outcome. I argue that in most cases, what is actually of interest are potential outcomes. I contrast modeling approaches built around observable vs. potential outcomes, and I recharacterize error rate-based algorithmic fairness metrics in terms of potential outcomes. I also aim to formally model the consequences of using confounded observable predictions to drive interventions.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {555–556},
numpages = {2},
keywords = {algorithmic fairness, risk assessment, recidivism, causal inference, bias, potential outcomes, machine learning},
location = {Honolulu, HI, USA},
series = {AIES '19}
}
  
"
"@inproceedings{10.1145/3375627.3375823,
author = {Cai, William and Gaebler, Johann and Garg, Nikhil and Goel, Sharad},
title = {Fair Allocation through Selective Information Acquisition},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375823},
doi = {10.1145/3375627.3375823},
abstract = {Public and private institutions must often allocate scarce resources under uncertainty. Banks, for example, extend credit to loan applicants based in part on their estimated likelihood of repaying a loan. But when the quality of information differs across candidates (e.g., if some applicants lack traditional credit histories), common lending strategies can lead to disparities across groups. Here we consider a setting in which decision makers---before allocating resources---can choose to spend some of their limited budget further screening select individuals. We present a computationally efficient algorithm for deciding whom to screen that maximizes a standard measure of social welfare. Intuitively, decision makers should screen candidates on the margin, for whom the additional information could plausibly alter the allocation. We formalize this idea by showing the problem can be reduced to solving a series of linear programs. Both on synthetic and real-world datasets, this strategy improves utility, illustrating the value of targeted information acquisition in such decisions. Further, when there is social value for distributing resources to groups for whom we have a priori poor information---like those without credit scores---our approach can substantially improve the allocation of limited assets.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {22–28},
numpages = {7},
keywords = {algorithmic fairness, lending, information acquisition},
location = {New York, NY, USA},
series = {AIES '20}
}
  
"
"@inproceedings{10.1145/3375627.3375807,
author = {Matthews, Jeanna Neefe and Northup, Graham and Grasso, Isabella and Lorenz, Stephen and Babaeianjelodar, Marzieh and Bashaw, Hunter and Mondal, Sumona and Matthews, Abigail and Njie, Mariama and Goldthwaite, Jessica},
title = {When Trusted Black Boxes Don't Agree: Incentivizing Iterative Improvement and Accountability in Critical Software Systems},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375807},
doi = {10.1145/3375627.3375807},
abstract = {Software increasingly plays a key role in regulated areas like housing, hiring, and credit, as well as major public functions such as criminal justice and elections. It is easy for there to be unintended defects with a large impact on the lives of individuals and society as a whole. Preventing, finding, and fixing software defects is a key focus of both industrial software development efforts as well as academic research in software engineering. In this paper, we discuss flaws in the larger socio-technical decision-making processes in which critical black-box software systems are developed, deployed, and trusted. We use criminal justice software, specifically probabilistic genotyping (PG) software, as a concrete example. We describe how PG software systems, designed to do the same job, produce different results. We highlight the under-appreciated impact of changes in key parameters and the disparate impact that one such parameter can have on different racial/ethnic groups. We propose concrete changes to the socio-technical decision-making processes surrounding the use of PG software that could be used to incentivize iterative improvements in the accuracy, fairness, reliability, and accountability of these systems.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {102–108},
numpages = {7},
keywords = {disparate impact, criminal justice software, software verification, probabilistic genotyping, algorithmic accountability},
location = {New York, NY, USA},
series = {AIES '20}
}
  
"
"@inproceedings{10.1145/3375627.3375860,
author = {Cruz Cort\'{e}s, Efr\'{e}n and Ghosh, Debashis},
title = {An Invitation to System-Wide Algorithmic Fairness},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375860},
doi = {10.1145/3375627.3375860},
abstract = {We propose a framework for analyzing and evaluating system-wide algorithmic fairness. The core idea is to use simulation techniques in order to extend the scope of current fairness assessments by incorporating context and feedback to a phenomenon of interest. By doing so, we expect to better understand the interaction among the social behavior giving rise to discrimination, automated decision making tools, and fairness-inspired statistical constraints. In particular, we invite the community to use agent based models as an explanatory tool for causal mechanisms of population level properties. We also propose embedding these into a reinforcement learning algorithm to find optimal actions for meaningful change. As an incentive for taking a system-wide approach , we show through a simple model of predictive policing and trials that if we limit our attention to one portion of the system, we may determine some blatantly unfair practices as fair, and be blind to overall unfairness.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {235–241},
numpages = {7},
keywords = {recidivism, ethical ai, fairness, agent based modeling},
location = {New York, NY, USA},
series = {AIES '20}
}
  
"
"@inproceedings{10.1145/3375627.3375872,
author = {Osoba, Osonde A. and Boudreaux, Benjamin and Yeung, Douglas},
title = {Steps Towards Value-Aligned Systems},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375872},
doi = {10.1145/3375627.3375872},
abstract = {Algorithmic (including AI/ML) decision-making artifacts are an established and growing part of our decision-making ecosystem. They are now indispensable tools that help us manage the flood of information we use to try to make effective decisions in a complex world. The current literature is full of examples of how individual artifacts violate societal norms and expectations (e.g. violations of fairness, privacy, or safety norms). Against this backdrop, this discussion highlights an under-emphasized perspective in the body of research focused on assessing value misalignment in AI-equipped sociotechnical systems. The research on value misalignment so far has a strong focus on the behavior of individual tech artifacts. This discussion argues for a more structured systems-level approach for assessing value-alignment in sociotechnical systems. We rely primarily on the research on fairness to make our arguments more concrete. And we use the opportunity to highlight how adopting a system perspective improves our ability to explain and address value misalignments better. Our discussion ends with an exploration of priority questions that demand attention if we are to assure the value alignment of whole systems, not just individual artifacts.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {332–336},
numpages = {5},
keywords = {sociotechnical systems, ml fairness, value alignment, systems analysis},
location = {New York, NY, USA},
series = {AIES '20}
}
  
"
"@inproceedings{10.1145/3375627.3375865,
author = {Sharma, Shubham and Zhang, Yunfeng and R\'{\i}os Aliaga, Jes\'{u}s M. and Bouneffouf, Djallel and Muthusamy, Vinod and Varshney, Kush R.},
title = {Data Augmentation for Discrimination Prevention and Bias Disambiguation},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375865},
doi = {10.1145/3375627.3375865},
abstract = {Machine learning models are prone to biased decisions due to biases in the datasets they are trained on. In this paper, we introduce a novel data augmentation technique to create a fairer dataset for model training that could also lend itself to understanding the type of bias existing in the dataset i.e. if bias arises from a lack of representation for a particular group (sampling bias) or if it arises because of human bias reflected in the labels (prejudice based bias). Given a dataset involving a protected attribute with a privileged and unprivileged group, we create an ""ideal world'' dataset: for every data sample, we create a new sample having the same features (except the protected attribute(s)) and label as the original sample but with the opposite protected attribute value. The synthetic data points are sorted in order of their proximity to the original training distribution and added successively to the real dataset to create intermediate datasets. We theoretically show that two different notions of fairness: statistical parity difference (independence) and average odds difference (separation) always change in the same direction using such an augmentation. We also show submodularity of the proposed fairness-aware augmentation approach that enables an efficient greedy algorithm. We empirically study the effect of training models on the intermediate datasets and show that this technique reduces the two bias measures while keeping the accuracy nearly constant for three datasets. We then discuss the implications of this study on the disambiguation of sample bias and prejudice based bias and discuss how pre-processing techniques should be evaluated in general. The proposed method can be used by policy makers who want to use unbiased datasets to train machine learning models for their applications to add a subset of synthetic points to an extent that they are comfortable with to mitigate unwanted bias.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {358–364},
numpages = {7},
keywords = {discrimination prevention, fairness in machine learning, responsible artificial intelligence},
location = {New York, NY, USA},
series = {AIES '20}
}
  
"
"@inproceedings{10.1145/3461702.3462516,
author = {Keswani, Vijay and Lease, Matthew and Kenthapadi, Krishnaram},
title = {Towards Unbiased and Accurate Deferral to Multiple Experts},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462516},
doi = {10.1145/3461702.3462516},
abstract = {Machine learning models are often implemented in cohort with humans in the pipeline, with the model having an option to defer to a domain expert in cases where it has low confidence in its inference. Our goal is to design mechanisms for ensuring accuracy and fairness in such prediction systems that combine machine learning model inferences and domain expert predictions. Prior work on ""deferral systems"" in classification settings has focused on the setting of a pipeline with a single expert and aimed to accommodate the inaccuracies and biases of this expert to simultaneously learn an inference model and a deferral system. Our work extends this framework to settings where multiple experts are available, with each expert having their own domain of expertise and biases. We propose a framework that simultaneously learns a classifier and a deferral system, with the deferral system choosing to defer to one or more human experts in cases of input where the classifier has low confidence. We test our framework on a synthetic dataset and a content moderation dataset with biased synthetic experts, and show that it significantly improves the accuracy and fairness of the final predictions, compared to the baselines. We also collect crowdsourced labels for the content moderation task to construct a real-world dataset for the evaluation of hybrid machine-human frameworks and show that our proposed framework outperforms baselines on this real-world dataset as well.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {154–165},
numpages = {12},
keywords = {fairness, deferral models, hybrid human-machine frameworks},
location = {Virtual Event, USA},
series = {AIES '21}
}
  
"
"@inproceedings{10.1145/3461702.3462585,
author = {Belitz, Clara and Jiang, Lan and Bosch, Nigel},
title = {Automating Procedurally Fair Feature Selection in Machine Learning},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462585},
doi = {10.1145/3461702.3462585},
abstract = {In recent years, machine learning has become more common in everyday applications. Consequently, numerous studies have explored issues of unfairness against specific groups or individuals in the context of these applications. Much of the previous work on unfairness in machine learning has focused on the fairness of outcomes rather than process. We propose a feature selection method inspired by fair process (procedural fairness) in addition to fair outcome. Specifically, we introduce the notion of unfairness weight, which indicates how heavily to weight unfairness versus accuracy when measuring the marginal benefit of adding a new feature to a model. Our goal is to maintain accuracy while reducing unfairness, as defined by six common statistical definitions. We show that this approach demonstrably decreases unfairness as the unfairness weight is increased, for most combinations of metrics and classifiers used. A small subset of all the combinations of datasets (4), unfairness metrics (6), and classifiers (3), however, demonstrated relatively low unfairness initially. For these specific combinations, neither unfairness nor accuracy were affected as unfairness weight changed, demonstrating that this method does not reduce accuracy unless there is also an equivalent decrease in unfairness. We also show that this approach selects unfair features and sensitive features for the model less frequently as the unfairness weight increases. As such, this procedure is an effective approach to constructing classifiers that both reduce unfairness and are less likely to include unfair features in the modeling process.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {379–389},
numpages = {11},
keywords = {machine learning, bias, feature selection, fairness},
location = {Virtual Event, USA},
series = {AIES '21}
}
  
"
"@inproceedings{10.1145/3461702.3462614,
author = {Islam, Rashidul and Pan, Shimei and Foulds, James R.},
title = {Can We Obtain Fairness For Free?},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462614},
doi = {10.1145/3461702.3462614},
abstract = {There is growing awareness that AI and machine learning systems can in some cases learn to behave in unfair and discriminatory ways with harmful consequences. However, despite an enormous amount of research, techniques for ensuring AI fairness have yet to see widespread deployment in real systems. One of the main barriers is the conventional wisdom that fairness brings a cost in predictive performance metrics such as accuracy which could affect an organization's bottom-line. In this paper we take a closer look at this concern. Clearly fairness/performance trade-offs exist, but are they inevitable? In contrast to the conventional wisdom, we find that it is frequently possible, indeed straightforward, to improve on a trained model's fairness without sacrificing predictive performance. We systematically study the behavior of fair learning algorithms on a range of benchmark datasets, showing that it is possible to improve fairness to some degree with no loss (or even an improvement) in predictive performance via a sensible hyper-parameter selection strategy. Our results reveal a pathway toward increasing the deployment of fair AI methods, with potentially substantial positive real-world impacts.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {586–596},
numpages = {11},
keywords = {deployment of fairness techniques, fairness/performance trade-offs, fairness in AI, practical barriers, AI and society},
location = {Virtual Event, USA},
series = {AIES '21}
}
  
"
"@inproceedings{10.1145/3461702.3462623,
author = {Jiang, Weijie and Pardos, Zachary A.},
title = {Towards Equity and Algorithmic Fairness in Student Grade Prediction},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462623},
doi = {10.1145/3461702.3462623},
abstract = {Equity of educational outcome and fairness of AI with respect to race have been topics of increasing importance in education. In this work, we address both with empirical evaluations of grade prediction in higher education, an important task to improve curriculum design, plan interventions for academic support, and offer course guidance to students. With fairness as the aim, we trial several strategies for both label and instance balancing to attempt to minimize differences in algorithm performance with respect to race. We find that an adversarial learning approach, combined with grade label balancing, achieved by far the fairest results. With equity of educational outcome as the aim, we trial strategies for boosting predictive performance on historically underserved groups and find success in sampling those groups in inverse proportion to their historic outcomes. With AI-infused technology supports increasingly prevalent on campuses, our methodologies fill a need for frameworks to consider performance trade-offs with respect to sensitive student attributes and allow institutions to instrument their AI resources in ways that are attentive to equity and fairness.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {608–617},
numpages = {10},
keywords = {grade prediction, equity, higher education, fairness},
location = {Virtual Event, USA},
series = {AIES '21}
}
  
"
"@inproceedings{10.1145/3461702.3462572,
author = {Lee, Michelle Seng Ah and Singh, Jatinder},
title = {Risk Identification Questionnaire for Detecting Unintended Bias in the Machine Learning Development Lifecycle},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462572},
doi = {10.1145/3461702.3462572},
abstract = {Unintended biases in machine learning (ML) models have the potential to introduce undue discrimination and exacerbate social inequalities. The research community has proposed various technical and qualitative methods intended to assist practitioners in assessing these biases. While frameworks for identifying the risks of harm due to unintended biases have been proposed, they have not yet been operationalised into practical tools to assist industry practitioners.In this paper, we link prior work on bias assessment methods to phases of a standard organisational risk management process (RMP), noting a gap in measures for helping practitioners identify bias- related risks. Targeting this gap, we introduce a bias identification methodology and questionnaire, illustrating its application through a real-world, practitioner-led use case. We validate the need and usefulness of the questionnaire through a survey of industry practitioners, which provides insights into their practical requirements and preferences. Our results indicate that such a questionnaire is helpful for proactively uncovering unexpected bias concerns, particularly where it is easy to integrate into existing processes, and facilitates communication with non-technical stakeholders. Ultimately, the effective end-to-end management of ML risks requires a more targeted identification of potential harm and its sources, so that appropriate mitigation strategies can be formulated. Towards this, our questionnaire provides a practical means to assist practitioners in identifying bias-related risks.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {704–714},
numpages = {11},
keywords = {fair ML, questionnaire, risk identification, risk management, algorithmic bias, algorithmic fairness},
location = {Virtual Event, USA},
series = {AIES '21}
}
  
"
"@inproceedings{10.1145/3461702.3462629,
author = {Perrone, Valerio and Donini, Michele and Zafar, Muhammad Bilal and Schmucker, Robin and Kenthapadi, Krishnaram and Archambeau, C\'{e}dric},
title = {Fair Bayesian Optimization},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462629},
doi = {10.1145/3461702.3462629},
abstract = {Given the increasing importance of machine learning (ML) in our lives, several algorithmic fairness techniques have been proposed to mitigate biases in the outcomes of the ML models. However, most of these techniques are specialized to cater to a single family of ML models and a specific definition of fairness, limiting their adaptibility in practice. We introduce a general constrained Bayesian optimization (BO) framework to optimize the performance of any ML model while enforcing one or multiple fairness constraints. BO is a model-agnostic optimization method that has been successfully applied to automatically tune the hyperparameters of ML models. We apply BO with fairness constraints to a range of popular models, including random forests, gradient boosting, and neural networks, showing that we can obtain accurate and fair solutions by acting solely on the hyperparameters. We also show empirically that our approach is competitive with specialized techniques that enforce model-specific fairness constraints, and outperforms preprocessing methods that learn fair representations of the input data. Moreover, our method can be used in synergy with such specialized fairness techniques to tune their hyperparameters. Finally, we study the relationship between fairness and the hyperparameters selected by BO. We observe a correlation between regularization and unbiased models, explaining why acting on the hyperparameters leads to ML models that generalize well and are fair.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {854–863},
numpages = {10},
keywords = {Bayesian optimization, fairness, bias, autoML, hyperparameter tuning},
location = {Virtual Event, USA},
series = {AIES '21}
}
  
"
"@inproceedings{10.1145/3461702.3462603,
author = {Prost, Flavien and Awasthi, Pranjal and Blumm, Nick and Kumthekar, Aditee and Potter, Trevor and Wei, Li and Wang, Xuezhi and Chi, Ed H. and Chen, Jilin and Beutel, Alex},
title = {Measuring Model Fairness under Noisy Covariates: A Theoretical Perspective},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462603},
doi = {10.1145/3461702.3462603},
abstract = {In this work we study the problem of measuring the fairness of a machine learning model under noisy information. Focusing on group fairness metrics, we investigate the particular but common situation when the evaluation requires controlling for the confounding effect of covariate variables. In a practical setting, we might not be able to jointly observe the covariate and group information, and a standard workaround is to then use proxies for one or more of these variables. Prior works have demonstrated the challenges with using a proxy for sensitive attributes, and strong independence assumptions are needed to provide guarantees on the accuracy of the noisy estimates. In contrast, in this work we study using a proxy for the covariate variable and present a theoretical analysis that aims to characterize weaker conditions under which accurate fairness evaluation is possible. Furthermore, our theory identifies potential sources of errors and decouples them into two interpretable parts y and E. The first part y depends solely on the performance of the proxy such as precision and recall, whereas the second part E captures correlations between all the variables of interest. We show that in many scenarios the error in the estimates is dominated by y via a linear dependence, whereas the dependence on the correlations E only constitutes a lower order term. As a result we expand the understanding of scenarios where measuring model fairness via proxies can be an effective approach. Finally, we compare, via simulations, the theoretical upper-bounds to the distribution of simulated estimation errors and show that assuming some structure on the data, even weak, is key to significantly improve both theoretical guarantees and empirical results.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {873–883},
numpages = {11},
keywords = {ml fairness, statistical parity, noisy covariates},
location = {Virtual Event, USA},
series = {AIES '21}
}
  
"
"@inproceedings{10.1145/3461702.3462594,
author = {Schumann, Candice and Ricco, Susanna and Prabhu, Utsav and Ferrari, Vittorio and Pantofaru, Caroline},
title = {A Step Toward More Inclusive People Annotations for Fairness},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462594},
doi = {10.1145/3461702.3462594},
abstract = {The Open Images Dataset contains approximately 9 million images and is a widely accepted dataset for computer vision research. As is common practice for large datasets, the annotations are not exhaustive, with bounding boxes and attribute labels for only a subset of the classes in each image. In this paper, we present a new set of annotations on a subset of the Open Images dataset called the MIAP (More Inclusive Annotations for People) subset, containing bounding boxes and attributes for all of the people visible in those images. The attributes and labeling methodology for the MIAP subset were designed to enable research into model fairness. In addition, we analyze the original annotation methodology for the person class and its subclasses, discussing the resulting patterns in order to inform future annotation efforts. By considering both the original and exhaustive annotation sets, researchers can also now study how systematic patterns in training annotations affect modeling.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {916–925},
numpages = {10},
keywords = {fairness, datasets, computer vision},
location = {Virtual Event, USA},
series = {AIES '21}
}
  
"
"@inproceedings{10.1145/3514094.3534147,
author = {Li, Nianyun and Goel, Naman and Ash, Elliott},
title = {Data-Centric Factors in Algorithmic Fairness},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534147},
doi = {10.1145/3514094.3534147},
abstract = {Notwithstanding the widely held view that data generation and data curation processes are prominent sources of bias in machine learning algorithms, there is little empirical research seeking to document and understand the specific data dimensions affecting algorithmic unfairness. Contra the previous work, which has focused on modeling using simple, small-scale benchmark datasets, we hold the model constant and methodically intervene on relevant dimensions of a much larger, more diverse dataset. For this purpose, we introduce a new dataset on recidivism in 1.5 million criminal cases from courts in the U.S. state of Wisconsin, 2000-2018. From this main dataset, we generate multiple auxiliary datasets to simulate different kinds of biases in the data. Focusing on algorithmic bias toward different race/ethnicity groups, we assess the relevance of training data size, base rate difference between groups, representation of groups in the training data, temporal aspects of data curation, including race/ethnicity or neighborhood characteristics as features, and training separate classifiers by race/ethnicity or crime type. We find that these factors often do influence fairness metrics holding the classifier specification constant, without having a corresponding effect on accuracy metrics. The methodology and the results in the paper provide a useful reference point for a data-centric approach to studying algorithmic fairness in recidivism prediction and beyond.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {396–410},
numpages = {15},
keywords = {algorithmic fairness, datasets, recidivism prediction, machine learning},
location = {Oxford, United Kingdom},
series = {AIES '22}
}
  
"
"@inproceedings{10.1145/3514094.3534126,
author = {Robertson, Jake and Stinson, Catherine and Hu, Ting},
title = {A Bio-Inspired Framework for Machine Bias Interpretation},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534126},
doi = {10.1145/3514094.3534126},
abstract = {Machine learning algorithms use the past and the present to predict the future. But when given biased historical data, these algorithms can quickly become discriminatory. The area of machine learning fairness has emerged to detect and de-bias these algorithms, but has received widespread criticism for its one-size-fits-all approach, which allows certain cases of bias to slip through the cracks. In this study, we take a deeper look at the mechanisms by which machine learning algorithms develop harmful bias. We introduce a new method to interpret discriminatory systems, an Evolutionary algorithm for Feature Interaction (EFI), which we apply to several commonly used machine learning algorithms in two real-world problem instances: violent crime and median house price prediction. In the results, we discover several complex forms of bias including the encoding of race through other seemingly unrelated attributes. Ultimately we suggest that more informative interpretation tools such as EFI can be used to not only explain machine learning outcomes, but supplement and improve existing machine bias detection approaches to provide a more robust and in-depth ethical evaluation of machine learning algorithms.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {588–598},
numpages = {11},
keywords = {feature interaction, interpretability, fairness, machine bias, feature importance},
location = {Oxford, United Kingdom},
series = {AIES '22}
}
  
"
"@inproceedings{10.1145/3514094.3539561,
author = {Pav\'{o}n P\'{e}rez, \'{A}ngel},
title = {Bias in Artificial Intelligence Models in Financial Services},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3539561},
doi = {10.1145/3514094.3539561},
abstract = {Nowadays, artificial intelligence models are widely used in financial services, from credit scoring to fraud detection, having a direct impact on our daily lives. Although such models have been developed to try to reduce human bias and thus bring greater fairness to financial services decisions, studies have found that there is still significant discrimination by both face-to-face and algorithmic lenders. In fact, Apple has recently been investigated for gender discrimination in assigning a credit limit to its users, demonstrating that there may still be inherent biases in the development of such algorithms and models. Furthermore, biases in financial services models may not only lead to unfair discrimination but were also linked to health problems and recovery prospects.This project aims to analyse and identify the different types of biases found in AI models and data used in the financial services industry. We propose a method using data analysis and explainable models to explain how these biases emerge throughout the process of developing AI models as well as applying state-of-the-art bias dealing techniques to avoid and mitigate them. Finally, we propose how to evaluate these models according to the business objectives and consider possible trade-offs between different definitions of fairness. Thus, the main questions that this project will try to answer are as follows: - What are the current biases in credit risk and fraud detection models and how to identify them? - In what ways understanding how biases emerge from the data can help us in bias mitigation? - To what extent could credit risk and fraud detection models bias be mitigated, and what are the implications of those mitigation techniques?Answering these questions, we hope to create a pipeline for building these models by understanding the key points where bias can emerge and the appropriate methods to avoid it.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {908},
numpages = {1},
keywords = {bias, machine learning, financial services, fairness},
location = {Oxford, United Kingdom},
series = {AIES '22}
}
  
"
"@inproceedings{10.1145/3551624.3555289,
author = {Singh, Shubham and Shah, Bhuvni and Kanich, Chris and Kash, Ian A.},
title = {Fair Decision-Making for Food Inspections},
year = {2022},
isbn = {9781450394772},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551624.3555289},
doi = {10.1145/3551624.3555289},
abstract = {We revisit the application of predictive models by the Chicago Department of Public Health to schedule restaurant inspections and prioritize the detection of critical food code violations. We perform the first analysis of the model’s fairness to the population served by the restaurants in terms of average time to find a critical violation. We find that the model treats inspections unequally based on the sanitarian who conducted the inspection and that, in turn, there are geographic disparities in the benefits of the model. We examine four alternate methods of model training and two alternative ways of scheduling using the model and find that the latter generate more desirable results. The challenges from this application point to important directions for future work around fairness with collective entities rather than individuals, the use of critical violations as a proxy, and the disconnect between fair classification and fairness in dynamic scheduling systems.},
booktitle = {Equity and Access in Algorithms, Mechanisms, and Optimization},
articleno = {5},
numpages = {11},
keywords = {fairness, food inspections, scheduling},
location = {Arlington, VA, USA},
series = {EAAMO '22}
}
  
"
"@inproceedings{10.1145/3551624.3555302,
author = {Andrews, Kenya and Ohannessian, Mesrob and Berger-Wolf, Tanya},
title = {Modeling Access Differences to Reduce Disparity in Resource Allocation},
year = {2022},
isbn = {9781450394772},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551624.3555302},
doi = {10.1145/3551624.3555302},
abstract = {Motivated by COVID-19 vaccine allocation, where vulnerable subpopulations are simultaneously more impacted in terms of health and more disadvantaged in terms of access to the vaccine, we formalize and study the problem of resource allocation when there are inherent access differences that correlate with advantage and disadvantage. We identify reducing resource disparity as a key goal in this context and show its role as a proxy to more nuanced downstream impacts. We develop a concrete access model that helps quantify how a given allocation translates to resource flow for the advantaged vs. the disadvantaged, based on the access gap between them. We then provide a methodology for access-aware allocation. Intuitively, the resulting allocation leverages more vaccines in locations with higher vulnerable populations to mitigate the access gap and reduce overall disparity. Surprisingly, knowledge of the access gap is often not needed to perform access-aware allocation. To support this formalism, we provide empirical evidence for our access model and show that access-aware allocation can significantly reduce resource disparity and thus improve downstream outcomes. We demonstrate this at various scales, including at county, state, national, and global levels.},
booktitle = {Equity and Access in Algorithms, Mechanisms, and Optimization},
articleno = {17},
numpages = {11},
location = {Arlington, VA, USA},
series = {EAAMO '22}
}
  
"
"@inproceedings{NEURIPS2018_09d37c08,
 author = {Madras, David and Pitassi, Toni and Zemel, Richard},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Predict Responsibly: Improving Fairness and Accuracy by Learning to Defer},
 url = {https://proceedings.neurips.cc/paper/2018/file/09d37c08f7b129e96277388757530c72-Paper.pdf},
 volume = {31},
 year = {2018}
}

"
"@inproceedings{NEURIPS2018_1f1baa5b,
 author = {Chen, Irene and Johansson, Fredrik D and Sontag, David},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Why Is My Classifier Discriminatory?},
 url = {https://proceedings.neurips.cc/paper/2018/file/1f1baa5b8edac74eb4eaa329f14a0361-Paper.pdf},
 volume = {31},
 year = {2018}
}

"
"@inproceedings{NEURIPS2018_6cd9313e,
 author = {Yeom, Samuel and Datta, Anupam and Fredrikson, Matt},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Hunting for Discriminatory Proxies in Linear Regression Models},
 url = {https://proceedings.neurips.cc/paper/2018/file/6cd9313ed34ef58bad3fdd504355e72c-Paper.pdf},
 volume = {31},
 year = {2018}
}

"
"@inproceedings{NEURIPS2019_201d5469,
 author = {Tan, Yi Chern and Celis, L. Elisa},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Assessing Social and Intersectional Biases in Contextualized Word Representations},
 url = {https://proceedings.neurips.cc/paper/2019/file/201d546992726352471cfea6b0df0a48-Paper.pdf},
 volume = {32},
 year = {2019}
}

"
"@inproceedings{NEURIPS2019_fc0de4e0,
 author = {Bagdasaryan, Eugene and Poursaeed, Omid and Shmatikov, Vitaly},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Differential Privacy Has Disparate Impact on Model Accuracy},
 url = {https://proceedings.neurips.cc/paper/2019/file/fc0de4e0396fff257ea362983c2dda5a-Paper.pdf},
 volume = {32},
 year = {2019}
}

"
"@inproceedings{NEURIPS2020_37d097ca,
 author = {Wang, Serena and Guo, Wenshuo and Narasimhan, Harikrishna and Cotter, Andrew and Gupta, Maya and Jordan, Michael},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {5190--5203},
 publisher = {Curran Associates, Inc.},
 title = {Robust Optimization for Fairness with Noisy Protected Groups},
 url = {https://proceedings.neurips.cc/paper/2020/file/37d097caf1299d9aa79c2c2b843d2d78-Paper.pdf},
 volume = {33},
 year = {2020}
}

"
"@inproceedings{NEURIPS2021_18de4beb,
 author = {Venkatesh, Praveen and Dutta, Sanghamitra and Mehta, Neil and Grover, Pulkit},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {3149--3162},
 publisher = {Curran Associates, Inc.},
 title = {Can Information Flows Suggest Targets for Interventions in Neural Circuits?},
 url = {https://proceedings.neurips.cc/paper/2021/file/18de4beb01f6a17b6e1dfb9813ba6045-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
"@inproceedings{NEURIPS2021_32e54441,
 author = {Ding, Frances and Hardt, Moritz and Miller, John and Schmidt, Ludwig},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {6478--6490},
 publisher = {Curran Associates, Inc.},
 title = {Retiring Adult: New Datasets for Fair Machine Learning},
 url = {https://proceedings.neurips.cc/paper/2021/file/32e54441e6382a7fbacbbbaf3c450059-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
"@inproceedings{NEURIPS2021_85dca1d2,
 author = {Zhang, Yiliang and Long, Qi},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {16007--16019},
 publisher = {Curran Associates, Inc.},
 title = {Assessing Fairness in the Presence of Missing Data},
 url = {https://proceedings.neurips.cc/paper/2021/file/85dca1d270f7f9aef00c9d372f114482-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
"@inproceedings{NEURIPS2021_91e50fe1,
 author = {Liu, Yang and Wang, Jialu},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {17467--17479},
 publisher = {Curran Associates, Inc.},
 title = {Can Less be More? When Increasing-to-Balancing Label Noise Rates Considered Beneficial},
 url = {https://proceedings.neurips.cc/paper/2021/file/91e50fe1e39af2869d3336eaaeebdb43-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
"@inproceedings{NEURIPS2021_ba9fab00,
 author = {van Breugel, Boris and Kyono, Trent and Berrevoets, Jeroen and van der Schaar, Mihaela},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {22221--22233},
 publisher = {Curran Associates, Inc.},
 title = {DECAF:  Generating Fair Synthetic Data Using Causally-Aware Generative Networks},
 url = {https://proceedings.neurips.cc/paper/2021/file/ba9fab001f67381e56e410575874d967-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
"@inproceedings{NEURIPS2021_e7e8f8e5,
 author = {Tran, Cuong and Dinh, My and Fioretto, Ferdinando},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {27555--27565},
 publisher = {Curran Associates, Inc.},
 title = {Differentially Private Empirical Risk Minimization under the Fairness Lens},
 url = {https://proceedings.neurips.cc/paper/2021/file/e7e8f8e5982b3298c8addedf6811d500-Paper.pdf},
 volume = {34},
 year = {2021}
}



@inproceedings{
wei2022learning,
title={Learning with Noisy Labels Revisited: A Study Using Real-World Human Annotations},
author={Jiaheng Wei and Zhaowei Zhu and Hao Cheng and Tongliang Liu and Gang Niu and Yang Liu},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=TBWA6PLJZQm}
}

"@inproceedings{
balakrishnan2022diverse,
title={Diverse Client Selection for Federated Learning via Submodular Maximization},
author={Ravikumar Balakrishnan and Tian Li and Tianyi Zhou and Nageen Himayat and Virginia Smith and Jeff Bilmes},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=nwKXyFvaUm}
}"
"@inproceedings{
vu2022distributionally,
title={Distributionally Robust Fair Principal Components via Geodesic Descents},
author={Hieu Vu and Toan Tran and Man-Chung Yue and Viet Anh Nguyen},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=9NVd-DMtThY}
}"
"@inproceedings{10.1145/3461702.3462596,
author = {Biswas, Arpita and Mukherjee, Suvam},
title = {Ensuring Fairness under Prior Probability Shifts},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462596},
doi = {10.1145/3461702.3462596},
abstract = {Prior probability shift is a phenomenon where the training and test datasets differ structurally within population subgroups. This phenomenon can be observed in the yearly records of several real-world datasets, for example, recidivism records and medical expenditure surveys. If unaccounted for, such shifts can cause the predictions of a classifier to become unfair towards specific population subgroups. While the fairness notion called Proportional Equality (PE) accounts for such shifts, a procedure to ensure PE-fairness was unknown. In this work, we design an algorithm, called CAPE, that ensures fair classification under such shifts. We introduce a metric, called prevalence difference, which CAPE attempts to minimize in order to achieve fairness under prior probability shifts. We theoretically establish that this metric exhibits several properties that are desirable for a fair classifier. We evaluate the efficacy of CAPE via a thorough empirical evaluation on synthetic datasets. We also compare the performance of CAPE with several state-of-the-art fair classifiers on real-world datasets like COMPAS (criminal risk assessment) and MEPS (medical expenditure panel survey). The results indicate that CAPE ensures a high degree of PE-fairness in its predictions, while performing well on other important metrics.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {414–424},
numpages = {11},
keywords = {classification, algorithmic fairness, distributional shifts, discrimination},
location = {Virtual Event, USA},
series = {AIES '21}
}
  
"
"@inproceedings{10.1145/3461702.3462561,
author = {Pandey, Akshat and Caliskan, Aylin},
title = {Disparate Impact of Artificial Intelligence Bias in Ridehailing Economy's Price Discrimination Algorithms},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462561},
doi = {10.1145/3461702.3462561},
abstract = {Ridehailing applications that collect mobility data from individuals to inform smart city planning predict each trip's fare pricing with automated algorithms that rely on artificial intelligence (AI). This type of AI algorithm, namely a price discrimination algorithm, is widely used in the industry's black box systems for dynamic individualized pricing. Lacking transparency, studying such AI systems for fairness and disparate impact has not been possible without access to data used in generating the outcomes of price discrimination algorithms. Recently, in an effort to enhance transparency in city planning, the city of Chicago regulation mandated that transportation providers publish anonymized data on ridehailing. As a result, we present the first large-scale measurement of the disparate impact of price discrimination algorithms used by ridehailing applications.The application of random effects models from the meta-analysis literature combines the city-level effects of AI bias on fare pricing from census tract attributes, aggregated from the American Community Survey. An analysis of 100 million ridehailing samples from the city of Chicago indicates a significant disparate impact in fare pricing of neighborhoods due to AI bias learned from ridehailing utilization patterns associated with demographic attributes. Neighborhoods with larger non-white populations, higher poverty levels, younger residents, and high education levels are significantly associated with higher fare prices, with combined effect sizes, measured in Cohen's d, of -0.32, -0.28, 0.69, and 0.24 for each demographic, respectively. Further, our methods hold promise for identifying and addressing the sources of disparate impact in AI algorithms learning from datasets that contain U.S. geolocations.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {822–833},
numpages = {12},
keywords = {prediction, algorithmic bias, disparate impact, price discrimination, AI ethics, geolocation},
location = {Virtual Event, USA},
series = {AIES '21}
}
  
"
"@inproceedings{10.1145/3461702.3462590,
author = {Park, Joon Sung and Bernstein, Michael S. and Brewer, Robin N. and Kamar, Ece and Morris, Meredith Ringel},
title = {Understanding the Representation and Representativeness of Age in AI Data Sets},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462590},
doi = {10.1145/3461702.3462590},
abstract = {A diverse representation of different demographic groups in AI training data sets is important in ensuring that the models will work for a large range of users. To this end, recent efforts in AI fairness and inclusion have advocated for creating AI data sets that are well-balanced across race, gender, socioeconomic status, and disability status. In this paper, we contribute to this line of work by focusing on the representation of age by asking whether older adults are represented proportionally to the population at large in AI data sets. We examine publicly-available information about 92 face data sets to understand how they codify age as a case study to investigate how the subjects' ages are recorded and whether older generations are represented. We find that older adults are very under-represented; five data sets in the study that explicitly documented the closed age intervals of their subjects included older adults (defined as older than 65 years), while only one included oldest-old adults (defined as older than 85 years). Additionally, we find that only 24 of the data sets include any age-related information in their documentation or metadata, and that there is no consistent method followed across these data sets to collect and record the subjects' ages. We recognize the unique difficulties in creating representative data sets in terms of age, but raise it as an important dimension that researchers and engineers interested in inclusive AI should consider.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {834–842},
numpages = {9},
keywords = {AI fate, datasets, accessibility, older adults, representation, inclusion, aging},
location = {Virtual Event, USA},
series = {AIES '21}
}
  
"
"@inproceedings{10.1145/3514094.3534173,
author = {Akpinar, Nil-Jana and DiCiccio, Cyrus and Nandy, Preetam and Basu, Kinjal},
title = {Long-Term Dynamics of Fairness Intervention in Connection Recommender Systems},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534173},
doi = {10.1145/3514094.3534173},
abstract = {Recommender system fairness has been studied from the perspectives of a variety of stakeholders including content producers, the content itself and recipients of recommendations. Regardless of which type of stakeholders are considered, most works in this area assess the efficacy of fairness intervention by evaluating a single fixed fairness criterion through the lens of a one-shot, static setting. Yet recommender systems constitute dynamical systems with feedback loops from the recommendations to the underlying population distributions which could lead to unforeseen and adverse consequences if not taken into account. In this paper, we study a connection recommender system patterned after the systems employed by web-scale social networks and analyze the long-term effects of intervening on fairness in the recommendations. We find that, although seemingly fair in aggregate, common exposure and utility parity interventions fail to mitigate amplification of biases in the long term. We theoretically characterize how certain fairness interventions impact the bias amplification dynamics in a stylized Polya urn model.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {22–35},
numpages = {14},
keywords = {recommender systems, fairness, machine learning},
location = {Oxford, United Kingdom},
series = {AIES '22}
}
  
"
"@inproceedings{10.1145/3514094.3534184,
author = {Butcher, Bradley and Robinson, Chris and Zilka, Miri and Fogliato, Riccardo and Ashurst, Carolyn and Weller, Adrian},
title = {Racial Disparities in the Enforcement of Marijuana Violations in the US},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534184},
doi = {10.1145/3514094.3534184},
abstract = {Racial disparities in US drug arrest rates have been observed for decades, but their causes and policy implications are still contested. Some have argued that the disparities largely reflect differences in drug use between racial groups, while others have hypothesized that discriminatory enforcement policies and police practices play a significant role. In this work, we analyze racial disparities in the enforcement of marijuana violations in the US. Using data from the National Incident-Based Reporting System (NIBRS) and the National Survey on Drug Use and Health (NSDUH) programs, we investigate whether marijuana usage and purchasing behaviors can explain the racial composition of offenders in police records. We examine potential driving mechanisms behind these disparities and the extent to which county-level socioeconomic factors are associated with corresponding disparities. Our results indicate that the significant racial disparities in reported incidents and arrests cannot be explained by differences in marijuana days-of-use alone. Variations in the location where marijuana is purchased and in the frequency of these purchases partially explain the observed disparities. We observe an increase in racial disparities across most counties over the last decade, with the greatest increases in states that legalized the use of marijuana within this timeframe. Income, high school graduation rate, and rate of employment positively correlate with larger racial disparities, while the rate of incarceration is negatively correlated. We conclude with a discussion of the implications of the observed racial disparities in the context of algorithmic fairness.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {130–143},
numpages = {14},
keywords = {marijuana, law enforcement, racial disparities},
location = {Oxford, United Kingdom},
series = {AIES '22}
}
  
"
"@inproceedings{10.1145/3514094.3534162,
author = {Caliskan, Aylin and Ajay, Pimparkar Parth and Charlesworth, Tessa and Wolfe, Robert and Banaji, Mahzarin R.},
title = {Gender Bias in Word Embeddings: A Comprehensive Analysis of Frequency, Syntax, and Semantics},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534162},
doi = {10.1145/3514094.3534162},
abstract = {Word embeddings are numeric representations of meaning derived from word co-occurrence statistics in corpora of human-produced texts. The statistical regularities in language corpora encode well-known social biases into word embeddings (e.g., the word vector for family is closer to the vector women than to men). Although efforts have been made to mitigate bias in word embeddings, with the hope of improving fairness in downstream Natural Language Processing (NLP) applications, these efforts will remain limited until we more deeply understand the multiple (and often subtle) ways that social biases can be reflected in word embeddings. Here, we focus on gender to provide a comprehensive analysis of group-based biases in widely-used static English word embeddings trained on internet corpora (GloVe 2014, fastText 2017). While some previous research has helped uncover biases in specific semantic associations between a group and a target domain (e.g., women - family), using the Single-Category Word Embedding Association Test, we demonstrate the widespread prevalence of gender biases that also show differences in: (1) frequencies of words associated with men versus women; (b) part-of-speech tags in gender-associated words; (c) semantic categories in gender-associated words; and (d) valence, arousal, and dominance in gender-associated words. We leave the analysis of non-binary gender to future work due to the challenges in accurate group representation caused by limitations inherent in data.First, in terms of word frequency: we find that, of the 1,000 most frequent words in the vocabulary, 77% are more associated with men than women, providing direct evidence of a masculine default in the everyday language of the English-speaking world. Second, turning to parts-of-speech: the top male-associated words are typically verbs (e.g., fight, overpower) while the top female-associated words are typically adjectives and adverbs (e.g., giving, emotionally). Gender biases in embeddings also permeate parts-of-speech. Third, for semantic categories: bottom-up, cluster analyses of the top 1,000 words associated with each gender. The top male-associated concepts include roles and domains of big tech, engineering, religion, sports, and violence; in contrast, the top female-associated concepts are less focused on roles, including, instead, female-specific slurs and sexual content, as well as appearance and kitchen terms. Fourth, using human ratings of word valence, arousal, and dominance from a ~20,000 word lexicon, we find that male-associated words are higher on arousal and dominance, while female-associated words are higher on valence. Ultimately, these findings move the study of gender bias in word embeddings beyond the basic investigation of semantic relationships to also study gender differences in multiple manifestations in text. Given the central role of word embeddings in NLP applications, it is essential to more comprehensively document where biases exist and may remain hidden, allowing them to persist without our awareness throughout large text corpora.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {156–170},
numpages = {15},
keywords = {word embeddings, ai bias, masculine default, representation, gender bias, psycholinguistics},
location = {Oxford, United Kingdom},
series = {AIES '22}
}
  
"
"@inproceedings{10.1145/3514094.3534157,
author = {Ghosh, Avijit and Shanbhag, Aalok and Wilson, Christo},
title = {FairCanary: Rapid Continuous Explainable Fairness},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534157},
doi = {10.1145/3514094.3534157},
abstract = {Systems that offer continuous model monitoring have emerged in response to (1) well-documented failures of deployed Machine Learning (ML) and Artificial Intelligence (AI) models and (2) new regulatory requirements impacting these models. Existing monitoring systems continuously track the performance of deployed ML models and compute feature importance (a.k.a. explanations) for each prediction to help developers identify the root causes of emergent model performance problems.We present Quantile Demographic Drift (QDD), a novel model bias quantification metric that uses quantile binning to measure differences in the overall prediction distributions over subgroups. QDD is ideal for continuous monitoring scenarios, does not suffer from the statistical limitations of conventional threshold-based bias metrics, and does not require outcome labels (which may not be available at runtime). We incorporate QDD into a continuous model monitoring system, called FairCanary, that reuses existing explanations computed for each individual prediction to quickly compute explanations for the QDD bias metrics. This optimization makes FairCanary an order of magnitude faster than previous work that has tried to generate feature-level bias explanations.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {307–316},
numpages = {10},
keywords = {fairness, continuous measurement, model explanation, drift},
location = {Oxford, United Kingdom},
series = {AIES '22}
}
  
"
"@inproceedings{10.1145/3514094.3534142,
author = {Li, Zhuoyan and Lu, Zhuoran and Yin, Ming},
title = {Towards Better Detection of Biased Language with Scarce, Noisy, and Biased Annotations},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534142},
doi = {10.1145/3514094.3534142},
abstract = {Biased language is prevalent in today's online social media. To reduce the amount of online biased language, one critical first step is to accurately detect such biased language, ideally automatically. This is a challenging problem, however, as the annotated data necessary for training a biased language classifier is either scarce and costly (e.g., when collected from experts), or noisy and potentially biased on their own (e.g., when collected from crowd workers). The biased language classifier built based on these annotations may thus be inaccurate, and sometimes unfair (e.g., have systematic accuracy disparities across texts with different political leanings). In this paper, we propose a novel method, CLEARE, for biased language detection, in which we utilize self-supervised contrastive learning to enhance the biased language classifier---we learn a robust encoder of the textual data through solving a min-max optimization problem, so that the encoder could help achieve the best classification performance even if the worst data augmentation strategy is selected. Extensive evaluations suggest that CLEARE shows substantial improvements compared to the state-of-art biased language detection methods on several benchmark datasets, in terms of improving both the accuracy and the fairness of the detection.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {411–423},
numpages = {13},
keywords = {biased language, contrastive learning, bias detection, fairness},
location = {Oxford, United Kingdom},
series = {AIES '22}
}
  
"
"@inproceedings{10.1145/3514094.3534194,
author = {Shimao, Hajime and Khern-am-nuai, Warut and Kannan, Karthik and Cohen, Maxime C.},
title = {Strategic Best Response Fairness in Fair Machine Learning},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534194},
doi = {10.1145/3514094.3534194},
abstract = {While artificial intelligence (AI) and machine learning (ML) have been increasingly used for decision-making, issues related to discrimination in AI/ML have become prominent. While several fair algorithms are proposed to alleviate these discrimination issues, most of them provide fairness by imposing constraints to eliminate disparity in prediction results. However, the use of these fair algorithms may change the behavior of prediction subjects. As such, even though the disparity in prediction results might be removed by fair algorithms, behavioral responses to the use of fair algorithms can still create disparity in behavior which may persist across different groups of prediction subjects. To study this issue, we define a notion called ""strategic best-response fairness"" (SBR-fair). It is defined in a context that includes different groups of prediction subjects who are ex-ante identical in terms of abilities and conditional payoffs. We utilize a game-theoretic model to investigate whether different types of fair algorithms lead to identical equilibrium behaviors among different groups of prediction subjects. If yes, such an algorithm is considered SBR-fair. We then demonstrate that many existing fair algorithms are not SBR-fair. As a result, implementing these algorithms may impose fairness on prediction results but actually induce disparity between privileged and unprivileged individuals in the long run.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {664},
numpages = {1},
keywords = {game-theoretic model, strategic best response, fair machine learning},
location = {Oxford, United Kingdom},
series = {AIES '22}
}
  
"

"@inproceedings{10.1145/3514094.3534169,
author = {Yik, William and Serafini, Limnanthes and Lindsey, Timothy and Monta\~{n}ez, George D.},
title = {Identifying Bias in Data Using Two-Distribution Hypothesis Tests},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534169},
doi = {10.1145/3514094.3534169},
abstract = {As machine learning models become more widely used in important decision-making processes, the need for identifying and mitigating potential sources of bias has increased substantially. Using two-distribution (specified complexity) hypothesis tests, we identify biases in training data with respect to proposed distributions and without the need to train a model, distinguishing our methods from common output-based fairness tests. Furthermore, our methods allow us to return a ""closest plausible explanation"" for a given dataset, potentially revealing underlying biases in the processes that generated them. We also show that a binomial variation of this hypothesis test could be used to identify bias in certain directions, or towards certain outcomes, and again return a closest plausible explanation. The benefits of this binomial variation are compared with other hypothesis tests, including the exact binomial. Lastly, potential industrial applications of our methods are shown using two real-world datasets.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {831–844},
numpages = {14},
keywords = {bias, statistics, fairness, machine learning, data analysis, hypothesis testing},
location = {Oxford, United Kingdom},
series = {AIES '22}
}
  
"
"@inproceedings{NEURIPS2019_373e4c5d,
 author = {Wick, Michael and panda, swetasudha and Tristan, Jean-Baptiste},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Unlocking Fairness: a Trade-off Revisited},
 url = {https://proceedings.neurips.cc/paper/2019/file/373e4c5d8edfa8b74fd4b6791d0cf6dc-Paper.pdf},
 volume = {32},
 year = {2019}
}

"
"@inproceedings{NEURIPS2020_7ec2442a,
 author = {Hiranandani, Gaurush and Narasimhan, Harikrishna and Koyejo, Sanmi},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {11083--11095},
 publisher = {Curran Associates, Inc.},
 title = {Fair Performance Metric Elicitation},
 url = {https://proceedings.neurips.cc/paper/2020/file/7ec2442aa04c157590b2fa1a7d093a33-Paper.pdf},
 volume = {33},
 year = {2020}
}

"
"@inproceedings{NEURIPS2020_d83de59e,
 author = {Ji, Disi and Smyth, Padhraic and Steyvers, Mark},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {18600--18612},
 publisher = {Curran Associates, Inc.},
 title = {Can I Trust My Fairness Metric? Assessing Fairness with Unlabeled Data and Bayesian Inference},
 url = {https://proceedings.neurips.cc/paper/2020/file/d83de59e10227072a9c034ce10029c39-Paper.pdf},
 volume = {33},
 year = {2020}
}

"
"@inproceedings{NEURIPS2021_4b26dc46,
 author = {Yao, Huihan and Chen, Ying and Ye, Qinyuan and Jin, Xisen and Ren, Xiang},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {8954--8967},
 publisher = {Curran Associates, Inc.},
 title = {Refining Language Models with Compositional Explanations},
 url = {https://proceedings.neurips.cc/paper/2021/file/4b26dc4663ccf960c8538d595d0a1d3a-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
"@inproceedings{NEURIPS2021_d800149d,
 author = {Maity, Subha and Mukherjee, Debarghya and Yurochkin, Mikhail and Sun, Yuekai},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {25773--25784},
 publisher = {Curran Associates, Inc.},
 title = {Does enforcing fairness mitigate biases caused by subpopulation shift?},
 url = {https://proceedings.neurips.cc/paper/2021/file/d800149d2f947ad4d64f34668f8b20f6-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
"@inproceedings{NEURIPS2021_dcf531ed,
 author = {Meyer, Anna and Albarghouthi, Aws and D\textquotesingle Antoni, Loris},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {26276--26288},
 publisher = {Curran Associates, Inc.},
 title = {Certifying Robustness to Programmable Data Bias in Decision Trees},
 url = {https://proceedings.neurips.cc/paper/2021/file/dcf531edc9b229acfe0f4b87e1e278dd-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
"@inproceedings{NEURIPS2021_08425b88,
 author = {Ma, Jiaqi and Deng, Junwei and Mei, Qiaozhu},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {1048--1061},
 publisher = {Curran Associates, Inc.},
 title = {Subgroup Generalization and Fairness of Graph Neural Networks},
 url = {https://proceedings.neurips.cc/paper/2021/file/08425b881bcde94a383cd258cea331be-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
"@inproceedings{NEURIPS2021_e97a4f04,
 author = {Chen, Yuan and Fei, Wenbo and Wang, Qinxia and Zeng, Donglin and Wang, Yuanjia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {27747--27760},
 publisher = {Curran Associates, Inc.},
 title = {Dynamic COVID risk assessment accounting for community virus exposure from a spatial-temporal transmission model},
 url = {https://proceedings.neurips.cc/paper/2021/file/e97a4f04ef1b914f6a1698caa364f693-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
"@inproceedings{10.1145/3278721.3278729,
author = {Dixon, Lucas and Li, John and Sorensen, Jeffrey and Thain, Nithum and Vasserman, Lucy},
title = {Measuring and Mitigating Unintended Bias in Text Classification},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278729},
doi = {10.1145/3278721.3278729},
abstract = {We introduce and illustrate a new approach to measuring and mitigating unintended bias in machine learning models. Our definition of unintended bias is parameterized by a test set and a subset of input features. We illustrate how this can be used to evaluate text classifiers using a synthetic test set and a public corpus of comments annotated for toxicity from Wikipedia Talk pages. We also demonstrate how imbalances in training data can lead to unintended bias in the resulting models, and therefore potentially unfair applications. We use a set of common demographic identity terms as the subset of input features on which we measure bias. This technique permits analysis in the common scenario where demographic information on authors and readers is unavailable, so that bias mitigation must focus on the content of the text itself. The mitigation method we introduce is an unsupervised approach based on balancing the training dataset. We demonstrate that this approach reduces the unintended bias without compromising overall model quality.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {67–73},
numpages = {7},
keywords = {fairness, algorithmic bias, text classification, machine learning, natural language processing},
location = {New Orleans, LA, USA},
series = {AIES '18}
}
  
"
"@inproceedings{NIPS2017_8cb22bdd,
 author = {Xie, Qizhe and Dai, Zihang and Du, Yulun and Hovy, Eduard and Neubig, Graham},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Controllable Invariance through Adversarial Feature Learning},
 url = {https://proceedings.neurips.cc/paper/2017/file/8cb22bdd0b7ba1ab13d742e22eed8da2-Paper.pdf},
 volume = {30},
 year = {2017}
}

"
"@inproceedings{NIPS2017_9a49a25d,
 author = {Calmon, Flavio and Wei, Dennis and Vinzamuri, Bhanukiran and Natesan Ramamurthy, Karthikeyan and Varshney, Kush R},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Optimized Pre-Processing for Discrimination Prevention},
 url = {https://proceedings.neurips.cc/paper/2017/file/9a49a25d845a483fae4be7e341368e36-Paper.pdf},
 volume = {30},
 year = {2017}
}

"
"@inproceedings{NIPS2017_e6384711,
 author = {Yao, Sirui and Huang, Bert},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Beyond Parity: Fairness Objectives for Collaborative Filtering},
 url = {https://proceedings.neurips.cc/paper/2017/file/e6384711491713d29bc63fc5eeb5ba4f-Paper.pdf},
 volume = {30},
 year = {2017}
}

"
"@inproceedings{NEURIPS2018_415185ea,
 author = {Moyer, Daniel and Gao, Shuyang and Brekelmans, Rob and Galstyan, Aram and Ver Steeg, Greg},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Invariant Representations without Adversarial Training},
 url = {https://proceedings.neurips.cc/paper/2018/file/415185ea244ea2b2bedeb0449b926802-Paper.pdf},
 volume = {31},
 year = {2018}
}

"
"@inproceedings{NEURIPS2019_8d5e957f,
 author = {Lamy, Alex and Zhong, Ziyuan and Menon, Aditya K and Verma, Nakul},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Noise-tolerant fair classification},
 url = {https://proceedings.neurips.cc/paper/2019/file/8d5e957f297893487bd98fa830fa6413-Paper.pdf},
 volume = {32},
 year = {2019}
}

"
"@inproceedings{NEURIPS2020_55d491cf,
 author = {Ruoss, Anian and Balunovic, Mislav and Fischer, Marc and Vechev, Martin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {7584--7596},
 publisher = {Curran Associates, Inc.},
 title = {Learning Certified Individually Fair Representations},
 url = {https://proceedings.neurips.cc/paper/2020/file/55d491cf951b1b920900684d71419282-Paper.pdf},
 volume = {33},
 year = {2020}
}

"
"@inproceedings{NEURIPS2020_af9c0e0c,
 author = {Oneto, Luca and Donini, Michele and Luise, Giulia and Ciliberto, Carlo and Maurer, Andreas and Pontil, Massimiliano},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {15360--15370},
 publisher = {Curran Associates, Inc.},
 title = {Exploiting MMD and Sinkhorn Divergences for Fair and Transferable Representation Learning},
 url = {https://proceedings.neurips.cc/paper/2020/file/af9c0e0c1dee63e5acad8b7ed1a5be96-Paper.pdf},
 volume = {33},
 year = {2020}
}

"
"@inproceedings{NEURIPS2021_191f8f85,
 author = {Li, Mingchen and Zhang, Xuechen and Thrampoulidis, Christos and Chen, Jiasi and Oymak, Samet},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {3163--3177},
 publisher = {Curran Associates, Inc.},
 title = {AutoBalance: Optimized Loss Functions for Imbalanced Data},
 url = {https://proceedings.neurips.cc/paper/2021/file/191f8f858acda435ae0daf994e2a72c2-Paper.pdf},
 volume = {34},
 year = {2021}
}

"
"@inproceedings{NEURIPS2021_64ff7983,
 author = {Du, Mengnan and Mukherjee, Subhabrata and Wang, Guanchu and Tang, Ruixiang and Awadallah, Ahmed and Hu, Xia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {12091--12103},
 publisher = {Curran Associates, Inc.},
 title = {Fairness via Representation Neutralization},
 url = {https://proceedings.neurips.cc/paper/2021/file/64ff7983a47d331b13a81156e2f4d29d-Paper.pdf},
 volume = {34},
 year = {2021}
}

@inproceedings{NEURIPS2021_ed277964,
 author = {Khalili, Mohammad Mahdi and Zhang, Xueru and Abroshan, Mahed},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {28144--28155},
 publisher = {Curran Associates, Inc.},
 title = {Fair Sequential Selection Using Supervised Learning Models},
 url = {https://proceedings.neurips.cc/paper/2021/file/ed277964a8959e72a0d987e598dfbe72-Paper.pdf},
 volume = {34},
 year = {2021}
}


@inproceedings{pmlr-v81-speicher18a,
	title        = {Potential for Discrimination in Online Targeted Advertising},
	author       = {Speicher, Till and Ali, Muhammad and Venkatadri, Giridhari and Ribeiro, Filipe Nunes and Arvanitakis, George and Benevenuto, Fabrício and Gummadi, Krishna P. and Loiseau, Patrick and Mislove, Alan},
	booktitle    = {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
	pages        = {5--19},
	year         = 2018,
	editor       = {Friedler, Sorelle A. and Wilson, Christo},
	volume       = 81,
	series       = {Proceedings of Machine Learning Research},
	month        = {23--24 Feb},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v81/speicher18a/speicher18a.pdf},
	url          = {https://proceedings.mlr.press/v81/speicher18a.html},
	abstract     = {Recently, online targeted advertising platforms like Facebook have been criticized for allowing advertisers to discriminate against users belonging to sensitive groups, i.e., to exclude users belonging to a certain race or gender from receiving their ads. Such criticisms have led, for instance, Facebook to disallow the use of attributes such as ethnic affinity from being used by advertisers when targeting ads related to housing or employment or financial services. In this paper, we show that such measures are far from sufficient and that the problem of discrimination in targeted advertising is much more pernicious.  We argue that discrimination measures should be based on the targeted population and not on the attributes used for targeting. We systematically investigate the different targeting methods offered by Facebook for their ability to enable discriminatory advertising.  We show that a malicious advertiser can create highly discriminatory ads without using sensitive attributes. Our findings call for exploring fundamentally new methods for mitigating discrimination in online targeted advertising.},
}

"
@inproceedings{pmlr-v81-datta18a,
	title        = {Discrimination in Online Advertising: A Multidisciplinary Inquiry},
	author       = {Datta, Amit and Datta, Anupam and Makagon, Jael and Mulligan, Deirdre K. and Tschantz, Michael Carl},
	booktitle    = {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
	pages        = {20--34},
	year         = 2018,
	editor       = {Friedler, Sorelle A. and Wilson, Christo},
	volume       = 81,
	series       = {Proceedings of Machine Learning Research},
	month        = {23--24 Feb},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v81/datta18a/datta18a.pdf},
	url          = {https://proceedings.mlr.press/v81/datta18a.html},
	abstract     = {We explore ways in which discrimination may arise in the targeting of job-related advertising, noting the potential for multiple parties to contribute to its occurrence.  We then examine the statutes and case law interpreting the prohibition on advertisements that indicate a preference based on protected class, and consider its application to online advertising.  We focus on its interaction with Section 230 of the Communications Decency Act, which provides interactive computer services with immunity for providing access to  information created by a third party.  We argue that such services can lose that immunity if they target ads toward or away from protected classes without explicit instructions from advertisers to do so.},
}
"
"
@inproceedings{pmlr-v81-ekstrand18a,
	title        = {Privacy for All: Ensuring Fair and Equitable Privacy Protections},
	author       = {Ekstrand, Michael D. and Joshaghani, Rezvan and Mehrpouyan, Hoda},
	booktitle    = {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
	pages        = {35--47},
	year         = 2018,
	editor       = {Friedler, Sorelle A. and Wilson, Christo},
	volume       = 81,
	series       = {Proceedings of Machine Learning Research},
	month        = {23--24 Feb},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v81/ekstrand18a/ekstrand18a.pdf},
	url          = {https://proceedings.mlr.press/v81/ekstrand18a.html},
	abstract     = {In this position paper, we argue for applying recent research on ensuring sociotechnical systems are fair and non-discriminatory to the privacy protections those systems may provide. Privacy literature seldom considers whether a proposed privacy scheme protects all persons uniformly, irrespective of membership in protected classes or particular risk in the face of privacy failure. Just as algorithmic decision-making systems may have discriminatory outcomes even without explicit or deliberate discrimination, so also privacy regimes may disproportionately fail to protect vulnerable members of their target population, resulting in disparate impact with respect to the effectiveness of privacy protections.We propose a research agenda that will illuminate this issue, along with related issues in the intersection of fairness and privacy, and present case studies that show how the outcomes of this research may change existing privacy and fairness research. We believe it is important to ensure that technologies and policies intended to protect the users and subjects of information systems provide such protection in an equitable fashion.},
}
"
"
@inproceedings{pmlr-v81-barabas18a,
	title        = {Interventions over Predictions: Reframing the Ethical Debate for Actuarial Risk Assessment},
	author       = {Barabas, Chelsea and Virza, Madars and Dinakar, Karthik and Ito, Joichi and Zittrain, Jonathan},
	booktitle    = {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
	pages        = {62--76},
	year         = 2018,
	editor       = {Friedler, Sorelle A. and Wilson, Christo},
	volume       = 81,
	series       = {Proceedings of Machine Learning Research},
	month        = {23--24 Feb},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v81/barabas18a/barabas18a.pdf},
	url          = {https://proceedings.mlr.press/v81/barabas18a.html},
	abstract     = {Actuarial risk assessments are frequently touted as a neutral way to counteract implicit bias and increase the fairness of decisions made at almost every juncture of the criminal justice system, from pretrial release to sentencing, parole and probation. In recent times these assessments have come under increased scrutiny, as critics claim that the statistical techniques underlying them might reproduce existing patterns of discrimination and historical biases that are reflected in the data. Much of this debate is centered around competing notions of fairness and predictive accuracy, which seek to problematize the use of variables that act as “proxies” for protected classes, such as race and gender. However, these debates fail to address the core ethical issue at hand - that current risk assessments are ill-equipped to support ethical punishment and rehabilitation practices in the criminal justice system, because they offer only a limited insight into the underlying drivers of criminal behavior. In this paper, we examine the prevailing paradigms of fairness currently under debate and propose an alternative methodology for identifying the underlying social and structural factors that drive criminal behavior. We argue that the core ethical debate surrounding the use of regression in risk assessments is not one of bias or accuracy. Rather, it’s one of purpose. If machine learning is operationalized merely in the service of predicting future crime, then it becomes difficult to break cycles of criminalization that are driven by the iatrogenic effects of the criminal justice system itself. We posit that machine learning should not be used for prediction, rather it should be used to surface covariates that are fed into a causal model for understanding the social, structural and psychological drivers of crime. We propose an alternative application of machine learning and causal inference away from predicting risk scores to risk mitigation.},
}
"
"
"
"
"
"
@inproceedings{pmlr-v81-dwork18a,
	title        = {Decoupled Classifiers for Group-Fair and Efficient Machine Learning},
	author       = {Dwork, Cynthia and Immorlica, Nicole and Kalai, Adam Tauman and Leiserson, Max},
	booktitle    = {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
	pages        = {119--133},
	year         = 2018,
	editor       = {Friedler, Sorelle A. and Wilson, Christo},
	volume       = 81,
	series       = {Proceedings of Machine Learning Research},
	month        = {23--24 Feb},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v81/dwork18a/dwork18a.pdf},
	url          = {https://proceedings.mlr.press/v81/dwork18a.html},
	abstract     = {When it is ethical and legal to use a sensitive attribute (such as gender or race) in machine learning systems, the question remains how to do so. We show that the naive application of machine learning algorithms using sensitive attributes leads to an inherent tradeoff in accuracy between groups. We provide a simple and efficient decoupling technique, that can be added on top of any black-box machine learning algorithm, to learn different classifiers for different groups. Transfer learning is used to mitigate the problem of having too little data on any one group.},
}
"
"
@inproceedings{pmlr-v81-chouldechova18a,
	title        = {A case study of algorithm-assisted decision making in child maltreatment hotline screening decisions},
	author       = {Chouldechova, Alexandra and Benavides-Prado, Diana and Fialko, Oleksandr and Vaithianathan, Rhema},
	booktitle    = {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
	pages        = {134--148},
	year         = 2018,
	editor       = {Friedler, Sorelle A. and Wilson, Christo},
	volume       = 81,
	series       = {Proceedings of Machine Learning Research},
	month        = {23--24 Feb},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v81/chouldechova18a/chouldechova18a.pdf},
	url          = {https://proceedings.mlr.press/v81/chouldechova18a.html},
	abstract     = {Every year there are more than 3.6 million referrals made to child protection agencies across the US. The practice of screening calls is left to each jurisdiction to follow local practices and policies, potentially leading to large variation in the way in which referrals are treated across the country. Whilst increasing access to linked administrative data is available, it is difficult for welfare workers to make systematic use of historical information about all the children and adults on a single referral call. Risk prediction models that use routinely collected administrative data can help call workers to better identify cases that are likely to result in adverse outcomes. However, the use of predictive analytics in the area of child welfare is contentious. There is a possibility that some communities—such as those in poverty or from particular racial and ethnic groups—will be disadvantaged by the reliance on government administrative data. On the other hand, these analytics tools can augment or replace human judgments, which themselves are biased and imperfect. In this paper we describe our work on developing, validating, fairness auditing, and deploying a risk prediction model in Allegheny County, Pennsylvania, USA. We discuss the results of our analysis to-date, and also highlight key problems and data bias issues that present challenges for model evaluation and deployment.},
}
"
"
"
"
@inproceedings{pmlr-v81-ensign18a,
	title        = {Runaway Feedback Loops in Predictive Policing},
	author       = {Ensign, Danielle and Friedler, Sorelle A. and Neville, Scott and Scheidegger, Carlos and Venkatasubramanian, Suresh},
	booktitle    = {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
	pages        = {160--171},
	year         = 2018,
	editor       = {Friedler, Sorelle A. and Wilson, Christo},
	volume       = 81,
	series       = {Proceedings of Machine Learning Research},
	month        = {23--24 Feb},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v81/ensign18a/ensign18a.pdf},
	url          = {https://proceedings.mlr.press/v81/ensign18a.html},
	abstract     = {Predictive policing systems are increasingly used to determine how to allocate police across a city in order to best prevent crime. Discovered crime data (e.g., arrest counts) are used to help update the model, and the process is repeated. Such systems have been shown susceptible to runaway feedback loops, where police are repeatedly sent back to the same neighborhoods regardless of the true crime rate.  In response, we develop a mathematical model of predictive policing that proves why this feedback loop occurs, show empirically that this model exhibits such problems, and demonstrate how to change the inputs to a predictive policing system (in a black-box manner) so the runaway feedback loop does not occur, allowing the true crime rate to be learned.   Our results are quantitative: we can establish a link (in our model) between the degree to which runaway feedback causes problems and the disparity in crime rates between areas. Moreover, we can also demonstrate the way in which reported incidents of crime (those reported by residents) and discovered incidents of crime (i.e those directly observed by police officers dispatched as a result of the predictive policing algorithm) interact: in brief, while reported incidents can attenuate the degree of runaway feedback, they cannot entirely remove it without the interventions we suggest.},
}
"
"
@inproceedings{pmlr-v81-kamishima18a,
	title        = {Recommendation Independence},
	author       = {Kamishima, Toshihiro and Akaho, Shotaro and Asoh, Hideki and Sakuma, Jun},
	booktitle    = {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
	pages        = {187--201},
	year         = 2018,
	editor       = {Friedler, Sorelle A. and Wilson, Christo},
	volume       = 81,
	series       = {Proceedings of Machine Learning Research},
	month        = {23--24 Feb},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v81/kamishima18a/kamishima18a.pdf},
	url          = {https://proceedings.mlr.press/v81/kamishima18a.html},
	abstract     = {This paper studies a recommendation algorithm whose outcomes are not influenced by specified information. It is useful in contexts potentially unfair decision should be avoided, such as job-applicant recommendations that are not influenced by socially sensitive information. An algorithm that could exclude the influence of sensitive information would thus be useful for job-matching with fairness. We call the condition between a recommendation outcome and a sensitive feature Recommendation Independence, which is formally defined as statistical independence between the outcome and the feature. Our previous independence-enhanced algorithms simply matched the means of predictions between sub-datasets consisting of the same sensitive value. However, this approach could not remove the sensitive information represented by the second or higher moments of distributions. In this paper, we develop new methods that can deal with the second moment, i.e., variance, of recommendation outcomes without increasing the computational complexity. These methods can more strictly remove the sensitive information, and experimental results demonstrate that our new algorithms can more effectively eliminate the factors that undermine fairness. Additionally, we explore potential applications for independence-enhanced recommendation, and discuss its relation to other concepts, such as recommendation diversity.},
}
"
"
@inproceedings{pmlr-v81-burke18a,
	title        = {Balanced Neighborhoods for Multi-sided Fairness in Recommendation},
	author       = {Burke, Robin and Sonboli, Nasim and Ordonez-Gauger, Aldo},
	booktitle    = {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
	pages        = {202--214},
	year         = 2018,
	editor       = {Friedler, Sorelle A. and Wilson, Christo},
	volume       = 81,
	series       = {Proceedings of Machine Learning Research},
	month        = {23--24 Feb},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v81/burke18a/burke18a.pdf},
	url          = {https://proceedings.mlr.press/v81/burke18a.html},
	abstract     = {Fairness has emerged as an important category of analysis for machine learning systems in some application areas. In extending the concept of fairness to recommender systems, there is an essential tension between the goals of fairness and those of personalization. However, there are contexts in which  equity across recommendation outcomes is a desirable goal. It is also the case that in some applications fairness may be a multisided concept, in which the impacts on multiple groups of individuals must be considered. In this paper, we examine two different cases of fairness-aware recommender systems: consumer-centered and provider-centered. We  explore the concept of a balanced neighborhood as a mechanism to preserve personalization in recommendation while enhancing the fairness of recommendation outcomes. We show that a modified version of the Sparse Linear Method (SLIM) can be used to improve the balance of user and item neighborhoods, with the result of achieving greater outcome fairness in real-world datasets with minimal loss in ranking performance.},
}
"
"@inproceedings{10.1145/3287560.3287567,
author = {Passi, Samir and Barocas, Solon},
title = {Problem Formulation and Fairness},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287567},
doi = {10.1145/3287560.3287567},
abstract = {Formulating data science problems is an uncertain and difficult process. It requires various forms of discretionary work to translate high-level objectives or strategic goals into tractable problems, necessitating, among other things, the identification of appropriate target variables and proxies. While these choices are rarely self-evident, normative assessments of data science projects often take them for granted, even though different translations can raise profoundly different ethical concerns. Whether we consider a data science project fair often has as much to do with the formulation of the problem as any property of the resulting model. Building on six months of ethnographic fieldwork with a corporate data science team---and channeling ideas from sociology and history of science, critical data studies, and early writing on knowledge discovery in databases---we describe the complex set of actors and activities involved in problem formulation. Our research demonstrates that the specification and operationalization of the problem are always negotiated and elastic, and rarely worked out with explicit normative considerations in mind. In so doing, we show that careful accounts of everyday data science work can help us better understand how and why data science problems are posed in certain ways---and why specific formulations prevail in practice, even in the face of what might seem like normatively preferable alternatives. We conclude by discussing the implications of our findings, arguing that effective normative interventions will require attending to the practical work of problem formulation.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {39–48},
numpages = {10},
keywords = {Fairness, Machine Learning, Data Science, Problem Formulation, Target Variable},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}
  
"
"@inproceedings{10.1145/3287560.3287600,
author = {Hutchinson, Ben and Mitchell, Margaret},
title = {50 Years of Test (Un)Fairness: Lessons for Machine Learning},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287600},
doi = {10.1145/3287560.3287600},
abstract = {Quantitative definitions of what is unfair and what is fair have been introduced in multiple disciplines for well over 50 years, including in education, hiring, and machine learning. We trace how the notion of fairness has been defined within the testing communities of education and hiring over the past half century, exploring the cultural and social context in which different fairness definitions have emerged. In some cases, earlier definitions of fairness are similar or identical to definitions of fairness in current machine learning research, and foreshadow current formal work. In other cases, insights into what fairness means and how to measure it have largely gone overlooked. We compare past and current notions of fairness along several dimensions, including the fairness criteria, the focus of the criteria (e.g., a test, a model, or its use), the relationship of fairness to individuals, groups, and subgroups, and the mathematical method for measuring fairness (e.g., classification, regression). This work points the way towards future research and measurement of (un)fairness that builds from our modern understanding of fairness while incorporating insights from the past.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {49–58},
numpages = {10},
keywords = {ML fairness, fairness, history, psychometrics, test fairness},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}
  
"
"@inproceedings{10.1145/3287560.3287598,
author = {Selbst, Andrew D. and Boyd, Danah and Friedler, Sorelle A. and Venkatasubramanian, Suresh and Vertesi, Janet},
title = {Fairness and Abstraction in Sociotechnical Systems},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287598},
doi = {10.1145/3287560.3287598},
abstract = {A key goal of the fair-ML community is to develop machine-learning based systems that, once introduced into a social context, can achieve social and legal outcomes such as fairness, justice, and due process. Bedrock concepts in computer science---such as abstraction and modular design---are used to define notions of fairness and discrimination, to produce fairness-aware learning algorithms, and to intervene at different stages of a decision-making pipeline to produce ""fair"" outcomes. In this paper, however, we contend that these concepts render technical interventions ineffective, inaccurate, and sometimes dangerously misguided when they enter the societal context that surrounds decision-making systems. We outline this mismatch with five ""traps"" that fair-ML work can fall into even as it attempts to be more context-aware in comparison to traditional data science. We draw on studies of sociotechnical systems in Science and Technology Studies to explain why such traps occur and how to avoid them. Finally, we suggest ways in which technical designers can mitigate the traps through a refocusing of design in terms of process rather than solutions, and by drawing abstraction boundaries to include social actors rather than purely technical ones.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {59–68},
numpages = {10},
keywords = {Interdisciplinary, Sociotechnical Systems, Fairness-aware Machine Learning},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}
  
"
"@inproceedings{10.1145/3287560.3287563,
author = {Green, Ben and Chen, Yiling},
title = {Disparate Interactions: An Algorithm-in-the-Loop Analysis of Fairness in Risk Assessments},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287563},
doi = {10.1145/3287560.3287563},
abstract = {Despite vigorous debates about the technical characteristics of risk assessments being deployed in the U.S. criminal justice system, remarkably little research has studied how these tools affect actual decision-making processes. After all, risk assessments do not make definitive decisions---they inform judges, who are the final arbiters. It is therefore essential that considerations of risk assessments be informed by rigorous studies of how judges actually interpret and use them. This paper takes a first step toward such research on human interactions with risk assessments through a controlled experimental study on Amazon Mechanical Turk. We found several behaviors that call into question the supposed efficacy and fairness of risk assessments: our study participants 1) underperformed the risk assessment even when presented with its predictions, 2) could not effectively evaluate the accuracy of their own or the risk assessment's predictions, and 3) exhibited behaviors fraught with ""disparate interactions,"" whereby the use of risk assessments led to higher risk predictions about black defendants and lower risk predictions about white defendants. These results suggest the need for a new ""algorithm-in-the-loop"" framework that places machine learning decision-making aids into the sociotechnical context of improving human decisions rather than the technical context of generating the best prediction in the abstract. If risk assessments are to be used at all, they must be grounded in rigorous evaluations of their real-world impacts instead of in their theoretical potential.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {90–99},
numpages = {10},
keywords = {Mechanical Turk, risk assessment, fairness, behavioral experiment},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}
  
"
"@inproceedings{10.1145/3287560.3287592,
author = {Kearns, Michael and Neel, Seth and Roth, Aaron and Wu, Zhiwei Steven},
title = {An Empirical Study of Rich Subgroup Fairness for Machine Learning},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287592},
doi = {10.1145/3287560.3287592},
abstract = {Kearns, Neel, Roth, and Wu [ICML 2018] recently proposed a notion of rich subgroup fairness intended to bridge the gap between statistical and individual notions of fairness. Rich subgroup fairness picks a statistical fairness constraint (say, equalizing false positive rates across protected groups), but then asks that this constraint hold over an exponentially or infinitely large collection of subgroups defined by a class of functions with bounded VC dimension. They give an algorithm guaranteed to learn subject to this constraint, under the condition that it has access to oracles for perfectly learning absent a fairness constraint. In this paper, we undertake an extensive empirical evaluation of the algorithm of Kearns et al. On four real datasets for which fairness is a concern, we investigate the basic convergence of the algorithm when instantiated with fast heuristics in place of learning oracles, measure the tradeoffs between fairness and accuracy, and compare this approach with the recent algorithm of Agarwal, Beygelzeimer, Dudik, Langford, and Wallach [ICML 2018], which implements weaker and more traditional marginal fairness constraints defined by individual protected attributes. We find that in general, the Kearns et al. algorithm converges quickly, large gains in fairness can be obtained with mild costs to accuracy, and that optimizing accuracy subject only to marginal fairness leads to classifiers with substantial subgroup unfairness. We also provide a number of analyses and visualizations of the dynamics and behavior of the Kearns et al. algorithm. Overall we find this algorithm to be effective on real data, and rich subgroup fairness to be a viable notion in practice.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {100–109},
numpages = {10},
keywords = {Fair Classification, Fairness Auditing, Algorithmic Bias, Subgroup Fairness},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}
  
"
"@inproceedings{10.1145/3287560.3287570,
author = {Chakraborty, Abhijnan and Patro, Gourab K. and Ganguly, Niloy and Gummadi, Krishna P. and Loiseau, Patrick},
title = {Equality of Voice: Towards Fair Representation in Crowdsourced Top-K Recommendations},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287570},
doi = {10.1145/3287560.3287570},
abstract = {To help their users to discover important items at a particular time, major websites like Twitter, Yelp, TripAdvisor or NYTimes provide Top-K recommendations (e.g., 10 Trending Topics, Top 5 Hotels in Paris or 10 Most Viewed News Stories), which rely on crowdsourced popularity signals to select the items. However, different sections of a crowd may have different preferences, and there is a large silent majority who do not explicitly express their opinion. Also, the crowd often consists of actors like bots, spammers, or people running orchestrated campaigns. Recommendation algorithms today largely do not consider such nuances, hence are vulnerable to strategic manipulation by small but hyper-active user groups.To fairly aggregate the preferences of all users while recommending top-K items, we borrow ideas from prior research on social choice theory, and identify a voting mechanism called Single Transferable Vote (STV) as having many of the fairness properties we desire in top-K item (s)elections. We develop an innovative mechanism to attribute preferences of silent majority which also make STV completely operational. We show the generalizability of our approach by implementing it on two different real-world datasets. Through extensive experimentation and comparison with state-of-the-art techniques, we show that our proposed approach provides maximum user satisfaction, and cuts down drastically on items disliked by most but hyper-actively promoted by a few users.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {129–138},
numpages = {10},
keywords = {Fairness in Recommendation, Twitter Trends, Most Popular News, Fair Representation, Top-K Recommendation},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}
  
"
"@inproceedings{10.1145/3287560.3287571,
author = {Elzayn, Hadi and Jabbari, Shahin and Jung, Christopher and Kearns, Michael and Neel, Seth and Roth, Aaron and Schutzman, Zachary},
title = {Fair Algorithms for Learning in Allocation Problems},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287571},
doi = {10.1145/3287560.3287571},
abstract = {Settings such as lending and policing can be modeled by a centralized agent allocating a scarce resource (e.g. loans or police officers) amongst several groups, in order to maximize some objective (e.g. loans given that are repaid, or criminals that are apprehended). Often in such problems fairness is also a concern. One natural notion of fairness, based on general principles of equality of opportunity, asks that conditional on an individual being a candidate for the resource in question, the probability of actually receiving it is approximately independent of the individual's group. For example, in lending this would mean that equally creditworthy individuals in different racial groups have roughly equal chances of receiving a loan. In policing it would mean that two individuals committing the same crime in different districts would have roughly equal chances of being arrested.In this paper, we formalize this general notion of fairness for allocation problems and investigate its algorithmic consequences. Our main technical results include an efficient learning algorithm that converges to an optimal fair allocation even when the allocator does not know the frequency of candidates (i.e. creditworthy individuals or criminals) in each group. This algorithm operates in a censored feedback model in which only the number of candidates who received the resource in a given allocation can be observed, rather than the true number of candidates in each group. This models the fact that we do not learn the creditworthiness of individuals we do not give loans to and do not learn about crimes committed if the police presence in a district is low.As an application of our framework and algorithm, we consider the predictive policing problem, in which the resource being allocated to each group is the number of police officers assigned to each district. The learning algorithm is trained on arrest data gathered from its own deployments on previous days, resulting in a potential feedback loop that our algorithm provably overcomes. In this case, the fairness constraint asks that the probability that an individual who has committed a crime is arrested should be independent of the district in which they live. We investigate the performance of our learning algorithm on the Philadelphia Crime Incidents dataset.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {170–179},
numpages = {10},
keywords = {online learning, censored feedback, algorithmic fairness, resource allocation},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}
  
"
"@inproceedings{10.1145/3287560.3287582,
author = {Babaioff, Moshe and Nisan, Noam and Talgam-Cohen, Inbal},
title = {Fair Allocation through Competitive Equilibrium from Generic Incomes},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287582},
doi = {10.1145/3287560.3287582},
abstract = {Two food banks catering to populations of different sizes with different needs must divide among themselves a donation of food items. What constitutes a ""fair"" allocation of the items among them?Competitive equilibrium from equal incomes (CEEI) is a classic solution to the problem of fair and efficient allocation of goods among equally entitled agents [Foley 1967, Varian 1974]. Every agent (foodbank) receives an equal endowment of artificial currency with which to ""purchase"" bundles of goods (food items). Prices for the goods are set high enough such that the agents can simultaneously get their favorite within-budget bundle, and low enough such that all goods are allocated (no waste). A CEEI satisfies mathematical notions of fairness like fair-share, and also has built-in transparency -- prices can be published so the agents can verify they're being treated equally. However, a CEEI is not guaranteed to exist when the items are indivisible.We study competitive equilibrium from generic incomes (CEGI), which is based on the idea of slightly perturbed endowments, and enjoys similar fairness, efficiency and transparency properties as CEEI. We show that when the two agents have almost equal endowments and additive preferences for the items, a CEGI always exists. We then consider agents who are a priori non-equal (like different-sized foodbanks); we formulate a new notion of fair allocation among non-equals satisfied by CEGI, and show existence in cases of interest (like when the agents have identical preferences). Experiments on simulated and Spliddit data (a popular fair division website) indicate more general existence. Our results open opportunities for future research on fairness through generic endowments, and on fair treatment of non-equals.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {180},
numpages = {1},
keywords = {Fisher markets, additive preferences, Market equilibrium, fairness, unequal entitlements},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}
  
"
"@inproceedings{10.1145/3287560.3287584,
author = {Heidari, Hoda and Loi, Michele and Gummadi, Krishna P. and Krause, Andreas},
title = {A Moral Framework for Understanding Fair ML through Economic Models of Equality of Opportunity},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287584},
doi = {10.1145/3287560.3287584},
abstract = {We map the recently proposed notions of algorithmic fairness to economic models of Equality of opportunity (EOP)---an extensively studied ideal of fairness in political philosophy. We formally show that through our conceptual mapping, many existing definition of algorithmic fairness, such as predictive value parity and equality of odds, can be interpreted as special cases of EOP. In this respect, our work serves as a unifying moral framework for understanding existing notions of algorithmic fairness. Most importantly, this framework allows us to explicitly spell out the moral assumptions underlying each notion of fairness, and interpret recent fairness impossibility results in a new light. Last but not least and inspired by luck egalitarian models of EOP, we propose a new family of measures for algorithmic fairness. We illustrate our proposal empirically and show that employing a measure of algorithmic (un)fairness when its underlying moral assumptions are not satisfied, can have devastating consequences for the disadvantaged group's welfare.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {181–190},
numpages = {10},
keywords = {Equality of Odds, Equality of Opportunity (EOP), Rawlsian and Luck Egalitarian EOP, Statistical Parity, Fairness for Machine Learning, Predictive Value Parity},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}
  
"
"@inproceedings{10.1145/3287560.3287577,
author = {Young, Meg and Rodriguez, Luke and Keller, Emily and Sun, Feiyang and Sa, Boyang and Whittington, Jan and Howe, Bill},
title = {Beyond Open vs. Closed: Balancing Individual Privacy and Public Accountability in Data Sharing},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287577},
doi = {10.1145/3287560.3287577},
abstract = {Data too sensitive to be ""open"" for analysis and re-purposing typically remains ""closed"" as proprietary information. This dichotomy undermines efforts to make algorithmic systems more fair, transparent, and accountable. Access to proprietary data in particular is needed by government agencies to enforce policy, researchers to evaluate methods, and the public to hold agencies accountable; all of these needs must be met while preserving individual privacy and firm competitiveness. In this paper, we describe an integrated legal-technical approach provided by a third-party public-private data trust designed to balance these competing interests. Basic membership allows firms and agencies to enable low-risk access to data for compliance reporting and core methods research, while modular data sharing agreements support a wide array of projects and use cases. Unless specifically stated otherwise in an agreement, all data access is initially provided to end users through customized synthetic datasets that offer a) strong privacy guarantees, b) removal of signals that could expose competitive advantage, and c) removal of biases that could reinforce discriminatory policies, all while maintaining fidelity to the original data. We find that using synthetic data in conjunction with strong legal protections over raw data strikes a balance between transparency, proprietorship, privacy, and research objectives. This legal-technical framework can form the basis for data trusts in a variety of contexts.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {191–200},
numpages = {10},
keywords = {data sharing, privacy, data ethics, data governance, algorithmic bias},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}
  
"
"@inproceedings{10.1145/3287560.3287565,
author = {Jiang, Shan and Martin, John and Wilson, Christo},
title = {Who's the Guinea Pig? Investigating Online A/B/n Tests in-the-Wild},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287565},
doi = {10.1145/3287560.3287565},
abstract = {A/B/n testing has been adopted by many technology companies as a data-driven approach to product design and optimization. These tests are often run on their websites without explicit consent from users. In this paper, we investigate such online A/B/n tests by using Optimizely as a lens. First, we provide measurement results of 575 websites that use Optimizely drawn from the Alexa Top-1M, and analyze the distributions of their audiences and experiments. Then, we use three case studies to discuss potential ethical pitfalls of such experiments, including involvement of political content, price discrimination, and advertising campaigns. We conclude with a suggestion for greater awareness of ethical concerns inherent in human experimentation and a call for increased transparency among A/B/n test operators.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {201–210},
numpages = {10},
keywords = {personalization, A/B/n testing, online controlled experiments},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}
  
"
"@inproceedings{10.1145/3287560.3287588,
author = {Albarghouthi, Aws and Vinitsky, Samuel},
title = {Fairness-Aware Programming},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287588},
doi = {10.1145/3287560.3287588},
abstract = {Increasingly, programming tasks involve automating and deploying sensitive decision-making processes that may have adverse impacts on individuals or groups of people. The issue of fairness in automated decision-making has thus become a major problem, attracting interdisciplinary attention. In this work, we aim to make fairness a first-class concern in programming. Specifically, we propose fairness-aware programming, where programmers can state fairness expectations natively in their code, and have a runtime system monitor decision-making and report violations of fairness.We present a rich and general specification language that allows a programmer to specify a range of fairness definitions from the literature, as well as others. As the decision-making program executes, the runtime maintains statistics on the decisions made and incrementally checks whether the fairness definitions have been violated, reporting such violations to the developer. The advantages of this approach are two fold: (i) Enabling declarative mathematical specifications of fairness in the programming language simplifies the process of checking fairness, as the programmer does not have to write ad hoc code for maintaining statistics. (ii) Compared to existing techniques for checking and ensuring fairness, our approach monitors a decision-making program in the wild, which may be running on a distribution that is unlike the dataset on which a classifier was trained and tested.We describe an implementation of our proposed methodology as a library in the Python programming language and illustrate its use on case studies from the algorithmic fairness literature.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {211–219},
numpages = {9},
keywords = {Runtime verification, Fairness, Assertion languages, Runtime monitoring, Probabilistic specifications},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}
  
"
"@inproceedings{10.1145/3287560.3287576,
author = {Milli, Smitha and Miller, John and Dragan, Anca D. and Hardt, Moritz},
title = {The Social Cost of Strategic Classification},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287576},
doi = {10.1145/3287560.3287576},
abstract = {Consequential decision-making typically incentivizes individuals to behave strategically, tailoring their behavior to the specifics of the decision rule. A long line of work has therefore sought to counteract strategic behavior by designing more conservative decision boundaries in an effort to increase robustness to the effects of strategic covariate shift.We show that these efforts benefit the institutional decision maker at the expense of the individuals being classified. Introducing a notion of social burden, we prove that any increase in institutional utility necessarily leads to a corresponding increase in social burden. Moreover, we show that the negative externalities of strategic classification can disproportionately harm disadvantaged groups in the population.Our results highlight that strategy-robustness must be weighed against considerations of social welfare and fairness.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {230–239},
numpages = {10},
keywords = {Strategic classification, fairness, machine learning},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}
  
"
"@inproceedings{10.1145/3287560.3287578,
author = {Kannan, Sampath and Roth, Aaron and Ziani, Juba},
title = {Downstream Effects of Affirmative Action},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287578},
doi = {10.1145/3287560.3287578},
abstract = {We study a two-stage model, in which students are 1) admitted to college on the basis of an entrance exam which is a noisy signal about their qualifications (type), and then 2) those students who were admitted to college can be hired by an employer as a function of their college grades, which are an independently drawn noisy signal of their type. Students are drawn from one of two populations, which might have different type distributions. We assume that the employer at the end of the pipeline is rational, in the sense that it computes a posterior distribution on student type conditional on all information that it has available (college admissions, grades, and group membership), and makes a decision based on posterior expectation. We then study what kinds of fairness goals can be achieved by the college by setting its admissions rule and grading policy. For example, the college might have the goal of guaranteeing equal opportunity across populations: that the probability of passing through the pipeline and being hired by the employer should be independent of group membership, conditioned on type. Alternately, the college might have the goal of incentivizing the employer to have a group blind hiring rule. We show that both goals can be achieved when the college does not report grades. On the other hand, we show that under reasonable conditions, these goals are impossible to achieve even in isolation when the college uses an (even minimally) informative grading policy.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {240–248},
numpages = {9},
keywords = {affirmative action, job market, Long-term fairness, college admissions},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}
  
"
"@inproceedings{10.1145/3287560.3287573,
author = {Glymour, Bruce and Herington, Jonathan},
title = {Measuring the Biases That Matter: The Ethical and Casual Foundations for Measures of Fairness in Algorithms},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287573},
doi = {10.1145/3287560.3287573},
abstract = {Measures of algorithmic bias can be roughly classified into four categories, distinguished by the conditional probabilistic dependencies to which they are sensitive. First, measures of ""procedural bias"" diagnose bias when the score returned by an algorithm is probabilistically dependent on a sensitive class variable (e.g. race or sex). Second, measures of ""outcome bias"" capture probabilistic dependence between class variables and the outcome for each subject (e.g. parole granted or loan denied). Third, measures of ""behavior-relative error bias"" capture probabilistic dependence between class variables and the algorithmic score, conditional on target behaviors (e.g. recidivism or loan default). Fourth, measures of ""score-relative error bias"" capture probabilistic dependence between class variables and behavior, conditional on score. Several recent discussions have demonstrated a tradeoff between these different measures of algorithmic bias, and at least one recent paper has suggested conditions under which tradeoffs may be minimized.In this paper we use the machinery of causal graphical models to show that, under standard assumptions, the underlying causal relations among variables forces some tradeoffs. We delineate a number of normative considerations that are encoded in different measures of bias, with reference to the philosophical literature on the wrongfulness of disparate treatment and disparate impact. While both kinds of error bias are nominally motivated by concern to avoid disparate impact, we argue that consideration of causal structures shows that these measures are better understood as complicated and unreliable measures of procedural biases (i.e. disparate treatment). Moreover, while procedural bias is indicative of disparate treatment, we show that the measure of procedural bias one ought to adopt is dependent on the account of the wrongfulness of disparate treatment one endorses. Finally, given that neither score-relative nor behavior-relative measures of error bias capture the relevant normative considerations, we suggest that error bias proper is best measured by score-based measures of accuracy, such as the Brier score.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {269–278},
numpages = {10},
keywords = {casual inference, Algorithmic decision-making, discrimination, fairness},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}
  
"
"@inproceedings{10.1145/3287560.3287575,
author = {Benthall, Sebastian and Haynes, Bruce D.},
title = {Racial Categories in Machine Learning},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287575},
doi = {10.1145/3287560.3287575},
abstract = {Controversies around race and machine learning have sparked debate among computer scientists over how to design machine learning systems that guarantee fairness. These debates rarely engage with how racial identity is embedded in our social experience, making for sociological and psychological complexity. This complexity challenges the paradigm of considering fairness to be a formal property of supervised learning with respect to protected personal attributes. Racial identity is not simply a personal subjective quality. For people labeled ""Black"" it is an ascribed political category that has consequences for social differentiation embedded in systemic patterns of social inequality achieved through both social and spatial segregation. In the United States, racial classification can best be understood as a system of inherently unequal status categories that places whites as the most privileged category while signifying the Negro/black category as stigmatized. Social stigma is reinforced through the unequal distribution of societal rewards and goods along racial lines that is reinforced by state, corporate, and civic institutions and practices. This creates a dilemma for society and designers: be blind to racial group disparities and thereby reify racialized social inequality by no longer measuring systemic inequality, or be conscious of racial categories in a way that itself reifies race. We propose a third option. By preceding group fairness interventions with unsupervised learning to dynamically detect patterns of segregation, machine learning systems can mitigate the root cause of social disparities, social segregation and stratification, without further anchoring status categories of disadvantage.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {289–298},
numpages = {10},
keywords = {machine learning, racial classification, segregation, fairness},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}
  
"
"@inproceedings{10.1145/3287560.3287561,
author = {Canetti, Ran and Cohen, Aloni and Dikkala, Nishanth and Ramnarayan, Govind and Scheffler, Sarah and Smith, Adam},
title = {From Soft Classifiers to Hard Decisions: How Fair Can We Be?},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287561},
doi = {10.1145/3287560.3287561},
abstract = {A popular methodology for building binary decision-making classifiers in the presence of imperfect information is to first construct a calibrated non-binary ""scoring"" classifier, and then to post-process this score to obtain a binary decision. We study various fairness (or, error-balance) properties of this methodology, when the non-binary scores are calibrated over all protected groups, and with a variety of post-processing algorithms. Specifically, we show:First, there does not exist a general way to post-process a calibrated classifier to equalize protected groups' positive or negative predictive value (PPV or NPV). For certain ""nice"" calibrated classifiers, either PPV or NPV can be equalized when the post-processor uses different thresholds across protected groups. Still, when the post-processing consists of a single global threshold across all groups, natural fairness properties, such as equalizing PPV in a nontrivial way, do not hold even for ""nice"" classifiers.Second, when the post-processing stage is allowed to defer on some decisions (that is, to avoid making a decision by handing off some examples to a separate process), then for the non-deferred decisions, the resulting classifier can be made to equalize PPV, NPV, false positive rate (FPR) and false negative rate (FNR) across the protected groups. This suggests a way to partially evade the impossibility results of Chouldechova and Kleinberg et al., which preclude equalizing all of these measures simultaneously. We also present different deferring strategies and show how they affect the fairness properties of the overall system.We evaluate our post-processing techniques using the COMPAS data set from 2016.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {309–318},
numpages = {10},
keywords = {post-processing, classification, algorithmic fairness},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}
  
"
"@inproceedings{10.1145/3287560.3287586,
author = {Celis, L. Elisa and Huang, Lingxiao and Keswani, Vijay and Vishnoi, Nisheeth K.},
title = {Classification with Fairness Constraints: A Meta-Algorithm with Provable Guarantees},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287586},
doi = {10.1145/3287560.3287586},
abstract = {Developing classification algorithms that are fair with respect to sensitive attributes of the data is an important problem due to the increased deployment of classification algorithms in societal contexts. Several recent works have focused on studying classification with respect to specific fairness metrics, modeled the corresponding fair classification problem as constrained optimization problems, and developed tailored algorithms to solve them. Despite this, there still remain important metrics for which there are no fair classifiers with theoretical guarantees; primarily because the resulting optimization problem is non-convex. The main contribution of this paper is a meta-algorithm for classification that can take as input a general class of fairness constraints with respect to multiple non-disjoint and multi-valued sensitive attributes, and which comes with provable guarantees. In particular, our algorithm can handle non-convex ""linear fractional"" constraints (which includes fairness constraints such as predictive parity) for which no prior algorithm was known. Key to our results is an algorithm for a family of classification problems with convex constraints along with a reduction from classification problems with linear fractional constraints to this family. Empirically, we observe that our algorithm is fast, can achieve near-perfect fairness with respect to various fairness metrics, and the loss in accuracy due to the imposed fairness constraints is often small.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {319–328},
numpages = {10},
keywords = {Algorithmic Fairness, Classification},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}"
"@inproceedings{10.1145/3287560.3287589,
author = {Friedler, Sorelle A. and Scheidegger, Carlos and Venkatasubramanian, Suresh and Choudhary, Sonam and Hamilton, Evan P. and Roth, Derek},
title = {A Comparative Study of Fairness-Enhancing Interventions in Machine Learning},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287589},
doi = {10.1145/3287560.3287589},
abstract = {Computers are increasingly used to make decisions that have significant impact on people's lives. Often, these predictions can affect different population subgroups disproportionately. As a result, the issue of fairness has received much recent interest, and a number of fairness-enhanced classifiers have appeared in the literature. This paper seeks to study the following questions: how do these different techniques fundamentally compare to one another, and what accounts for the differences? Specifically, we seek to bring attention to many under-appreciated aspects of such fairness-enhancing interventions that require investigation for these algorithms to receive broad adoption.We present the results of an open benchmark we have developed that lets us compare a number of different algorithms under a variety of fairness measures and existing datasets. We find that although different algorithms tend to prefer specific formulations of fairness preservations, many of these measures strongly correlate with one another. In addition, we find that fairness-preserving algorithms tend to be sensitive to fluctuations in dataset composition (simulated in our benchmark by varying training-test splits) and to different forms of preprocessing, indicating that fairness interventions might be more brittle than previously thought.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {329–338},
numpages = {10},
keywords = {Fairness-aware machine learning, benchmarks},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}
  
"
"@inproceedings{10.1145/3287560.3287594,
author = {Chen, Jiahao and Kallus, Nathan and Mao, Xiaojie and Svacha, Geoffry and Udell, Madeleine},
title = {Fairness Under Unawareness: Assessing Disparity When Protected Class Is Unobserved},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287594},
doi = {10.1145/3287560.3287594},
abstract = {Assessing the fairness of a decision making system with respect to a protected class, such as gender or race, is challenging when class membership labels are unavailable. Probabilistic models for predicting the protected class based on observable proxies, such as surname and geolocation for race, are sometimes used to impute these missing labels for compliance assessments. Empirically, these methods are observed to exaggerate disparities, but the reason why is unknown. In this paper, we decompose the biases in estimating outcome disparity via threshold-based imputation into multiple interpretable bias sources, allowing us to explain when over- or underestimation occurs. We also propose an alternative weighted estimator that uses soft classification, and show that its bias arises simply from the conditional covariance of the outcome with the true class membership. Finally, we illustrate our results with numerical simulations and a public dataset of mortgage applications, using geolocation as a proxy for race. We confirm that the bias of threshold-based imputation is generally upward, but its magnitude varies strongly with the threshold chosen. Our new weighted estimator tends to have a negative bias that is much simpler to analyze and reason about.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {339–348},
numpages = {10},
keywords = {race imputation, protected class, probablistic proxy model, fair lending, disparate impact, racial discrimination, Bayesian Improved Surname Geocoding},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}
  
"
"@inproceedings{10.1145/3287560.3287564,
author = {Madras, David and Creager, Elliot and Pitassi, Toniann and Zemel, Richard},
title = {Fairness through Causal Awareness: Learning Causal Latent-Variable Models for Biased Data},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287564},
doi = {10.1145/3287560.3287564},
abstract = {How do we learn from biased data? Historical datasets often reflect historical prejudices; sensitive or protected attributes may affect the observed treatments and outcomes. Classification algorithms tasked with predicting outcomes accurately from these datasets tend to replicate these biases. We advocate a causal modeling approach to learning from biased data, exploring the relationship between fair classification and intervention. We propose a causal model in which the sensitive attribute confounds both the treatment and the outcome. Building on prior work in deep learning and generative modeling, we describe how to learn the parameters of this causal model from observational data alone, even in the presence of unobserved confounders. We show experimentally that fairness-aware causal modeling provides better estimates of the causal effects between the sensitive attribute, the treatment, and the outcome. We further present evidence that estimating these causal effects can help learn policies that are both more accurate and fair, when presented with a historically biased dataset.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {349–358},
numpages = {10},
keywords = {fairness in machine learning, causal inference, variational inference},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}
  
"
"@inproceedings{10.1145/3287560.3287599,
author = {Mouzannar, Hussein and Ohannessian, Mesrob I. and Srebro, Nathan},
title = {From Fair Decision Making To Social Equality},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287599},
doi = {10.1145/3287560.3287599},
abstract = {The study of fairness in intelligent decision systems has mostly ignored long-term influence on the underlying population. Yet fairness considerations (e.g. affirmative action) have often the implicit goal of achieving balance among groups within the population. The most basic notion of balance is eventual equality between the qualifications of the groups. How can we incorporate influence dynamics in decision making? How well do dynamics-oblivious fairness policies fare in terms of reaching equality? In this paper, we propose a simple yet revealing model that encompasses (1) a selection process where an institution chooses from multiple groups according to their qualifications so as to maximize an institutional utility and (2) dynamics that govern the evolution of the groups' qualifications according to the imposed policies. We focus on demographic parity as the formalism of affirmative action.We first give conditions under which an unconstrained policy reaches equality on its own. In this case, surprisingly, imposing demographic parity may break equality. When it doesn't, one would expect the additional constraint to reduce utility, however, we show that utility may in fact increase. In real world scenarios, unconstrained policies do not lead to equality. In such cases, we show that although imposing demographic parity may remedy it, there is a danger that groups settle at a worse set of qualifications. As a silver lining, we also identify when the constraint not only leads to equality, but also improves all groups. These cases and trade-offs are instrumental in determining when and how imposing demographic parity can be beneficial in selection processes, both for the institution and for society on the long run.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {359–368},
numpages = {10},
keywords = {social equality, influence on society, demographic parity, selection processes, dynamics, affirmative action, fairness},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}
  
"
"@inproceedings{10.1145/3351095.3372874,
author = {Katell, Michael and Young, Meg and Dailey, Dharma and Herman, Bernease and Guetler, Vivian and Tam, Aaron and Bintz, Corinne and Raz, Daniella and Krafft, P. M.},
title = {Toward Situated Interventions for Algorithmic Equity: Lessons from the Field},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372874},
doi = {10.1145/3351095.3372874},
abstract = {Research to date aimed at the fairness, accountability, and transparency of algorithmic systems has largely focused on topics such as identifying failures of current systems and on technical interventions intended to reduce bias in computational processes. Researchers have given less attention to methods that account for the social and political contexts of specific, situated technical systems at their points of use. Co-developing algorithmic accountability interventions in communities supports outcomes that are more likely to address problems in their situated context and re-center power with those most disparately affected by the harms of algorithmic systems. In this paper we report on our experiences using participatory and co-design methods for algorithmic accountability in a project called the Algorithmic Equity Toolkit. The main insights we gleaned from our experiences were: (i) many meaningful interventions toward equitable algorithmic systems are non-technical; (ii) community organizations derive the most value from localized materials as opposed to what is ""scalable"" beyond a particular policy context; (iii) framing harms around algorithmic bias suggests that more accurate data is the solution, at the risk of missing deeper questions about whether some technologies should be used at all. More broadly, we found that community-based methods are important inroads to addressing algorithmic harms in their situated contexts.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {45–55},
numpages = {11},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
"@inproceedings{10.1145/3351095.3372827,
author = {Sendak, Mark and Elish, Madeleine Clare and Gao, Michael and Futoma, Joseph and Ratliff, William and Nichols, Marshall and Bedoya, Armando and Balu, Suresh and O'Brien, Cara},
title = {""The Human Body is a Black Box"": Supporting Clinical Decision-Making with Deep Learning},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372827},
doi = {10.1145/3351095.3372827},
abstract = {Machine learning technologies are increasingly developed for use in healthcare. While research communities have focused on creating state-of-the-art models, there has been less focus on real world implementation and the associated challenges to fairness, transparency, and accountability that come from actual, situated use. Serious questions remain underexamined regarding how to ethically build models, interpret and explain model output, recognize and account for biases, and minimize disruptions to professional expertise and work cultures. We address this gap in the literature and provide a detailed case study covering the development, implementation, and evaluation of Sepsis Watch, a machine learning-driven tool that assists hospital clinicians in the early diagnosis and treatment of sepsis. Sepsis is a severe infection that can lead to organ failure or death if not treated in time and is the leading cause of inpatient deaths in US hospitals. We, the team that developed and evaluated the tool, discuss our conceptualization of the tool not as a model deployed in the world but instead as a socio-technical system requiring integration into existing social and professional contexts. Rather than focusing solely on model interpretability to ensure fair and accountable machine learning, we point toward four key values and practices that should be considered when developing machine learning to support clinical decision-making: rigorously define the problem in context, build relationships with stakeholders, respect professional discretion, and create ongoing feedback loops with stakeholders. Our work has significant implications for future research regarding mechanisms of institutional accountability and considerations for responsibly designing machine learning systems. Our work underscores the limits of model interpretability as a solution to ensure transparency, accuracy, and accountability in practice. Instead, our work demonstrates other means and goals to achieve FATML values in design and in practice.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {99–109},
numpages = {11},
keywords = {interpretability, trust, deep learning, expertise, medicine},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
"@inproceedings{10.1145/3351095.3373154,
author = {Kallus, Nathan and Mao, Xiaojie and Zhou, Angela},
title = {Assessing Algorithmic Fairness with Unobserved Protected Class Using Data Combination},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3373154},
doi = {10.1145/3351095.3373154},
abstract = {The increasing impact of algorithmic decisions on people's lives compels us to scrutinize their fairness and, in particular, the disparate impacts that ostensibly-color-blind algorithms can have on different groups. Examples include credit decisioning, hiring, advertising, criminal justice, personalized medicine, and targeted policymaking, where in some cases legislative or regulatory frameworks for fairness exist and define specific protected classes. In this paper we study a fundamental challenge to assessing disparate impacts, or performance disparities in general, in practice: protected class membership is often not observed in the data. This is particularly a problem in lending and healthcare. We consider the use of an auxiliary dataset, such as the US census, that includes protected class labels but not decisions or outcomes. We show that a variety of common disparity measures are generally unidentifiable aside for some unrealistic cases, providing a new perspective on the documented biases of popular proxy-based methods. We provide exact characterizations of the sharpest-possible partial identification set of disparities either under no assumptions or when we incorporate mild smoothness constraints. We further provide optimization-based algorithms for computing and visualizing these sets of simultaneously achievable pairwise disparties for assessing disparities that arise between multiple groups, which enables reliable and robust assessments - an important tool when disparity assessment can have far-reaching policy implications. We demonstrate this in two case studies with real data: mortgage lending and personalized medicine dosing.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {110},
numpages = {1},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
"@inproceedings{10.1145/3351095.3372845,
author = {Black, Emily and Yeom, Samuel and Fredrikson, Matt},
title = {FlipTest: Fairness Testing via Optimal Transport},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372845},
doi = {10.1145/3351095.3372845},
abstract = {We present FlipTest, a black-box technique for uncovering discrimination in classifiers. FlipTest is motivated by the intuitive question: had an individual been of a different protected status, would the model have treated them differently? Rather than relying on causal information to answer this question, FlipTest leverages optimal transport to match individuals in different protected groups, creating similar pairs of in-distribution samples. We show how to use these instances to detect discrimination by constructing a flipset: the set of individuals whose classifier output changes post-translation, which corresponds to the set of people who may be harmed because of their group membership. To shed light on why the model treats a given subgroup differently, FlipTest produces a transparency report: a ranking of features that are most associated with the model's behavior on the flipset. Evaluating the approach on three case studies, we show that this provides a computationally inexpensive way to identify subgroups that may be harmed by model discrimination, including in cases where the model satisfies group fairness criteria.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {111–121},
numpages = {11},
keywords = {disparate impact, machine learning, fairness, optimal transport},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
"@inproceedings{10.1145/3351095.3372867,
author = {Marcinkowski, Frank and Kieslich, Kimon and Starke, Christopher and L\""{u}nich, Marco},
title = {Implications of AI (Un-)Fairness in Higher Education Admissions: The Effects of Perceived AI (Un-)Fairness on Exit, Voice and Organizational Reputation},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372867},
doi = {10.1145/3351095.3372867},
abstract = {Algorithmic decision-making (ADM) is becoming increasingly important in all areas of social life. In higher education, machine-learning systems have manifold uses because they can efficiently process large amounts of student data and use these data to arrive at effective decisions. Despite the potential upsides of ADM systems, fairness concerns are gaining momentum in academic and public discourses. The criticism largely focuses on the disparate effects of ADM. That is, algorithms may not serve as objective and fair decision-makers but, rather, reproduce biases existing within the respective training data. This study adopted a different approach by focusing on individual perceptions of fairness. Specifically, we looked at two different dimensions of perceived fairness: (i) procedural fairness and (ii) distributive fairness. Using cross-sectional survey data (n = 304) from a large German university, we tested whether students' assessments of fairness differ with respect to algorithmic vs. human decision-making (HDM) within the higher education context. Furthermore, we investigated whether fairness perceptions have subsequent effects on three different outcome variables, which are hugely important for universities: (1) exit, (2) voice, and (3) organizational reputation. The results of our survey suggest that participants evaluated ADM higher than HDM in terms of both procedural and distributive fairness. Concerning the subsequent effects of fairness perceptions, we find that (1) distributive fairness as well as procedural fairness perceptions have a negative impact on the intention to protest against an ADM system, whereas (2) only procedural fairness perceptions negatively affect the likelihood of exiting. Finally, (3) distributive fairness, but not procedural fairness perceptions have a positive effect on organizational reputation. For universities aiming to implement ADM systems, it is crucial, therefore, to take possible fairness issues and their further implications into account.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {122–130},
numpages = {9},
keywords = {distributive fairness, higher education systems, artificial intelligence, voice, exit, reputation, procedural fairness, algorithmic decision making},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
"@inproceedings{10.1145/3351095.3372863,
author = {Rodolfa, Kit T. and Salomon, Erika and Haynes, Lauren and Mendieta, Iv\'{a}n Higuera and Larson, Jamie and Ghani, Rayid},
title = {Case Study: Predictive Fairness to Reduce Misdemeanor Recidivism through Social Service Interventions},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372863},
doi = {10.1145/3351095.3372863},
abstract = {The criminal justice system is currently ill-equipped to improve outcomes of individuals who cycle in and out of the system with a series of misdemeanor offenses. Often due to constraints of caseload and poor record linkage, prior interactions with an individual may not be considered when an individual comes back into the system, let alone in a proactive manner through the application of diversion programs. The Los Angeles City Attorney's Office recently created a new Recidivism Reduction and Drug Diversion unit (R2D2) tasked with reducing recidivism in this population. Here we describe a collaboration with this new unit as a case study for the incorporation of predictive equity into machine learning based decision making in a resource-constrained setting. The program seeks to improve outcomes by developing individually-tailored social service interventions (i.e., diversions, conditional plea agreements, stayed sentencing, or other favorable case disposition based on appropriate social service linkage rather than traditional sentencing methods) for individuals likely to experience subsequent interactions with the criminal justice system, a time and resource-intensive undertaking that necessitates an ability to focus resources on individuals most likely to be involved in a future case. Seeking to achieve both efficiency (through predictive accuracy) and equity (improving outcomes in traditionally under-served communities and working to mitigate existing disparities in criminal justice outcomes), we discuss the equity outcomes we seek to achieve, describe the corresponding choice of a metric for measuring predictive fairness in this context, and explore a set of options for balancing equity and efficiency when building and selecting machine learning models in an operational public policy setting.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {142–153},
numpages = {12},
keywords = {algorithmic fairness, criminal justice, machine learning disparities, racial bias},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
"@inproceedings{10.1145/3351095.3372868,
author = {Malgieri, Gianclaudio},
title = {The Concept of Fairness in the GDPR: A Linguistic and Contextual Interpretation},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372868},
doi = {10.1145/3351095.3372868},
abstract = {There is a growing attention on the notion of fairness in the GDPR in the European legal literature. However, the principle of fairness in the Data Protection framework is still ambiguous and uncertain, as computer science literature and interpretative guidelines reveal.This paper looks for a better understanding of the concept of fairness in the data protection field through two parallel methodological tools: linguistic comparison and contextual interpretation.In terms of linguistic comparison, the paper analyses all translations of the world ""fair"" in the GDPR in the EU official languages, as the CJEU suggests in CILFIT Case for the interpretation of the EU law.The analysis takes into account also the translation of the notion of fairness in other contiguous fields (e.g. at Article 8 of the EU Charter of fundamental rights or in the Consumer field, e.g. Unfair terms directive or Unfair commercial practice directive).In general, the notion of fairness is translated with several different nuances (in accordance or in discordance with the previous Data protection Directive and with Article 8 of the Charter)In some versions different words are used interchangeably (it is the case of French, Spanish and Portuguese texts), in other versions there seems to be a specific rationale for using different terms in different parts of the GDPR (it is the case of German and Greek version).The analysis reveals three mean semantic notions: correctness (Italian, Swedish, Romanian), loyalty (French, Spanish, Portuguese and the German version of ""Treu und Glaube"") and equitability (French, Spanish and Portuguese).Interestingly, these three notions have common roots in the Western legal history: the Roman law notion of ""bona fide"".Taking into account both the value of ""bona fide"" in the current European legal contexts and also a contextual interpretation of the role of fairness in the GDPR, the preliminary conclusions is that fairness refers to a substantial balancing of interests among data controllers and data subjects.The approach of fairness is effect-based: what is relevant is not the formal respect of procedures (in terms of transparency, lawfulness or accountability), but the substantial mitigation of unfair imbalances that create situations of ""vulnerability"". Building on these reflections, the paper analyses how the notion of fairness and imbalance are related to the idea of vulnerability, within and beyond the GDPR.In sum, the article suggests that the best interpretation of the fairness principles in the GDPR (taking into account both the notion of procedural fairness and of fair balancing) is the mitigation of data subjects' vulnerabilities through specific safeguards and measures.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {154–166},
numpages = {13},
keywords = {linguistic comparison, fairness, data protection, GDPR},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
"@inproceedings{10.1145/3351095.3372859,
author = {Barabas, Chelsea and Doyle, Colin and Rubinovitz, JB and Dinakar, Karthik},
title = {Studying up: Reorienting the Study of Algorithmic Fairness around Issues of Power},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372859},
doi = {10.1145/3351095.3372859},
abstract = {Research within the social sciences and humanities has long characterized the work of data science as a sociotechnical process, comprised of a set of logics and techniques that are inseparable from specific social norms, expectations and contexts of development and use. Yet all too often the assumptions and premises underlying data analysis remain unexamined, even in contemporary debates about the fairness of algorithmic systems. This blindspot exists in part because the methodological toolkit used to evaluate the fairness of algorithmic systems remains limited to a narrow set of computational and legal modes of analysis. In this paper, we expand on Elish and Boyd's [17] call for data scientists to develop more robust frameworks for understanding their work as situated practice by examining a specific methodological debate within the field of anthropology, frequently referred to as the practice of ""studying up"". We reflect on the contributions that the call to ""study up"" has made in the field of anthropology before making the case that the field of algorithmic fairness would similarly benefit from a reorientation ""upward"". A case study from our own work illustrates what it looks like to reorient one's research questions ""up"" in a high-profile debate regarding the fairness of an algorithmic system - namely, pretrial risk assessment in American criminal law. We discuss the limitations of contemporary fairness discourse with regard to pretrial risk assessment before highlighting the insights gained when we reframe our research questions to focus on those who inhabit positions of power and authority within the U.S. court system. Finally, we reflect on the challenges we have encountered in implementing data science projects that ""study up"". In the process, we surface new insights and questions about what it means to ethically engage in data science work that directly confronts issues of power and authority.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {167–176},
numpages = {10},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
"@inproceedings{10.1145/3351095.3372853,
author = {Kulynych, Bogdan and Overdorf, Rebekah and Troncoso, Carmela and G\""{u}rses, Seda},
title = {POTs: Protective Optimization Technologies},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372853},
doi = {10.1145/3351095.3372853},
abstract = {Algorithmic fairness aims to address the economic, moral, social, and political impact that digital systems have on populations through solutions that can be applied by service providers. Fairness frameworks do so, in part, by mapping these problems to a narrow definition and assuming the service providers can be trusted to deploy countermeasures. Not surprisingly, these decisions limit fairness frameworks' ability to capture a variety of harms caused by systems.We characterize fairness limitations using concepts from requirements engineering and from social sciences. We show that the focus on algorithms' inputs and outputs misses harms that arise from systems interacting with the world; that the focus on bias and discrimination omits broader harms on populations and their environments; and that relying on service providers excludes scenarios where they are not cooperative or intentionally adversarial.We propose Protective Optimization Technologies (POTs). POTs, provide means for affected parties to address the negative impacts of systems in the environment, expanding avenues for political contestation. POTs intervene from outside the system, do not require service providers to cooperate, and can serve to correct, shift, or expose harms that systems impose on populations and their environments. We illustrate the potential and limitations of POTs in two case studies: countering road congestion caused by traffic beating applications, and recalibrating credit scoring for loan applicants.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {177–188},
numpages = {12},
keywords = {protective optimization technologies, fairness and accountability},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
"@inproceedings{10.1145/3351095.3372872,
author = {Pujol, David and McKenna, Ryan and Kuppam, Satya and Hay, Michael and Machanavajjhala, Ashwin and Miklau, Gerome},
title = {Fair Decision Making Using Privacy-Protected Data},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372872},
doi = {10.1145/3351095.3372872},
abstract = {Data collected about individuals is regularly used to make decisions that impact those same individuals. We consider settings where sensitive personal data is used to decide who will receive resources or benefits. While it is well known that there is a trade-off between protecting privacy and the accuracy of decisions, we initiate a first-of-its-kind study into the impact of formally private mechanisms (based on differential privacy) on fair and equitable decision-making. We empirically investigate novel tradeoffs on two real-world decisions made using U.S. Census data (allocation of federal funds and assignment of voting rights benefits) as well as a classic apportionment problem.Our results show that if decisions are made using an ∈-differentially private version of the data, under strict privacy constraints (smaller ∈), the noise added to achieve privacy may disproportionately impact some groups over others. We propose novel measures of fairness in the context of randomized differentially private algorithms and identify a range of causes of outcome disparities. We also explore improved algorithms to remedy the unfairness observed.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {189–199},
numpages = {11},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
"@inproceedings{10.1145/3351095.3372839,
author = {Slack, Dylan and Friedler, Sorelle A. and Givental, Emile},
title = {Fairness Warnings and Fair-MAML: Learning Fairly with Minimal Data},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372839},
doi = {10.1145/3351095.3372839},
abstract = {Motivated by concerns surrounding the fairness effects of sharing and transferring fair machine learning tools, we propose two algorithms: Fairness Warnings and Fair-MAML. The first is a model-agnostic algorithm that provides interpretable boundary conditions for when a fairly trained model may not behave fairly on similar but slightly different tasks within a given domain. The second is a fair meta-learning approach to train models that can be quickly fine-tuned to specific tasks from only a few number of sample instances while balancing fairness and accuracy. We demonstrate experimentally the individual utility of each model using relevant baselines and provide the first experiment to our knowledge of K-shot fairness, i.e. training a fair model on a new task with only K data points. Then, we illustrate the usefulness of both algorithms as a combined method for training models from a few data points on new tasks while using Fairness Warnings as interpretable boundary conditions under which the newly trained model may not be fair.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {200–209},
numpages = {10},
keywords = {covariate shift, meta-learning, fairness, machine learning},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
"@inproceedings{10.1145/3351095.3373152,
author = {Terzis, Petros},
title = {Onward for the Freedom of Others: Marching beyond the AI Ethics},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3373152},
doi = {10.1145/3351095.3373152},
abstract = {The debate on the ethics of Artificial Intelligence brought together different stakeholders including but not limited to academics, policymakers, CEOs, activists, workers' representatives, lobbyists, journalists, and 'moral machines'. Prominent political institutions crafted principles for the 'ethical being' of the AI companies while tech giants were documenting ethics in a series of self-written guidelines. In parallel, a large community started to flourish, focusing on how to technically embed ethical parameters into algorithmic systems. Founded upon the philosophical work of Simone de Beauvoir and Jean-Paul Sartre, this paper explores the philosophical antinomies of the 'AI Ethics' debate as well as the conceptual disorientation of the 'fairness discussion'. By bringing the philosophy of existentialism to the dialogue, this paper attempts to challenge the dialectical monopoly of utilitarianism and sheds fresh light on the -already glaring- AI arena. Why is 'the AI Ethics guidelines' a futile battle doomed to dangerous abstraction? How this battle can harm our sense of collective freedom? Which is the uncomfortable reality that remains obscured by the smoke-gas of the 'AI Ethics' discussion? And eventually, what's the alternative? There seems to be a different pathway for discussing and implementing ethics; A pathway that sets the freedom of others at the epicenter of the battle for a sustainable and open to all future.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {220–229},
numpages = {10},
keywords = {existentialism, philosophy, algorithms, ethics, artificial intelligence},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
"@inproceedings{10.1145/3351095.3375784,
author = {Noriega-Campero, Alejandro and Garcia-Bulle, Bernardo and Cantu, Luis Fernando and Bakker, Michiel A. and Tejerina, Luis and Pentland, Alex},
title = {Algorithmic Targeting of Social Policies: Fairness, Accuracy, and Distributed Governance},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375784},
doi = {10.1145/3351095.3375784},
abstract = {Targeted social policies are the main strategy for poverty alleviation across the developing world. These include targeted cash transfers (CTs), as well as targeted subsidies in health, education, housing, energy, childcare, and others. Due to the scale, diversity, and widespread relevance of targeted social policies like CTs, the algorithmic rules that decide who is eligible to benefit from them---and who is not---are among the most important algorithms operating in the world today. Here we report on a year-long engagement towards improving social targeting systems in a couple of developing countries. We demonstrate that a shift towards the use of AI methods in poverty-based targeting can substantially increase accuracy, extending the coverage of the poor by nearly a million people in two countries, without increasing expenditure. However, we also show that, absent explicit parity constraints, both status quo and AI-based systems induce disparities across population subgroups. Moreover, based on qualitative interviews with local social institutions, we find a lack of consensus on normative standards for prioritization and fairness criteria. Hence, we close by proposing a decision-support platform for distributed governance, which enables a diversity of institutions to customize the use of AI-based insights into their targeting decisions.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {241–251},
numpages = {11},
keywords = {algorithmic fairness, proxy means tests, AI for social good, cash transfers, targeted social programs},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
"@inproceedings{10.1145/3351095.3372871,
author = {Abebe, Rediet and Barocas, Solon and Kleinberg, Jon and Levy, Karen and Raghavan, Manish and Robinson, David G.},
title = {Roles for Computing in Social Change},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372871},
doi = {10.1145/3351095.3372871},
abstract = {A recent normative turn in computer science has brought concerns about fairness, bias, and accountability to the core of the field. Yet recent scholarship has warned that much of this technical work treats problematic features of the status quo as fixed, and fails to address deeper patterns of injustice and inequality. While acknowledging these critiques, we posit that computational research has valuable roles to play in addressing social problems --- roles whose value can be recognized even from a perspective that aspires toward fundamental social change. In this paper, we articulate four such roles, through an analysis that considers the opportunities as well as the significant risks inherent in such work. Computing research can serve as a diagnostic, helping us to understand and measure social problems with precision and clarity. As a formalizer, computing shapes how social problems are explicitly defined --- changing how those problems, and possible responses to them, are understood. Computing serves as rebuttal when it illuminates the boundaries of what is possible through technical means. And computing acts as synecdoche when it makes long-standing social problems newly salient in the public eye. We offer these paths forward as modalities that leverage the particular strengths of computational work in the service of social change, without overclaiming computing's capacity to solve social problems on its own.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {252–260},
numpages = {9},
keywords = {discrimination, societal implications of AI, social change, inequality},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
"@inproceedings{10.1145/3351095.3372834,
author = {Toreini, Ehsan and Aitken, Mhairi and Coopamootoo, Kovila and Elliott, Karen and Zelaya, Carlos Gonzalez and van Moorsel, Aad},
title = {The Relationship between Trust in AI and Trustworthy Machine Learning Technologies},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372834},
doi = {10.1145/3351095.3372834},
abstract = {To design and develop AI-based systems that users and the larger public can justifiably trust, one needs to understand how machine learning technologies impact trust. To guide the design and implementation of trusted AI-based systems, this paper provides a systematic approach to relate considerations about trust from the social sciences to trustworthiness technologies proposed for AI-based services and products. We start from the ABI+ (Ability, Benevolence, Integrity, Predictability) framework augmented with a recently proposed mapping of ABI+ on qualities of technologies that support trust. We consider four categories of trustworthiness technologies for machine learning, namely these for Fairness, Explainability, Auditability and Safety (FEAS) and discuss if and how these support the required qualities. Moreover, trust can be impacted throughout the life cycle of AI-based systems, and we therefore introduce the concept of Chain of Trust to discuss trustworthiness technologies in all stages of the life cycle. In so doing we establish the ways in which machine learning technologies support trusted AI-based systems. Finally, FEAS has obvious relations with known frameworks and therefore we relate FEAS to a variety of international 'principled AI' policy and technology frameworks that have emerged in recent years.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {272–283},
numpages = {12},
keywords = {machine learning, trustworthiness, artificial intelligence, trust},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
"@inproceedings{10.1145/3351095.3372829,
author = {Jo, Eun Seo and Gebru, Timnit},
title = {Lessons from Archives: Strategies for Collecting Sociocultural Data in Machine Learning},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372829},
doi = {10.1145/3351095.3372829},
abstract = {A growing body of work shows that many problems in fairness, accountability, transparency, and ethics in machine learning systems are rooted in decisions surrounding the data collection and annotation process. In spite of its fundamental nature however, data collection remains an overlooked part of the machine learning (ML) pipeline. In this paper, we argue that a new specialization should be formed within ML that is focused on methodologies for data collection and annotation: efforts that require institutional frameworks and procedures. Specifically for sociocultural data, parallels can be drawn from archives and libraries. Archives are the longest standing communal effort to gather human information and archive scholars have already developed the language and procedures to address and discuss many challenges pertaining to data collection such as consent, power, inclusivity, transparency, and ethics & privacy. We discuss these five key approaches in document collection practices in archives that can inform data collection in sociocultural ML. By showing data collection practices from another field, we encourage ML research to be more cognizant and systematic in data collection and draw from interdisciplinary expertise.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {306–316},
numpages = {11},
keywords = {data collection, machine learning, archives, datasets, sociocultural data, ML fairness},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
"@inproceedings{10.1145/3351095.3375783,
author = {Nasr, Milad and Tschantz, Michael Carl},
title = {Bidding Strategies with Gender Nondiscrimination Constraints for Online Ad Auctions},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375783},
doi = {10.1145/3351095.3375783},
abstract = {Interactions between bids to show ads online can lead to an advertiser's ad being shown to more men than women even when the advertiser does not target towards men. We design bidding strategies that advertisers can use to avoid such emergent discrimination without having to modify the auction mechanism. We mathematically analyze the strategies to determine the additional cost to the advertiser for avoiding discrimination, proving our strategies to be optimal in some settings. We use simulations to understand other settings.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {337–347},
numpages = {11},
keywords = {targeted advertising, MDPs, online auctions, fairness constraints},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
"@inproceedings{10.1145/3351095.3372848,
author = {Ilvento, Christina and Jagadeesan, Meena and Chawla, Shuchi},
title = {Multi-Category Fairness in Sponsored Search Auctions},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372848},
doi = {10.1145/3351095.3372848},
abstract = {Fairness in advertising is a topic of particular concern motivated by theoretical and empirical observations in both the computer science and economics literature. We examine the problem of fairness in advertising for general purpose platforms that service advertisers from many different categories. First, we propose inter-category and intra-category fairness desiderata that take inspiration from individual fairness and envy-freeness. Second, we investigate the ""platform utility"" (a proxy for the quality of allocation) achievable by mechanisms satisfying these desiderata. More specifically, we compare the utility of fair mechanisms against the unfair optimum, and show by construction that our fairness desiderata are compatible with utility. Our mechanisms also enjoy nice implementation properties including metric-obliviousness, which allows the platform to produce fair allocations without needing to know the specifics of the fairness requirements.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {348–358},
numpages = {11},
keywords = {envy-freeness, algorithmic fairness, advertisement auctions, individual fairness, utility},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
"@inproceedings{10.1145/3351095.3372837,
author = {Sweeney, Chris and Najafian, Maryam},
title = {Reducing Sentiment Polarity for Demographic Attributes in Word Embeddings Using Adversarial Learning},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372837},
doi = {10.1145/3351095.3372837},
abstract = {The use of word embedding models in sentiment analysis has gained a lot of traction in the Natural Language Processing (NLP) community. However, many inherently neutral word vectors describing demographic identity have unintended implicit correlations with negative or positive sentiment, resulting in unfair downstream machine learning algorithms. We leverage adversarial learning to decorrelate demographic identity term word vectors with positive or negative sentiment, and re-embed them into the word embeddings. We show that our method effectively minimizes unfair positive/negative sentiment polarity while retaining the semantic accuracy of the word embeddings. Furthermore, we show that our method effectively reduces unfairness in downstream sentiment regression and can be extended to reduce unfairness in toxicity classification tasks.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {359–368},
numpages = {10},
keywords = {embeddings, fairness, NLP},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
"@inproceedings{10.1145/3351095.3372831,
author = {Harrison, Galen and Hanson, Julia and Jacinto, Christine and Ramirez, Julio and Ur, Blase},
title = {An Empirical Study on the Perceived Fairness of Realistic, Imperfect Machine Learning Models},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372831},
doi = {10.1145/3351095.3372831},
abstract = {There are many competing definitions of what statistical properties make a machine learning model fair. Unfortunately, research has shown that some key properties are mutually exclusive. Realistic models are thus necessarily imperfect, choosing one side of a trade-off or the other. To gauge perceptions of the fairness of such realistic, imperfect models, we conducted a between-subjects experiment with 502 Mechanical Turk workers. Each participant compared two models for deciding whether to grant bail to criminal defendants. The first model equalized one potentially desirable model property, with the other property varying across racial groups. The second model did the opposite. We tested pairwise trade-offs between the following four properties: accuracy; false positive rate; outcomes; and the consideration of race. We also varied which racial group the model disadvantaged. We observed a preference among participants for equalizing the false positive rate between groups over equalizing accuracy. Nonetheless, no preferences were overwhelming, and both sides of each trade-off we tested were strongly preferred by a non-trivial fraction of participants. We observed nuanced distinctions between participants considering a model ""unbiased"" and considering it ""fair."" Furthermore, even when a model within a trade-off pair was seen as fair and unbiased by a majority of participants, we did not observe consensus that a machine learning model was preferable to a human judge. Our results highlight challenges for building machine learning models that are perceived as fair and broadly acceptable in realistic situations.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {392–402},
numpages = {11},
keywords = {fairness, machine learning, accountability, survey, data science},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
"@inproceedings{10.1145/3351095.3375623,
author = {Liang, Lizhen and Acuna, Daniel E.},
title = {Artificial Mental Phenomena: Psychophysics as a Framework to Detect Perception Biases in AI Models},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375623},
doi = {10.1145/3351095.3375623},
abstract = {Detecting biases in artificial intelligence has become difficult because of the impenetrable nature of deep learning. The central difficulty is in relating unobservable phenomena deep inside models with observable, outside quantities that we can measure from inputs and outputs. For example, can we detect gendered perceptions of occupations (e.g., female librarian, male electrician) using questions to and answers from a word embedding-based system? Current techniques for detecting biases are often customized for a task, dataset, or method, affecting their generalization. In this work, we draw from Psychophysics in Experimental Psychology---meant to relate quantities from the real world (i.e., ""Physics"") into subjective measures in the mind (i.e., ""Psyche"")---to propose an intellectually coherent and generalizable framework to detect biases in AI. Specifically, we adapt the two-alternative forced choice task (2AFC) to estimate potential biases and the strength of those biases in black-box models. We successfully reproduce previously-known biased perceptions in word embeddings and sentiment analysis predictions. We discuss how concepts in experimental psychology can be naturally applied to understanding artificial mental phenomena, and how psychophysics can form a useful methodological foundation to study fairness in AI.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {403–412},
numpages = {10},
keywords = {biases in word embeddings, artificial psychophysics, two-alternative forced choice task, biases in sentiment analysis},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"


@InProceedings{agarwal2018reductions,
  title = 	 {A Reductions Approach to Fair Classification},
  author =       {Agarwal, Alekh and Beygelzimer, Alina and Dudik, Miroslav and Langford, John and Wallach, Hanna},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {60--69},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/agarwal18a/agarwal18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/agarwal18a.html},
  abstract = 	 {We present a systematic approach for achieving fairness in a binary classification setting. While we focus on two well-known quantitative definitions of fairness, our approach encompasses many other previously studied definitions as special cases. The key idea is to reduce fair classification to a sequence of cost-sensitive classification problems, whose solutions yield a randomized classifier with the lowest (empirical) error subject to the desired constraints. We introduce two reductions that work for any representation of the cost-sensitive classifier and compare favorably to prior baselines on a variety of data sets, while overcoming several of their disadvantages.}
}


@article{
obermeyer2019dissecting,
author = {Ziad Obermeyer  and Brian Powers  and Christine Vogeli  and Sendhil Mullainathan },
title = {Dissecting racial bias in an algorithm used to manage the health of populations},
journal = {Science},
volume = {366},
number = {6464},
pages = {447-453},
year = {2019},
doi = {10.1126/science.aax2342},
URL = {https://www.science.org/doi/abs/10.1126/science.aax2342},
eprint = {https://www.science.org/doi/pdf/10.1126/science.aax2342},
abstract = {The U.S. health care system uses commercial algorithms to guide health decisions. Obermeyer et al. find evidence of racial bias in one widely used algorithm, such that Black patients assigned the same level of risk by the algorithm are sicker than White patients (see the Perspective by Benjamin). The authors estimated that this racial bias reduces the number of Black patients identified for extra care by more than half. Bias occurs because the algorithm uses health costs as a proxy for health needs. Less money is spent on Black patients who have the same level of need, and the algorithm thus falsely concludes that Black patients are healthier than equally sick White patients. Reformulating the algorithm so that it no longer uses costs as a proxy for needs eliminates the racial bias in predicting who needs extra care. Science, this issue p. 447; see also p. 421 A health algorithm that uses health costs as a proxy for health needs leads to racial bias against Black patients. Health systems rely on commercial prediction algorithms to identify and help patients with complex health needs. We show that a widely used algorithm, typical of this industry-wide approach and affecting millions of patients, exhibits significant racial bias: At a given risk score, Black patients are considerably sicker than White patients, as evidenced by signs of uncontrolled illnesses. Remedying this disparity would increase the percentage of Black patients receiving additional help from 17.7 to 46.5\%. The bias arises because the algorithm predicts health care costs rather than illness, but unequal access to care means that we spend less money caring for Black patients than for White patients. Thus, despite health care cost appearing to be an effective proxy for health by some measures of predictive accuracy, large racial biases arise. We suggest that the choice of convenient, seemingly effective proxies for ground truth can be an important source of algorithmic bias in many contexts.}}

"@inproceedings{10.1145/3351095.3373156,
author = {Castelle, Michael},
title = {The Social Lives of Generative Adversarial Networks},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3373156},
doi = {10.1145/3351095.3373156},
abstract = {Generative adversarial networks (GANs) are a genre of deep learning model of significant practical and theoretical interest for their facility in producing photorealistic 'fake' images which are plausibly similar, but not identical, to a corpus of training data. But from the perspective of a sociologist, the distinctive architecture of GANs is highly suggestive. First, a convolutional neural network for classification, on its own, is (at present) popularly considered to be an 'AI'; and a generative neural network is a kind of inversion of such a classification network (i.e. a layered transformation from a vector of numbers to an image, as opposed to a transformation from an image to a vector of numbers). If, then, in the training of GANs, these two 'AIs' interact with each other in a dyadic fashion, shouldn't we consider that form of learning... social? This observation can lead to some surprising associations as we compare and contrast GANs with the theories of the sociologist Pierre Bourdieu, whose concept of the so-called habitus is one which is simultaneously cognitive and social: a productive perception in which classification practices and practical action cannot be fully disentangled.Bourdieu had long been concerned with the reproduction of social stratification: his early works studied formal public schooling in France not as an egalitarian system but instead as one which unintentionally maintained the persistence of class distinctions. It was, he argued, through the cultural inculcation of an embodied and partially unconscious habitus---a ""durably installed generative principle of regulated improvisations""---that, he argued, students from the upper classes are given an advantage which is only further reinforced throughout their educational trajectories. For Bourdieu, institutions of schooling instill ""deeply interiorized master patterns"" of behavior and thought (and classification) which in turn direct the acquisition of subsequent patterns, whose character is determined not simply by this cognitive layering but by their actual use in lived practice, especially early in childhood development.In this work I develop a productive analogy between the GAN architecture and Bourdieu's habitus, in three ways. First, I call attention to the fact that connectionist approaches and Bourdieu's theories were both conceived as revolts against rule-bound paradigms. In the 1980s, Rumelhart and McClelland used a multilayer neural network to learn the phonology of English past-tense verbs because ""sometimes we don't follow the rules... language is full of exceptions to the rules""; and in the case of Bourdieu, the habitus was an answer to a long-standing question: ""how can behaviour be regulated without being the product of obedience to rules?"" Bourdieu strove to transgress what was then seen in the social sciences as a conceptual opposition between structure-based theories of social life and those which emphasized an embodied agency.Second, I suggest that concerns about bias and discrimination in machine learning in recent years can in part be attributed due to the increased use of ML models not just for static classification but for practical action. Similarly, the habitus for Bourdieu is simultaneously durable and transposable: its judgments may be relatively stable, but are capable of being deployed dynamically in novel and varying social situations---or what ML practitioners might call generalizability. We can thus theorize generative models (including GANs) as biased not just in their stereotyped classifications, but through their potential for actively generating new biased data. These generated actions then recursively become part of the social arena Bourdieu called the field, into which new agents are 'born' and for which they may know few alternatives.Finally, it is intriguing that GAN researchers and Bourdieu both extensively use metaphors from game theory. Goodfellow described the GAN architecture as a ""two-player minimax game with value function V(G,D)"", meaning that there is a single abstract function whose output value the discriminator is trying to maximize and which the generator is trying to minimize; but the dynamic nature of the GAN training process means that convergence to Nash equilibrium is nontrivial. But for Bourdieu, such a utility-based approach to artistic creation could not be more crude when compared to the social reality of art worlds: utilitarianism is, for him, ""the degree zero of sociology"", by which he means an isolated, inert, and amodal---and therefore not particularly sociological---starting point. Moreover, 19th-century bohemian culture was characterized primarily by its inversion of financial incentives, in which failure is a kind of success, and ""selling out"" (i.e. maximizing profit) worst of all; and thus the relentless optimization of neural networks may be fundamentally at odds with the ""value functions"" of many human artists. I conclude that deep learning, while primarily understood as a scientific and technical achievement, may also intentionally or unintentionally constitute a nascent, independent reinvention of social theory.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {413},
numpages = {1},
keywords = {generative adversarial networks, sociological theory, habitus, bias, game theory},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
"@inproceedings{10.1145/3351095.3372832,
author = {Bates, Jo and Cameron, David and Checco, Alessandro and Clough, Paul and Hopfgartner, Frank and Mazumdar, Suvodeep and Sbaffi, Laura and Stordy, Peter and de la Vega de Le\'{o}n, Antonio},
title = {Integrating FATE/Critical Data Studies into Data Science Curricula: Where Are We Going and How Do We Get There?},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372832},
doi = {10.1145/3351095.3372832},
abstract = {There have been multiple calls for integrating topics related to fairness, accountability, transparency, ethics (FATE) and social justice into Data Science curricula, but little exploration of how this might work in practice. This paper presents the findings of a collaborative auto-ethnography (CAE) engaged in by a MSc Data Science teaching team based at University of Sheffield (UK) Information School where FATE/Critical Data Studies (CDS) topics have been a core part of the curriculum since 2015/16. In this paper, we adopt the CAE approach to reflect on our experiences of working at the intersection of disciplines, and our progress and future plans for integrating FATE/CDS into the curriculum. We identify a series of challenges for deeper FATE/CDS integration related to our own competencies and the wider socio-material context of Higher Education in the UK. We conclude with recommendations for ourselves and the wider FATE/CDS orientated Data Science community.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {425–435},
numpages = {11},
keywords = {critical data studies, data science, higher education, FATE},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
"@inproceedings{10.1145/3351095.3372843,
author = {Papakyriakopoulos, Orestis and Hegelich, Simon and Serrano, Juan Carlos Medina and Marco, Fabienne},
title = {Bias in Word Embeddings},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372843},
doi = {10.1145/3351095.3372843},
abstract = {Word embeddings are a widely used set of natural language processing techniques that map words to vectors of real numbers. These vectors are used to improve the quality of generative and predictive models. Recent studies demonstrate that word embeddings contain and amplify biases present in data, such as stereotypes and prejudice. In this study, we provide a complete overview of bias in word embeddings. We develop a new technique for bias detection for gendered languages and use it to compare bias in embeddings trained on Wikipedia and on political social media data. We investigate bias diffusion and prove that existing biases are transferred to further machine learning models. We test two techniques for bias mitigation and show that the generally proposed methodology for debiasing models at the embeddings level is insufficient. Finally, we employ biased word embeddings and illustrate that they can be used for the detection of similar biases in new data. Given that word embeddings are widely used by commercial companies, we discuss the challenges and required actions towards fair algorithmic implementations and applications.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {446–457},
numpages = {12},
keywords = {diffusion, word embeddings, sexism, racism, mitigation, bias, homophobia, fairness, detection},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
"@inproceedings{10.1145/3351095.3372849,
author = {S\'{a}nchez-Monedero, Javier and Dencik, Lina and Edwards, Lilian},
title = {What Does It Mean to 'solve' the Problem of Discrimination in Hiring? Social, Technical and Legal Perspectives from the UK on Automated Hiring Systems},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372849},
doi = {10.1145/3351095.3372849},
abstract = {Discriminatory practices in recruitment and hiring are an ongoing issue that is a concern not just for workplace relations, but also for wider understandings of economic justice and inequality. The ability to get and keep a job is a key aspect of participating in society and sustaining livelihoods. Yet the way decisions are made on who is eligible for jobs, and why, are rapidly changing with the advent and growth in uptake of automated hiring systems (AHSs) powered by data-driven tools. Evidence of the extent of this uptake around the globe is scarce, but a recent report estimated that 98% of Fortune 500 companies use Applicant Tracking Systems of some kind in their hiring process, a trend driven by perceived efficiency measures and cost-savings. Key concerns about such AHSs include the lack of transparency and potential limitation of access to jobs for specific profiles. In relation to the latter, however, several of these AHSs claim to detect and mitigate discriminatory practices against protected groups and promote diversity and inclusion at work. Yet whilst these tools have a growing user-base around the world, such claims of 'bias mitigation' are rarely scrutinised and evaluated, and when done so, have almost exclusively been from a US socio-legal perspective.In this paper, we introduce a perspective outside the US by critically examining how three prominent automated hiring systems (AHSs) in regular use in the UK, HireVue, Pymetrics and Applied, understand and attempt to mitigate bias and discrimination. These systems have been chosen as they explicitly claim to address issues of discrimination in hiring and, unlike many of their competitors, provide some information about how their systems work that can inform an analysis. Using publicly available documents, we describe how their tools are designed, validated and audited for bias, highlighting assumptions and limitations, before situating these in the socio-legal context of the UK. The UK has a very different legal background to the US in terms not only of hiring and equality law, but also in terms of data protection (DP) law. We argue that this might be important for addressing concerns about transparency and could mean a challenge to building bias mitigation into AHSs definitively capable of meeting EU legal standards. This is significant as these AHSs, especially those developed in the US, may obscure rather than improve systemic discrimination in the workplace.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {458–468},
numpages = {11},
keywords = {social justice, fairness, automated hiring, discrimination, GDPR, algorithmic decision-making, socio-technical systems},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
"@inproceedings{10.1145/3351095.3372828,
author = {Raghavan, Manish and Barocas, Solon and Kleinberg, Jon and Levy, Karen},
title = {Mitigating Bias in Algorithmic Hiring: Evaluating Claims and Practices},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372828},
doi = {10.1145/3351095.3372828},
abstract = {There has been rapidly growing interest in the use of algorithms in hiring, especially as a means to address or mitigate bias. Yet, to date, little is known about how these methods are used in practice. How are algorithmic assessments built, validated, and examined for bias? In this work, we document and analyze the claims and practices of companies offering algorithms for employment assessment. In particular, we identify vendors of algorithmic pre-employment assessments (i.e., algorithms to screen candidates), document what they have disclosed about their development and validation procedures, and evaluate their practices, focusing particularly on efforts to detect and mitigate bias. Our analysis considers both technical and legal perspectives. Technically, we consider the various choices vendors make regarding data collection and prediction targets, and explore the risks and trade-offs that these choices pose. We also discuss how algorithmic de-biasing techniques interface with, and create challenges for, antidiscrimination law.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {469–481},
numpages = {13},
keywords = {algorithmic bias, algorithmic hiring, discrimination law},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
"@inproceedings{10.1145/3351095.3372846,
author = {Lum, Kristian and Boudin, Chesa and Price, Megan},
title = {The Impact of Overbooking on a Pre-Trial Risk Assessment Tool},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372846},
doi = {10.1145/3351095.3372846},
abstract = {Pre-trial risk assessment tools are used to make recommendations to judges about appropriate conditions of pre-trial supervision for people who have been arrested. Increasingly, there is concern about whether these models are operating fairly, including concerns about whether the models' input factors are fair measures of one's criminal activity. In this paper, we assess the impact of booking charges that do not result in a conviction on a popular risk assessment tool, the Arnold Public Safety Assessment. Using data from a pilot run of the tool in San Francisco, CA, we find that booking charges that do not result in a conviction (i.e. charges that are dropped or end in an acquittal) increased the recommended level of pre-trial supervision in around 27% of cases evaluated by the tool.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {482–491},
numpages = {10},
keywords = {risk assessment, police accountability, overbooking, fairness},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
"@inproceedings{10.1145/3351095.3372877,
author = {Bogen, Miranda and Rieke, Aaron and Ahmed, Shazeda},
title = {Awareness in Practice: Tensions in Access to Sensitive Attribute Data for Antidiscrimination},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372877},
doi = {10.1145/3351095.3372877},
abstract = {Organizations cannot address demographic disparities that they cannot see. Recent research on machine learning and fairness has emphasized that awareness of sensitive attributes, such as race and sex, is critical to the development of interventions. However, on the ground, the existence of these data cannot be taken for granted.This paper uses the domains of employment, credit, and healthcare in the United States to surface conditions that have shaped the availability of sensitive attribute data. For each domain, we describe how and when private companies collect or infer sensitive attribute data for antidiscrimination purposes. An inconsistent story emerges: Some companies are required by law to collect sensitive attribute data, while others are prohibited from doing so. Still others, in the absence of legal mandates, have determined that collection and imputation of these data are appropriate to address disparities.This story has important implications for fairness research and its future applications. If companies that mediate access to life opportunities are unable or hesitant to collect or infer sensitive attribute data, then proposed techniques to detect and mitigate bias in machine learning models might never be implemented outside the lab. We conclude that today's legal requirements and corporate practices, while highly inconsistent across domains, offer lessons for how to approach the collection and inference of sensitive data in appropriate circumstances. We urge stakeholders, including machine learning practitioners, to actively help chart a path forward that takes both policy goals and technical needs into account.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {492–500},
numpages = {9},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
"@inproceedings{10.1145/3351095.3372826,
author = {Hanna, Alex and Denton, Emily and Smart, Andrew and Smith-Loud, Jamila},
title = {Towards a Critical Race Methodology in Algorithmic Fairness},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372826},
doi = {10.1145/3351095.3372826},
abstract = {We examine the way race and racial categories are adopted in algorithmic fairness frameworks. Current methodologies fail to adequately account for the socially constructed nature of race, instead adopting a conceptualization of race as a fixed attribute. Treating race as an attribute, rather than a structural, institutional, and relational phenomenon, can serve to minimize the structural aspects of algorithmic unfairness. In this work, we focus on the history of racial categories and turn to critical race theory and sociological work on race and ethnicity to ground conceptualizations of race for fairness research, drawing on lessons from public health, biomedical research, and social survey research. We argue that algorithmic fairness researchers need to take into account the multidimensionality of race, take seriously the processes of conceptualizing and operationalizing race, focus on social processes which produce racial inequality, and consider perspectives of those most affected by sociotechnical systems.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {501–512},
numpages = {12},
keywords = {algorithmic fairness, race and ethnicity, critical race theory},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
"@inproceedings{10.1145/3351095.3375674,
author = {Hu, Lily and Kohler-Hausmann, Issa},
title = {What's Sex Got to Do with Machine Learning?},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375674},
doi = {10.1145/3351095.3375674},
abstract = {The debate about fairness in machine learning has largely centered around competing substantive definitions of what fairness or nondiscrimination between groups requires. However, very little attention has been paid to what precisely a group is. Many recent approaches have abandoned observational, or purely statistical, definitions of fairness in favor of definitions that require one to specify a causal model of the data generating process. The implicit ontological assumption of these exercises is that a racial or sex group is a collection of individuals who share a trait or attribute, for example: the group ""female"" simply consists in grouping individuals who share female-coded sex features. We show this by exploring the formal assumption of modularity in causal models using directed acyclic graphs (DAGs), which hold that the dependencies captured by one causal pathway are invariant to interventions on any other causal pathways. Modeling sex, for example, as a node in a causal model aimed at elucidating fairness questions proposes two substantive claims: 1) There exists a feature, sex-on-its-own, that is an inherent trait of an individual that then (causally) brings about social phenomena external to it in the world; and 2) the relations between sex and its downstream effects can be modified in whichever ways and the former node would still retain the meaning that sex has in our world. Together, these claims suggest sex to be a category that could be different in its (causal) relations with other features of our social world via hypothetical interventions yet still mean what it means in our world. This fundamental stability of categories and causes (unless explicitly intervened on) is essential in the methodology of causal inference, because without it, causal operations can alter the meaning of a category, fundamentally change how it is situated within a causal diagram, and undermine the validity of any inferences drawn on the diagram as corresponding to any real phenomena in the world.We argue that these methods' ontological assumptions about social groups such as sex are conceptual errors. Many of the ""effects"" that sex purportedly ""causes"" are in fact constitutive features of sex as a social status. They constitute what it means to be sexed. In other words, together, they give the social meaning of sex features. These social meanings are precisely, we argue, what makes sex discrimination a distinctively morally problematic type of act that differs from mere irrationality or meanness on the basis of a physical feature.Correcting this conceptual error has a number of important implications for how analytical models can be used to detect discrimination. If what makes something discrimination on the basis of a particular social grouping is that the practice acts on what it means to be in that group in a way that we deem wrongful, then what we need from analytical diagrams is a model of what constitutes the social grouping. Such a model would allow us to explain the special moral (and legal) reasons we have to be concerned with the treatment of this category by reference to the empirical social relations and meanings that establish the category as what it is. Only then can we have the normative debate about what is fair or nondiscriminatory vis-\`{a}-vis that group. We suggest that formal diagrams of constitutive relations would present an entirely different path toward reasoning about discrimination (and relatedly, counterfactuals) because they proffer a model of how the meaning of a social group emerges from its constitutive features. Whereas the value of causal diagrams is to guide the construction and testing of sophisticated modular counterfactuals, the value of constitutive diagrams would be to identify a different kind of counterfactual as central to our inquiry into discrimination: one that asks how the social meaning of a group would be changed if its non-modular features were altered.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {513},
numpages = {1},
keywords = {law, social philosophy, algorithmic fairness, discrimination, machine learning, causal inference},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
"@inproceedings{10.1145/3351095.3372864,
author = {Binns, Reuben},
title = {On the Apparent Conflict between Individual and Group Fairness},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372864},
doi = {10.1145/3351095.3372864},
abstract = {A distinction has been drawn in fair machine learning research between 'group' and 'individual' fairness measures. Many technical research papers assume that both are important, but conflicting, and propose ways to minimise the trade-offs between these measures. This paper argues that this apparent conflict is based on a misconception. It draws on discussions from within the fair machine learning research, and from political and legal philosophy, to argue that individual and group fairness are not fundamentally in conflict. First, it outlines accounts of egalitarian fairness which encompass plausible motivations for both group and individual fairness, thereby suggesting that there need be no conflict in principle. Second, it considers the concept of individual justice, from legal philosophy and jurisprudence, which seems similar but actually contradicts the notion of individual fairness as proposed in the fair machine learning literature. The conclusion is that the apparent conflict between individual and group fairness is more of an artefact of the blunt application of fairness measures, rather than a matter of conflicting principles. In practice, this conflict may be resolved by a nuanced consideration of the sources of 'unfairness' in a particular deployment context, and the carefully justified application of measures to mitigate it.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {514–524},
numpages = {11},
keywords = {individual fairness, discrimination, machine learning, fairness, justice, statistical parity},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
"@inproceedings{10.1145/3351095.3372878,
author = {D'Amour, Alexander and Srinivasan, Hansa and Atwood, James and Baljekar, Pallavi and Sculley, D. and Halpern, Yoni},
title = {Fairness is Not Static: Deeper Understanding of Long Term Fairness via Simulation Studies},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372878},
doi = {10.1145/3351095.3372878},
abstract = {As machine learning becomes increasingly incorporated within high impact decision ecosystems, there is a growing need to understand the long-term behaviors of deployed ML-based decision systems and their potential consequences. Most approaches to understanding or improving the fairness of these systems have focused on static settings without considering long-term dynamics. This is understandable; long term dynamics are hard to assess, particularly because they do not align with the traditional supervised ML research framework that uses fixed data sets. To address this structural difficulty in the field, we advocate for the use of simulation as a key tool in studying the fairness of algorithms. We explore three toy examples of dynamical systems that have been previously studied in the context of fair decision making for bank loans, college admissions, and allocation of attention. By analyzing how learning agents interact with these systems in simulation, we are able to extend previous work, showing that static or single-step analyses do not give a complete picture of the long-term consequences of an ML-based decision system. We provide an extensible open-source software framework for implementing fairness-focused simulation studies and further reproducible research, available at https://github.com/google/ml-fairness-gym.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {525–534},
numpages = {10},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
"@inproceedings{10.1145/3351095.3372857,
author = {Hu, Lily and Chen, Yiling},
title = {Fair Classification and Social Welfare},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372857},
doi = {10.1145/3351095.3372857},
abstract = {Now that machine learning algorithms lie at the center of many important resource allocation pipelines, computer scientists have been unwittingly cast as partial social planners. Given this state of affairs, important questions follow. How do leading notions of fairness as defined by computer scientists map onto longer-standing notions of social welfare? In this paper, we present a welfare-based analysis of fair classification regimes. Our main findings assess the welfare impact of fairness-constrained empirical risk minimization programs on the individuals and groups who are subject to their outputs. We fully characterize the ranges of Δ'e perturbations to a fairness parameter 'e in a fair Soft Margin SVM problem that yield better, worse, and neutral outcomes in utility for individuals and by extension, groups. Our method of analysis allows for fast and efficient computation of ""fairness-to-welfare"" solution paths, thereby allowing practitioners to easily assess whether and which fair learning procedures result in classification outcomes that make groups better-off. Our analyses show that applying stricter fairness criteria codified as parity constraints can worsen welfare outcomes for both groups. More generally, always preferring ""more fair"" classifiers does not abide by the Pareto Principle---a fundamental axiom of social choice theory and welfare economics. Recent work in machine learning has rallied around these notions of fairness as critical to ensuring that algorithmic systems do not have disparate negative impact on disadvantaged social groups. By showing that these constraints often fail to translate into improved outcomes for these groups, we cast doubt on their effectiveness as a means to ensure fairness and justice.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {535–545},
numpages = {11},
location = {Barcelona, Spain},
series = {FAT* '20}
}
@misc{ftcANPR2023,
	title        = {Commercial Surveillance and Data Security Rulemaking},
	author       = {Federal Trade Commission},
	year         = 2022,
	url          = {https://www.ftc.gov/legal-library/browse/federal-register-notices/commercial-surveillance-data-security-rulemaking},
}
@misc{house2022blueprint,
	title        = {Blueprint for an ai bill of rights: Making automated systems work for the american people},
	author       = {House, White},
	year         = 2022,
}
@article{ai2023artificial,
	title        = {Artificial Intelligence Risk Management Framework (AI RMF 1.0)},
	author       = {AI, NIST},
	year         = 2023,
}
"
"@inproceedings{10.1145/3351095.3373155,
author = {Kim, Michael P. and Korolova, Aleksandra and Rothblum, Guy N. and Yona, Gal},
title = {Preference-Informed Fairness},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3373155},
doi = {10.1145/3351095.3373155},
abstract = {In this work, we study notions of fairness in decision-making systems when individuals have diverse preferences over the possible outcomes of the decisions. Our starting point is the seminal work of Dwork et al. [ITCS 2012] which introduced a notion of individual fairness (IF): given a task-specific similarity metric, every pair of individuals who are similarly qualified according to the metric should receive similar outcomes. We show that when individuals have diverse preferences over outcomes, requiring IF may unintentionally lead to less-preferred outcomes for the very individuals that IF aims to protect (e.g. a protected minority group). A natural alternative to IF is the classic notion of fair division, envy-freeness (EF): no individual should prefer another individual's outcome over their own. Although EF allows for solutions where all individuals receive a highly-preferred outcome, EF may also be overly-restrictive for the decision-maker. For instance, if many individuals agree on the best outcome, then if any individual receives this outcome, they all must receive it, regardless of each individual's underlying qualifications for the outcome.We introduce and study a new notion of preference-informed individual fairness (PIIF) that is a relaxation of both individual fairness and envy-freeness. At a high-level, PIIF requires that outcomes satisfy IF-style constraints, but allows for deviations provided they are in line with individuals' preferences. We show that PIIF can permit outcomes that are more favorable to individuals than any IF solution, while providing considerably more flexibility to the decision-maker than EF. In addition, we show how to efficiently optimize any convex objective over the outcomes subject to PIIF for a rich class of individual preferences. Finally, we demonstrate the broad applicability of the PIIF framework by extending our definitions and algorithms to the multiple-task targeted advertising setting introduced by Dwork and Ilvento [ITCS 2019].},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {546},
numpages = {1},
keywords = {algorithmic fairness},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
"@inproceedings{10.1145/3351095.3375709,
author = {Yang, Kaiyu and Qinami, Klint and Fei-Fei, Li and Deng, Jia and Russakovsky, Olga},
title = {Towards Fairer Datasets: Filtering and Balancing the Distribution of the People Subtree in the ImageNet Hierarchy},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375709},
doi = {10.1145/3351095.3375709},
abstract = {Computer vision technology is being used by many but remains representative of only a few. People have reported misbehavior of computer vision models, including offensive prediction results and lower performance for underrepresented groups. Current computer vision models are typically developed using datasets consisting of manually annotated images or videos; the data and label distributions in these datasets are critical to the models' behavior. In this paper, we examine ImageNet, a large-scale ontology of images that has spurred the development of many modern computer vision methods. We consider three key factors within the person subtree of ImageNet that may lead to problematic behavior in downstream computer vision technology: (1) the stagnant concept vocabulary of WordNet, (2) the attempt at exhaustive illustration of all categories with images, and (3) the inequality of representation in the images within concepts. We seek to illuminate the root causes of these concerns and take the first steps to mitigate them constructively.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {547–558},
numpages = {12},
keywords = {representative datasets, fairness, computer vision, dataset construction},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
"@inproceedings{10.1145/3351095.3372835,
author = {Mustafaraj, Eni and Lurie, Emma and Devine, Claire},
title = {The Case for Voter-Centered Audits of Search Engines during Political Elections},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372835},
doi = {10.1145/3351095.3372835},
abstract = {Search engines, by ranking a few links ahead of million others based on opaque rules, open themselves up to criticism of bias. Previous research has focused on measuring political bias of search engine algorithms to detect possible search engine manipulation effects on voters or unbalanced ideological representation in search results. Insofar that these concerns are related to the principle of fairness, this notion of fairness can be seen as explicitly oriented toward election candidates or political processes and only implicitly oriented toward the public at large. Thus, we ask the following research question: how should an auditing framework that is explicitly centered on the principle of ensuring and maximizing fairness for the public (i.e., voters) operate? To answer this question, we qualitatively explore four datasets about elections and politics in the United States: 1) a survey of eligible U.S. voters about their information needs ahead of the 2018 U.S. elections, 2) a dataset of biased political phrases used in a large-scale Google audit ahead of the 2018 U.S. election, 3) Google's ""related searches"" phrases for two groups of political candidates in the 2018 U.S. election (one group is composed entirely of women), and 4) autocomplete suggestions and result pages for a set of searches on the day of a statewide election in the U.S. state of Virginia in 2019. We find that voters have much broader information needs than the search engine audit literature has accounted for in the past, and that relying on political science theories of voter modeling provides a good starting point for informing the design of voter-centered audits.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {559–569},
numpages = {11},
keywords = {voters, search engines, Google, bias, algorithm audits, elections},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
"@inproceedings{10.1145/3351095.3372851,
author = {Coston, Amanda and Mishler, Alan and Kennedy, Edward H. and Chouldechova, Alexandra},
title = {Counterfactual Risk Assessments, Evaluation, and Fairness},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372851},
doi = {10.1145/3351095.3372851},
abstract = {Algorithmic risk assessments are increasingly used to help humans make decisions in high-stakes settings, such as medicine, criminal justice and education. In each of these cases, the purpose of the risk assessment tool is to inform actions, such as medical treatments or release conditions, often with the aim of reducing the likelihood of an adverse event such as hospital readmission or recidivism. Problematically, most tools are trained and evaluated on historical data in which the outcomes observed depend on the historical decision-making policy. These tools thus reflect risk under the historical policy, rather than under the different decision options that the tool is intended to inform. Even when tools are constructed to predict risk under a specific decision, they are often improperly evaluated as predictors of the target outcome.Focusing on the evaluation task, in this paper we define counterfactual analogues of common predictive performance and algorithmic fairness metrics that we argue are better suited for the decision-making context. We introduce a new method for estimating the proposed metrics using doubly robust estimation. We provide theoretical results that show that only under strong conditions can fairness according to the standard metric and the counterfactual metric simultaneously hold. Consequently, fairness-promoting methods that target parity in a standard fairness metric may---and as we show empirically, do---induce greater imbalance in the counterfactual analogue. We provide empirical comparisons on both synthetic data and a real world child welfare dataset to demonstrate how the proposed method improves upon standard practice.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {582–593},
numpages = {12},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
"@inproceedings{10.1145/3351095.3372869,
author = {Green, Ben},
title = {The False Promise of Risk Assessments: Epistemic Reform and the Limits of Fairness},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372869},
doi = {10.1145/3351095.3372869},
abstract = {Risk assessments have proliferated in the United States criminal justice system. The theory of change motivating their adoption involves two key assumptions: first, that risk assessments will reduce human biases by making objective decisions, and second, that risk assessments will promote criminal justice reform. In this paper I interrogate both of these assumptions, concluding that risk assessments are an ill-advised tool for challenging the centrality and legitimacy of incarceration within the criminal justice system. First, risk assessments fail to provide objectivity, as their use creates numerous sites of discretion. Second, risk assessments provide no guarantee of reducing incarceration; instead, they risk legitimizing the criminal justice system's structural racism. I then consider, via an ""epistemic reform,"" the path forward for criminal justice reform. I reinterpret recent results regarding the ""impossibility of fairness"" as not simply a tension between mathematical metrics but as evidence of a deeper tension between notions of equality. This expanded frame challenges the formalist, colorblind proceduralism at the heart of the criminal justice system and suggests a more structural approach to reform. Together, this analysis highlights how algorithmic fairness narrows the scope of judgments about justice and how ""fair"" algorithms can reinforce discrimination.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {594–606},
numpages = {13},
keywords = {criminal justice system, risk assessment, fairness, social justice},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
"@inproceedings{10.1145/3351095.3372855,
author = {Panigutti, Cecilia and Perotti, Alan and Pedreschi, Dino},
title = {Doctor XAI: An Ontology-Based Approach to Black-Box Sequential Data Classification Explanations},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372855},
doi = {10.1145/3351095.3372855},
abstract = {Several recent advancements in Machine Learning involve blackbox models: algorithms that do not provide human-understandable explanations in support of their decisions. This limitation hampers the fairness, accountability and transparency of these models; the field of eXplainable Artificial Intelligence (XAI) tries to solve this problem providing human-understandable explanations for black-box models. However, healthcare datasets (and the related learning tasks) often present peculiar features, such as sequential data, multi-label predictions, and links to structured background knowledge. In this paper, we introduce Doctor XAI, a model-agnostic explainability technique able to deal with multi-labeled, sequential, ontology-linked data. We focus on explaining Doctor AI, a multilabel classifier which takes as input the clinical history of a patient in order to predict the next visit. Furthermore, we show how exploiting the temporal dimension in the data and the domain knowledge encoded in the medical ontology improves the quality of the mined explanations.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {629–639},
numpages = {11},
keywords = {explainable artificial intelligence, healthcare data, machine learning},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
"@inproceedings{10.1145/3351095.3372847,
author = {Donahue, Kate and Kleinberg, Jon},
title = {Fairness and Utilization in Allocating Resources with Uncertain Demand},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372847},
doi = {10.1145/3351095.3372847},
abstract = {Resource allocation problems are a fundamental domain in which to evaluate the fairness properties of algorithms. The trade-offs between fairness and utilization have a long history in this domain. A recent line of work has considered fairness questions for resource allocation when the demands for the resource are distributed across multiple groups and drawn from probability distributions. In such cases, a natural fairness requirement is that individuals from different groups should have (approximately) equal probabilities of receiving the resource. A largely open question in this area has been to bound the gap between the maximum possible utilization of the resource and the maximum possible utilization subject to this fairness condition.Here, we obtain some of the first provable upper bounds on this gap. We obtain an upper bound for arbitrary distributions, as well as much stronger upper bounds for specific families of distributions that are typically used to model levels of demand. In particular, we find --- somewhat surprisingly --- that there are natural families of distributions (including Exponential and Weibull) for which the gap is non-existent: it is possible to simultaneously achieve maximum utilization and the given notion of fairness. Finally, we show that for power-law distributions, there is a non-trivial gap between the solutions, but this gap can be bounded by a constant factor independent of the parameters of the distribution.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {658–668},
numpages = {11},
keywords = {algorithmic fairness, power law distribution, resource allocation, uncertainty, weibull distribution},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
"@inproceedings{10.1145/3351095.3372842,
author = {Elzayn, Hadi and Fish, Benjamin},
title = {The Effects of Competition and Regulation on Error Inequality in Data-Driven Markets},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372842},
doi = {10.1145/3351095.3372842},
abstract = {Recent work has documented instances of unfairness in deployed machine learning models, and significant researcher effort has been dedicated to creating algorithms that intrinsically consider fairness. In this work, we highlight another source of unfairness: market forces that drive differential investment in the data pipeline for differing groups. We develop a high-level model to study this question. First, we show that our model predicts unfairness in a monopoly setting. Then, we show that under all but the most extreme models, competition does not eliminate this tendency, and may even exacerbate it. Finally, we consider two avenues for regulating a machine-learning driven monopolist - relative error inequality and absolute error-bounds - and quantify the price of fairness (and who pays it). These models imply that mitigating fairness concerns may require policy-driven solutions, not only technological ones.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {669–679},
numpages = {11},
keywords = {algorithmic fairness, industrial organization, game theory, learning theory, economics, data markets},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
"@inproceedings{10.1145/3351095.3372838,
author = {Lundgard, Alan},
title = {Measuring Justice in Machine Learning},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372838},
doi = {10.1145/3351095.3372838},
abstract = {How can we build more just machine learning systems? To answer this question, we need to know both what justice is and how to tell whether one system is more or less just than another. That is, we need both a definition and a measure of justice. Theories of distributive justice hold that justice can be measured (in part) in terms of the fair distribution of benefits and burdens across people in society. Recently, the field known as fair machine learning has turned to John Rawls's theory of distributive justice for inspiration and operationalization. However, philosophers known as capability theorists have long argued that Rawls's theory uses the wrong measure of justice, thereby encoding biases against people with disabilities. If these theorists are right, is it possible to operationalize Rawls's theory in machine learning systems without also encoding its biases? In this paper, I draw on examples from fair machine learning to suggest that the answer to this question is no: the capability theorists' arguments against Rawls's theory carry over into machine learning systems. But capability theorists don't only argue that Rawls's theory uses the wrong measure, they also offer an alternative measure. Which measure of justice is right? And has fair machine learning been using the wrong one?},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {680},
numpages = {1},
keywords = {measure, operationalization, disability, blind, accessibility, fairness, distributive justice, philosophy, machine learning, theory, capability, bias, non-visual access, justice, discrimination, web accessibility},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
"@inproceedings{10.1145/3351095.3375680,
author = {Baxter, Kathy and Schlesinger, Yoav and Aerni, Sarah and Baker, Lewis and Dawson, Julie and Kenthapadi, Krishnaram and Kloumann, Isabel and Wallach, Hanna},
title = {Bridging the Gap from AI Ethics Research to Practice},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375680},
doi = {10.1145/3351095.3375680},
abstract = {The study of fairness in machine learning applications has seen significant academic inquiry, research and publication in recent years. Concurrently, technology companies have begun to instantiate nascent program in AI ethics and product ethics more broadly. As a result of these efforts, AI ethics practitioners have piloted new processes to evaluate and ensure fairness in their machine learning applications. In this session, six industry practitioners, hailing from LinkedIn, Yoti, Microsoft, Pymetrics, Facebook, and Salesforce share insights from the work they have undertaken in the area of fairness, what has worked and what has not, lessons learned and best practices instituted as a result.• Krishnaram Kenthapadi presents LinkedIn's fairness-aware reranking for talent search.• Julie Dawson shares how Yoti applies ML fairness research to age estimation in their digital identity platform.• Hanna Wallach contributes how Microsoft is applying fairness principles in practice.• Lewis Baker presents Pymetric's fairness mechanisms in their hiring algorithm.• Isabel Kloumann presents Facebook's fairness assessment framework through a case study of fairness in a content moderation system.• Sarah Aerni contributes how Salesforce is building fairness features into the Einstein AI platform.Building on those insights, we discuss insights and brainstorm modalities through which to build upon the practitioners' work. Opportunities for further research or collaboration are identified, with the goal of developing a shared understanding of experiences and needs of AI ethics practitioners. Ultimately, the aim is to develop a playbook for more ethical and fair AI product development and deployment.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {682},
numpages = {1},
keywords = {algorithmic decision-making, ethics, artificial intelligence, fairness, ML fairness, practitioners, data},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
"@inproceedings{10.1145/3351095.3375697,
author = {Pritchard, Helen and Snodgrass, Eric and Morrison, Romi Ron and Britton, Loren and Moll, Joana},
title = {Burn, Dream and Reboot! Speculating Backwards for the Missing Archive on Non-Coercive Computing},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375697},
doi = {10.1145/3351095.3375697},
abstract = {Whether one is speaking of barbed wire, the assembly line or computer operating systems, the history of coercive technologies for the automation of tasks has focused on optimization, determinate outcomes and an ongoing disciplining of components and bodies. Automated technologies of the present emerge and are marked by this lineage of coercive modes of implementation, whose scarred history of techniques of discrimination, exploitation and extraction point to an archive of automated injustices in computing, a history that continues to charge present paradigms and practices of computing.This workshop addresses the history of coercive technologies through attuning to how we perform speculation within practices of computing through a renewed attention to this history. We go backwards into the archive, rather than racing forward and proposing ever new speculative futures of automation. This is because speculative creative approaches are often conceived and positioned as methodological toolkits for addressing computing practices by imagining for/with others for a ""future otherwise"". We argue that ""speculation"" as the easy-go-to of designers and artists trying to address automated injustices needs some undoing, as without work it will always be confined within ongoing legacies of coercive modes of computing practice. Instead of creating more just-worlds, the generation of ever-new futures by creative speculation often merely reinforces the project of coercive computing.For this workshop, drawing on queer approaches to resisting futures and informed by activist feminist engagements with archives, we invite participants to temporarily resist imagining futures and instead to speculate backwards. We speculate backwards to various moments, artefacts and practices within computing history. What does it mean to understand techniques of computing and automation as coercive infrastructures? How did so many of the dreams and seeming promises of computing turn into the coercive practices that we see today? Has computing as a practice become so imbued with coercive techniques that we find it hard to imagine otherwise? Together, we will build a speculative understanding and possible archive of non-coercive computing. In the words of Alexis Pauline Gumbs, the emerging archive proposes ""how did their dreams make rooms to dream in""... or not, in the case of coercive practices of computing. And ""what if she changes her dream?"" What if we reboot this dream?1},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {683},
numpages = {1},
keywords = {optimization, social justice, automation, critical computing, speculative design, trans*feminist technoscience},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
"@inproceedings{10.1145/3351095.3375686,
author = {Givens, Alexandra Reeve and Morris, Meredith Ringel},
title = {Centering Disability Perspectives in Algorithmic Fairness, Accountability, & Transparency},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375686},
doi = {10.1145/3351095.3375686},
abstract = {It is vital to consider the unique risks and impacts of algorithmic decision-making for people with disabilities. The diverse nature of potential disabilities poses unique challenges for approaches to fairness, accountability, and transparency. Many disabled people choose not to disclose their disabilities, making auditing and accountability tools particularly hard to design and operate. Further, the variety inherent in disability poses challenges for collecting representative training data in any quantity sufficient to better train more inclusive and accountable algorithms.This panel highlights areas of concern, present emerging research efforts, and enlist more researchers and advocates to study the potential impacts of algorithmic decision-making on people with disabilities. A key objective is to surface new research projects and collaborations, including by integrating a critical disability perspective into existing research and advocacy efforts focused on identifying sources of bias and advancing equity.In the technology space, discussion topics will include methods to assess the fairness of current AI systems, and strategies to develop new systems and bias mitigation approaches that ensure fairness for people with disabilities. For example, how do today's currently-deployed AI systems impact people with disabilities? If developing inclusive datasets is part of the solution, how can researchers ethically gather such data, and what risks might centralizing data about disability pose? What new privacy solutions must developers create to reduce the risk of deductive disclosure of identities of people with disabilities in ""anonymized"" datasets? How can AI models and bias mitigation techniques be developed that handle the unique challenges of disability, i.e., the ""long tail"" and low incidence of many types of disability - for instance, how do we ensure that data about disability are not treated as outliers? What are the pros and cons of developing custom/personalized AI models for people with disabilities versus ensuring that general models are inclusive?In the law and policy space, the framework for people with disabilities requires specific study. For example, the Americans with Disabilities Act (ADA) requires employers to adopt ""reasonable accommodations"" for qualified individuals with a disability. But what is a ""reasonable accommodation"" in the context of machine learning and AI? How will the ADA's unique standards interact with case law and scholarship about algorithmic bias against other protected groups? When the ADA governs what questions employers can ask about a candidate's disability, and HIPAA and the Genetic Information Privacy Act regulate the sharing of health information, how should we think about inferences from data that approximate such questions?Panelists will bring varied perspectives to this conversation, including backgrounds in computer science, disability studies, legal studies, and activism. In addition to their scholarly expertise, several panelists have direct lived experience with disability. The session format will consist of brief position statements from each panelist, followed by questions from the moderator, and then open questions from and discussion with the audience.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {684},
numpages = {1},
keywords = {accessibility, disability studies, algorithmic bias, AI FATE},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
"@inproceedings{10.1145/3351095.3375689,
author = {Sassaman, Hannah and Lee, Jennifer and Irvine, Jenessa and Narayan, Shankar},
title = {Creating Community-Based Tech Policy: Case Studies, Lessons Learned, and What Technologists and Communities Can Do Together},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375689},
doi = {10.1145/3351095.3375689},
abstract = {What are the core ways the field of data science can center community voice and power throughout all the processes involved in conceptualizing, creating, and disseminating technology?? What are the most possible and most urgent ways communities can shape the field of algorithmic decision-making to center community power in the next few years? This interactive workshop will highlight some of the following lessons learned through our combined experience engaging with communities challenging technology in Seattle and Philadelphia, cities in the United States. We will discuss the historical context of disproportionate impacts of technology on marginalized and vulnerable communities; case studies including criminal justice risk assessments, face surveillance technologies, and surveillance regulations; and work in small-group and break-out sessions to engage questions about when and where technologists hold power, serve as gatekeepers, and can work in accountable partnership with impacted communities.By the end of the session, we hope that participants will learn how to actively center diverse communities in creating technology by examining successes, challenges, and ongoing work in Seattle and Philadelphia, through the following lessons we have learned:• that communities, policy-makers, and technologists need to work intimately together to lift up each other's' goals• that communities need to gain data justice and data literacy to understand and independently audit how a system is impacting them• that scientific analyses of algorithmic bias are powerful but heard most clearly when lifted up by local community members and stakeholders in decisions where algorithms might be deployed• that anecdotal stories of harm are most impactful on decisionmakers when tied to rigorous scientific analysis and examples from other communities that amplify and ground those stories• that communities and community goals and standards are often not heard in conversations between data scientists and people who deploy algorithms, as well as in decision-makers' conversations about what policy should look like• and that we need to begin to craft what it means for those with the least power in conversations about algorithmic fairness - those judged by those tools - to have far more, or even the most power in the future of their design or implementation.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {685},
numpages = {1},
keywords = {disproportionate impact, surveillance, community-centered, algorithms, criminal justice, bias, case studies},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
"@inproceedings{10.1145/3351095.3375692,
author = {Hanna, Alex and Denton, Emily},
title = {CtrlZ.AI Zine Fair: Critical Perspectives},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375692},
doi = {10.1145/3351095.3375692},
abstract = {The FAT* conference has begun the necessary conversation on the normative implications and ethical ramifications of sociotechnical systems. However, many scholars have pointed to the limitations in methodologies and scope of analysis (e.g. [8, 11]). In addition to these critiques, we add in the fact that those who are most affected by this technology do not have the skills, training, or technical aptitude to participate in these conversations. With the exception of the 2018 FAT* tutorial which featured Terrance Wilkerson (who had been labeled as likely to highly recidivate by COMPAS) and his partner, there has been silence from those most impacted by algorithmic unfairness at FAT*. This silence has been deafening, as FAT* conversations - with a few notable exceptions (e.g. [1, 4]) - have failed to discuss anti-racist politics, prison abolition, and social justice.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {686},
numpages = {1},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
"@inproceedings{10.1145/3351095.3375688,
author = {Allhutter, Doris and Berendt, Bettina},
title = {Deconstructing FAT: Using Memories to Collectively Explore Implicit Assumptions, Values and Context in Practices of Debiasing and Discrimination-Awareness},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375688},
doi = {10.1145/3351095.3375688},
abstract = {Research in fairness, accountability, and transparency (FAT) in socio-technical systems needs to take into account how practices of computing are entrenched with power relations in complex and multi-layered ways. Trying to disentangle the way in which structural discrimination and normative computational concepts and methods are intertwined, this frequently raises the question of WHO are the actors that shape technologies and research agendas---who gets to speak and to define bias, (un)fairness, and discrimination? ""Deconstructing FAT"" is a CRAFT workshop that aims at complicating this question by asking how ""we"" as researchers in FAT (often unknowingly) mobilize implicit assumptions, values and beliefs that reflect our own embeddedness in power relations, our disciplinary ways of thinking, and our historically, locally, and culturally-informed ways of solving computational problems or approaching our research. This is a vantage point to make visible and analyze the normativity of technical approaches, concepts and methods that are part of the repertoire of FAT research. Inspired by a previous international workshop [1], this CRAFT workshop engages an interdisciplinary panel of FAT researchers in a deconstruction exercise that traces the following issues:(1) FAT research frequently speaks of social bias that is amplified by algorithmic systems, of the problem of discriminatory consequences that is to be solved, and of underprivileged or vulnerable groups that need to be protected. What does this perspectivity imply in terms of the approaches, methods and metrics that are being applied? How do methods of debiasing and discrimination-awareness enact the epistemic power of a perspective of privilege as their norm?(2) FAT research has emphasized the need for multi- or interdisciplinary approaches to get a grip on the complex intertwining of social power relations and the normativity of computational methods, norms and practices. Clearly, multi- and interdisciplinary research includes different normative frameworks and ways of thinking that need to be negotiated. This is complicated by the fact that these frameworks are not fully transparent and ready for reflection. What are the normative implications of interdisciplinary collaboration in FAT research? (3) While many problems of discrimination, marginalization and exploitation can be similar across places, they can also have specific local shapes. How can FAT research e.g. consider historically grown specifics such as the effects of different colonial histories? If these specifics make patterns of discrimination have different and more nuanced dimensions than clear-cut 'redlining', what does this imply?To explore these questions, we use the method of 'mind scripting' which is based in theories of discourse, ideology, memory and affect and aims at investigating hidden patterns of meaning making in written memories of the panelists [2]. The workshop strives to challenge some of the implicit norms and tensions in FAT research and to trigger future directions.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {687},
numpages = {1},
keywords = {deconstruction, discrimination-awareness in machine learning},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
"@inproceedings{10.1145/3351095.3375690,
author = {Ahmad, Muhammad Aurangzeb and Teredesai, Ankur and Eckert, Carly},
title = {Fairness, Accountability, Transparency in AI at Scale: Lessons from National Programs},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375690},
doi = {10.1145/3351095.3375690},
abstract = {The panel aims to elucidate how different national govenmental programs are implementing accountability of machine learning systems in healthcare and how accountability is operationlized in different cultural settings in legislation, policy and deployment. We have representatives from three different govenments, UAE, Singapore and Maldives who will discuss what accountability of AI and machine learning means in their contexts and use cases. We hope to have a fruitful conversation around FAT ML as it is operationalized ccross cultures, national boundries and legislative constraints.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {690},
numpages = {1},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
"@inproceedings{10.1145/3351095.3375685,
author = {Wan, Evelyn and de Groot, Aviva and Jameson, Shazade and P\u{a}un, Mara and L\""{u}cking, Phillip and Klumbyte, Goda and L\""{a}mmerhirt, Danny},
title = {Lost in Translation: An Interactive Workshop Mapping Interdisciplinary Translations for Epistemic Justice},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375685},
doi = {10.1145/3351095.3375685},
abstract = {There are gaps in understanding in and between those who design systems of AI/ ML, those who critique them, and those positioned between these discourses. This gap can be defined in multiple ways - e.g. methodological, epistemological, linguistic, or cultural. To bridge this gap requires a set of translations: the generation of a collaborative space and a new set of shared sensibilities that traverse disciplinary boundaries. This workshop aims to explore translations across multiple fields, and translations between theory and practice, as well as how interdisciplinary work could generate new operationalizable approaches.We define 'knowledge' as a social product (L. Code) which requires fair and broad epistemic cooperation in its generation, development, and dissemination. As a ""marker for truth"" (B. Williams) and therefore a basis for action, knowledge circulation sustains the systems of power which produce it in the first place (M. Foucault). Enabled by epistemic credence, authority or knowledge, epistemic power can be an important driver of, but also result from, other (e.g. economic, political) powers.To produce reliable output, our standards and methods should serve us all and exclude no-one. Critical theorists have long revealed failings of epistemic practices, resulting in the marginalization and exclusion of some types of knowledge. How can we cultivate more reflexive epistemic practices in the interdisciplinary research setting of FAT*?We frame this ideal as 'epistemic justice' (M. Geuskens), the positive of 'epistemic injustice', defined by M. Fricker as injustice that exists when people are wronged as a knower or as an epistemic subject. Epistemic justice is the proper use and allocation of epistemic power; the inclusion and balancing of all epistemic sources.As S. Jasanoff reminds us, any authoritative way of seeing must be legitimized in discourse and practice, showing that practices can be developed to value and engage with other viewpoints and possibly reshape our ways of knowing.Our workshop aims to address the following questions: how could critical theory or higher level critiques be translated into and anchored in ML/AI design practices - and vice versa? What kind of cartographies and methodologies are needed in order to identify issues that can act as the basis of collaborative research and design? How can we (un)learn our established ways of thinking for such collaborative work to take place? During the workshop, participants will create, share and explode prototypical workflows of designing, researching and critiquing algorithmic systems. We will identify moments in which translations and interdisciplinary interventions could or should happen in order to build actionable steps and methodological frameworks that advance epistemic justice and are conducive to future interdisciplinary collaboration.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {692},
numpages = {1},
keywords = {workflow, algorithm development, epistemic justice, critical theory, methodologies, interdisciplinary collaboration},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
"@inproceedings{10.1145/3351095.3375682,
author = {Goss, Ezra and Hu, Lily and Sabin, Manuel and Teeple, Stephanie},
title = {Manifesting the Sociotechnical: Experimenting with Methods for Social Context and Social Justice},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375682},
doi = {10.1145/3351095.3375682},
abstract = {Critiques of 'algorithmic fairness' have counseled against a purely technical approach. Recent work from the FAT* conference has warned specifically about abstracting away the social context that these automated systems are operating within and has suggested that ""[fairness work] require[s] technical researchers to learn new skills or partner with social scientists"" [Fairness and abstraction in sociotechnical systems, Selbst et al. 2019, FAT* '19]. That ""social context"" includes groups outside the academy organizing for data and/or tech justice (e.g., Allied Media Projects, Stop LAPD Spying Coalition, data4blacklives, etc). These struggles have deep historical roots but have become prominent in the past several years alongside broader citizen-science efforts. In this CRAFT session we as STEM researchers hope to initiate conversation about methods used by community organizers to analyze power relations present in that social context. We will take this time to learn together and discuss if/how these and other methods, collaborations and efforts can be used to actualize oft-mentioned critiques of algorithmic fairness and move toward a data justice-oriented approach.Many scholars and activists have spoken on how to approach social context when discussing algorithmic fairness interventions. Community organizing and attendant methods for power analysis present one such approach: documenting all stakeholders and entities relevant to an issue and the nature of the power differentials between them. The facilitators for this session are not experts in community organizing theory or practice. Instead, we will share what we have learned from our readings of decades of rich work and writings from community organizers. This session is a collective, interdisciplinary learning experience, open to all who see their interests as relevant to the conversation.We will open with a discussion of community organizing practice: What is community organizing, what are its goals, methods, past and ongoing examples? What disciplines and intellectual lineages does it draw from? We will incorporate key sources we have found helpful for synthesizing this knowledge so that participants can continue exposing themselves to the field after the conference. We will also consider the concept of social power, including power that the algorithmic fairness community holds. Noting that there are many ways to theorize and understand power, we will share the framings that have been most useful to us. We plan to present different tools, models and procedures for doing power analysis in various organizing settings.We will propose to our group that we conduct a power analysis of our own. We have prepared a hypothetical but realistic scenario involving risk assessment in a hospital setting as an example. However, we encourage participants to bring their own experiences to the table, especially if they pertain in any way to data injustice. We also invite participants to bring examples of ongoing organizing efforts with which algorithmic fairness researchers could act in solidarity. Participants will walk away from this session with 1) an understanding of the key terms and sources necessary to gain further exposure to these topics and 2) preliminary experience analyzing power in realistic, grounded scenarios.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {693},
numpages = {1},
keywords = {community organizing, power analysis, data justice, fairness},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
"@inproceedings{10.1145/3351095.3375672,
author = {Rakova, Bogdana and Chowdhury, Rumman and Yang, Jingying},
title = {Assessing the Intersection of Organizational Structure and FAT* Efforts within Industry: Implications Tutorial},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375672},
doi = {10.1145/3351095.3375672},
abstract = {The work within the Fairness, Accountability, and Transparency of ML (fair-ML) community will positively benefit from appreciating the role of organizational culture and structure in the effective practice of fair-ML efforts of individuals, teams, and initiatives within industry. In this tutorial session we will explore various organizational structures and possible leverage points to effectively intervene in the process of design, development, and deployment of AI systems, towards contributing to positive fair-ML outcomes. We will begin by presenting the results of interviews conducted during an ethnographic study among practitioners working in industry, including themes related to: origination and evolution, common challenges, ethical tensions, and effective enablers. The study was designed through the lens of Industrial Organizational Psychology and aims to create a mapping of the current state of the fair-ML organizational structures inside major AI companies. We also look at the most-desired future state to enable effective work to increase algorithmic accountability, as well as the key elements in the transition from the current to that future state. We investigate drivers for change as well as the tensions between creating an 'ethical' system vs one that is 'ethical' enough. After presenting our preliminary findings, the rest of the tutorial will be highly interactive. Starting with a facilitated activity in break out groups, we will discuss the already identified challenges, best practices, and mitigation strategies. Finally, we hope to create space for productive discussion among AI practitioners in industry, academic researchers within various fields working directly on algorithmic accountability and transparency, advocates for various communities most impacted by technology, and others. Based on the interactive component of the tutorial, facilitators and interested participants will collaborate on further developing the discussed challenges into scenarios and guidelines that will be published as a follow up report.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {697},
numpages = {1},
keywords = {organizational structure, fair machine learning, need-finding, empirical study, I/O psychology},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
"@inproceedings{10.1145/3351095.3375673,
author = {Oswald, Marion and Powell, David},
title = {Can an Algorithmic System Be a 'friend' to a Police Officer's Discretion? ACM FAT 2020 Translation Tutorial},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375673},
doi = {10.1145/3351095.3375673},
abstract = {This tutorial aims to increase understanding of the importance of discretion in police decision-making. It will guide computer scientists, policy-makers, lawyers and others in considering practical and technical issues crucial to avoiding the prejudicial and instead develop algorithms that are supportive - a 'friend'- to legitimate discretionary decision-making. It combines explanation of the relevant law and related literature with discussion based upon deep operational experience in the area of preventative and protective policing work.Autonomy and discretion are fundamental to police work, not only in relation to strategy and policy but for day-to-day operational decisions taken by front line officers. Such discretion 'recognizes the fallibility of interfacing rules with their field of application.' (Hildebrandt 2016). This discretion is not unbounded however and English common law expects discretion to be exercised reasonably and fairly. Conversely, discretion must not be fettered unlawfully, by failing to take a relevant factor into account when making a decision, or by abdicating responsibility to another person, body or 'thing'. Algorithmic systems have the potential to contribute to factors relevant to the decision in question at the point of interaction between their outputs and the real-world outcome for the victim, offender and/or community.Algorithmic decision tools present a number of challenges to legitimate discretionary police decision-making. Unnuanced outputs could be highly influential on the human decision-maker (Cooke and Michie 2012) and may undermine discretionary power to deal with atypical cases and 'un-thought of' factors that rely upon uncodified knowledge (Oswald 2018).Practical and technical considerations will be crucial to developing MLA that are supportive to discretionary decision-making. These include the methodological approach, design of the humancomputer interface having regard the decision-maker's responsibility to give reasons for their decision, the avoidance of unnuanced or over-confident framing of results, understanding of the policing context in which the MLA will operate, and consideration of the implications of organisational culture and processes to the MLA's influence.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {698},
numpages = {1},
keywords = {algorithms, discretion, machine learning, police},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
"@inproceedings{10.1145/3351095.3375664,
author = {Gade, Krishna and Geyik, Sahin Cem and Kenthapadi, Krishnaram and Mithal, Varun and Taly, Ankur},
title = {Explainable AI in Industry: Practical Challenges and Lessons Learned: Implications Tutorial},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375664},
doi = {10.1145/3351095.3375664},
abstract = {Artificial Intelligence is increasingly playing an integral role in determining our day-to-day experiences. Moreover, with the proliferation of AI based solutions in areas such as hiring, lending, criminal justice, healthcare, and education, the resulting personal and professional implications of AI have become far-reaching. The dominant role played by AI models in these domains has led to a growing concern regarding potential bias in these models, and a demand for model transparency and interpretability [2, 4]. Model explainability is considered a prerequisite for building trust and adoption of AI systems in high stakes domains such as lending and healthcare [1] requiring reliability, safety, and fairness. It is also critical to automated transportation, and other industrial applications with significant socio-economic implications such as predictive maintenance, exploration of natural resources, and climate change modeling.As a consequence, AI researchers and practitioners have focused their attention on explainable AI to help them better trust and understand models at scale [5, 6, 8]. In fact, the field of explainability in AI/ML is at an inflexion point. There is a tremendous need from the societal, regulatory, commercial, end-user, and model developer perspectives. Consequently, practical and scalable explainability approaches are rapidly becoming available. The challenges for the research community include: (i) achieving consensus on the right notion of model explainability, (ii) identifying and formalizing explainability tasks from the perspectives of various stakeholders, and (iii) designing measures for evaluating explainability techniques.In this tutorial, we will first motivate the need for model interpretability and explainability in AI [3] from various perspectives. We will then provide a brief overview of several explainability techniques and tools. The rest of the tutorial will focus on the real-world application of explainability techniques in industry. We will present case studies spanning several domains such as:• Search and Recommendation systems: Understanding of search and recommendations systems, as well as how retrieval and ranking decisions happen in real-time [7]. Example applications include explanation of decisions made by an AI system towards job recommendations, ranking of potential candidates for job posters, and content recommendations.• Sales: Understanding of sales predictions in terms of customer up-sell/churn.• Fraud Detection: Examining and explaining AI systems that determine whether a content or event is fraudulent.• Lending: How to understand/interpret lending decisions made by an AI system.We will focus on the sociotechnical dimensions, practical challenges, and lessons learned during development and deployment of these systems, which would be beneficial for researchers and practitioners interested in explainable AI. Finally, we will discuss open challenges and research directions for the community.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {699},
numpages = {1},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
"@inproceedings{10.1145/3351095.3375670,
author = {Burke, Robin Douglas and Mansoury, Masoud and Sonboli, Nasim},
title = {Experimentation with Fairness-Aware Recommendation Using Librec-Auto: Hands-on Tutorial},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375670},
doi = {10.1145/3351095.3375670},
abstract = {The field of machine learning fairness has developed metrics, methodologies, and data sets for experimenting with classification algorithms. However, equivalent research is lacking in the area of personalized recommender systems. This 180-minute hands-on tutorial will introduce participants to concepts in fairness-aware recommendation, and metrics and methodologies in evaluating recommendation fairness. Participants will also gain hands-on experience with conducting fairness-aware recommendation experiments with the LibRec recommendation system using the libauto{} scripting platform, and learn the steps required to configure their own experiments, incorporate their own data sets, and design their own algorithms and metrics.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {700},
numpages = {1},
keywords = {fairness, recommender systems, evaluation, software},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
"@inproceedings{10.1145/3351095.3375665,
author = {Cath, Corinne and Latonero, Mark and Marda, Vidushi and Pakzad, Roya},
title = {Leap of FATE: Human Rights as a Complementary Framework for AI Policy and Practice},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375665},
doi = {10.1145/3351095.3375665},
abstract = {The premise of this translation tutorial is that human rights serves as a complementary framework - in addition to Fairness, Accountability, Transparency, and Ethics - for guiding and governing artificial intelligence (AI) and machine learning research and development. Attendees will participate in a case study, which will demonstrate show how a human rights framework, grounded in international law, fundamental values, and global systems of accountability, can offer the technical community a practical approach to addressing global AI risks and harms. This tutorial discusses how human rights frameworks can inform, guide and govern AI policy and practice in a manner that is complementary to Fairness, Accountability, Transparency, and Ethics (FATE) frameworks. Using the case study of researchers developing a facial recognition API at a tech company and its use by a law enforcement client, we will engage the audience to think through the benefits and challenges of applying human rights frameworks to AI system design and deployment. We will do so by providing a brief overview of the international human rights law, and various non-binding human rights frameworks in relation to our current discussions around FATE and then apply them to contemporary debates and case studies},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {702},
numpages = {1},
keywords = {human rights, practice, policy, governance, ethics, law, AI, FAT},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
"@inproceedings{10.1145/3351095.3375662,
author = {Wexler, James and Pushkarna, Mahima and Robinson, Sara and Bolukbasi, Tolga and Zaldivar, Andrew},
title = {Probing ML Models for Fairness with the What-If Tool and SHAP: Hands-on Tutorial},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375662},
doi = {10.1145/3351095.3375662},
abstract = {As more and more industries use machine learning, it's important to understand how these models make predictions, and where bias can be introduced in the process. In this tutorial we'll walk through two open source frameworks for analyzing your models from a fairness perspective. We'll start with the What-If Tool, a visualization tool that you can run inside a Python notebook to analyze an ML model. With the What-If Tool, you can identify dataset imbalances, see how individual features impact your model's prediction through partial dependence plots, and analyze human-centered ML models from a fairness perspective using various optimization strategies.Then we'll look at SHAP, a tool for interpreting the output of any machine learning model, and seeing how a model arrived at predictions for individual datapoints. We will then show how to use SHAP and the What-If Tool together. After the tutorial you'll have the skills to get started with both of these tools on your own datasets, and be better equipped to analyze your models from a fairness perspective.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {705},
numpages = {1},
keywords = {fairness, machine learning, data visualization},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
"@inproceedings{10.1145/3351095.3375671,
author = {Jacobs, Abigail Z. and Blodgett, Su Lin and Barocas, Solon and Daum\'{e}, Hal and Wallach, Hanna},
title = {The Meaning and Measurement of Bias: Lessons from Natural Language Processing},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375671},
doi = {10.1145/3351095.3375671},
abstract = {The recent interest in identifying and mitigating bias in computational systems has introduced a wide range of different---and occasionally incomparable---proposals for what constitutes bias in such systems. This tutorial introduces the language of measurement modeling from the quantitative social sciences as a framework for examining how social, organizational, and political values enter computational systems and unpacking the varied normative concerns operationalized in different techniques for measuring ""bias."" We show that this framework helps to clarify the way unobservable theoretical constructs---such as ""creditworthiness,"" ""risk to society,"" or ""tweet toxicity""---are turned into measurable quantities and how this process may introduce fairness-related harms. In particular, we demonstrate how to systematically assess the construct validity and reliability of these measurements to detect and characterize specific types of harms, which arise from mismatches between constructs and their operationalizations. We then take a critical look at existing approaches to examining ""bias"" in NLP models, ranging from work on embedding spaces to machine translation and hate speech detection. We show that measurement modeling can help uncover the implicit constructs that such work aims to capture when measuring ""bias."" In so doing, we illustrate the limits of current ""debiasing"" techniques, which have obscured the specific harms whose measurements they implicitly aim to reduce. By introducing the language of measurement modeling, we provide the FAT* community with a framework for making explicit and testing assumptions about unobservable theoretical constructs embedded in computational systems, thereby clarifying and uniting our understandings of fairness-related harms.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {706},
numpages = {1},
keywords = {word embeddings, fairness, measurement, bias, construct validity},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
"@inproceedings{10.1145/3351095.3375663,
author = {Ganesh, Maya Indira and Dechesne, Francien and Waseem, Zeerak},
title = {Two Computer Scientists and a Cultural Scientist Get Hit by a Driver-Less Car: A Method for Situating Knowledge in the Cross-Disciplinary Study of F-A-T in Machine Learning: Translation Tutorial},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375663},
doi = {10.1145/3351095.3375663},
abstract = {In a workshop organized in December 2017 in Leiden, the Netherlands, a group of lawyers, computer scientists, artists, activists and social and cultural scientists collectively read a computer science paper about 'improving fairness'. This session was perceived by many participants as eye-opening on how different epistemologies shape approaches to the problem, method and solutions, thus enabling further cross-disciplinary discussions during the rest of the workshop. For many participants it was both refreshing and challenging, in equal measure, to understand how another discipline approached the problem of fairness. Now, as a follow-up we propose a translation tutorial that will engage participants at the FAT* conference in a similar exercise. We will invite participants to work in small groups reading excerpts of academic papers from different disciplinary perspectives on the same theme. We argue that most of us do not read outside our disciplines and thus are not familiar with how the same issues might be framed and addressed by our peers. Thus the purpose will be to have participants reflect on the different genealogies of knowledge in research, and how they erect walls, or generate opportunities for more productive inter-disciplinary work. We argue that addressing, through technical measures or otherwise, matters of ethics, bias and discrimination in AI/ML technologies in society is complicated by the different constructions of knowledge about what ethics (or bias or discrimination) means to different groups of practitioners. In the current academic structure, there are scarce resources to test, build on-or even discard-methods to talk across disciplinary lines. This tutorial is thus proposed to see if this particular method might work.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {707},
numpages = {1},
keywords = {discrimination, epistemology, social sciences, situated knowledge, natural language processing, bias, humanities, cross-disciplinary, ethics, science, methodology},
location = {Barcelona, Spain},
series = {FAT* '20}
}
  
"
"@inproceedings{10.1145/3442188.3445929,
author = {Hampton, Lelia Marie},
title = {Black Feminist Musings on Algorithmic Oppression},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445929},
doi = {10.1145/3442188.3445929},
abstract = {This paper uses a theory of oppression to ground and extend algorithmic oppression. Algorithmic oppression is then situated through a Black feminist lens part of which entails highlighting the double bind of technology. To reconcile algorithmic oppression with respect to the fairness, accountability, and transparency community, I critique the language of the community. Lastly, I place algorithmic oppression in a broader conversation of feminist science, technology, and society studies to ground the discussion of ways forward through abolition and empowering marginalized communities.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1},
numpages = {1},
keywords = {abolition, Black feminism, Algorithmic oppression},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
"@inproceedings{10.1145/3442188.3445864,
author = {Cohen, Maxime C. and Elmachtoub, Adam N. and Lei, Xiao},
title = {Price Discrimination with Fairness Constraints},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445864},
doi = {10.1145/3442188.3445864},
abstract = {Price discrimination - offering different prices to different customers - has become common practice. While it allows sellers to increase their profits, it also raises several concerns in terms of fairness. This topic has received extensive attention from media, industry, and regulatory agencies. In this paper, we consider the problem of setting prices for different groups under fairness constraints.In this paper, we propose a formal framework for pricing with fairness, including several definitions of fairness and their potential impact on consumers, sellers, and society at large. In a first step towards the ambitious agenda of designing pricing strategies that are fair, we consider the simplest scenario of a single-product seller facing consumers who can be partitioned into two groups based on a single, binary feature observable to the seller. For each group, we assume that the seller knows the valuation distribution and the population size. The seller's goal is to maximize profit by optimally selecting a price for each group, subject to a fairness constraint which may be self-imposed or explicitly enforced by laws and regulations.We first propose four definitions: fairness in price, demand, consumer surplus, and no-purchase valuation. With our model and definitions in place, we first show that satisfying all four fairness goals simultaneously is impossible unless the mean valuations are the same for both groups. In fact, even achieving two fairness measures simultaneously cannot be done in basic settings. We then consider the impact of imposing each fairness criterion separately, and identify conditions under which the consumer surplus and the social welfare increase or decrease. Under linear or exponential demand, we show that imposing a small amount of fairness in price or no-purchase valuation increases social welfare, whereas fairness in demand or surplus reduces social welfare. We fully characterize the impact of imposing different types of fairness for linear demand. We discover that imposing too much price fairness may result in a lower social welfare relative to imposing no price fairness. Imposing demand and surplus fairness always decreases social welfare. However, imposing no-purchase valuation fairness always increases social welfare. We also extend our results to the cases when there are multiple groups or there is an unprotected feature.Finally, we computationally show that most of our findings continue to hold for three common nonlinear demand models. Our results and insights provide a first step in understanding the impact of imposing fairness in the context of pricing.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {2},
numpages = {1},
keywords = {price discrimination, social welfare, fairness, personalization},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
"@inproceedings{10.1145/3442188.3445865,
author = {Singh, Harvineet and Singh, Rina and Mhasawade, Vishwali and Chunara, Rumi},
title = {Fairness Violations and Mitigation under Covariate Shift},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445865},
doi = {10.1145/3442188.3445865},
abstract = {We study the problem of learning fair prediction models for unseen test sets distributed differently from the train set. Stability against changes in data distribution is an important mandate for responsible deployment of models. The domain adaptation literature addresses this concern, albeit with the notion of stability limited to that of prediction accuracy. We identify sufficient conditions under which stable models, both in terms of prediction accuracy and fairness, can be learned. Using the causal graph describing the data and the anticipated shifts, we specify an approach based on feature selection that exploits conditional independencies in the data to estimate accuracy and fairness metrics for the test set. We show that for specific fairness definitions, the resulting model satisfies a form of worst-case optimality. In context of a healthcare task, we illustrate the advantages of the approach in making more equitable decisions.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {3–13},
numpages = {11},
keywords = {domain adaptation, causal inference, algorithmic fairness, covariate shift},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
"@inproceedings{10.1145/3442188.3445868,
author = {Ron, Tom and Ben-Porat, Omer and Shalit, Uri},
title = {Corporate Social Responsibility via Multi-Armed Bandits},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445868},
doi = {10.1145/3442188.3445868},
abstract = {We propose a multi-armed bandit setting where each arm corresponds to a subpopulation, and pulling an arm is equivalent to granting an opportunity to this subpopulation. In this setting the decision-maker's fairness policy governs the number of opportunities each subpopulation should receive, which typically depends on the (unknown) reward from granting an opportunity to this subpopulation. The decision-maker can decide whether to provide these opportunities, or pay a pre-defined monetary value for every withheld opportunity. The decision-maker's objective is to maximize her utility, which is the sum of rewards minus the cost paid for withheld opportunities. We provide a no-regret algorithm that maximizes the decision-maker's utility and complement our analysis with an almost-tight lower bound. Finally, we discuss the fairness policy and demonstrate its downstream implications on the utility and opportunities via simulations.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {26–40},
numpages = {15},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
"@inproceedings{10.1145/3442188.3445870,
author = {Park, Joon Sung and Bragg, Danielle and Kamar, Ece and Morris, Meredith Ringel},
title = {Designing an Online Infrastructure for Collecting AI Data From People With Disabilities},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445870},
doi = {10.1145/3442188.3445870},
abstract = {AI technology offers opportunities to expand virtual and physical access for people with disabilities. However, an important part of bringing these opportunities to fruition is ensuring that upcoming AI technology works well for people with a wide range of abilities. In this paper, we identify the lack of data from disabled populations as one of the challenges to training and benchmarking fair and inclusive AI systems. As a potential solution, we envision an online infrastructure that can enable large-scale, remote data contributions from disability communities. We investigate the motivations, concerns, and challenges that people with disabilities might experience when asked to collect and upload various forms of AI-relevant data through a semi-structured interview and an online survey that simulated a data contribution process by collecting example data files through an online portal. Based on our findings, we outline design guidelines for developers creating online infrastructures for gathering data from people with disabilities.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {52–63},
numpages = {12},
keywords = {disability, accessibility, AI FATE, inclusion, representation, datasets},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
"@inproceedings{10.1145/3442188.3445872,
author = {Chasalow, Kyla and Levy, Karen},
title = {Representativeness in Statistics, Politics, and Machine Learning},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445872},
doi = {10.1145/3442188.3445872},
abstract = {Representativeness is a foundational yet slippery concept. Though familiar at first blush, it lacks a single precise meaning. Instead, meanings range from typical or characteristic, to a proportionate match between sample and population, to a more general sense of accuracy, generalizability, coverage, or inclusiveness. Moreover, the concept has long been contested. In statistics, debates about the merits and methods of selecting a representative sample date back to the late 19th century; in politics, debates about the value of likeness as a logic of political representation are older still. Today, as the concept crops up in the study of fairness and accountability in machine learning, we need to carefully consider the term's meanings in order to communicate clearly and account for their normative implications. In this paper, we ask what representativeness means, how it is mobilized socially, and what values and ideals it communicates or confronts. We trace the concept's history in statistics and discuss normative tensions concerning its relationship to likeness, exclusion, authority, and aspiration. We draw on these analyses to think through how representativeness is used in FAccT debates, with emphasis on data, shift, participation, and power.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {77–89},
numpages = {13},
keywords = {sampling, fairness, representativeness, inclusion, participation, bias},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
"@inproceedings{10.1145/3442188.3445876,
author = {R\""{a}z, Tim},
title = {Group Fairness: Independence Revisited},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445876},
doi = {10.1145/3442188.3445876},
abstract = {This paper critically examines arguments against independence, a measure of group fairness also known as statistical parity and as demographic parity. In recent discussions of fairness in computer science, some have maintained that independence is not a suitable measure of group fairness. This position is at least partially based on two influential papers (Dwork et al., 2012, Hardt et al., 2016) that provide arguments against independence. We revisit these arguments, and we find that the case against independence is rather weak. We also give arguments in favor of independence, showing that it plays a distinctive role in considerations of fairness. Finally, we discuss how to balance different fairness considerations.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {129–137},
numpages = {9},
keywords = {separation, accuracy, affirmative action, demographic parity, fairness, statistical parity, sufficiency, independence},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
"@inproceedings{10.1145/3442188.3445878,
author = {Zhang, Hongjing and Davidson, Ian},
title = {Towards Fair Deep Anomaly Detection},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445878},
doi = {10.1145/3442188.3445878},
abstract = {Anomaly detection aims to find instances that are considered unusual and is a fundamental problem of data science. Recently, deep anomaly detection methods were shown to achieve superior results particularly in complex data such as images. Our work focuses on deep one-class classification for anomaly detection which learns a mapping only from the normal samples. However, the non-linear transformation performed by deep learning can potentially find patterns associated with social bias. The challenge with adding fairness to deep anomaly detection is to ensure both making fair and correct anomaly predictions simultaneously. In this paper, we propose a new architecture for the fair anomaly detection approach (Deep Fair SVDD) and train it using an adversarial network to de-correlate the relationships between the sensitive attributes and the learned representations. This differs from how fairness is typically added namely as a regularizer or a constraint. Further, we propose two effective fairness measures and empirically demonstrate that existing deep anomaly detection methods are unfair. We show that our proposed approach can remove the unfairness largely with minimal loss on the anomaly detection performance. Lastly, we conduct an in-depth analysis to show the strength and limitations of our proposed model, including parameter analysis, feature visualization, and run-time analysis.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {138–148},
numpages = {11},
keywords = {anomaly detection, deep learning, algorithmic fairness, adversarial learning, machine learning},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
"@inproceedings{10.1145/3442188.3445879,
author = {Cheng, Victoria and Suriyakumar, Vinith M. and Dullerud, Natalie and Joshi, Shalmali and Ghassemi, Marzyeh},
title = {Can You Fake It Until You Make It? Impacts of Differentially Private Synthetic Data on Downstream Classification Fairness},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445879},
doi = {10.1145/3442188.3445879},
abstract = {The recent adoption of machine learning models in high-risk settings such as medicine has increased demand for developments in privacy and fairness. Rebalancing skewed datasets using synthetic data created by generative adversarial networks (GANs) has shown potential to mitigate disparate impact on minoritized subgroups. However, such generative models are subject to privacy attacks that can expose sensitive data from the training dataset. Differential privacy (DP) is the current leading solution for privacy-preserving machine learning. Differentially private GANs (DP GANs) are often considered a potential solution for improving model fairness while maintaining privacy of sensitive training data. We investigate the impact of using synthetic images from DP GANs on downstream classification model utility and fairness. We demonstrate that existing DP GANs cannot simultaneously maintain model utility, privacy, and fairness. The images generated from GAN models trained with DP exhibit extreme decreases in image quality and utility which leads to poor downstream classification model performance. Our evaluation highlights the friction between privacy, fairness, and utility and how this directly translates into real loss of performance and representation in common machine learning settings. Our results show that additional work improving the utility and fairness of DP generative models is required before they can be utilized as a potential solution to privacy and fairness issues stemming from lack of diversity in the training dataset.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {149–160},
numpages = {12},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
"@inproceedings{10.1145/3442188.3445882,
author = {Donahue, Kate and Barocas, Solon},
title = {Better Together? How Externalities of Size Complicate Notions of Solidarity and Actuarial Fairness},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445882},
doi = {10.1145/3442188.3445882},
abstract = {Consider a cost-sharing game with players of different costs: an example might be an insurance company calculating premiums for a population of mixed-risk individuals. Two natural and competing notions of fairness might be to a) charge each individual the same or b) charge each individual according to the cost that they bring to the pool. In the insurance literature, these approaches are referred to as ""solidarity"" and ""actuarial fairness"" and are commonly viewed as opposites. However, in insurance (and many other natural settings), the cost-sharing game also exhibits externalities of size: all else being equal, larger groups have lower average cost. In the insurance case, we analyze model where costs strictly decreases with pooling due to a reduction in the variability of losses. In this paper, we explore how this complicates traditional understandings of fairness, drawing on literature in cooperative game theory.First, we explore solidarity: we show that it is possible for both groups (high risk and low risk) to strictly benefit by joining an insurance pool where costs are evenly split, as opposed to being in separate risk pools. We build on this by producing a pricing scheme that maximally subsidizes the high risk group, while maintaining an incentive for lower risk people to stay in the insurance pool. Next, we demonstrate that with this new model, the price charged to each individual has to depend on the risk of other participants, making naive actuarial fairness inefficient. Furthermore, we prove that stable pricing schemes must be ones where players have the antisocial incentive desiring riskier partners, contradicting motivations for using actuarial fairness. Finally, we describe how these results relate to debates about fairness in machine learning and potential avenues for future research.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {185–195},
numpages = {11},
keywords = {actuarial fairness, cooperative game theory, insurance, submodular cost function, fair cost sharing, solidarity},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
"@inproceedings{10.1145/3442188.3445884,
author = {Awasthi, Pranjal and Beutel, Alex and Kleindessner, Matth\""{a}us and Morgenstern, Jamie and Wang, Xuezhi},
title = {Evaluating Fairness of Machine Learning Models Under Uncertain and Incomplete Information},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445884},
doi = {10.1145/3442188.3445884},
abstract = {Training and evaluation of fair classifiers is a challenging problem. This is partly due to the fact that most fairness metrics of interest depend on both the sensitive attribute information and label information of the data points. In many scenarios it is not possible to collect large datasets with such information. An alternate approach that is commonly used is to separately train an attribute classifier on data with sensitive attribute information, and then use it later in the ML pipeline to evaluate the bias of a given classifier. While such decoupling helps alleviate the problem of demographic scarcity, it raises several natural questions such as: how should the attribute classifier be trained?, and how should one use a given attribute classifier for accurate bias estimation? In this work we study this question from both theoretical and empirical perspectives.We first experimentally demonstrate that the test accuracy of the attribute classifier is not always correlated with its effectiveness in bias estimation for a downstream model. In order to further investigate this phenomenon, we analyze an idealized theoretical model and characterize the structure of the optimal classifier. Our analysis has surprising and counter-intuitive implications where in certain regimes one might want to distribute the error of the attribute classifier as unevenly as possible among the different subgroups. Based on our analysis we develop heuristics for both training and using attribute classifiers for bias estimation in the data scarce regime. We empirically demonstrate the effectiveness of our approach on real and simulated data.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {206–214},
numpages = {9},
keywords = {Model Auditing, Active Learning, Bias Estimation},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
"@inproceedings{10.1145/3442188.3445885,
author = {Vincent, Nicholas and Li, Hanlin and Tilly, Nicole and Chancellor, Stevie and Hecht, Brent},
title = {Data Leverage: A Framework for Empowering the Public in Its Relationship with Technology Companies},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445885},
doi = {10.1145/3442188.3445885},
abstract = {Many powerful computing technologies rely on implicit and explicit data contributions from the public. This dependency suggests a potential source of leverage for the public in its relationship with technology companies: by reducing, stopping, redirecting, or otherwise manipulating data contributions, the public can reduce the effectiveness of many lucrative technologies. In this paper, we synthesize emerging research that seeks to better understand and help people action this data leverage. Drawing on prior work in areas including machine learning, human-computer interaction, and fairness and accountability in computing, we present a framework for understanding data leverage that highlights new opportunities to change technology company behavior related to privacy, economic inequality, content moderation and other areas of societal concern. Our framework also points towards ways that policymakers can bolster data leverage as a means of changing the balance of power between the public and tech companies.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {215–227},
numpages = {13},
keywords = {data leverage, data poisoning, conscious data contribution, data strikes},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
"@inproceedings{10.1145/3442188.3445886,
author = {Kasirzadeh, Atoosa and Smart, Andrew},
title = {The Use and Misuse of Counterfactuals in Ethical Machine Learning},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445886},
doi = {10.1145/3442188.3445886},
abstract = {The use of counterfactuals for considerations of algorithmic fairness and explainability is gaining prominence within the machine learning community and industry. This paper argues for more caution with the use of counterfactuals when the facts to be considered are social categories such as race or gender. We review a broad body of papers from philosophy and social sciences on social ontology and the semantics of counterfactuals, and we conclude that the counterfactual approach in machine learning fairness and social explainability can require an incoherent theory of what social categories are. Our findings suggest that most often the social categories may not admit counterfactual manipulation, and hence may not appropriately satisfy the demands for evaluating the truth or falsity of counterfactuals. This is important because the widespread use of counterfactuals in machine learning can lead to misleading results when applied in high-stakes domains. Accordingly, we argue that even though counterfactuals play an essential part in some causal inferences, their use for questions of algorithmic fairness and social explanations can create more problems than they resolve. Our positive result is a set of tenets about using counterfactuals for fairness and explanations in machine learning.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {228–236},
numpages = {9},
keywords = {Fairness, Counterfactuals, Social kind, Ethics of AI, Algorithmic Fairness, Philosophy of AI, Social category, Philosophy, Ethical AI, Social ontology, Explanation, Machine learning, Explainable AI},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
"@inproceedings{10.1145/3442188.3445887,
author = {Mehrotra, Anay and Celis, L. Elisa},
title = {Mitigating Bias in Set Selection with Noisy Protected Attributes},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445887},
doi = {10.1145/3442188.3445887},
abstract = {Subset selection algorithms are ubiquitous in AI-driven applications, including, online recruiting portals and image search engines, so it is imperative that these tools are not discriminatory on the basis of protected attributes such as gender or race. Currently, fair subset selection algorithms assume that the protected attributes are known as part of the dataset. However, protected attributes may be noisy due to errors during data collection or if they are imputed (as is often the case in real-world settings). While a wide body of work addresses the effect of noise on the performance of machine learning algorithms, its effect on fairness remains largely unexamined. We find that in the presence of noisy protected attributes, in attempting to increase fairness without considering noise, one can, in fact, decrease the fairness of the result!Towards addressing this, we consider an existing noise model in which there is probabilistic information about the protected attributes (e.g., [19, 32, 44, 56]), and ask is fair selection possible under noisy conditions? We formulate a ""denoised"" selection problem which functions for a large class of fairness metrics; given the desired fairness goal, the solution to the denoised problem violates the goal by at most a small multiplicative amount with high probability. Although this denoised problem turns out to be NP-hard, we give a linear-programming based approximation algorithm for it. We evaluate this approach on both synthetic and real-world datasets. Our empirical results show that this approach can produce subsets which significantly improve the fairness metrics despite the presence of noisy protected attributes, and, compared to prior noise-oblivious approaches, has better Pareto-tradeoffs between utility and fairness.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {237–248},
numpages = {12},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
"@inproceedings{10.1145/3442188.3445888,
author = {Andrus, McKane and Spitzer, Elena and Brown, Jeffrey and Xiang, Alice},
title = {What We Can't Measure, We Can't Understand: Challenges to Demographic Data Procurement in the Pursuit of Fairness},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445888},
doi = {10.1145/3442188.3445888},
abstract = {As calls for fair and unbiased algorithmic systems increase, so too does the number of individuals working on algorithmic fairness in industry. However, these practitioners often do not have access to the demographic data they feel they need to detect bias in practice. Even with the growing variety of toolkits and strategies for working towards algorithmic fairness, they almost invariably require access to demographic attributes or proxies. We investigated this dilemma through semi-structured interviews with 38 practitioners and professionals either working in or adjacent to algorithmic fairness. Participants painted a complex picture of what demographic data availability and use look like on the ground, ranging from not having access to personal data of any kind to being legally required to collect and use demographic data for discrimination assessments. In many domains, demographic data collection raises a host of difficult questions, including how to balance privacy and fairness, how to define relevant social categories, how to ensure meaningful consent, and whether it is appropriate for private companies to infer someone's demographics. Our research suggests challenges that must be considered by businesses, regulators, researchers, and community groups in order to enable practitioners to address algorithmic bias in practice. Critically, we do not propose that the overall goal of future work should be to simply lower the barriers to collecting demographic data. Rather, our study surfaces a swath of normative questions about how, when, and whether this data should be procured, and, in cases where it is not, what should still be done to mitigate bias.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {249–260},
numpages = {12},
keywords = {fairness, demographic data, special category data, anti-discrimination, data privacy, sensitive data},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
"@inproceedings{10.1145/3442188.3445891,
author = {Kilby, Angela E.},
title = {Algorithmic Fairness in Predicting Opioid Use Disorder Using Machine Learning},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445891},
doi = {10.1145/3442188.3445891},
abstract = {There has been recent interest by payers, health care systems, and researchers in the development of machine learning and artificial intelligence models that predict an individual's probability of developing opioid use disorder. The scores generated by these algorithms can be used by physicians to tailor the prescribing of opioids for the treatment of pain, reducing or foregoing prescribing to individuals deemed to be at high risk, or increasing prescribing for patients deemed to be at low risk. This paper constructs a machine learning algorithm to predict opioid use disorder risk using commercially available claims data similar to those utilized in the development of proprietary opioid use disorder prediction algorithms. We study risk scores generated by the machine learning model in a setting with quasi-experimental variation in the likelihood that doctors prescribe opioids, generated by changes in the legal structure for monitoring physician prescribing. We find that machine-predicted risk scores do not appear to correlate at all with the individual-specific heterogeneous treatment effect of receiving opioids. The paper identifies a new source of algorithmic unfairness in machine learning applications for health care and precision medicine, arising from the researcher's choice of objective function. While precision medicine should guide physician treatment decisions based on the heterogeneous causal impact of a course of treatment for an individual, allocating treatments to individuals receiving the most benefit and recommending caution for those most likely to experience harmful side effects, ML models in health care are often trained on proxies like individual baseline risk, and are not necessarily informative in deciding who would most benefit, or be harmed, by a course of treatment.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {272},
numpages = {1},
keywords = {evaluations, algorithm development, critical data/algorithm studies, fairness, auditing, ethics, disability studies, causality, algorithmic impacts on social phenomena, social and organizational processes},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
"@inproceedings{10.1145/3442188.3445892,
author = {Yeom, Samuel and Tschantz, Michael Carl},
title = {Avoiding Disparity Amplification under Different Worldviews},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445892},
doi = {10.1145/3442188.3445892},
abstract = {We mathematically compare four competing definitions of group-level nondiscrimination: demographic parity, equalized odds, predictive parity, and calibration. Using the theoretical framework of Friedler et al., we study the properties of each definition under various worldviews, which are assumptions about how, if at all, the observed data is biased. We argue that different worldviews call for different definitions of fairness, and we specify the worldviews that, when combined with the desire to avoid a criterion for discrimination that we call disparity amplification, motivate demographic parity and equalized odds. We also argue that predictive parity and calibration are insufficient for avoiding disparity amplification because predictive parity allows an arbitrarily large inter-group disparity and calibration is not robust to post-processing. Finally, we define a worldview that is more realistic than the previously considered ones, and we introduce a new notion of fairness that corresponds to this worldview.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {273–283},
numpages = {11},
keywords = {equalized odds, worldview, predictive parity, disparity amplification, demographic parity, fairness, calibration},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
"@inproceedings{10.1145/3442188.3445894,
author = {Black, Emily and Fredrikson, Matt},
title = {Leave-One-out Unfairness},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445894},
doi = {10.1145/3442188.3445894},
abstract = {We introduce leave-one-out unfairness, which characterizes how likely a model's prediction for an individual will change due to the inclusion or removal of a single other person in the model's training data. Leave-one-out unfairness appeals to the idea that fair decisions are not arbitrary: they should not be based on the chance event of any one person's inclusion in the training data. Leave-one-out unfairness is closely related to algorithmic stability, but it focuses on the consistency of an individual point's prediction outcome over unit changes to the training data, rather than the error of the model in aggregate. Beyond formalizing leave-one-out unfairness, we characterize the extent to which deep models behave leave-one-out unfairly on real data, including in cases where the generalization error is small. Further, we demonstrate that adversarial training and randomized smoothing techniques have opposite effects on leave-one-out fairness, which sheds light on the relationships between robustness, memorization, individual fairness, and leave-one-out fairness in deep models. Finally, we discuss salient practical applications that may be negatively affected by leave-one-out unfairness.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {285–295},
numpages = {11},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
"@inproceedings{10.1145/3442188.3445895,
author = {Kallus, Nathan and Zhou, Angela},
title = {Fairness, Welfare, and Equity in Personalized Pricing},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445895},
doi = {10.1145/3442188.3445895},
abstract = {We study the interplay of fairness, welfare, and equity considerations in personalized pricing based on customer features. Sellers are increasingly able to conduct price personalization based on predictive modeling of demand conditional on covariates: setting customized interest rates, targeted discounts of consumer goods, and personalized subsidies of scarce resources with positive externalities like vaccines and bed nets. These different application areas may lead to different concerns around fairness, welfare, and equity on different objectives: price burdens on consumers, price envy, firm revenue, access to a good, equal access, and distributional consequences when the good in question further impacts downstream outcomes of interest. We conduct a comprehensive literature review in order to disentangle these different normative considerations and propose a taxonomy of different objectives with mathematical definitions. We focus on observational metrics that do not assume access to an underlying valuation distribution which is either unobserved due to binary feedback or ill-defined due to overriding behavioral concerns regarding interpreting revealed preferences. In the setting of personalized pricing for the provision of goods with positive benefits, we discuss how price optimization may provide unambiguous benefit by achieving a ""triple bottom line"": personalized pricing enables expanding access, which in turn may lead to gains in welfare due to heterogeneous utility, and improve revenue or budget utilization. We empirically demonstrate the potential benefits of personalized pricing in two settings: pricing subsidies for an elective vaccine, and the effects of personalized interest rates on downstream outcomes in microcredit.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {296–314},
numpages = {19},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
"@inproceedings{10.1145/3442188.3445896,
author = {Sambasivan, Nithya and Arnesen, Erin and Hutchinson, Ben and Doshi, Tulsee and Prabhakaran, Vinodkumar},
title = {Re-Imagining Algorithmic Fairness in India and Beyond},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445896},
doi = {10.1145/3442188.3445896},
abstract = {Conventional algorithmic fairness is West-centric, as seen in its subgroups, values, and methods. In this paper, we de-center algorithmic fairness and analyse AI power in India. Based on 36 qualitative interviews and a discourse analysis of algorithmic deployments in India, we find that several assumptions of algorithmic fairness are challenged. We find that in India, data is not always reliable due to socio-economic factors, ML makers appear to follow double standards, and AI evokes unquestioning aspiration. We contend that localising model fairness alone can be window dressing in India, where the distance between models and oppressed communities is large. Instead, we re-imagine algorithmic fairness in India and provide a roadmap to re-contextualise data and models, empower oppressed communities, and enable Fair-ML ecosystems.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {315–328},
numpages = {14},
keywords = {anti-caste politics, caste, gender, class, religion, critical algorithmic studies, India, decoloniality, feminism, ability, algorithmic fairness},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
"@inproceedings{10.1145/3442188.3445900,
author = {Barbosa, Simone Diniz Junqueira and Barbosa, Gabriel Diniz Junqueira and Souza, Clarisse Sieckenius de and Leit\~{a}o, Carla Faria},
title = {A Semiotics-Based Epistemic Tool to Reason about Ethical Issues in Digital Technology Design and Development},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445900},
doi = {10.1145/3442188.3445900},
abstract = {One of the important challenges regarding the development of morally responsible and ethically qualified digital technologies is how to support designers and developers in producing those technologies, especially when conceptualizing their vision of what the technology will be, how it will benefit users, and avoid doing harm. However, traditional software design and development life cycles do not explicitly support the reflection upon either ethical or moral issues. In this paper we look at how a number of ethical issues may be dealt with during digital technology design and development, to prevent damage and improve technological fairness, accountability, and transparency. Starting from mature work on semiotic theory and methods in human-computer interaction, we propose to extend the core artifact used in semiotic engineering of human-centered technology design, so as to directly address moral responsibility and ethical issues. The resulting extension is an epistemic tool, that is, an instrument to create and elaborate on this specific kind of knowledge. The paper describes the tool, illustrates how it is to be used, and discusses its promises and limitations against the background of related work. It also includes proposed empirical studies, accompanied by briefly described methodological challenges and considerations that deserve our attention.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {363–374},
numpages = {12},
keywords = {semiotic engineering, epistemic tool, ethics},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
"@inproceedings{10.1145/3442188.3445901,
author = {Jacobs, Abigail Z. and Wallach, Hanna},
title = {Measurement and Fairness},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445901},
doi = {10.1145/3442188.3445901},
abstract = {We propose measurement modeling from the quantitative social sciences as a framework for understanding fairness in computational systems. Computational systems often involve unobservable theoretical constructs, such as socioeconomic status, teacher effectiveness, and risk of recidivism. Such constructs cannot be measured directly and must instead be inferred from measurements of observable properties (and other unobservable theoretical constructs) thought to be related to them---i.e., operationalized via a measurement model. This process, which necessarily involves making assumptions, introduces the potential for mismatches between the theoretical understanding of the construct purported to be measured and its operationalization. We argue that many of the harms discussed in the literature on fairness in computational systems are direct results of such mismatches. We show how some of these harms could have been anticipated and, in some cases, mitigated if viewed through the lens of measurement modeling. To do this, we contribute fairness-oriented conceptualizations of construct reliability and construct validity that unite traditions from political science, education, and psychology and provide a set of tools for making explicit and testing assumptions about constructs and their operationalizations. We then turn to fairness itself, an essentially contested construct that has different theoretical understandings in different contexts. We argue that this contestedness underlies recent debates about fairness definitions: although these debates appear to be about different operationalizations, they are, in fact, debates about different theoretical understandings of fairness. We show how measurement modeling can provide a framework for getting to the core of these debates.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {375–385},
numpages = {11},
keywords = {fairness, construct validity, measurement, construct reliability},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
"@inproceedings{10.1145/3442188.3445902,
author = {Mishler, Alan and Kennedy, Edward H. and Chouldechova, Alexandra},
title = {Fairness in Risk Assessment Instruments: Post-Processing to Achieve Counterfactual Equalized Odds},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445902},
doi = {10.1145/3442188.3445902},
abstract = {In domains such as criminal justice, medicine, and social welfare, decision makers increasingly have access to algorithmic Risk Assessment Instruments (RAIs). RAIs estimate the risk of an adverse outcome such as recidivism or child neglect, potentially informing high-stakes decisions such as whether to release a defendant on bail or initiate a child welfare investigation. It is important to ensure that RAIs are fair, so that the benefits and harms of such decisions are equitably distributed.The most widely used algorithmic fairness criteria are formulated with respect to observable outcomes, such as whether a person actually recidivates, but these criteria are misleading when applied to RAIs. Since RAIs are intended to inform interventions that can reduce risk, the prediction itself affects the downstream outcome. Recent work has argued that fairness criteria for RAIs should instead utilize potential outcomes, i.e. the outcomes that would occur in the absence of an appropriate intervention [11]. However, no methods currently exist to satisfy such fairness criteria.In this paper, we target one such criterion, counterfactual equalized odds. We develop a post-processed predictor that is estimated via doubly robust estimators, extending and adapting previous postprocessing approaches [16] to the counterfactual setting. We also provide doubly robust estimators of the risk and fairness properties of arbitrary fixed post-processed predictors. Our predictor converges to an optimal fair predictor at fast rates. We illustrate properties of our method and show that it performs well on both simulated and real data.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {386–400},
numpages = {15},
keywords = {counterfactual, risk assessment, fairness, post-processing},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
"@inproceedings{10.1145/3442188.3445906,
author = {Ghadiri, Mehrdad and Samadi, Samira and Vempala, Santosh},
title = {Socially Fair K-Means Clustering},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445906},
doi = {10.1145/3442188.3445906},
abstract = {We show that the popular k-means clustering algorithm (Lloyd's heuristic), used for a variety of scientific data, can result in outcomes that are unfavorable to subgroups of data (e.g., demographic groups). Such biased clusterings can have deleterious implications for human-centric applications such as resource allocation. We present a fair k-means objective and algorithm to choose cluster centers that provide equitable costs for different groups. The algorithm, Fair-Lloyd, is a modification of Lloyd's heuristic for k-means, inheriting its simplicity, efficiency, and stability. In comparison with standard Lloyd's, we find that on benchmark datasets, Fair-Lloyd exhibits unbiased performance by ensuring that all groups have equal costs in the output k-clustering, while incurring a negligible increase in running time, thus making it a viable fair option wherever k-means is currently used.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {438–448},
numpages = {11},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
"@inproceedings{10.1145/3442188.3445910,
author = {Nanda, Vedant and Dooley, Samuel and Singla, Sahil and Feizi, Soheil and Dickerson, John P.},
title = {Fairness Through Robustness: Investigating Robustness Disparity in Deep Learning},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445910},
doi = {10.1145/3442188.3445910},
abstract = {Deep neural networks (DNNs) are increasingly used in real-world applications (e.g. facial recognition). This has resulted in concerns about the fairness of decisions made by these models. Various notions and measures of fairness have been proposed to ensure that a decision-making system does not disproportionately harm (or benefit) particular subgroups of the population. In this paper, we argue that traditional notions of fairness that are only based on models' outputs are not sufficient when the model is vulnerable to adversarial attacks. We argue that in some cases, it may be easier for an attacker to target a particular subgroup, resulting in a form of robustness bias. We show that measuring robustness bias is a challenging task for DNNs and propose two methods to measure this form of bias. We then conduct an empirical study on state-of-the-art neural networks on commonly used real-world datasets such as CIFAR-10, CIFAR-100, Adience, and UTKFace and show that in almost all cases there are subgroups (in some cases based on sensitive attributes like race, gender, etc) which are less robust and are thus at a disadvantage. We argue that this kind of bias arises due to both the data distribution and the highly complex nature of the learned decision boundary in the case of DNNs, thus making mitigation of such biases a non-trivial task. Our results show that robustness bias is an important criterion to consider while auditing real-world systems that rely on DNNs for decision making. Code to reproduce all our results can be found here: https://github.com/nvedant07/Fairness-Through-Robustness},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {466–477},
numpages = {12},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
"@inproceedings{10.1145/3442188.3445912,
author = {Finocchiaro, Jessie and Maio, Roland and Monachou, Faidra and Patro, Gourab K and Raghavan, Manish and Stoica, Ana-Andreea and Tsirtsis, Stratis},
title = {Bridging Machine Learning and Mechanism Design towards Algorithmic Fairness},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445912},
doi = {10.1145/3442188.3445912},
abstract = {Decision-making systems increasingly orchestrate our world: how to intervene on the algorithmic components to build fair and equitable systems is therefore a question of utmost importance; one that is substantially complicated by the context-dependent nature of fairness and discrimination. Modern decision-making systems that involve allocating resources or information to people (e.g., school choice, advertising) incorporate machine-learned predictions in their pipelines, raising concerns about potential strategic behavior or constrained allocation, concerns usually tackled in the context of mechanism design. Although both machine learning and mechanism design have developed frameworks for addressing issues of fairness and equity, in some complex decision-making systems, neither framework is individually sufficient. In this paper, we develop the position that building fair decision-making systems requires overcoming these limitations which, we argue, are inherent to each field. Our ultimate objective is to build an encompassing framework that cohesively bridges the individual frameworks of mechanism design and machine learning. We begin to lay the ground work towards this goal by comparing the perspective each discipline takes on fair decision-making, teasing out the lessons each field has taught and can teach the other, and highlighting application domains that require a strong collaboration between these disciplines.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {489–503},
numpages = {15},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
"@inproceedings{10.1145/3442188.3445913,
author = {Abbasi, Mohsen and Bhaskara, Aditya and Venkatasubramanian, Suresh},
title = {Fair Clustering via Equitable Group Representations},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445913},
doi = {10.1145/3442188.3445913},
abstract = {What does it mean for a clustering to be fair? One popular approach seeks to ensure that each cluster contains groups in (roughly) the same proportion in which they exist in the population. The normative principle at play is balance: any cluster might act as a representative of the data, and thus should reflect its diversity.But clustering also captures a different form of representativeness. A core principle in most clustering problems is that a cluster center should be representative of the cluster it represents, by being ""close"" to the points associated with it. This is so that we can effectively replace the points by their cluster centers without significant loss in fidelity, and indeed is a common ""use case"" for clustering. For such a clustering to be fair, the centers should ""represent"" different groups equally well. We call such a clustering a group-representative clustering.In this paper, we study the structure and computation of group-representative clusterings. We show that this notion naturally parallels the development of fairness notions in classification, with direct analogs of ideas like demographic parity and equal opportunity. We demonstrate how these notions are distinct from and cannot be captured by balance-based notions of fairness. We present approximation algorithms for group representative k-median clustering and couple this with an empirical evaluation on various real-world data sets. We also extend this idea to facility location, motivated by the current problem of assigning polling locations for voting},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {504–514},
numpages = {11},
keywords = {clustering, algorithmic fairness, representation},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
"@inproceedings{10.1145/3442188.3445915,
author = {Wang, Jialu and Liu, Yang and Levy, Caleb},
title = {Fair Classification with Group-Dependent Label Noise},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445915},
doi = {10.1145/3442188.3445915},
abstract = {This work examines how to train fair classifiers in settings where training labels are corrupted with random noise, and where the error rates of corruption depend both on the label class and on the membership function for a protected subgroup. Heterogeneous label noise models systematic biases towards particular groups when generating annotations. We begin by presenting analytical results which show that naively imposing parity constraints on demographic disparity measures, without accounting for heterogeneous and group-dependent error rates, can decrease both the accuracy and the fairness of the resulting classifier. Our experiments demonstrate these issues arise in practice as well. We address these problems by performing empirical risk minimization with carefully defined surrogate loss functions and surrogate constraints that help avoid the pitfalls introduced by heterogeneous label noise. We provide both theoretical and empirical justifications for the efficacy of our methods. We view our results as an important example of how imposing fairness on biased data sets without proper care can do at least as much harm as it does good.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {526–536},
numpages = {11},
keywords = {algorithmic fairness, learning with noisy and biased labels, machine learning},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
"@inproceedings{10.1145/3442188.3445916,
author = {Yang, Eddie and Roberts, Margaret E.},
title = {Censorship of Online Encyclopedias: Implications for NLP Models},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445916},
doi = {10.1145/3442188.3445916},
abstract = {While artificial intelligence provides the backbone for many tools people use around the world, recent work has brought to attention that the algorithms powering AI are not free of politics, stereotypes, and bias. While most work in this area has focused on the ways in which AI can exacerbate existing inequalities and discrimination, very little work has studied how governments actively shape training data. We describe how censorship has affected the development of Wikipedia corpuses, text data which are regularly used for pre-trained inputs into NLP algorithms. We show that word embeddings trained on Baidu Baike, an online Chinese encyclopedia, have very different associations between adjectives and a range of concepts about democracy, freedom, collective action, equality, and people and historical events in China than its regularly blocked but uncensored counterpart - Chinese language Wikipedia. We examine the implications of these discrepancies by studying their use in downstream AI applications. Our paper shows how government repression, censorship, and self-censorship may impact training data and the applications that draw from them.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {537–548},
numpages = {12},
keywords = {censorship, machine learning, word embeddings, training data},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
"@inproceedings{10.1145/3442188.3445917,
author = {Hamon, Ronan and Junklewitz, Henrik and Malgieri, Gianclaudio and Hert, Paul De and Beslay, Laurent and Sanchez, Ignacio},
title = {Impossible Explanations? Beyond Explainable AI in the GDPR from a COVID-19 Use Case Scenario},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445917},
doi = {10.1145/3442188.3445917},
abstract = {Can we achieve an adequate level of explanation for complex machine learning models in high-risk AI applications when applying the EU data protection framework? In this article, we address this question, analysing from a multidisciplinary point of view the connection between existing legal requirements for the explainability of AI systems and the current state of the art in the field of explainable AI.We present a case study of a real-life scenario designed to illustrate the application of an AI-based automated decision making process for the medical diagnosis of COVID-19 patients. The scenario exemplifies the trend in the usage of increasingly complex machine-learning algorithms with growing dimensionality of data and model parameters. Based on this setting, we analyse the challenges of providing human legible explanations in practice and we discuss their legal implications following the General Data Protection Regulation (GDPR).Although it might appear that there is just one single form of explanation in the GDPR, we conclude that the context in which the decision-making system operates requires that several forms of explanation are considered. Thus, we propose to design explanations in multiple forms, depending on: the moment of the disclosure of the explanation (either ex ante or ex post); the audience of the explanation (explanation for an expert or a data controller and explanation for the final data subject); the layer of granularity (such as general, group-based or individual explanations); the level of the risks of the automated decision regarding fundamental rights and freedoms. Consequently, explanations should embrace this multifaceted environment.Furthermore, we highlight how the current inability of complex, deep learning based machine learning models to make clear causal links between input data and final decisions represents a limitation for providing exact, human-legible reasons behind specific decisions. This makes the provision of satisfactorily, fair and transparent explanations a serious challenge. Therefore, there are cases where the quality of possible explanations might not be assessed as an adequate safeguard for automated decision-making processes under Article 22(3) GDPR. Accordingly, we suggest that further research should focus on alternative tools in the GDPR (such as algorithmic impact assessments from Article 35 GDPR or algorithmic lawfulness justifications) that might be considered to complement the explanations of automated decision-making.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {549–559},
numpages = {11},
keywords = {GDPR, Black-Box, Automated Decision-Making, Data Protection, Machine Learning, Explainability, AI},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
"@inproceedings{10.1145/3442188.3445919,
author = {Kasy, Maximilian and Abebe, Rediet},
title = {Fairness, Equality, and Power in Algorithmic Decision-Making},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445919},
doi = {10.1145/3442188.3445919},
abstract = {Much of the debate on the impact of algorithms is concerned with fairness, defined as the absence of discrimination for individuals with the same ""merit."" Drawing on the theory of justice, we argue that leading notions of fairness suffer from three key limitations: they legitimize inequalities justified by ""merit;"" they are narrowly bracketed, considering only differences of treatment within the algorithm; and they consider between-group and not within-group differences. We contrast this fairness-based perspective with two alternate perspectives: the first focuses on inequality and the causal impact of algorithms and the second on the distribution of power. We formalize these perspectives drawing on techniques from causal inference and empirical economics, and characterize when they give divergent evaluations. We present theoretical results and empirical examples which demonstrate this tension. We further use these insights to present a guide for algorithmic auditing and discuss the importance of inequality- and power-centered frameworks in algorithmic decision-making.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {576–586},
numpages = {11},
keywords = {power, empirical economics, Algorithmic fairness, inequality, auditing},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
"@inproceedings{10.1145/3442188.3445927,
author = {Taskesen, Bahar and Blanchet, Jose and Kuhn, Daniel and Nguyen, Viet Anh},
title = {A Statistical Test for Probabilistic Fairness},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445927},
doi = {10.1145/3442188.3445927},
abstract = {Algorithms are now routinely used to make consequential decisions that affect human lives. Examples include college admissions, medical interventions or law enforcement. While algorithms empower us to harness all information hidden in vast amounts of data, they may inadvertently amplify existing biases in the available datasets. This concern has sparked increasing interest in fair machine learning, which aims to quantify and mitigate algorithmic discrimination. Indeed, machine learning models should undergo intensive tests to detect algorithmic biases before being deployed at scale. In this paper, we use ideas from the theory of optimal transport to propose a statistical hypothesis test for detecting unfair classifiers. Leveraging the geometry of the feature space, the test statistic quantifies the distance of the empirical distribution supported on the test samples to the manifold of distributions that render a pre-trained classifier fair. We develop a rigorous hypothesis testing mechanism for assessing the probabilistic fairness of any pre-trained logistic classifier, and we show both theoretically as well as empirically that the proposed test is asymptotically correct. In addition, the proposed framework offers interpretability by identifying the most favorable perturbation of the data so that the given classifier becomes fair.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {648–665},
numpages = {18},
keywords = {Wasserstein distance, equalized odds, algorithmic bias, fairness, equal opportunity},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
"@inproceedings{10.1145/3442188.3445928,
author = {Wilson, Christo and Ghosh, Avijit and Jiang, Shan and Mislove, Alan and Baker, Lewis and Szary, Janelle and Trindel, Kelly and Polli, Frida},
title = {Building and Auditing Fair Algorithms: A Case Study in Candidate Screening},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445928},
doi = {10.1145/3442188.3445928},
abstract = {Academics, activists, and regulators are increasingly urging companies to develop and deploy sociotechnical systems that are fair and unbiased. Achieving this goal, however, is complex: the developer must (1) deeply engage with social and legal facets of ""fairness"" in a given context, (2) develop software that concretizes these values, and (3) undergo an independent algorithm audit to ensure technical correctness and social accountability of their algorithms. To date, there are few examples of companies that have transparently undertaken all three steps.In this paper we outline a framework for algorithmic auditing by way of a case-study of pymetrics, a startup that uses machine learning to recommend job candidates to their clients. We discuss how pymetrics approaches the question of fairness given the constraints of ethical, regulatory, and client demands, and how pymetrics' software implements adverse impact testing. We also present the results of an independent audit of pymetrics' candidate screening tool.We conclude with recommendations on how to structure audits to be practical, independent, and constructive, so that companies have better incentive to participate in third party audits, and that watchdog groups can be better prepared to investigate companies.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {666–677},
numpages = {12},
keywords = {algorithm auditing, fairness, four-fifths rule, adverse impact testing},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
"@inproceedings{10.1145/3442188.3445931,
author = {Kasinidou, Maria and Kleanthous, Styliani and Barlas, P\i{}nar and Otterbacher, Jahna},
title = {I Agree with the Decision, but They Didn't Deserve This: Future Developers' Perception of Fairness in Algorithmic Decisions},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445931},
doi = {10.1145/3442188.3445931},
abstract = {While professionals are increasingly relying on algorithmic systems for making a decision, on some occasions, algorithmic decisions may be perceived as biased or not just. Prior work has looked into the perception of algorithmic decision-making from the user's point of view. In this work, we investigate how students in fields adjacent to algorithm development perceive algorithmic decisionmaking. Participants (N=99) were asked to rate their agreement with statements regarding six constructs that are related to facets of fairness and justice in algorithmic decision-making in three separate scenarios. Two of the three scenarios were independent of each other, while the third scenario presented three different outcomes of the same algorithmic system, demonstrating perception changes triggered by different outputs. Quantitative analysis indicates that a) 'agreeing' with a decision does not mean the person 'deserves the outcome', b) perceiving the factors used in the decision-making as 'appropriate' does not make the decision of the system 'fair' and c) perceiving a system's decision as 'not fair' is affecting the participants' 'trust' in the system. In addition, participants found proportional distribution of benefits more fair than other approaches. Qualitative analysis provides further insights into that information the participants find essential to judge and understand an algorithmic decision-making system's fairness. Finally, the level of academic education has a role to play in the perception of fairness and justice in algorithmic decision-making.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {690–700},
numpages = {11},
keywords = {algorithmic transparency, algorithmic decision-making, algorithmic accountability, algorithmic fairness},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
"@inproceedings{10.1145/3442188.3445934,
author = {Suriyakumar, Vinith M. and Papernot, Nicolas and Goldenberg, Anna and Ghassemi, Marzyeh},
title = {Chasing Your Long Tails: Differentially Private Prediction in Health Care Settings},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445934},
doi = {10.1145/3442188.3445934},
abstract = {Machine learning models in health care are often deployed in settings where it is important to protect patient privacy. In such settings, methods for differentially private (DP) learning provide a general-purpose approach to learn models with privacy guarantees. Modern methods for DP learning ensure privacy through the addition of calibrated noise. The resulting privacy-preserving models are unable to learn too much information about the tails of a data distribution, resulting in a loss of accuracy that can disproportionately affect small groups. In this paper, we study the effects of DP learning in health care. We use state-of-the-art methods for DP learning to train privacy-preserving models in clinical prediction tasks, including x-ray classification of images and mortality prediction in time series data. We use these models to perform a comprehensive empirical investigation of the tradeoffs between privacy, utility, robustness to dataset shift and fairness. Our results highlight lesser-known limitations of methods for DP learning in health care, models that exhibit steep tradeoffs between privacy and utility, and models whose predictions are disproportionately influenced by large demographic groups in the training data. We discuss the costs and benefits of differentially private learning in health care with open directions for differential privacy, machine learning and health care.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {723–734},
numpages = {12},
keywords = {privacy, robustness, fairness, health care, machine learning},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
"@inproceedings{10.1145/3442188.3445936,
author = {Hertweck, Corinna and Heitz, Christoph and Loi, Michele},
title = {On the Moral Justification of Statistical Parity},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445936},
doi = {10.1145/3442188.3445936},
abstract = {A crucial but often neglected aspect of algorithmic fairness is the question of how we justify enforcing a certain fairness metric from a moral perspective. When fairness metrics are proposed, they are typically argued for by highlighting their mathematical properties. Rarely are the moral assumptions beneath the metric explained. Our aim in this paper is to consider the moral aspects associated with the statistical fairness criterion of independence (statistical parity). To this end, we consider previous work, which discusses the two worldviews ""What You See Is What You Get"" (WYSIWYG) and ""We're All Equal"" (WAE) and by doing so provides some guidance for clarifying the possible assumptions in the design of algorithms. We present an extension of this work, which centers on morality. The most natural moral extension is that independence needs to be fulfilled if and only if differences in predictive features (e.g. high school grades and standardized test scores are predictive of performance at university) between socio-demographic groups are caused by unjust social disparities or measurement errors. Through two counterexamples, we demonstrate that this extension is not universally true. This means that the question of whether independence should be used or not cannot be satisfactorily answered by only considering the justness of differences in the predictive features.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {747–757},
numpages = {11},
keywords = {statistical parity, distributive justice, bias, independence, fairness},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
"@inproceedings{10.1145/3442188.3445938,
author = {Krafft, P. M. and Young, Meg and Katell, Michael and Lee, Jennifer E. and Narayan, Shankar and Epstein, Micah and Dailey, Dharma and Herman, Bernease and Tam, Aaron and Guetler, Vivian and Bintz, Corinne and Raz, Daniella and Jobe, Pa Ousman and Putz, Franziska and Robick, Brian and Barghouti, Bissan},
title = {An Action-Oriented AI Policy Toolkit for Technology Audits by Community Advocates and Activists},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445938},
doi = {10.1145/3442188.3445938},
abstract = {Motivated by the extensive documented disparate harms of artificial intelligence (AI), many recent practitioner-facing reflective tools have been created to promote responsible AI development. However, the use of such tools internally by technology development firms addresses responsible AI as an issue of closed-door compliance rather than a matter of public concern. Recent advocate and activist efforts intervene in AI as a public policy problem, inciting a growing number of cities to pass bans or other ordinances on AI and surveillance technologies. In support of this broader ecology of political actors, we present a set of reflective tools intended to increase public participation in technology advocacy for AI policy action. To this end, the Algorithmic Equity Toolkit (the AEKit) provides a practical policy-facing definition of AI, a flowchart for assessing technologies against that definition, a worksheet for decomposing AI systems into constituent parts, and a list of probing questions that can be posed to vendors, policy-makers, or government agencies. The AEKit carries an action-orientation towards political encounters between community groups in the public and their representatives, opening up the work of AI reflection and remediation to multiple points of intervention. Unlike current reflective tools available to practitioners, our toolkit carries with it a politics of community participation and activism.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {772–781},
numpages = {10},
keywords = {algorithmic justice, surveillance, algorithmic equity, regulation, accountability, participatory action research, Participatory design},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
"@inproceedings{10.1145/3442188.3445940,
author = {Beretta, Elena and Vetr\`{o}, Antonio and Lepri, Bruno and Martin, Juan Carlos De},
title = {Detecting Discriminatory Risk through Data Annotation Based on Bayesian Inferences},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445940},
doi = {10.1145/3442188.3445940},
abstract = {Thanks to the increasing growth of computational power and data availability, the research in machine learning has advanced with tremendous rapidity. Nowadays, the majority of automatic decision making systems are based on data. However, it is well known that machine learning systems can present problematic results if they are built on partial or incomplete data. In fact, in recent years several studies have found a convergence of issues related to the ethics and transparency of these systems in the process of data collection and how they are recorded. Although the process of rigorous data collection and analysis is fundamental in the model design, this step is still largely overlooked by the machine learning community. For this reason, we propose a method of data annotation based on Bayesian statistical inference that aims to warn about the risk of discriminatory results of a given data set. In particular, our method aims to deepen knowledge and promote awareness about the sampling practices employed to create the training set, highlighting that the probability of success or failure conditioned to a minority membership is given by the structure of the data available. We empirically test our system on three datasets commonly accessed by the machine learning community and we investigate the risk of racial discrimination.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {794–804},
numpages = {11},
keywords = {data ethics, sampling bias, data labeling, race discrimination, human annotation, machine learning},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
"@inproceedings{10.1145/3442188.3445942,
author = {Creel, Kathleen and Hellman, Deborah},
title = {The Algorithmic Leviathan: Arbitrariness, Fairness, and Opportunity in Algorithmic Decision Making Systems},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445942},
doi = {10.1145/3442188.3445942},
abstract = {Automated decision-making systems implemented in public life are typically standardized. One algorithmic decision-making system can replace thousands of human deciders. Each of the humans so replaced had her own decision-making criteria: some good, some bad, and some arbitrary. Is such arbitrariness of moral concern?We argue that an isolated arbitrary decision need not morally wrong the individual whom it misclassifies. However, if the same algorithms are applied across a public sphere, such as hiring or lending, a person could be excluded from a large number of opportunities. This harm persists even when the automated decision-making systems are ""fair"" on standard metrics of fairness. We argue that such arbitrariness at scale is morally problematic and propose technically informed solutions that can lessen the impact of algorithms at scale and so mitigate or avoid the moral harms we identify.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {816},
numpages = {1},
keywords = {fairness, algorithmic decision making, opportunity, automated hiring, machine learning, arbitrariness},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
"@inproceedings{10.1145/3442188.3445908,
author = {Williams, Joshua and Kolter, J. Zico},
title = {A Bayesian Model of Cash Bail Decisions},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445908},
doi = {10.1145/3442188.3445908},
abstract = {The use of cash bail as a mechanism for detaining defendants pretrial is an often-criticized system that many have argued violates the presumption of ""innocent until proven guilty."" Many studies have sought to understand both the long-term effects of cash bail's use and the disparate rate of cash bail assignments along demographic lines (race, gender, etc). However, such work is often susceptible to problems of infra-marginality - that the data we observe can only describe average outcomes, and not the outcomes associated with the marginal decision. In this work, we address this problem by creating a hierarchical Bayesian model of cash bail assignments. Specifically, our approach models cash bail decisions as a probabilistic process whereby judges balance the relative costs of assigning cash bail with the cost of defendants potentially skipping court dates, and where these skip probabilities are estimated based upon features of the individual case. We then use Monte Carlo inference to sample the distribution over these costs for different magistrates and across different races. We fit this model to a data set we have collected of over 50,000 court cases in the Allegheny and Philadelphia counties in Pennsylvania. Our analysis of 50 separate judges shows that they are uniformly more likely to assign cash bail to black defendants than to white defendants, even given identical likelihood of skipping a court appearance. This analysis raises further questions about the equity of the practice of cash bail, irrespective of its underlying legal justification.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {827–837},
numpages = {11},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
"@inproceedings{10.1145/3442188.3445877,
author = {Akpinar, Nil-Jana and De-Arteaga, Maria and Chouldechova, Alexandra},
title = {The Effect of Differential Victim Crime Reporting on Predictive Policing Systems},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445877},
doi = {10.1145/3442188.3445877},
abstract = {Police departments around the world have been experimenting with forms of place-based data-driven proactive policing for over two decades. Modern incarnations of such systems are commonly known as hot spot predictive policing. These systems predict where future crime is likely to concentrate such that police can allocate patrols to these areas and deter crime before it occurs. Previous research on fairness in predictive policing has concentrated on the feedback loops which occur when models are trained on discovered crime data, but has limited implications for models trained on victim crime reporting data. We demonstrate how differential victim crime reporting rates across geographical areas can lead to outcome disparities in common crime hot spot prediction models. Our analysis is based on a simulation1 patterned after district-level victimization and crime reporting survey data for Bogot\'{a}, Colombia. Our results suggest that differential crime reporting rates can lead to a displacement of predicted hotspots from high crime but low reporting areas to high or medium crime and high reporting areas. This may lead to misallocations both in the form of over-policing and under-policing.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {838–849},
numpages = {12},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
"@inproceedings{10.1145/3442188.3445971,
author = {Shen, Hong and Deng, Wesley H. and Chattopadhyay, Aditi and Wu, Zhiwei Steven and Wang, Xu and Zhu, Haiyi},
title = {Value Cards: An Educational Toolkit for Teaching Social Impacts of Machine Learning through Deliberation},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445971},
doi = {10.1145/3442188.3445971},
abstract = {Recently, there have been increasing calls for computer science curricula to complement existing technical training with topics related to Fairness, Accountability, Transparency and Ethics (FATE). In this paper, we present Value Cards, an educational toolkit to inform students and practitioners the social impacts of different machine learning models via deliberation. This paper presents an early use of our approach in a college-level computer science course. Through an in-class activity, we report empirical data for the initial effectiveness of our approach. Our results suggest that the use of the Value Cards toolkit can improve students' understanding of both the technical definitions and trade-offs of performance metrics and apply them in real-world contexts, help them recognize the significance of considering diverse social values in the development and deployment of algorithmic systems, and enable them to communicate, negotiate and synthesize the perspectives of diverse stakeholders. Our study also demonstrates a number of caveats we need to consider when using the different variants of the Value Cards toolkit. Finally, we discuss the challenges as well as future applications of our approach.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {850–861},
numpages = {12},
keywords = {Deliberation, CS Education, Value Cards, Machine Learning, Fairness},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
"@inproceedings{10.1145/3442188.3445944,
author = {Dash, Abhisek and Chakraborty, Abhijnan and Ghosh, Saptarshi and Mukherjee, Animesh and Gummadi, Krishna P.},
title = {When the Umpire is Also a Player: Bias in Private Label Product Recommendations on E-Commerce Marketplaces},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445944},
doi = {10.1145/3442188.3445944},
abstract = {Algorithmic recommendations mediate interactions between millions of customers and products (in turn, their producers and sellers) on large e-commerce marketplaces like Amazon. In recent years, the producers and sellers have raised concerns about the fairness of black-box recommendation algorithms deployed on these marketplaces. Many complaints are centered around marketplaces biasing the algorithms to preferentially favor their own 'private label' products over competitors. These concerns are exacerbated as marketplaces increasingly de-emphasize or replace 'organic' recommendations with ad-driven 'sponsored' recommendations, which include their own private labels. While these concerns have been covered in popular press and have spawned regulatory investigations, to our knowledge, there has not been any public audit of these marketplace algorithms. In this study, we bridge this gap by performing an end-to-end systematic audit of related item recommendations on Amazon. We propose a network-centric framework to quantify and compare the biases across organic and sponsored related item recommendations. Along a number of our proposed bias measures, we find that the sponsored recommendations are significantly more biased toward Amazon private label products compared to organic recommendations. While our findings are primarily interesting to producers and sellers on Amazon, our proposed bias measures are generally useful for measuring link formation bias in any social or content networks.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {873–884},
numpages = {12},
keywords = {e-commerce marketplace, Recommendation, algorithmic auditing},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
  
"
"@inproceedings{10.1145/3531146.3533074,
author = {Goyal, Priya and Soriano, Adriana Romero and Hazirbas, Caner and Sagun, Levent and Usunier, Nicolas},
title = {Fairness Indicators for Systematic Assessments of Visual Feature Extractors},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533074},
doi = {10.1145/3531146.3533074},
abstract = {Does everyone equally benefit from computer vision systems? Answers to this question become more and more important as computer vision systems are deployed at large scale, and can spark major concerns when they exhibit vast performance discrepancies between people from various demographic and social backgrounds. Systematic diagnosis of fairness, harms, and biases of computer vision systems is an important step towards building socially responsible systems. To initiate an effort towards standardized fairness audits, we propose three fairness indicators, which aim at quantifying harms and biases of visual systems. Our indicators use existing publicly available datasets collected for fairness evaluations, and focus on three main types of harms and bias identified in the literature, namely harmful label associations, disparity in learned representations of social and demographic traits, and biased performance on geographically diverse images from across the world. We define precise experimental protocols applicable to a wide range of computer vision models. These indicators are part of an ever-evolving suite of fairness probes and are not intended to be a substitute for a thorough analysis of the broader impact of the new computer vision technologies. Yet, we believe it is a necessary first step towards (1) facilitating the widespread adoption and mandate of the fairness assessments in computer vision research, and (2) tracking progress towards building socially responsible models. To study the practical effectiveness and broad applicability of our proposed indicators to any visual system, we apply them to “off-the-shelf” models built using widely adopted model training paradigms which vary in their ability to whether they can predict labels on a given image or only produce the embeddings. We also systematically study the effect of data domain and model size. The results of our fairness indicators on these systems suggest that blatant disparities still exist, which highlight the importance on the relationship between the context of the task and contents of a datasets. The code will be released to encourage the use of indicators.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {70–88},
numpages = {19},
keywords = {Fairness, benchmarks, Computer Vision, metrics},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3533076,
author = {Simbeck, Katharina},
title = {FAccT-Check on AI Regulation: Systematic Evaluation of AI Regulation on the Example of the Legislation on the Use of AI in the Public Sector in the German Federal State of Schleswig-Holstein},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533076},
doi = {10.1145/3531146.3533076},
abstract = {In the framework of the current discussions about regulating Artificial Intelligence (AI) and machine learning (ML), the small Federal State of Schleswig-Holstein in Northern Germany hurries ahead and adopts legislation on the Use of AI in the public sector. The legislation aims on the one hand to enable the use of AI in the public sector by creating a legal framework and to limit its potential discriminatory effect on the other hand. Contrary to the European AI Act, which is valid for all companies and organizations in Europe, and contrary to the Chinese administrative rule on Internet information recommender systems, the Schleswig-Holstein “IT Deployment Law” (ITDL) would therefore only apply to public administrations and agencies in the federal state. The legislation addresses several AI risks, including fairness and transparency, and mitigates them with approaches quite different from the proposed European AI Act (AIA). In this paper, the legislation will be systematically reviewed and discussed with regards to its definition of AI, risk handling, fairness, accountability, and transparency.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {89–96},
numpages = {8},
keywords = {AI regulation, AI transparency, AI fairness},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3533079,
author = {Wang, Xiuling and Wang, Wendy Hui},
title = {Providing Item-Side Individual Fairness for Deep Recommender Systems},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533079},
doi = {10.1145/3531146.3533079},
abstract = {Recent advent of deep learning techniques have reinforced the development of new recommender systems. Although these systems have been demonstrated as efficient and effective, the issue of item popularity bias in these recommender systems has raised serious concerns. While most of the existing works focus on group fairness at item side, individual fairness at item side is left largely unexplored. To address this issue, in this paper, first, we define a new notion of individual fairness from the perspective of items, namely (α, β)-fairness, to deal with item popularity bias in recommendations. In particular, (α, β)-fairness requires that similar items should receive similar coverage in the recommendations, where α and β control item similarity and coverage similarity respectively, and both item and coverage similarity metrics are defined as task specific for deep recommender systems. Next, we design two bias mitigation methods, namely embedding-based re-ranking (ER) and greedy substitution (GS), for deep recommender systems. ER is an in-processing mitigation method that equips (α, β)-fairness as a constraint to the objective function of the recommendation algorithm, while GS is a post-processing approach that accepts the biased recommendations as the input, and substitutes high-coverage items with low-coverage ones in the recommendations to satisfy (α, β)-fairness. We evaluate the performance of both mitigation algorithms on two real-world datasets and a set of state-of-the-art deep recommender systems. Our results demonstrate that both ER and GS outperform the existing minimum-coverage (MC) mitigation solutions [Koutsopoulos and Halkidi 2018; Patro et al. 2020] in terms of both fairness and accuracy of recommendations. Furthermore, ER delivers the best trade-off between fairness and recommendation accuracy among a set of alternative mitigation methods, including GS, the hybrid of ER and GS, and the existing MC solutions.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {117–127},
numpages = {11},
keywords = {deep recommender systems, algorithmic fairness in machine learning, Individual fairness},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3533080,
author = {Engelmann, Severin and Ullstein, Chiara and Papakyriakopoulos, Orestis and Grossklags, Jens},
title = {What People Think AI Should Infer From Faces},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533080},
doi = {10.1145/3531146.3533080},
abstract = {Faces play an indispensable role in human social life. At present, computer vision artificial intelligence (AI) captures and interprets human faces for a variety of digital applications and services. The ambiguity of facial information has recently led to a debate among scholars in different fields about the types of inferences AI should make about people based on their facial looks. AI research often justifies facial AI inference-making by referring to how people form impressions in first-encounter scenarios. Critics raise concerns about bias and discrimination and warn that facial analysis AI resembles an automated version of physiognomy. What has been missing from this debate, however, is an understanding of how “non-experts” in AI ethically evaluate facial AI inference-making. In a two-scenario vignette study with 24 treatment groups, we show that non-experts (N = 3745) reject facial AI inferences such as trustworthiness and likability from portrait images in a low-stake advertising and a high-stake hiring context. In contrast, non-experts agree with facial AI inferences such as skin color or gender in the advertising but not the hiring decision context. For each AI inference, we ask non-experts to justify their evaluation in a written response. Analyzing 29,760 written justifications, we find that non-experts are either “evidentialists” or “pragmatists”: they assess the ethical status of a facial AI inference based on whether they think faces warrant sufficient or insufficient evidence for an inference (evidentialist justification) or whether making the inference results in beneficial or detrimental outcomes (pragmatist justification). Non-experts’ justifications underscore the normative complexity behind facial AI inference-making. AI inferences with insufficient evidence can be rationalized by considerations of relevance while irrelevant inferences can be justified by reference to sufficient evidence. We argue that participatory approaches contribute valuable insights for the development of ethical AI in an increasingly visual data culture.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {128–141},
numpages = {14},
keywords = {computer vision, participatory AI ethics, artificial intelligence, human faces},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3533081,
author = {Papadaki, Afroditi and Martinez, Natalia and Bertran, Martin and Sapiro, Guillermo and Rodrigues, Miguel},
title = {Minimax Demographic Group Fairness in Federated Learning},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533081},
doi = {10.1145/3531146.3533081},
abstract = {Federated learning is an increasingly popular paradigm that enables a large number of entities to collaboratively learn better models. In this work, we study minimax group fairness in federated learning scenarios where different participating entities may only have access to a subset of the population groups during the training phase. We formally analyze how our proposed group fairness objective differs from existing federated learning fairness criteria that impose similar performance across participants instead of demographic groups. We provide an optimization algorithm – FedMinMax – for solving the proposed problem that provably enjoys the performance guarantees of centralized learning algorithms. We experimentally compare the proposed approach against other state-of-the-art methods in terms of group fairness in various federated learning setups, showing that our approach exhibits competitive or superior performance.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {142–159},
numpages = {18},
keywords = {Algorithmic Fairness, Minimax Group Fairness, Federated Learning},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3533088,
author = {Weidinger, Laura and Uesato, Jonathan and Rauh, Maribeth and Griffin, Conor and Huang, Po-Sen and Mellor, John and Glaese, Amelia and Cheng, Myra and Balle, Borja and Kasirzadeh, Atoosa and Biles, Courtney and Brown, Sasha and Kenton, Zac and Hawkins, Will and Stepleton, Tom and Birhane, Abeba and Hendricks, Lisa Anne and Rimell, Laura and Isaac, William and Haas, Julia and Legassick, Sean and Irving, Geoffrey and Gabriel, Iason},
title = {Taxonomy of Risks Posed by Language Models},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533088},
doi = {10.1145/3531146.3533088},
abstract = {Responsible innovation on large-scale Language Models (LMs) requires foresight into and in-depth understanding of the risks these models may pose. This paper develops a comprehensive taxonomy of ethical and social risks associated with LMs. We identify twenty-one risks, drawing on expertise and literature from computer science, linguistics, and the social sciences. We situate these risks in our taxonomy of six risk areas: I. Discrimination, Hate speech and Exclusion, II. Information Hazards, III. Misinformation Harms, IV. Malicious Uses, V. Human-Computer Interaction Harms, and VI. Environmental and Socioeconomic harms. For risks that have already been observed in LMs, the causal mechanism leading to harm, evidence of the risk, and approaches to risk mitigation are discussed. We further describe and analyse risks that have not yet been observed but are anticipated based on assessments of other language technologies, and situate these in the same taxonomy. We underscore that it is the responsibility of organizations to engage with the mitigations we discuss throughout the paper. We close by highlighting challenges and directions for further research on risk evaluation and mitigation with the goal of ensuring that language models are developed responsibly.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {214–229},
numpages = {16},
keywords = {responsible innovation, language models, risk assessment, technology risks, responsible AI},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3533092,
author = {Tedjopurnomo, David and Bao, Zhifeng and Choudhury, Farhana and Luo, Hui and Qin, A. K.},
title = {Equitable Public Bus Network Optimization for Social Good: A Case Study of Singapore},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533092},
doi = {10.1145/3531146.3533092},
abstract = {Public bus transport is a major backbone of many cities’ socioeconomic activities. As such, the topic of public bus network optimization has received substantial attention in Geographic Information System (GIS) research. Unfortunately, most of the current literature are focused on improving only the efficiency of the bus network, neglecting the important equity factors. Optimizing only the efficiency of a bus network may cause these limited public transportation resources to be shifted away from areas with disadvantaged demographics, compounding the equity problem. In this work, we make the first attempt to explore the intricacies of the equitable public bus network optimization problem by performing a case study of Singapore’s public bus network. We describe the challenges in designing an equitable public bus network, tackle the fundamental problem of formulating efficiency and equity metrics, perform exploratory experiments to assess each metric’s real-life impact, and analyze the challenges of the equitable bus network optimization task. For our experiments, we have curated and combined Singapore’s bus network data, road network data, census area boundaries data, and demographics data into a unified dataset which we released publicly. Our objective is not only to explore this important yet relatively unexplored problem, but also to inspire more discussion and research.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {278–288},
numpages = {11},
keywords = {datasets, bus network, equity, public transportation, metric},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3533094,
author = {Sikdar, Sandipan and Lemmerich, Florian and Strohmaier, Markus},
title = {GetFair: Generalized Fairness Tuning of Classification Models},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533094},
doi = {10.1145/3531146.3533094},
abstract = {We present GetFair, a novel framework for tuning fairness of classification models. The fair classification problem deals with training models for a given classification task where data points have sensitive attributes. The goal of fair classification models is to not only generate accurate classification results but also to prevent discrimination against subpopulations (i.e., individuals with a specific value for the sensitive attribute). Existing methods for enhancing fairness of classification models, however, are often specifically designed for a particular fairness metric or a classifier model. They may also not be suitable for scenarios with incomplete training data or where optimizing for multiple fairness metrics is important. GetFair represents a general solution to this problem. The GetFair approach works in the following way: First, a given classifier is trained on training data without any fairness objective. This is followed by a reinforcement learning inspired tuning procedure which updates the parameters of the learned model on a given fairness objective. This disentangles classifier training from fairness tuning, making our framework more general and allowing for the adoption of any parameterized classifier model. Because fairness metrics are designed as reward functions during tuning, GetFair generalizes across any fairness metric. We demonstrate the generalizability of GetFair via evaluation over a benchmark suite of datasets, classification models, and fairness metrics. In addition, GetFair can also be deployed in settings where the training data is incomplete or the classifier needs to be tuned on multiple fairness metrics. GetFair not only contributes a flexible method to the repertoire of tools available to improve the fairness of classification models, it also seamlessly adapts to settings where existing fair classification methods may not be suitable or applicable.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {289–299},
numpages = {11},
keywords = {fairness metrics, sensitive attribute, Fair classification, classifier models},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3533097,
author = {Jakesch, Maurice and Bu\c{c}inca, Zana and Amershi, Saleema and Olteanu, Alexandra},
title = {How Different Groups Prioritize Ethical Values for Responsible AI},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533097},
doi = {10.1145/3531146.3533097},
abstract = {Private companies, public sector organizations, and academic groups have outlined ethical values they consider important for responsible artificial intelligence technologies. While their recommendations converge on a set of central values, little is known about the values a more representative public would find important for the AI technologies they interact with and might be affected by. We conducted a survey examining how individuals perceive and prioritize responsible AI values across three groups: a representative sample of the US population (N=743), a sample of crowdworkers (N=755), and a sample of AI practitioners (N=175). Our results empirically confirm a common concern: AI practitioners’ value priorities differ from those of the general public. Compared to the US-representative sample, AI practitioners appear to consider responsible AI values as less important and emphasize a different set of values. In contrast, self-identified women and black respondents found responsible AI values more important than other groups. Surprisingly, more liberal-leaning participants, rather than participants reporting experiences with discrimination, were more likely to prioritize fairness than other groups. Our findings highlight the importance of paying attention to who gets to define “responsible AI.”},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {310–323},
numpages = {14},
keywords = {Responsible AI, empirical ethics, value-sensitive design},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3533099,
author = {Wang, Angelina and Barocas, Solon and Laird, Kristen and Wallach, Hanna},
title = {Measuring Representational Harms in Image Captioning},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533099},
doi = {10.1145/3531146.3533099},
abstract = {Previous work has largely considered the fairness of image captioning systems through the underspecified lens of “bias.” In contrast, we present a set of techniques for measuring five types of representational harms, as well as the resulting measurements obtained for two of the most popular image captioning datasets using a state-of-the-art image captioning system. Our goal was not to audit this image captioning system, but rather to develop normatively grounded measurement techniques, in turn providing an opportunity to reflect on the many challenges involved. We propose multiple measurement techniques for each type of harm. We argue that by doing so, we are better able to capture the multi-faceted nature of each type of harm, in turn improving the (collective) validity of the resulting measurements. Throughout, we discuss the assumptions underlying our measurement approach and point out when they do not hold.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {324–335},
numpages = {12},
keywords = {fairness measurement, harm propagation, image captioning},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3533101,
author = {Wang, Angelina and Ramaswamy, Vikram V and Russakovsky, Olga},
title = {Towards Intersectionality in Machine Learning: Including More Identities, Handling Underrepresentation, and Performing Evaluation},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533101},
doi = {10.1145/3531146.3533101},
abstract = {Research in machine learning fairness has historically considered a single binary demographic attribute; however, the reality is of course far more complicated. In this work, we grapple with questions that arise along three stages of the machine learning pipeline when incorporating intersectionality as multiple demographic attributes: (1) which demographic attributes to include as dataset labels, (2) how to handle the progressively smaller size of subgroups during model training, and (3) how to move beyond existing evaluation metrics when benchmarking model fairness for more subgroups. For each question, we provide thorough empirical evaluation on tabular datasets derived from the US Census, and present constructive recommendations for the machine learning community. First, we advocate for supplementing domain knowledge with empirical validation when choosing which demographic attribute labels to train on, while always evaluating on the full set of demographic attributes. Second, we warn against using data imbalance techniques without considering their normative implications and suggest an alternative using the structure in the data. Third, we introduce new evaluation metrics which are more appropriate for the intersectional setting. Overall, we provide substantive suggestions on three necessary (albeit not sufficient!) considerations when incorporating intersectionality into machine learning.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {336–349},
numpages = {14},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3533102,
author = {Roth, Jonathan and Saint-Jacques, Guillaume and Yu, YinYin},
title = {An Outcome Test of Discrimination for Ranked Lists},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533102},
doi = {10.1145/3531146.3533102},
abstract = {This paper extends Becker [3]’s outcome test of discrimination to settings where a (human or algorithmic) decision-maker produces a ranked list of candidates. Ranked lists are particularly relevant in the context of online platforms that produce search results or feeds, and also arise when human decisionmakers express ordinal preferences over a list of candidates. We show that non-discrimination implies a system of moment inequalities, which intuitively impose that one cannot permute the position of a lower-ranked candidate from one group with a higher-ranked candidate from a second group and systematically improve the objective. Moreover, we show that that these moment inequalities are the only testable implications of non-discrimination when the auditor observes only outcomes and group membership by rank. We show how to statistically test the implied inequalities, and validate our approach in an application using data from LinkedIn.1},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {350–356},
numpages = {7},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3533105,
author = {Lum, Kristian and Zhang, Yunfeng and Bower, Amanda},
title = {De-Biasing “Bias” Measurement},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533105},
doi = {10.1145/3531146.3533105},
abstract = {When a model’s performance differs across socially or culturally relevant groups–like race, gender, or the intersections of many such groups–it is often called ”biased.” While much of the work in algorithmic fairness over the last several years has focused on developing various definitions of model fairness (the absence of group-wise model performance disparities) and eliminating such “bias,” much less work has gone into rigorously measuring it. In practice, it important to have high quality, human digestible measures of model performance disparities and associated uncertainty quantification about them that can serve as inputs into multi-faceted decision-making processes. In this paper, we show both mathematically and through simulation that many of the metrics used to measure group-wise model performance disparities are themselves statistically biased estimators of the underlying quantities they purport to represent. We argue that this can cause misleading conclusions about the relative group-wise model performance disparities along different dimensions, especially in cases where some sensitive variables consist of categories with few members. We propose the “double-corrected” variance estimator, which provides unbiased estimates and uncertainty quantification of the variance of model performance across groups. It is conceptually simple and easily implementable without statistical software package or numerical optimization. We demonstrate the utility of this approach through simulation and show on a real dataset that while statistically biased estimators of model group-wise model performance disparities indicate statistically significant between-group model performance disparities, when accounting for statistical bias in the estimator, the estimated group-wise disparities in model performance are no longer statistically significant.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {379–389},
numpages = {11},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3533107,
author = {Laufer, Benjamin and Jain, Sameer and Cooper, A. Feder and Kleinberg, Jon and Heidari, Hoda},
title = {Four Years of FAccT: A Reflexive, Mixed-Methods Analysis of Research Contributions, Shortcomings, and Future Prospects},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533107},
doi = {10.1145/3531146.3533107},
abstract = {Fairness, Accountability, and Transparency (FAccT) for socio-technical systems has been a thriving area of research in recent years. An ACM conference bearing the same name has been the central venue for scholars in this area to come together, provide peer feedback to one another, and publish their work. This reflexive study aims to shed light on FAccT’s activities to date and identify major gaps and opportunities for translating contributions into broader positive impact. To this end, we utilize a mixed-methods research design. On the qualitative front, we develop a protocol for reviewing and coding prior FAccT papers, tracing their distribution of topics, methods, datasets, and disciplinary roots. We also design and administer a questionnaire to reflect the voices of FAccT community members and affiliates on a wide range of topics. On the quantitative front, we use the full text and citation network associated with prior FAccT publications to provide further evidence about topics and values represented in FAccT. We organize the findings from our analysis into four main dimensions: the themes present in FAccT scholarship, the values that underpin the work, the impact of the contributions both within academic circles and beyond, and the practices and informal norms of the community that has formed around FAccT. Finally, our work identifies several suggestions on directions for change, as voiced by community members.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {401–426},
numpages = {26},
keywords = {community perspectives, reflexivity, topics, impact, mixed methods, values, FAccT},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3533113,
author = {Deng, Wesley Hanwen and Nagireddy, Manish and Lee, Michelle Seng Ah and Singh, Jatinder and Wu, Zhiwei Steven and Holstein, Kenneth and Zhu, Haiyi},
title = {Exploring How Machine Learning Practitioners (Try To) Use Fairness Toolkits},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533113},
doi = {10.1145/3531146.3533113},
abstract = {Recent years have seen the development of many open-source ML fairness toolkits aimed at helping ML practitioners assess and address unfairness in their systems. However, there has been little research investigating how ML practitioners actually use these toolkits in practice. In this paper, we conducted the first in-depth empirical exploration of how industry practitioners (try to) work with existing fairness toolkits. In particular, we conducted think-aloud interviews to understand how participants learn about and use fairness toolkits, and explored the generality of our findings through an anonymous online survey. We identified several opportunities for fairness toolkits to better address practitioner needs and scaffold them in using toolkits effectively and responsibly. Based on these findings, we highlight implications for the design of future open-source fairness toolkits that can support practitioners in better contextualizing, communicating, and collaborating around ML fairness efforts.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {473–484},
numpages = {12},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3533114,
author = {Kong, Youjin},
title = {Are “Intersectionally Fair” AI Algorithms Really Fair to Women of Color? A Philosophical Analysis},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533114},
doi = {10.1145/3531146.3533114},
abstract = {A growing number of studies on fairness in artificial intelligence (AI) use the notion of intersectionality to measure AI fairness. Most of these studies take intersectional fairness to be a matter of statistical parity among intersectional subgroups: an AI algorithm is “intersectionally fair” if the probability of the outcome is roughly the same across all subgroups defined by different combinations of the protected attributes. This paper identifies and examines three fundamental problems with this dominant interpretation of intersectional fairness in AI. First, the dominant approach is so preoccupied with the intersection of attributes/categories (e.g., race, gender) that it fails to address the intersection of oppression (e.g., racism, sexism), which is more central to intersectionality as a critical framework. Second, the dominant approach faces a dilemma between infinite regress and fairness gerrymandering: it either keeps splitting groups into smaller subgroups or arbitrarily selects protected groups. Lastly, the dominant view fails to capture what it really means for AI algorithms to be fair, in terms of both distributive and non-distributive fairness. I distinguish a strong sense of AI fairness from a weak sense that is prevalent in the literature, and conclude by envisioning paths towards strong intersectional fairness in AI.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {485–494},
numpages = {10},
keywords = {Intersectionality, Philosophical Analysis of Fairness, Fairness and Bias in AI, Feminist and Critical Race Social Philosophy},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3533115,
author = {Zhang, Marilyn},
title = {Affirmative Algorithms: Relational Equality as Algorithmic Fairness},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533115},
doi = {10.1145/3531146.3533115},
abstract = {Many statistical fairness notions have been proposed for algorithmic decision-making systems, and especially public safety pretrial risk assessment (PSPRA) algorithms such as COMPAS. Most fairness notions equalize something between groups, whether it is false positive rates or accuracy. In fact, I demonstrate that most prominent notions have their basis in equalizing some form of accuracy. However, statistical fairness metrics often do not capture the substantive point of equality. I argue that equal accuracy is not only difficult to measure but also unsatisfactory for ensuring equal justice. In response, I introduce philosopher Elizabeth Anderson’s theory of relational equality as a fruitful alternative framework: to relate as equals, people need access to certain basic capabilities. I show that relational equality requires Affirmative PSPRA algorithms that lower risk scores for Black defendants. This is because fairness based on relational equality means considering the impact of PSPRA algorithms’ decisions on access to basic capabilities. This impact is racially asymmetric in an unjust society. I make three main contributions: (1) I illustrate the shortcomings of statistical fairness notions in their reliance on equalizing some form of accuracy; (2) I present the first comprehensive ethical defense of Affirmative PSPRA algorithms, based on fairness in terms of relational equality instead; and (3) I show that equalizing accuracy is neither sufficient nor necessary for fairness based on relational equality. Overall, this work serves narrowly as a reason to re-evaluate algorithmic fairness for PSPRA algorithms, and serves broadly as an example of how discussions of algorithmic fairness can benefit from egalitarian philosophical frameworks.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {495–507},
numpages = {13},
keywords = {algorithmic fairness, affirmative algorithms, pretrial risk assessments, philosophy, fairness, criminal justice, relational equality},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3533121,
author = {Niu, Mingzi and Kannan, Sampath and Roth, Aaron and Vohra, Rakesh},
title = {Best vs. All: Equity and Accuracy of Standardized Test Score Reporting},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533121},
doi = {10.1145/3531146.3533121},
abstract = {We study a game theoretic model of standardized testing for college admissions. Students are of two types; High and Low. There is a college that would like to admit the High type students. Students take a potentially costly standardized exam which provides a noisy signal of their type. The students come from two populations, which are identical in talent (i.e. the type distribution is the same), but differ in their access to resources: the higher resourced population can at their option take the exam multiple times, whereas the lower resourced population can only take the exam once. We study two models of score reporting, which capture existing policies used by colleges. The first policy (sometimes known as “super-scoring”) allows students to report the max of the scores they achieve. The other policy requires that all scores be reported. We find in our model that requiring that all scores be reported results in superior outcomes in equilibrium, both from the perspective of the college (the admissions rule is more accurate), and from the perspective of equity across populations: a student’s probability of admission is independent of their population, conditional on their type. In particular, the false positive rates and false negative rates are identical in this setting, across the highly and poorly resourced student populations. This is the case despite the fact that the more highly resourced students can—at their option—either report a more accurate signal of their type, or pool with the lower resourced population under this policy. This represents an unusual situation in the algorithmic fairness literature where the goals of accuracy and equity are in alignment, and do not need to be traded off against one another.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {574–586},
numpages = {13},
keywords = {screen accuracy, score reporting policies, algorithm fairness},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3533126,
author = {Fong, Hortense and Kumar, Vineet and Mehrotra, Anay and Vishnoi, Nisheeth K.},
title = {Fairness for AUC via Feature Augmentation},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533126},
doi = {10.1145/3531146.3533126},
abstract = {We study fairness in the context of classification where the performance is measured by the area under the curve (AUC) of the receiver operating characteristic. AUC is commonly used when both Type I (false positive) and Type II (false negative) errors are important. However, the same classifier can have significantly varying AUCs for different protected groups and, in real-world applications, it is often desirable to reduce such cross-group differences. We address the problem of how to select additional features to most greatly improve AUC for the disadvantaged group. Our results establish that the unconditional variance of features does not inform us about AUC fairness but class-conditional variance does. Using this connection, we develop a novel approach, fairAUC, based on feature augmentation (adding features) to mitigate bias between identifiable groups. We evaluate fairAUC on synthetic and real-world (COMPAS) datasets and find that it significantly improves AUC for the disadvantaged group relative to benchmarks maximizing overall AUC and minimizing bias between groups.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {610},
numpages = {1},
keywords = {Classification, Area under the ROC curve (AUC), feature augmentation, data collection and curation},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3533128,
author = {Ghosh, Avijit and Jagielski, Matthew and Wilson, Christo},
title = {Subverting Fair Image Search with Generative Adversarial Perturbations},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533128},
doi = {10.1145/3531146.3533128},
abstract = {In this work we explore the intersection fairness and robustness in the context of ranking: when a ranking model has been calibrated to achieve some definition of fairness, is it possible for an external adversary to make the ranking model behave unfairly without having access to the model or training data? To investigate this question, we present a case study in which we develop and then attack a state-of-the-art, fairness-aware image search engine using images that have been maliciously modified using a Generative Adversarial Perturbation (GAP) model [75]. These perturbations attempt to cause the fair re-ranking algorithm to unfairly boost the rank of images containing people from an adversary-selected subpopulation. We present results from extensive experiments demonstrating that our attacks can successfully confer significant unfair advantage to people from the majority class relative to fairly-ranked baseline search results. We demonstrate that our attacks are robust across a number of variables, that they have close to zero impact on the relevance of search results, and that they succeed under a strict threat model. Our findings highlight the danger of deploying fair machine learning algorithms in-the-wild when (1) the data necessary to achieve fairness may be adversarially manipulated, and (2) the models themselves are not robust against attacks.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {637–650},
numpages = {14},
keywords = {Adversarial Machine Learning, Information Retrieval, Fair Ranking, Demographic Inference},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3533132,
author = {Suresh, Harini and Movva, Rajiv and Dogan, Amelia Lee and Bhargava, Rahul and Cruxen, Isadora and Cuba, Angeles Martinez and Taurino, Guilia and So, Wonyoung and D'Ignazio, Catherine},
title = {Towards Intersectional Feminist and Participatory ML: A Case Study in Supporting Feminicide Counterdata Collection},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533132},
doi = {10.1145/3531146.3533132},
abstract = {Data ethics and fairness have emerged as important areas of research in recent years. However, much work in this area focuses on retroactively auditing and “mitigating bias” in existing, potentially flawed systems, without interrogating the deeper structural inequalities underlying them. There are not yet examples of how to apply feminist and participatory methodologies from the start, to conceptualize and design machine learning-based tools that center and aim to challenge power inequalities. Our work targets this more prospective goal. Guided by the framework of data feminism, we co-design datasets and machine learning models to support the efforts of activists who collect and monitor data about feminicide — gender-based killings of women and girls. We describe how intersectional feminist goals and participatory processes shaped each stage of our approach, from problem conceptualization to data collection to model evaluation. We highlight several methodological contributions, including 1) an iterative data collection and annotation process that targets model weaknesses and interrogates framing concepts (such as who is included/excluded in “feminicide”), 2) models that explicitly focus on intersectional identities rather than statistical majorities, and 3) a multi-step evaluation process — with quantitative, qualitative and participatory steps — focused on context-specific relevance. We also distill insights and tensions that arise from bridging intersectional feminist goals with ML. These include reflections on how ML may challenge power, embrace pluralism, rethink binaries and consider context, as well as the inherent limitations of any technology-based solution to address durable structural inequalities.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {667–678},
numpages = {12},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3533136,
author = {Nandy, Preetam and DiCiccio, Cyrus and Venugopalan, Divya and Logan, Heloise and Basu, Kinjal and El Karoui, Noureddine},
title = {Achieving Fairness via Post-Processing in Web-Scale Recommender Systems},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533136},
doi = {10.1145/3531146.3533136},
abstract = {Building fair recommender systems is a challenging and crucial area of study due to its immense impact on society. We extended the definitions of two commonly accepted notions of fairness to recommender systems, namely equality of opportunity and equalized odds. These fairness measures ensure that equally “qualified” (or “unqualified”) candidates are treated equally regardless of their protected attribute status (such as gender or race). We propose scalable methods for achieving equality of opportunity and equalized odds in rankings in the presence of position bias, which commonly plagues data generated from recommender systems. Our algorithms are model agnostic in the sense that they depend only on the final scores provided by a model, making them easily applicable to virtually all web-scale recommender systems. We conduct extensive simulations as well as real-world experiments to show the efficacy of our approach.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {715–725},
numpages = {11},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3533138,
author = {Hundt, Andrew and Agnew, William and Zeng, Vicky and Kacianka, Severin and Gombolay, Matthew},
title = {Robots Enact Malignant Stereotypes},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533138},
doi = {10.1145/3531146.3533138},
abstract = {Stereotypes, bias, and discrimination have been extensively documented in Machine Learning (ML) methods such as Computer Vision (CV) [18, 80], Natural Language Processing (NLP) [6], or both, in the case of large image and caption models such as OpenAI CLIP [14]. In this paper, we evaluate how ML bias manifests in robots that physically and autonomously act within the world. We audit one of several recently published CLIP-powered robotic manipulation methods, presenting it with objects that have pictures of human faces on the surface which vary across race and gender, alongside task descriptions that contain terms associated with common stereotypes. Our experiments definitively show robots acting out toxic stereotypes with respect to gender, race, and scientifically-discredited physiognomy, at scale. Furthermore, the audited methods are less likely to recognize Women and People of Color. Our interdisciplinary sociotechnical analysis synthesizes across fields and applications such as Science Technology and Society (STS), Critical Studies, History, Safety, Robotics, and AI. We find that robots powered by large datasets and Dissolution Models (sometimes called “foundation models”, e.g. CLIP) that contain humans risk physically amplifying malignant stereotypes in general; and that merely correcting disparities will be insufficient for the complexity and scale of the problem. Instead, we recommend that robot learning methods that physically manifest stereotypes or other harmful outcomes be paused, reworked, or even wound down when appropriate, until outcomes can be proven safe, effective, and just. Finally, we discuss comprehensive policy changes and the potential of new interdisciplinary research on topics like Identity Safety Assessment Frameworks and Design Justice to better understand and address these harms.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {743–756},
numpages = {14},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3533140,
author = {Rieke, Aaron and Southerland, Vincent and Svirsky, Dan and Hsu, Mingwei},
title = {Imperfect Inferences: A Practical Assessment},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533140},
doi = {10.1145/3531146.3533140},
abstract = {Measuring racial disparities is challenging, especially when demographic labels are unavailable. Recently, some researchers and advocates have argued that companies should infer race and other demographic factors to help them understand and address discrimination. Others have been more skeptical, emphasizing the inaccuracy of racial inferences, critiquing the conceptualization of demographic categories themselves, and arguing that the use of demographic data might encourage algorithmic tweaks where more radical interventions are needed. We conduct a novel empirical analysis that informs this debate, using a dataset of self-reported demographic information provided by users of the ride-hailing service Uber who consented to share this information for research purposes. As a threshold matter, we show how this data reflects the enduring power of racism in society. We find differences by race across a range of outcomes. For example, among self-reported African-American riders, we see racial differences on factors from iOS use to local pollution levels. We then turn to a practical assessment of racial inference methodologies and offer two key findings. First, every inference method we tested has significant errors, miscategorizing people relative to their self-reports (even as the self-reports themselves suffer from selection bias). Second, and most importantly, we found that the inference methods worked: they reliably confirmed directional racial disparities that we knew were reflected in our dataset. Our analysis also suggests that the choice of inference methods should be informed by the measurement task. For example, disparities that are geographic in nature might be best captured by inferences that rely on geography; discrimination based on a person’s name might be best detected by inferences that rely on names. In conclusion, our analysis shows that common racial inference methods have real and practical utility in shedding light on aggregate, directional disparities, despite their imperfections. While the recent literature has identified notable challenges regarding the collection and use of this data, these challenges should not be seen as dispositive.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {767–777},
numpages = {11},
keywords = {demographics, fairness, race, civil rights, discrimination, inference},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3533145,
author = {Zamfirescu-Pereira, J.D. and Chen, Jerry and Wen, Emily and Koenecke, Allison and Garg, Nikhil and Pierson, Emma},
title = {Trucks Don’t Mean Trump: Diagnosing Human Error in Image Analysis},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533145},
doi = {10.1145/3531146.3533145},
abstract = {Algorithms provide powerful tools for detecting and dissecting human bias and error. Here, we develop machine learning methods to to analyze how humans err in a particular high-stakes task: image interpretation. We leverage a unique dataset of 16,135,392 human predictions of whether a neighborhood voted for Donald Trump or Joe Biden in the 2020 US election, based on a Google Street View image. We show that by training a machine learning estimator of the Bayes optimal decision for each image, we can provide an actionable decomposition of human error into bias, variance, and noise terms, and further identify specific features (like pickup trucks) which lead humans astray. Our methods can be applied to ensure that human-in-the-loop decision-making is accurate and fair and are also applicable to black-box algorithmic systems.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {799–813},
numpages = {15},
keywords = {human error, diagnosing bias, image analysis},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3533146,
author = {Dai, Zhen and Makarychev, Yury and Vakilian, Ali},
title = {Fair Representation Clustering with Several Protected Classes},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533146},
doi = {10.1145/3531146.3533146},
abstract = {We study the problem of fair k-median where each cluster is required to have a fair representation of individuals from different groups. In the fair representation k-median problem, we are given a set of points X in a metric space. Each point x ∈ X belongs to one of ℓ groups. Further, we are given fair representation parameters αj and βj for each group j ∈ [ℓ]. We say that a k-clustering C1, ⋅⋅⋅, Ck fairly represents all groups if the number of points from group j in cluster Ci is between αj|Ci| and βj|Ci| for every j ∈ [ℓ] and i ∈ [k]. The goal is to find a set of k centers and an assignment such that the clustering defined by fairly represents all groups and minimizes the ℓ1-objective ∑x ∈ Xd(x, ϕ(x)). We present an O(log k)-approximation algorithm that runs in time nO(ℓ). Note that the known algorithms for the problem either (i) violate the fairness constraints by an additive term or (ii) run in time that is exponential in both k and ℓ. We also consider an important special case of the problem where and for all j ∈ [ℓ]. For this special case, we present an O(log k)-approximation algorithm that runs in time.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {814–823},
numpages = {10},
keywords = {approximation algorithm, randomized algorithm, clustering, fair k-median},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3533149,
author = {Black, Emily and Raghavan, Manish and Barocas, Solon},
title = {Model Multiplicity: Opportunities, Concerns, and Solutions},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533149},
doi = {10.1145/3531146.3533149},
abstract = {Recent scholarship has brought attention to the fact that there often exist multiple models for a given prediction task with equal accuracy that differ in their individual-level predictions or aggregate properties. This phenomenon—which we call model multiplicity—can introduce a good deal of flexibility into the model selection process, creating a range of exciting opportunities. By demonstrating that there are many different ways of making equally accurate predictions, multiplicity gives model developers the freedom to prioritize other values in their model selection process without having to abandon their commitment to maximizing accuracy. However, multiplicity also brings to light a concerning truth: model selection on the basis of accuracy alone—the default procedure in many deployment scenarios—fails to consider what might be meaningful differences between equally accurate models with respect to other criteria such as fairness, robustness, and interpretability. Unless these criteria are taken into account explicitly, developers might end up making unnecessary trade-offs or could even mask intentional discrimination. Furthermore, the prospect that there might exist another model of equal accuracy that flips a prediction for a particular individual may lead to a crisis in justifiability: why should an individual be subject to an adverse model outcome if there exists an equally accurate model that treats them more favorably? In this work, we investigate how to take advantage of the flexibility afforded by model multiplicity while addressing the concerns with justifiability that it might raise?},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {850–863},
numpages = {14},
keywords = {Model multiplicity, recourse, procedural multiplicity, fairness, arbitrariness, predictive multiplicity, discrimination},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3533151,
author = {Fischer, Maximilian T. and Hirsbrunner, Simon David and Jentner, Wolfgang and Miller, Matthias and Keim, Daniel A. and Helm, Paula},
title = {Promoting Ethical Awareness in Communication Analysis: Investigating Potentials and Limits of Visual Analytics for Intelligence Applications},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533151},
doi = {10.1145/3531146.3533151},
abstract = {Digital systems for analyzing human communication data have become prevalent in recent years. This may be related to the increasing abundance of data that can be harnessed but can hardly be managed manually. Intelligence analysis of communications data in investigative journalism, criminal intelligence, and law present particularly interesting cases, as they must take into account the often highly sensitive properties of the underlying operations and data. At the same time, these are areas where increasingly automated, sophisticated approaches and tailored systems can be particularly useful and relevant, especially in terms of Big Data manageability. However, by the shifting of responsibilities, this also poses dangers. In addition to privacy concerns, these dangers relate to uncertain or poor data quality, leading to discrimination and potentially misleading insights. Other problems relate to a lack of transparency and traceability, making it difficult to accurately identify problems and determine appropriate remedial strategies. Visual analytics combines machine learning methods with interactive visual interfaces to enable human sense- and decision-making. This technique can be key for designing and operating meaningful interactive communication analysis systems that consider these ethical challenges. In this interdisciplinary work, a joint endeavor of computer scientists, ethicists, and scholars in Science & Technology Studies, we investigate and evaluate opportunities and risks involved in using Visual analytics approaches for communication analysis in intelligence applications in particular. We introduce, at first, the common technological systems used in communication analysis, with a special focus on intelligence analysis in criminal investigations, further discussing the domain-specific ethical implications, tensions, and risks involved. We then make the case of how tailored Visual Analytics approaches may reduce and mitigate the described problems, both theoretically and through practical examples. Offering interactive analysis capabilities and what-if explorations while facilitating guidance, provenance generation, and bias awareness (through nudges, for example) can improve analysts’ understanding of their data, increasing trustworthiness, accountability, and generating knowledge. We show that finding Visual Analytics design solutions for ethical issues is not a mere optimization task with an ideal final solution. Design solutions for specific ethical problems (e.g., privacy) often trigger new ethical issues (e.g., accountability) in other areas. Balancing out and negotiating these trade-offs has, as we argue, to be an integral aspect of the system design process from the outset. Finally, our work identifies existing gaps and highlights research opportunities, further describing how our results can be transferred to other domains. With this contribution, we aim at informing more ethically-aware approaches to communication analysis in intelligence operations.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {877–889},
numpages = {13},
keywords = {Interdisciplinary Research, Critical Algorithm Studies, Machine Learning, Intelligence Analysis, Communication Analysis, Ethic Awareness, Visual Analytics, Science & Technology Studies, Critical Data Studies},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3533152,
author = {McLaughlin, Bryce and Spiess, Jann and Gillis, Talia},
title = {On the Fairness of Machine-Assisted Human Decisions},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533152},
doi = {10.1145/3531146.3533152},
abstract = {When machine-learning algorithms are deployed in high-stakes decisions, we want to ensure that their deployment leads to fair and equitable outcomes. This concern has motivated a fast-growing literature that focuses on diagnosing and addressing disparities in machine predictions. However, many machine predictions are deployed to assist in decisions where a human decision-maker retains the ultimate decision authority. In this article, we therefore consider how properties of machine predictions affect the resulting human decisions. We show in a formal model that the inclusion of a biased human decision-maker can revert common relationships between the structure of the algorithm and the qualities of resulting decisions. Specifically, we document that excluding information about protected groups from the prediction may fail to reduce, and may even increase, ultimate disparities. While our concrete results rely on specific assumptions about the data, algorithm, and decision-maker, they show more broadly that any study of critical properties of complex decision systems, such as the fairness of machine-assisted human decisions, should go beyond focusing on the underlying algorithmic predictions in isolation.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {890},
numpages = {1},
keywords = {Machine Learning, Decision Support Systems, Fairness, Human Computer Interaction, Protected Classes},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3533154,
author = {Chien, Isabel and Deliu, Nina and Turner, Richard and Weller, Adrian and Villar, Sofia and Kilbertus, Niki},
title = {Multi-Disciplinary Fairness Considerations in Machine Learning for Clinical Trials},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533154},
doi = {10.1145/3531146.3533154},
abstract = {While interest in the application of machine learning to improve healthcare has grown tremendously in recent years, a number of barriers prevent deployment in medical practice. A notable concern is the potential to exacerbate entrenched biases and existing health disparities in society. The area of fairness in machine learning seeks to address these issues of equity; however, appropriate approaches are context-dependent, necessitating domain-specific consideration. We focus on clinical trials, i.e., research studies conducted on humans to evaluate medical treatments. Clinical trials are a relatively under-explored application in machine learning for healthcare, in part due to complex ethical, legal, and regulatory requirements and high costs. Our aim is to provide a multi-disciplinary assessment of how fairness for machine learning fits into the context of clinical trials research and practice. We start by reviewing the current ethical considerations and guidelines for clinical trials and examine their relationship with common definitions of fairness in machine learning. We examine potential sources of unfairness in clinical trials, providing concrete examples, and discuss the role machine learning might play in either mitigating potential biases or exacerbating them when applied without care. Particular focus is given to adaptive clinical trials, which may employ machine learning. Finally, we highlight concepts that require further investigation and development, and emphasize new approaches to fairness that may be relevant to the design of clinical trials.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {906–924},
numpages = {19},
keywords = {machine learning for healthcare, adaptive clinical trials, health informatics, clinical trials},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3533156,
author = {Sloane, Mona and Zakrzewski, Janina},
title = {German AI Start-Ups and “AI Ethics”: Using A Social Practice Lens for Assessing and Implementing Socio-Technical Innovation},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533156},
doi = {10.1145/3531146.3533156},
abstract = {The current AI ethics discourse focuses on developing computational interpretations of ethical concerns, normative frameworks, and concepts for socio-technical innovation. There is less emphasis on understanding how AI practitioners themselves understand ethics and socially organize to operationalize ethical concerns. This is particularly true for AI start-ups, despite their significance as a conduit for the cultural production of innovation and progress, especially in the US and European context. This gap in empirical research intensifies the risk of a disconnect between scholarly research, innovation and application. This risk materializes acutely as mounting pressures to identify and mitigate the potential harms of AI systems have created an urgent need to rapidly assess and implement socio-technical innovation focused on fairness, accountability, and transparency. In this paper, we address this need. Building on social practice theory, we propose a framework that allows AI researchers, practitioners, and regulators to systematically analyze existing cultural understandings, histories, and social practices of “ethical AI” to define appropriate strategies for effectively implementing socio-technical innovations. We argue that this approach is needed because socio-technical innovation “sticks” better if it sustains the cultural meaning of socially shared (ethical) AI practices, rather than breaking them. By doing so, it creates pathways for technical and socio-technical innovations to be integrated into already existing routines. Against that backdrop, our contributions are threefold: (1) we introduce a practice-based approach for understanding “ethical AI”; (2) we present empirical findings from our study on the operationalization of “ethics” in German AI start-ups to underline that AI ethics and social practices must be understood in their specific cultural and historical contexts; and (3) based on our empirical findings, suggest that “ethical AI” practices can be broken down into principles, needs, narratives, materializations, and cultural genealogies to form a useful backdrop for considering socio-technical innovations. We conclude with critical reflections and practical implications of our work, as well as recommendations for future research.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {935–947},
numpages = {13},
keywords = {socio-cultural history, AI ethics, start-ups, innovation, social practice, fairness, accountability, transparency, regulation, organizations},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3533157,
author = {Birhane, Abeba and Ruane, Elayne and Laurent, Thomas and S. Brown, Matthew and Flowers, Johnathan and Ventresque, Anthony and L. Dancy, Christopher},
title = {The Forgotten Margins of AI Ethics},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533157},
doi = {10.1145/3531146.3533157},
abstract = {How has recent AI Ethics literature addressed topics such as fairness and justice in the context of continued social and structural power asymmetries? We trace both the historical roots and current landmark work that have been shaping the field and categorize these works under three broad umbrellas: (i) those grounded in Western canonical philosophy, (ii) mathematical and statistical methods, and (iii) those emerging from critical data/algorithm/information studies. We also survey the field and explore emerging trends by examining the rapidly growing body of literature that falls under the broad umbrella of AI Ethics. To that end, we read and annotated peer-reviewed papers published over the past four years in two premier conferences: FAccT and AIES. We organize the literature based on an annotation scheme we developed according to three main dimensions: whether the paper deals with concrete applications, use-cases, and/or people’s lived experience; to what extent it addresses harmed, threatened, or otherwise marginalized groups; and if so, whether it explicitly names such groups. We note that although the goals of the majority of FAccT and AIES papers were often commendable, their consideration of the negative impacts of AI on traditionally marginalized groups remained shallow. Taken together, our conceptual analysis and the data from annotated papers indicate that the field would benefit from an increased focus on ethical analysis grounded in concrete use-cases, people’s experiences, and applications as well as from approaches that are sensitive to structural and historical power asymmetries.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {948–958},
numpages = {11},
keywords = {Trends, AI Ethics, AIES, Justice, FAccT},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3533159,
author = {Pahl, Jaspar and Rieger, Ines and M\""{o}ller, Anna and Wittenberg, Thomas and Schmid, Ute},
title = {Female, White, 27? Bias Evaluation on Data and Algorithms for Affect Recognition in Faces},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533159},
doi = {10.1145/3531146.3533159},
abstract = {Nowadays, Artificial Intelligence (AI) algorithms show a strong performance for many use cases, making them desirable for real-world scenarios where the algorithms provide high-impact decisions. However, one major drawback of AI algorithms is their susceptibility to bias and resulting unfairness. This has a huge influence for their application, as they have a higher failure rate for certain subgroups. In this paper, we focus on the field of affective computing and particularly on the detection of bias for facial expressions. Depending on the deployment scenario, bias in facial expression models can have a disadvantageous impact and it is therefore essential to evaluate the bias and limitations of the model. In order to analyze the metadata distribution in affective computing datasets, we annotate several benchmark training datasets, containing both Action Units and categorical emotions, with age, gender, ethnicity, glasses, and beards. We show that there is a significantly skewed distribution, particularly for ethnicity and age. Based on this metadata annotation, we evaluate two trained state-of-the-art affective computing algorithms. Our evaluation shows that the strongest bias is in age, with the best performance for persons under 34 and a sharp decrease for older persons. Furthermore, we see an ethnicity bias with varying direction depending on the algorithm, a slight gender bias and worse performance for facial parts occluded by glasses.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {973–987},
numpages = {15},
keywords = {algorithm evaluation, action units, bias, data evaluation, categorical emotions, fairness, metadata post-annotation, affective computing},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3533160,
author = {So, Wonyoung and Lohia, Pranay and Pimplikar, Rakesh and Hosoi, A.E. and D'Ignazio, Catherine},
title = {Beyond Fairness: Reparative Algorithms to Address Historical Injustices of Housing Discrimination in the US},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533160},
doi = {10.1145/3531146.3533160},
abstract = {Fairness in Machine Learning (ML) has mostly focused on interrogating the fairness of a particular decision point with assumptions made that the people represented in the data have been fairly treated throughout history. However, fairness cannot be ultimately achieved if such assumptions are not valid. This is the case for mortgage lending discrimination in the US, which should be critically understood as the result of historically accumulated injustices that were enacted through public policies and private practices including redlining, racial covenants, exclusionary zoning, and predatory inclusion, among others. With the erroneous assumptions of historical fairness in ML, Black borrowers with low income and low wealth are considered as a given condition in a lending algorithm, thus rejecting loans to them would be considered a “fair” decision even though Black borrowers were historically excluded from homeownership and wealth creation. To emphasize such issues, we introduce case studies using contemporary mortgage lending data as well as historical census data in the US. First, we show that historical housing discrimination has differentiated each racial group’s baseline wealth which is a critical input for algorithmically determining mortgage loans. The second case study estimates the cost of housing reparations in the algorithmic lending context to redress historical harms because of such discriminatory housing policies. Through these case studies, we envision what reparative algorithms would look like in the context of housing discrimination in the US. This work connects to emerging scholarship on how algorithmic systems can contribute to redressing past harms through engaging with reparations policies and programs.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {988–1004},
numpages = {17},
keywords = {reparations, racial wealth gap, housing, mortgage lending, fairness},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"


@InProceedings{pmlr-v80-kilbertus18a,
  title = 	 {Blind Justice: Fairness with Encrypted Sensitive Attributes},
  author =       {Kilbertus, Niki and Gascon, Adria and Kusner, Matt and Veale, Michael and Gummadi, Krishna and Weller, Adrian},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {2630--2639},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/kilbertus18a/kilbertus18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/kilbertus18a.html},
  abstract = 	 {Recent work has explored how to train machine learning models which do not discriminate against any subgroup of the population as determined by sensitive attributes such as gender or race. To avoid disparate treatment, sensitive attributes should not be considered. On the other hand, in order to avoid disparate impact, sensitive attributes must be examined, e.g., in order to learn a fair model, or to check if a given model is fair. We introduce methods from secure multi-party computation which allow us to avoid both. By encrypting sensitive attributes, we show how an outcome-based fair model may be learned, checked, or have its outputs verified and held to account, without users revealing their sensitive attributes.}
}


@InProceedings{pmlr-v80-komiyama18a,
  title = 	 {Nonconvex Optimization for Regression with Fairness Constraints},
  author =       {Komiyama, Junpei and Takeda, Akiko and Honda, Junya and Shimao, Hajime},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {2737--2746},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/komiyama18a/komiyama18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/komiyama18a.html},
  abstract = 	 {The unfairness of a regressor is evaluated by measuring the correlation between the estimator and the sensitive attribute (e.g., race, gender, age), and the coefficient of determination (CoD) is a natural extension of the correlation coefficient when more than one sensitive attribute exists. As is well known, there is a trade-off between fairness and accuracy of a regressor, which implies a perfectly fair optimizer does not always yield a useful prediction. Taking this into consideration, we optimize the accuracy of the estimation subject to a user-defined level of fairness. However, a fairness level as a constraint induces a nonconvexity of the feasible region, which disables the use of an off-the-shelf convex optimizer. Despite such nonconvexity, we show an exact solution is available by using tools of global optimization theory. Furthermore, we propose a nonlinear extension of the method by kernel representation. Unlike most of existing fairness-aware machine learning methods, our method allows us to deal with numeric and multiple sensitive attributes.}
}



"@inproceedings{10.1145/3531146.3533161,
author = {Lu, Christina and Kay, Jackie and McKee, Kevin},
title = {Subverting Machines, Fluctuating Identities: Re-Learning Human Categorization},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533161},
doi = {10.1145/3531146.3533161},
abstract = {Most machine learning systems that interact with humans construct some notion of a person’s “identity,” yet the default paradigm in AI research envisions identity with essential attributes that are discrete and static. In stark contrast, strands of thought within critical theory present a conception of identity as malleable and constructed entirely through interaction; a doing rather than a being. In this work, we distill some of these ideas for machine learning practitioners and introduce a theory of identity as autopoiesis, circular processes of formation and function. We argue that the default paradigm of identity used by the field immobilizes existing identity categories and the power differentials that co-occur, due to the absence of iterative feedback to our models. This includes a critique of emergent AI fairness practices that continue to impose the default paradigm. Finally, we apply our theory to sketch approaches to autopoietic identity through multilevel optimization and relational learning. While these ideas raise many open questions, we imagine the possibilities of machines that are capable of expressing human identity as a relationship perpetually in flux.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1005–1015},
numpages = {11},
keywords = {theories of identity, social construction, identity systems, algorithmic fairness},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3533165,
author = {Du, Yuhao and Ionescu, Stefania and Sage, Melanie and Joseph, Kenneth},
title = {A Data-Driven Simulation of the New York State Foster Care System},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533165},
doi = {10.1145/3531146.3533165},
abstract = {We introduce an analytic pipeline to model and simulate youth trajectories through the New York state foster care system. Our goal in doing so is to forecast how proposed interventions may impact the foster care system’s ability to achieve it’s stated goals before these interventions are actually implemented and impact the lives of thousands of youth. Here, we focus on two specific stated goals of the system: racial equity, and, as codified most recently by the 2018 Family First Prevention Services Act (FFPSA), a focus on keeping all youth out of foster care. We also focus on one specific potential intervention— a predictive model, proposed in prior work and implemented elsewhere in the U.S., which aims to determine whether or not a youth is in need of care. We use our method to explore how the implementation of this predictive model in New York would impact racial equity and the number of youth in care. While our findings, as in any simulation model, ultimately rely on modeling assumptions, we find evidence that the model would not necessarily achieve either goal. Primarily, then, we aim to further promote the use of data-driven simulation to help understand the ramifications of algorithmic interventions in public systems.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1028–1038},
numpages = {11},
keywords = {Social Policy, Simulation, Racial Equity, Child Welfare},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3533166,
author = {Pfohl, Stephen and Xu, Yizhe and Foryciarz, Agata and Ignatiadis, Nikolaos and Genkins, Julian and Shah, Nigam},
title = {Net Benefit, Calibration, Threshold Selection, and Training Objectives for Algorithmic Fairness in Healthcare},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533166},
doi = {10.1145/3531146.3533166},
abstract = {A growing body of work uses the paradigm of algorithmic fairness to frame the development of techniques to anticipate and proactively mitigate the introduction or exacerbation of health inequities that may follow from the use of model-guided decision-making. We evaluate the interplay between measures of model performance, fairness, and the expected utility of decision-making to offer practical recommendations for the operationalization of algorithmic fairness principles for the development and evaluation of predictive models in healthcare. We conduct an empirical case-study via development of models to estimate the ten-year risk of atherosclerotic cardiovascular disease to inform statin initiation in accordance with clinical practice guidelines. We demonstrate that approaches that incorporate fairness considerations into the model training objective typically do not improve model performance or confer greater net benefit for any of the studied patient populations compared to the use of standard learning paradigms followed by threshold selection concordant with patient preferences, evidence of intervention effectiveness, and model calibration. These results hold when the measured outcomes are not subject to differential measurement error across patient populations and threshold selection is unconstrained, regardless of whether differences in model performance metrics, such as in true and false positive error rates, are present. In closing, we argue for focusing model development efforts on developing calibrated models that predict outcomes well for all patient populations while emphasizing that such efforts are complementary to transparent reporting, participatory design, and reasoning about the impact of model-informed interventions in context.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1039–1052},
numpages = {14},
keywords = {cardiovascular disease, fairness, healthcare},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3533167,
author = {Mishler, Alan and Kennedy, Edward H.},
title = {FADE: FAir Double Ensemble Learning for Observable and Counterfactual Outcomes},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533167},
doi = {10.1145/3531146.3533167},
abstract = {Methods for building fair predictors often involve tradeoffs between fairness and accuracy and between different fairness criteria. Recent work seeks to characterize these tradeoffs in specific problem settings, but these methods often do not accommodate users who wish to improve the fairness of an existing benchmark model without sacrificing accuracy, or vice versa. These results are also typically restricted to observable accuracy and fairness criteria. We develop a flexible framework for fair ensemble learning that allows users to efficiently explore the fairness-accuracy space or to improve the fairness or accuracy of a benchmark model. Our framework can simultaneously target multiple observable or counterfactual fairness criteria, and it enables users to combine a large number of previously trained and newly trained predictors. We provide theoretical guarantees that our estimators converge at fast rates. We apply our method on both simulated and real data, with respect to both observable and counterfactual accuracy and fairness criteria. We show that, surprisingly, multiple unfairness measures can sometimes be minimized simultaneously with little impact on accuracy, relative to unconstrained predictors or existing benchmark models.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1053},
numpages = {1},
keywords = {ensemble learning, semiparametric, fairness, counterfactual},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3533169,
author = {Buyl, Maarten and Cociancig, Christina and Frattone, Cristina and Roekens, Nele},
title = {Tackling Algorithmic Disability Discrimination in the Hiring Process: An Ethical, Legal and Technical Analysis},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533169},
doi = {10.1145/3531146.3533169},
abstract = {Tackling algorithmic discrimination against persons with disabilities (PWDs) demands a distinctive approach that is fundamentally different to that applied to other protected characteristics, due to particular ethical, legal, and technical challenges. We address these challenges specifically in the context of artificial intelligence (AI) systems used in hiring processes (or automated hiring systems, AHSs), in which automated assessment procedures are subject to unique ethical and legal considerations and have an undeniable adverse impact on PWDs. In this paper, we discuss concerns and opportunities raised by AI-driven hiring in relation to disability discrimination. Ultimately, we aim to encourage further research into this topic. Hence, we establish some starting points and design a roadmap for ethicists, lawmakers, advocates as well as AI practitioners alike.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1071–1082},
numpages = {12},
keywords = {social justice, data protection law, automated hiring systems, algorithmic discrimination, persons with disabilities, reasonable accommodation, Artificial Intelligence Act, equality law, ethics of discrimination},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3533171,
author = {Mashiat, Tasfia and Gitiaux, Xavier and Rangwala, Huzefa and Fowler, Patrick and Das, Sanmay},
title = {Trade-Offs between Group Fairness Metrics in Societal Resource Allocation},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533171},
doi = {10.1145/3531146.3533171},
abstract = {We consider social resource allocations that deliver an array of scarce supports to a diverse population. Such allocations pervade social service delivery, such as provision of homeless services and assignment of refugees to cities, among others. At issue is whether allocations are fair across sociodemographic groups and intersectional identities. Our paper shows that necessary trade-offs exist for fairness in the context of scarcity; many reasonable definitions of equitable outcomes cannot hold simultaneously except under stringent conditions. For example, defining fairness in terms of improvement over a baseline inherently conflicts with defining fairness in terms of loss compared with the best possible outcome. Moreover, we demonstrate that the fairness trade-offs stem from heterogeneity across groups in intervention responses. Administrative records on homeless service delivery offer a real-world example. Building on prior work, we measure utilities for each household as the probability of reentry into homeless services if given three homeless services. Heterogeneity in utility distributions (conditional on received services) for several sociodemographic groups (e.g. single women with children versus without children) generates divergence across fairness metrics. We argue that such heterogeneity, and thus, fairness trade-offs, pervade many social policy contexts.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1095–1105},
numpages = {11},
keywords = {algorithmic fairness, Resource allocation, fairness metrics},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3533175,
author = {Lucchesi, Lydia R. and Kuhnert, Petra M. and Davis, Jenny L. and Xie, Lexing},
title = {Smallset Timelines: A Visual Representation of Data Preprocessing Decisions},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533175},
doi = {10.1145/3531146.3533175},
abstract = {Data preprocessing is a crucial stage in the data analysis pipeline, with both technical and social aspects to consider. Yet, the attention it receives is often lacking in research practice and dissemination. We present the Smallset Timeline, a visualisation to help reflect on and communicate data preprocessing decisions. A “Smallset” is a small selection of rows from the original dataset containing instances of dataset alterations. The Timeline is comprised of Smallset snapshots representing different points in the preprocessing stage and captions to describe the alterations visualised at each point. Edits, additions, and deletions to the dataset are highlighted with colour. We develop the R software package, smallsets, that can create Smallset Timelines from R and Python data preprocessing scripts. Constructing the figure asks practitioners to reflect on and revise decisions as necessary, while sharing it aims to make the process accessible to a diverse range of audiences. We present two case studies to illustrate use of the Smallset Timeline for visualising preprocessing decisions. Case studies include software defect data and income survey benchmark data, in which preprocessing affects levels of data loss and group fairness in prediction tasks, respectively. We envision Smallset Timelines as a go-to data provenance tool, enabling better documentation and communication of preprocessing tasks at large.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1136–1153},
numpages = {18},
keywords = {open-source software, reflexivity, data preprocessing, visualization, communication},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3533176,
author = {Botes, Marietjie Wilhelmina Maria},
title = {Brain Computer Interfaces and Human Rights: Brave New Rights for a Brave New World},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533176},
doi = {10.1145/3531146.3533176},
abstract = {Digital health applications include a wide range of wearable, implantable, injectable and ingestible digital medical devices. Many of these devices use machine learning algorithms to assist medical prognosis and decision-making. One of the most compelling digital medical device developments is brain-computer interfaces (BCIs) which entails the connecting of a person's brain to a computer, or to another device outside the human body. BCIs allow bidirectional communication and control between the human brain and the outside world by exporting brain data or altering brain activity. Although being marveled at for its clinical promises, this technological advancement also raises novel ethical, legal, social and technical implications (ELSTI). Debates in this regard centers around patient autonomy, equity, trustworthiness in healthcare, data protection and security, risks of dehumanization, the limitations of machine learning-based decision-making, and the influence that BCIs have on what it means to be human and human rights. Since the adoption of the Universal Declaration of Human Rights (UDHR) after World War II, the landscape that give rise to these human rights has evolved enormously. Human life and humans’ role in society are being transformed and threatened by technologies that were never imagined at the time the UDHR was adopted. BCIs, in particular, harbor the greatest possibility of social and individual disruption through its capability to record, interpret, manipulate, or alter brain activity that may potentially alter what it means to be human and how we control humans in future. Cutting edge technological innovations that increasingly blur the lines between human and computer beg the rethinking and extension of existing human rights to remain relevant in a digitized world. In this paper sui generis human rights such as mental privacy, the right to identity or self, agency or free will and fair access to cognitive augmentation will be discussed and how a regulatory framework must be adapted to act as technology enablers, whilst ensuring fairness, accountability, and transparency in sociotechnical systems.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1154–1161},
numpages = {8},
keywords = {identity, brain computer interfaces, human rights, neurological privacy, autonomy},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3533178,
author = {Blum, Avrim and Stangl, Kevin and Vakilian, Ali},
title = {Multi Stage Screening: Enforcing Fairness and Maximizing Efficiency in a Pre-Existing Pipeline},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533178},
doi = {10.1145/3531146.3533178},
abstract = {Consider an actor making selection decisions (e.g., hiring) using a series of classifiers, which we term a sequential screening process. The early stages (e.g. resume screen, coding screen, phone interview) filter out some of the applicants, and in the final stage an expensive but accurate test (e.g. a full interview) is applied to those individuals that make it to the final stage. Since the final stage is expensive, if there are multiple groups with different fractions of positives in them at the penultimate stage (even if a slight gap), then the firm may naturally only choose to apply the final (interview) stage solely to the highest precision group which would be clearly unfair to the other groups. Even if the firm is required to interview all those who pass to the final round, the tests themselves could have the property that qualified individuals from some groups pass more easily than qualified individuals from others. Accordingly, we consider requiring Equality of Opportunity (qualified members of each group have the same chance of reaching the final stage and being interviewed). We then examine the goal of maximizing quantities of interest to the decision maker subject to this constraint, via modification of the probabilities of promotion through the screening process at each stage based on performance at the previous stage. We exhibit algorithms for satisfying Equal Opportunity over the selection process and maximizing precision (the fraction of interviews that yield qualified candidates) as well as linear combinations of precision and recall (recall determines the number of applicants needed per hire) at the end of the final stage. We also present examples showing that the solution space is non-convex, which motivate our combinatorial exact and (FPTAS) approximation algorithms for maximizing the linear combination of precision and recall. Finally, we discuss the ‘price of’ adding additional restrictions, such as not allowing the decision-maker to use group membership in its decision process.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1178–1193},
numpages = {16},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3533179,
author = {Balagopalan, Aparna and Zhang, Haoran and Hamidieh, Kimia and Hartvigsen, Thomas and Rudzicz, Frank and Ghassemi, Marzyeh},
title = {The Road to Explainability is Paved with Bias: Measuring the Fairness of Explanations},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533179},
doi = {10.1145/3531146.3533179},
abstract = {Machine learning models in safety-critical settings like healthcare are often “blackboxes”: they contain a large number of parameters which are not transparent to users. Post-hoc explainability methods where a simple, human-interpretable model imitates the behavior of these blackbox models are often proposed to help users trust model predictions. In this work, we audit the quality of such explanations for different protected subgroups using real data from four settings in finance, healthcare, college admissions, and the US justice system. Across two different blackbox model architectures and four popular explainability methods, we find that the approximation quality of explanation models, also known as the fidelity, differs significantly between subgroups. We also demonstrate that pairing explainability methods with recent advances in robust machine learning can improve explanation fairness in some settings. However, we highlight the importance of communicating details of non-zero fidelity gaps to users, since a single solution might not exist across all settings. Finally, we discuss the implications of unfair explanation models as a challenging and understudied problem facing the machine learning community.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1194–1206},
numpages = {13},
keywords = {fairness, explainability, machine learning},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3533180,
author = {Diana, Emily and Gill, Wesley and Kearns, Michael and Kenthapadi, Krishnaram and Roth, Aaron and Sharifi-Malvajerdi, Saeed},
title = {Multiaccurate Proxies for Downstream Fairness},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533180},
doi = {10.1145/3531146.3533180},
abstract = {We study the problem of training a model that must obey demographic fairness conditions when the sensitive features are not available at training time — in other words, how can we train a model to be fair by race when we don’t have data about race? We adopt a fairness pipeline perspective, in which an “upstream” learner that does have access to the sensitive features will learn a proxy model for these features from the other attributes. The goal of the proxy is to allow a general “downstream” learner — with minimal assumptions on their prediction task — to be able to use the proxy to train a model that is fair with respect to the true sensitive features. We show that obeying multiaccuracy constraints with respect to the downstream model class suffices for this purpose, provide sample- and oracle efficient-algorithms and generalization bounds for learning such proxies, and conduct an experimental evaluation. In general, multiaccuracy is much easier to satisfy than classification accuracy, and can be satisfied even when the sensitive features are hard to predict.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1207–1239},
numpages = {33},
keywords = {proxy variables, game theory, multiaccuracy, algorithmic fairness},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3533181,
author = {Rahmattalabi, Aida and Vayanos, Phebe and Dullerud, Kathryn and Rice, Eric},
title = {Learning Resource Allocation Policies from Observational Data with an Application to Homeless Services Delivery},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533181},
doi = {10.1145/3531146.3533181},
abstract = {We study the problem of learning, from observational data, fair and interpretable policies that effectively match heterogeneous individuals to scarce resources of different types. We model this problem as a multi-class multi-server queuing system where both individuals and resources arrive stochastically over time. Each individual, upon arrival, is assigned to a queue where they wait to be matched to a resource. The resources are assigned in a first come first served (FCFS) fashion according to an eligibility structure that encodes the resource types that serve each queue. We propose a methodology based on techniques in modern causal inference to construct the individual queues as well as learn the matching outcomes and provide a mixed-integer optimization (MIO) formulation to optimize the eligibility structure. The MIO problem maximizes policy outcome subject to wait time and fairness constraints. It is very flexible, allowing for additional linear domain constraints. We conduct extensive analyses using synthetic and real-world data. In particular, we evaluate our framework using data from the U.S. Homeless Management Information System (HMIS). We obtain wait times as low as an FCFS policy while improving the rate of exit from homelessness for underserved or vulnerable groups (7% higher for the Black individuals and 15% higher for those below 17 years old) and overall.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1240–1256},
numpages = {17},
keywords = {Causal Inference, Observational Data, Mixed-integer Optimization, Fairness in AI},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3533183,
author = {Wolfe, Robert and Caliskan, Aylin},
title = {Markedness in Visual Semantic AI},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533183},
doi = {10.1145/3531146.3533183},
abstract = {We evaluate the state-of-the-art multimodal ”visual semantic” model CLIP (”Contrastive Language Image Pretraining”) for biases related to the marking of age, gender, and race or ethnicity. Given the option to label an image as ”a photo of a person” or to select a label denoting race or ethnicity, CLIP chooses the ”person” label 47.9% of the time for White individuals, compared with 5.0% or less for individuals who are Black, East Asian, Southeast Asian, Indian, or Latino or Hispanic. The model is also more likely to rank the unmarked ”person” label higher than labels denoting gender for Male individuals (26.7% of the time) vs. Female individuals (15.2% of the time). Age also affects whether an individual is marked by the model: Female individuals under the age of 20 are more likely than Male individuals to be marked with a gender label, but less likely to be marked with an age label, while Female individuals over the age of 40 are more likely to be marked based on age than Male individuals. We trace our results back to the CLIP embedding space by examining the self-similarity (mean pairwise cosine similarity) for each social group, where higher self-similarity denotes greater attention directed by CLIP to the shared characteristics (i.e., age, race, or gender) of the social group. The results indicate that, as age increases, the self-similarity of representations of Female individuals increases at a higher rate than for Male individuals, with the disparity most pronounced at the ”more than 70” age range. Six of the ten least self-similar social groups are individuals who are White and Male, while all ten of the most self-similar social groups are individuals under the age of 10 or over the age of 70, and six of the ten are Female individuals. Our results yield evidence that bias in CLIP is intersectional: existing biases of self-similarity and markedness between Male and Female gender groups are further exacerbated when the groups compared are individuals who are White and Male and individuals who are Black and Female. CLIP is an English-language model trained on internet content gathered based on a query list generated from an American website (Wikipedia), and results indicate that CLIP reflects the biases of the language and society which produced this training data.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1269–1279},
numpages = {11},
keywords = {language-and-vision AI, bias in AI, age bias, visual semantics, multimodal, markedness},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3533186,
author = {Ehsan, Upol and Singh, Ranjit and Metcalf, Jacob and Riedl, Mark},
title = {The Algorithmic Imprint},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533186},
doi = {10.1145/3531146.3533186},
abstract = {When algorithmic harms emerge, a reasonable response is to stop using the algorithm to resolve concerns related to fairness, accountability, transparency, and ethics (FATE). However, just because an algorithm is removed does not imply its FATE-related issues cease to exist. In this paper, we introduce the notion of the “algorithmic imprint” to illustrate how merely removing an algorithm does not necessarily undo or mitigate its consequences. We operationalize this concept and its implications through the 2020 events surrounding the algorithmic grading of the General Certificate of Education (GCE) Advanced (A) Level exams, an internationally recognized UK-based high school diploma exam administered in over 160 countries. While the algorithmic standardization was ultimately removed due to global protests, we show how the removal failed to undo the algorithmic imprint on the sociotechnical infrastructures that shape students’, teachers’, and parents’ lives. These events provide a rare chance to analyze the state of the world both with and without algorithmic mediation. We situate our case study in Bangladesh to illustrate how algorithms made in the Global North disproportionately impact stakeholders in the Global South. Chronicling more than a year-long community engagement consisting of 47 interviews, we present the first coherent timeline of “what” happened in Bangladesh, contextualizing “why” and “how” they happened through the lenses of the algorithmic imprint and situated algorithmic fairness. Analyzing these events, we highlight how the contours of the algorithmic imprints can be inferred at the infrastructural, social, and individual levels. We share conceptual and practical implications around how imprint-awareness can (a) broaden the boundaries of how we think about algorithmic impact, (b) inform how we design algorithms, and (c) guide us in AI governance. The imprint-aware design mindset can make the algorithmic development process more human-centered and sociotechnically-informed.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1305–1317},
numpages = {13},
keywords = {Algorithmic Imprint, Situated Fairness, Infrastructure, Algorithmic Impact Assessment, Global South, Folk Theories of Algorithms, User Perceptions},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3533194,
author = {Young, Meg and Katell, Michael and Krafft, P.M.},
title = {Confronting Power and Corporate Capture at the FAccT Conference},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533194},
doi = {10.1145/3531146.3533194},
abstract = {Fields such as medicine and public health attest to deep conflict of interest concerns present when private companies fund evaluation of their own products and services. We draw on these lessons to consider corporate capture of the ACM Fairness, Accountability, and Transparency (FAccT) conference. We situate our analysis within scholarship on the entanglement of industry and academia and focus on the silences it produces in the research record. Our analysis of the institutional design at FAccT indicates the conference’s neglect of those people most negatively impacted by algorithmic systems. We focus on a 2021 paper by Wilson et al., “Building and auditing fair algorithms: A case study in candidate screening” as a key example of conflicted research accepted via peer review at FAccT. We call on the conference to (1) lead on models for how to manage conflicts of interest in the field of computing beyond individual disclosure of funding sources, (2) hold space for advocates and activists able to speak directly to questions of algorithmic harm, and (3) reconstitute the conference with attention to fostering agonistic dissensus—un-making the present manufactured consensus and nurturing challenges to power. These changes will position our community to contend with the political dimensions of research on AI harms.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1375–1386},
numpages = {12},
keywords = {pymetrics, agonism, corporate capture, research funding, industry engagement, conflict of interest},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3533197,
author = {Buet-Golfouse, Francois and Utyagulov, Islam},
title = {Towards Fair Unsupervised Learning},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533197},
doi = {10.1145/3531146.3533197},
abstract = {Bias-mitigating techniques are now well established in the supervised learning literature and have shown their ability to tackle fairness-accuracy, as well as fairness-fairness trade-offs. These are usually predicated on different conceptions of fairness, such as demographic parity or equal odds that depend on the available labels in the dataset. However, it is often the case in practice that unsupervised learning is used as part of a machine learning pipeline (for instance, to perform dimensionality reduction or representation learning via SVD) or as a standalone model (for example, to derive a customer segmentation via k-means). It is thus crucial to develop approaches towards fair unsupervised learning. This work investigates fair unsupervised learning within the broad framework of generalised low-rank models (GLRM). Importantly, we introduce the concept of fairness functional that encompasses both traditional unsupervised learning techniques and min-max algorithms (whereby one minimises the maximum group loss). To do so, we design straightforward alternate convex search or biconvex gradient descent algorithms that also provide partial debiasing techniques. Finally, we show on benchmark datasets that our fair generalised low-rank models (“fGLRM”) perform well and help reduce disparity amongst groups while only incurring small runtime overheads.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1399–1409},
numpages = {11},
keywords = {PCA, Clustering, Unsupervised Learning, Fairness},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3533198,
author = {Susser, Daniel},
title = {Decision Time: Normative Dimensions of Algorithmic Speed},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533198},
doi = {10.1145/3531146.3533198},
abstract = {Existing discussions about automated decision-making focus primarily on its inputs and outputs, raising questions about data collection and privacy on one hand and accuracy and fairness on the other. Less attention has been devoted to critically examining the temporality of decision-making processes—the speed at which automated decisions are reached. In this paper, I identify four dimensions of algorithmic speed that merit closer analysis. Duration (how much time it takes to reach a judgment), timing (when automated systems intervene in the activity being evaluated), frequency (how often evaluations are performed), and lived time (the human experience of algorithmic speed) are interrelated, but distinct, features of automated decision-making. Choices about the temporal structure of automated decision-making systems have normative implications, which I describe in terms of ”disruption,” ”displacement,” ”re-calibration,” and ”temporal fairness,” with values such as accuracy, fairness, accountability, and legitimacy hanging in the balance. As computational tools are increasingly tasked with making judgments about human activities and practices, the designers of decision-making systems will have to reckon, I argue, with when—and how fast—judgments ought to be rendered. Though computers are capable of reaching decisions at incredible speeds, failing to account for the temporality of automated decision-making risks misapprehending the costs and benefits automation promises.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1410–1420},
numpages = {11},
keywords = {AI ethics, speed, automated decision-making, automation, time, temporality, data ethics},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3533199,
author = {Rateike, Miriam and Majumdar, Ayan and Mineeva, Olga and Gummadi, Krishna P. and Valera, Isabel},
title = {Don’t Throw It Away! The Utility of Unlabeled Data in Fair Decision Making},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533199},
doi = {10.1145/3531146.3533199},
abstract = {Decision making algorithms, in practice, are often trained on data that exhibits a variety of biases. Decision-makers often aim to take decisions based on some ground-truth target that is assumed or expected to be unbiased, i.e., equally distributed across socially salient groups. In many practical settings, the ground-truth cannot be directly observed, and instead, we have to rely on a biased proxy measure of the ground-truth, i.e., biased labels, in the data. In addition, data is often selectively labeled, i.e., even the biased labels are only observed for a small fraction of the data that received a positive decision. To overcome label and selection biases, recent work proposes to learn stochastic, exploring decision policies via i) online training of new policies at each time-step and ii) enforcing fairness as a constraint on performance. However, the existing approach uses only labeled data, disregarding a large amount of unlabeled data, and thereby suffers from high instability and variance in the learned decision policies at different times. In this paper, we propose a novel method based on a variational autoencoder for practical fair decision-making. Our method learns an unbiased data representation leveraging both labeled and unlabeled data and uses the representations to learn a policy in an online process. Using synthetic data, we empirically validate that our method converges to the optimal (fair) policy according to the ground-truth with low variance. In real-world experiments, we further show that our training approach not only offers a more stable learning process but also yields policies with higher fairness as well as utility than previous approaches.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1421–1433},
numpages = {13},
keywords = {label bias, fairness, fair representation, selection bias, decision making, variational autoencoder},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3533204,
author = {Black, Emily and Elzayn, Hadi and Chouldechova, Alexandra and Goldin, Jacob and Ho, Daniel},
title = {Algorithmic Fairness and Vertical Equity: Income Fairness with IRS Tax Audit Models},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533204},
doi = {10.1145/3531146.3533204},
abstract = {This study examines issues of algorithmic fairness in the context of systems that inform tax audit selection by the United States Internal Revenue Service (IRS). While the field of algorithmic fairness has developed primarily around notions of treating like individuals alike, we instead explore the concept of vertical equity—appropriately accounting for relevant differences across individuals—which is a central component of fairness in many public policy settings. Applied to the design of the U.S. individual income tax system, vertical equity relates to the fair allocation of tax and enforcement burdens across taxpayers of different income levels. Through a unique collaboration with the Treasury Department and IRS, we use access to detailed, anonymized individual taxpayer microdata, risk-selected audits, and random audits from 2010-14 to study vertical equity in tax administration. In particular, we assess how the adoption of modern machine learning methods for selecting taxpayer audits may affect vertical equity. Our paper makes four contributions. First, we show how the adoption of more flexible machine learning (classification) methods—as opposed to simpler models—shapes vertical equity by shifting audit burdens from high to middle-income taxpayers. Second, given concerns about high audit rates of low-income taxpayers, we investigate how existing algorithmic fairness techniques would change the audit distribution. We find that such methods can mitigate some disparities across income buckets, but that these come at a steep cost to performance. Third, we show that the choice of whether to treat risk of underreporting as a classification or regression problem is highly consequential. Moving from a classification approach to a regression approach to predict the expected magnitude of underreporting shifts the audit burden substantially toward high income individuals, while increasing revenue. Last, we investigate the role of differential audit cost in shaping the distribution of audits. Audits of lower income taxpayers, for instance, are typically conducted by mail and hence pose much lower cost to the IRS. We show that a narrow focus on return-on-investment can undermine vertical equity. Our results have implications for ongoing policy debates and the design of algorithmic tools across the public sector.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1479–1503},
numpages = {25},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3533205,
author = {Neumann, Terrence and De-Arteaga, Maria and Fazelpour, Sina},
title = {Justice in Misinformation Detection Systems: An Analysis of Algorithms, Stakeholders, and Potential Harms},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533205},
doi = {10.1145/3531146.3533205},
abstract = {Faced with the scale and surge of misinformation on social media, many platforms and fact-checking organizations have turned to algorithms for automating key parts of misinformation detection pipelines. While offering a promising solution to the challenge of scale, the ethical and societal risks associated with algorithmic misinformation detection are not well-understood. In this paper, we employ and extend upon the notion of informational justice to develop a framework for explicating issues of justice relating to representation, participation, distribution of benefits and burdens, and credibility in the misinformation detection pipeline. Drawing on the framework: (1) we show how injustices materialize for stakeholders across three algorithmic stages in the pipeline; (2) we suggest empirical measures for assessing these injustices; and (3) we identify potential sources of these harms. This framework should help researchers, policymakers, and practitioners reason about potential harms or risks associated with these algorithms and provide conceptual guidance for the design of algorithmic fairness audits in this domain.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1504–1515},
numpages = {12},
keywords = {machine learning, informational justice, justice, algorithmic fairness, misinformation detection},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3533207,
author = {Klumbytundefined, Goda and Draude, Claude and Taylor, Alex S.},
title = {Critical Tools for Machine Learning: Working with Intersectional Critical Concepts in Machine Learning Systems Design},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533207},
doi = {10.1145/3531146.3533207},
abstract = {This paper investigates how intersectional critical theoretical concepts from social sciences and humanities research can be worked with in machine learning systems design. It does so by presenting a case study of a series of speculative design workshops, conducted in 2021. These workshops drew on intersectional feminist methodologies to construct interdisciplinary interventions in the design of machine learning systems, towards more inclusive, accountable, and contextualized systems design. The concepts of “situating/situated knowledges”, ""figuration"", ""diffraction"", and “critical fabulation/speculation” were taken up as theoretical and methodological tools for concept-led design workshops. This paper presents the design framework of the workshops and highlights tensions and possibilities with regards to interdisciplinary machine learning systems design towards more inclusive, contextualized, and accountable systems. It discusses the role that critical theoretical concepts can play in a design process and shows how such concepts can work as methodological tools that nonetheless require an open-ended experimental space to function. It presents insights and discussion points regarding what it means to work with critical intersectional knowledge that is inextricably connected to its historical and socio-political roots, and how this reframes what it might mean to design fair and accountable systems.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1528–1541},
numpages = {14},
keywords = {Intersectionality, Machine learning systems design, Experimental practice, Interdisciplinary methodologies, Feminist epistemologies},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3533209,
author = {Agarwal, Sushant and Deshpande, Amit},
title = {On the Power of Randomization in Fair Classification and Representation},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533209},
doi = {10.1145/3531146.3533209},
abstract = {Fair classification and fair representation learning are two important problems in supervised and unsupervised fair machine learning, respectively. Fair classification asks for a classifier that maximizes accuracy on a given data distribution subject to fairness constraints. Fair representation maps a given data distribution over the original feature space to a distribution over a new representation space such that all classifiers over the representation satisfy fairness. In this paper, we examine the power of randomization in both these problems to minimize the loss of accuracy that results when we impose fairness constraints. Previous work on fair classification has characterized the optimal fair classifiers on a given data distribution that maximize accuracy subject to fairness constraints, e.g., Demographic Parity (DP), Equal Opportunity (EO), and Predictive Equality (PE). We refine these characterizations to demonstrate when the optimal randomized fair classifiers can surpass their deterministic counterparts in accuracy. We also show how the optimal randomized fair classifier that we characterize can be obtained as a solution to a convex optimization problem. Recent work has provided techniques to construct fair representations for a given data distribution such that any classifier over this representation satisfies DP. However, the classifiers on these fair representations either come with no or weak accuracy guarantees when compared to the optimal fair classifier on the original data distribution. Extending our ideas for randomized fair classification, we improve on these works, and construct DP-fair, EO-fair, and PE-fair representations that have provably optimal accuracy and suffer no accuracy loss compared to the optimal DP-fair, EO-fair, and PE-fair classifiers respectively on the original data distribution.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1542–1551},
numpages = {10},
keywords = {randomization, classification, representation, fairness, equal opportunity, demographic parity, machine learning},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3533211,
author = {Almuzaini, Abdulaziz A. and Bhatt, Chidansh A. and Pennock, David M. and Singh, Vivek K.},
title = {ABCinML: Anticipatory Bias Correction in Machine Learning Applications},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533211},
doi = {10.1145/3531146.3533211},
abstract = {The idealization of a static machine-learned model, trained once and deployed forever, is not practical. As input distributions change over time, the model will not only lose accuracy, any constraints to reduce bias against a protected class may fail to work as intended. Thus, researchers have begun to explore ways to maintain algorithmic fairness over time. One line of work focuses on dynamic learning: retraining after each batch, and the other on robust learning which tries to make algorithms robust against all possible future changes. Dynamic learning seeks to reduce biases soon after they have occurred and robust learning often yields (overly) conservative models. We propose an anticipatory dynamic learning approach for correcting the algorithm to mitigate bias before it occurs. Specifically, we make use of anticipations regarding the relative distributions of population subgroups (e.g., relative ratios of male and female applicants) in the next cycle to identify the right parameters for an importance weighing fairness approach. Results from experiments over multiple real-world datasets suggest that this approach has promise for anticipatory bias correction.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1552–1560},
numpages = {9},
keywords = {algorithmic bias, classification, fairness},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3533212,
author = {Irion, Kristina},
title = {Algorithms Off-Limits? If Digital Trade Law Restricts Access to Source Code of Software Then Accountability Will Suffer},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533212},
doi = {10.1145/3531146.3533212},
abstract = {Free trade agreements are increasingly used to construct an additional layer of protection for source code of software. This comes in the shape of a new prohibition for governments to require access to, or transfer of, source code of software, subject to certain exceptions. A clause on software source code is also part and parcel of an ambitious set of new rules on trade-related aspects of electronic commerce currently negotiated by 86 members of the World Trade Organization. Our understanding to date of how such a commitment inside trade law impacts on governments right to regulate digital technologies and the policy space that is allowed under trade law is limited. Access to software source code is for example necessary to meet regulatory and judicial needs in order to ensure that digital technologies are in conformity with individuals’ human rights and societal values. This article will unpack and analyze the implications of such a source code clause for current and future digital policies by governments that aim to ensure transparency, fairness and accountability of computer and machine learning algorithms.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1561–1570},
numpages = {10},
keywords = {Fairness, Accountability, International trade law, Application Programming Interface, Transparency, Software, Computer algorithms, Source code, Digital policy},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3533216,
author = {Sachdeva, Pratik S. and Barreto, Renata and von Vacano, Claudia and Kennedy, Chris J.},
title = {Assessing Annotator Identity Sensitivity via Item Response Theory: A Case Study in a Hate Speech Corpus},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533216},
doi = {10.1145/3531146.3533216},
abstract = {Content Warning: This paper contains content considered profane, hateful, and offensive. Annotators, by labeling data samples, play an essential role in the production of machine learning datasets. Their role is increasingly prevalent for more complex tasks such as hate speech or disinformation classification, where labels may be particularly subjective, as evidenced by low inter-annotator agreement statistics. Annotators may exhibit observable differences in their labeling patterns when grouped by their self-reported demographic identities, such as race, gender, etc. We frame these patterns as annotator identity sensitivities, referring to an annotator’s increased likelihood of assigning a particular label on a data sample, conditional on a self-reported identity group. We purposefully refrain from using the term annotator bias, which we argue is problematic terminology in such subjective scenarios. Since annotator identity sensitivities can play a role in the patterns learned by machine learning algorithms, quantifying and characterizing them is of paramount importance for fairness and accountability in machine learning. In this work, we utilize item response theory (IRT), a methodological approach developed for measurement theory, to quantify annotator identity sensitivity. IRT models can be constructed to incorporate diverse factors that influence a label on a specific data sample, such as the data sample itself, the annotator, and the labeling instrument’s wording and response options. An IRT model captures the contributions of these facets to the label via a latent-variable probabilistic model, thereby allowing the direct quantification of annotator sensitivity. As a case study, we examine a hate speech corpus containing over 50,000 social media comments from Reddit, YouTube, and Twitter, rated by 10,000 annotators on 10 components of hate speech (e.g., sentiment, respect, violence, dehumanization, etc.). We leverage three different IRT techniques which are complementary in that they quantify sensitivity from different perspectives: separated measurements, annotator-level interactions, and group-level interactions. We use these techniques to assess whether an annotator’s racial identity is associated with their ratings on comments that target different racial identities. We find that, after controlling for the estimated hatefulness of social media comments, annotators tended to be more sensitive when rating comments targeting a group they identify with. Specifically, annotators were more likely to rate comments targeting their own racial identity as possessing elements of hate speech. Our results identify a correspondence between annotator identity and the target identity of hate speech comments, and provide a set of tools that can assess annotator identity sensitivity in machine learning datasets at large.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1585–1603},
numpages = {19},
keywords = {differential rater functioning, item response theory, annotation, hate speech, annotator sensitivity},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3533218,
author = {Schoeffer, Jakob and Kuehl, Niklas and Machowski, Yvette},
title = {“There Is Not Enough Information”: On the Effects of Explanations on Perceptions of Informational Fairness and Trustworthiness in Automated Decision-Making},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533218},
doi = {10.1145/3531146.3533218},
abstract = {Automated decision systems (ADS) are increasingly used for consequential decision-making. These systems often rely on sophisticated yet opaque machine learning models, which do not allow for understanding how a given decision was arrived at. In this work, we conduct a human subject study to assess people’s perceptions of informational fairness (i.e., whether people think they are given adequate information on and explanation of the process and its outcomes) and trustworthiness of an underlying ADS when provided with varying types of information about the system. More specifically, we instantiate an ADS in the area of automated loan approval and generate different explanations that are commonly used in the literature. We randomize the amount of information that study participants get to see by providing certain groups of people with the same explanations as others plus additional explanations. From our quantitative analyses, we observe that different amounts of information as well as people’s (self-assessed) AI literacy significantly influence the perceived informational fairness, which, in turn, positively relates to perceived trustworthiness of the ADS. A comprehensive analysis of qualitative feedback sheds light on people’s desiderata for explanations, among which are (i) consistency (both with people’s expectations and across different explanations), (ii) disclosure of monotonic relationships between features and outcome, and (iii) actionability of recommendations.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1616–1628},
numpages = {13},
keywords = {informational fairness, trustworthiness, Automated decision-making, machine learning, explanations, perceptions},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3533219,
author = {Brubach, Brian and Ballarin, Audrey and Nazeer, Heeba},
title = {Characterizing Properties and Trade-Offs of Centralized Delegation Mechanisms in Liquid Democracy},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533219},
doi = {10.1145/3531146.3533219},
abstract = {Liquid democracy is a form of transitive delegative democracy that has received a flurry of scholarly attention from the computer science community in recent years. In its simplest form, every agent starts with one vote and may have other votes assigned to them via delegation from other agents. They can choose to delegate all votes assigned to them to another agent or vote directly with all votes assigned to them. However, many proposed realizations of liquid democracy allow for agents to express their delegation/voting preferences in more complex ways (e.g., a ranked list of potential delegates) and employ a centralized delegation mechanism to compute the final vote tally. In doing so, centralized delegation mechanisms can make decisions that affect the outcome of a vote and where/whether agents are able to delegate their votes. Much of the analysis thus far has focused on the ability of these mechanisms to make a correct choice. We extend this analysis by introducing and formalizing other important properties of a centralized delegation mechanism in liquid democracy with respect to crucial features such as accountability, transparency, explainability, fairness, and user agency. In addition, we evaluate existing methods in terms of these properties, show how some prior work can be augmented to achieve desirable properties, prove impossibility results for achieving certain sets of properties simultaneously, and highlight directions for future work.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1629–1638},
numpages = {10},
keywords = {accountability, voting, fairness, computational social choice, liquid democracy, transparency},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3533221,
author = {Donahue, Kate and Chouldechova, Alexandra and Kenthapadi, Krishnaram},
title = {Human-Algorithm Collaboration: Achieving Complementarity and Avoiding Unfairness},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533221},
doi = {10.1145/3531146.3533221},
abstract = {Much of machine learning research focuses on predictive accuracy: given a task, create a machine learning model (or algorithm) that maximizes accuracy. In many settings, however, the final prediction or decision of a system is under the control of a human, who uses an algorithm’s output along with their own personal expertise in order to produce a combined prediction. One ultimate goal of such collaborative systems is complementarity: that is, to produce lower loss (equivalently, greater payoff or utility) than either the human or algorithm alone. However, experimental results have shown that even in carefully-designed systems, complementary performance can be elusive. Our work provides three key contributions. First, we provide a theoretical framework for modeling simple human-algorithm systems and demonstrate that multiple prior analyses can be expressed within it. Next, we use this model to prove conditions where complementarity is impossible, and give constructive examples of where complementarity is achievable. Finally, we discuss the implications of our findings, especially with respect to the fairness of a classifier. In sum, these results deepen our understanding of key factors influencing the combined performance of human-algorithm systems, giving insight into how algorithmic tools can best be designed for collaborative environments.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1639–1656},
numpages = {18},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3287560.3287586,
author = {Celis, L. Elisa and Huang, Lingxiao and Keswani, Vijay and Vishnoi, Nisheeth K.},
title = {Classification with Fairness Constraints: A Meta-Algorithm with Provable Guarantees},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287586},
doi = {10.1145/3287560.3287586},
abstract = {Developing classification algorithms that are fair with respect to sensitive attributes of the data is an important problem due to the increased deployment of classification algorithms in societal contexts. Several recent works have focused on studying classification with respect to specific fairness metrics, modeled the corresponding fair classification problem as constrained optimization problems, and developed tailored algorithms to solve them. Despite this, there still remain important metrics for which there are no fair classifiers with theoretical guarantees; primarily because the resulting optimization problem is non-convex. The main contribution of this paper is a meta-algorithm for classification that can take as input a general class of fairness constraints with respect to multiple non-disjoint and multi-valued sensitive attributes, and which comes with provable guarantees. In particular, our algorithm can handle non-convex ""linear fractional"" constraints (which includes fairness constraints such as predictive parity) for which no prior algorithm was known. Key to our results is an algorithm for a family of classification problems with convex constraints along with a reduction from classification problems with linear fractional constraints to this family. Empirically, we observe that our algorithm is fast, can achieve near-perfect fairness with respect to various fairness metrics, and the loss in accuracy due to the imposed fairness constraints is often small.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {319–328},
numpages = {10},
keywords = {Algorithmic Fairness, Classification},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}"
"@inproceedings{10.1145/3531146.3533226,
author = {Andrus, McKane and Villeneuve, Sarah},
title = {Demographic-Reliant Algorithmic Fairness: Characterizing the Risks of Demographic Data Collection in the Pursuit of Fairness},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533226},
doi = {10.1145/3531146.3533226},
abstract = {Most proposed algorithmic fairness techniques require access to demographic data in order to make performance comparisons and standardizations across groups, however this data is largely unavailable in practice, hindering the widespread adoption of algorithmic fairness. Through this paper, we consider calls to collect more data on demographics to enable algorithmic fairness and challenge the notion that discrimination can be overcome with smart enough technical methods and sufficient data. We show how these techniques largely ignore broader questions of data governance and systemic oppression when categorizing individuals for the purpose of fairer algorithmic processing. In this work, we explore under what conditions demographic data should be collected and used to enable algorithmic fairness methods by characterizing a range of social risks to individuals and communities. For the risks to individuals we consider the unique privacy risks of sensitive attributes, the possible harms of miscategorization and misrepresentation, and the use of sensitive data beyond data subjects’ expectations. Looking more broadly, the risks to entire groups and communities include the expansion of surveillance infrastructure in the name of fairness, misrepresenting and mischaracterizing what it means to be part of a demographic group, and ceding the ability to define what constitutes biased or unfair treatment. We argue that, by confronting these questions before and during the collection of demographic data, algorithmic fairness methods are more likely to actually mitigate harmful treatment disparities without reinforcing systems of oppression. Towards this end, we assess privacy-focused methods of data collection and use and participatory data governance structures as proposals for more responsibly collecting demographic data.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1709–1721},
numpages = {13},
keywords = {categorization, demographic data, measurement, race, sensitive data, gender, identity, discrimination, fairness, sexuality},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3533228,
author = {Abebe, Rediet and Hardt, Moritz and Jin, Angela and Miller, John and Schmidt, Ludwig and Wexler, Rebecca},
title = {Adversarial Scrutiny of Evidentiary Statistical Software},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533228},
doi = {10.1145/3531146.3533228},
abstract = {The U.S. criminal legal system increasingly relies on software output to convict and incarcerate people. In a large number of cases each year, the government makes these consequential decisions based on evidence from statistical software—such as probabilistic genotyping, environmental audio detection and toolmark analysis tools—that the defense counsel cannot fully cross-examine or scrutinize. This undermines the commitments of the adversarial criminal legal system, which relies on the defense’s ability to probe and test the prosecution’s case to safeguard individual rights. Responding to this need to adversarially scrutinize output from such software, we propose robust adversarial testing as a framework to examine the validity of evidentiary statistical software. We define and operationalize this notion of robust adversarial testing for defense use by drawing on a large body of recent work in robust machine learning and algorithmic fairness. We demonstrate how this framework both standardizes the process for scrutinizing such tools and empowers defense lawyers to examine their validity for instances most relevant to the case at hand. We further discuss existing structural and institutional challenges within the U.S. criminal legal system which may create barriers for implementing this framework and close with a discussion on policy changes that could help address these concerns.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1733–1746},
numpages = {14},
keywords = {robust machine learning, evidentiary software, statistical software, black-box software, adversarial scrutiny},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3533230,
author = {Reader, Lydia and Nokhiz, Pegah and Power, Cathleen and Patwari, Neal and Venkatasubramanian, Suresh and Friedler, Sorelle},
title = {Models for Understanding and Quantifying Feedback in Societal Systems},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533230},
doi = {10.1145/3531146.3533230},
abstract = {When it comes to long-term fairness in decision-making settings, many studies have focused on closed systems with a specific appointed decision-maker and certain engagement rules in place. However, if the objective is to achieve equity in a broader societal system, studying the system in isolation is insufficient. In a societal system, neither a singular decision maker nor defined agent behavior rules exist. Additionally, analysis of societal systems can be complicated by the presence of feedback, in which historical and current inequities influence future inequity. In this paper, we present a model to quantify feedback in social systems so that the long-term effects of a policy or decision process may be investigated, even when the feedback mechanisms are not individually characterized. We explore the dynamics of real social systems and find that many examples of feedback are qualitatively similar in their temporal characteristics. Using a key idea in linear systems theory, namely proportional-integral-derivative (PID) feedback, we propose a model to quantify three types of feedback. We illustrate how different components of the PID capture analogous aspects of societal dynamics such as the persistence of current inequity, the cumulative effects of long-term inequity, and the response to the speed at which society is changing. Our model does not attempt to describe underlying systems or capture individual actions. It is a system-based approach to study inequity in feedback loops, and as a result unlocks a direction to study social systems that would otherwise be almost impossible to model and can only be observed. Our framework helps elucidate the ability of fair policies to produce and sustain equity in the long-term.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1765–1775},
numpages = {11},
keywords = {inequity, feedback, societal systems},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3533236,
author = {Grabowicz, Przemyslaw A. and Perello, Nicholas and Mishra, Aarshee},
title = {Marrying Fairness and Explainability in Supervised Learning},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533236},
doi = {10.1145/3531146.3533236},
abstract = {Machine learning algorithms that aid human decision-making may inadvertently discriminate against certain protected groups. Therefore, we formalize direct discrimination as a direct causal effect of the protected attributes on the decisions, while induced discrimination as a change in the causal influence of non-protected features associated with the protected attributes. The measurements of marginal direct effect (MDE) and SHapley Additive exPlanations (SHAP) reveal that state-of-the-art fair learning methods can induce discrimination via association or reverse discrimination in synthetic and real-world datasets. To inhibit discrimination in algorithmic systems, we propose to nullify the influence of the protected attribute on the output of the system, while preserving the influence of remaining features. We introduce and study post-processing methods achieving such objectives, finding that they yield relatively high model accuracy, prevent direct discrimination, and diminishes various disparity measures, e.g., demographic disparity.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1905–1916},
numpages = {12},
keywords = {algorithmic fairness, supervised learning, machine learning, explainability, discrimination},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3533237,
author = {Ramesh, Divya and Kameswaran, Vaishnav and Wang, Ding and Sambasivan, Nithya},
title = {How Platform-User Power Relations Shape Algorithmic Accountability: A Case Study of Instant Loan Platforms and Financially Stressed Users in India},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533237},
doi = {10.1145/3531146.3533237},
abstract = {Accountability, a requisite for responsible AI, can be facilitated through transparency mechanisms such as audits and explainability. However, prior work suggests that the success of these mechanisms may be limited to Global North contexts; understanding the limitations of current interventions in varied socio-political conditions is crucial to help policymakers facilitate wider accountability. To do so, we examined the mediation of accountability in the existing interactions between vulnerable users and a ‘high-risk’ AI system in a Global South setting. We report on a qualitative study with 29 financially-stressed users of instant loan platforms in India. We found that users experienced intense feelings of indebtedness for the ‘boon’ of instant loans, and perceived huge obligations towards loan platforms. Users fulfilled obligations by accepting harsh terms and conditions, over-sharing sensitive data, and paying high fees to unknown and unverified lenders. Users demonstrated a dependence on loan platforms by persisting with such behaviors despite risks of harms such as abuse, recurring debts, discrimination, privacy harms, and self-harm to them. Instead of being enraged with loan platforms, users assumed responsibility for their negative experiences, thus releasing the high-powered loan platforms from accountability obligations. We argue that accountability is shaped by platform-user power relations, and urge caution to policymakers in adopting a purely technical approach to fostering algorithmic accountability. Instead, we call for situated interventions that enhance agency of users, enable meaningful transparency, reconfigure designer-user relations, and prompt a critical reflection in practitioners towards wider accountability. We conclude with implications for responsibly deploying AI in FinTech applications in India and beyond.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1917–1928},
numpages = {12},
keywords = {socio-technical systems, instant loans, human-ai interaction, algorithmic fairness, algorithmic accountability},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3533238,
author = {Patro, Gourab K. and Porcaro, Lorenzo and Mitchell, Laura and Zhang, Qiuyue and Zehlike, Meike and Garg, Nikhil},
title = {Fair Ranking: A Critical Review, Challenges, and Future Directions},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533238},
doi = {10.1145/3531146.3533238},
abstract = {Ranking, recommendation, and retrieval systems are widely used in online platforms and other societal systems, including e-commerce, media-streaming, admissions, gig platforms, and hiring. In the recent past, a large “fair ranking” research literature has been developed around making these systems fair to the individuals, providers, or content that are being ranked. Most of this literature defines fairness for a single instance of retrieval, or as a simple additive notion for multiple instances of retrievals over time. This work provides a critical overview of this literature, detailing the often context-specific concerns that such approaches miss: the gap between high ranking placements and true provider utility, spillovers and compounding effects over time, induced strategic incentives, and the effect of statistical uncertainty. We then provide a path forward for a more holistic and impact-oriented fair ranking research agenda, including methodological lessons from other fields and the role of the broader stakeholder community in overcoming data bottlenecks and designing effective regulatory environments.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1929–1942},
numpages = {14},
keywords = {Exposure, Recommendation, Algorithmic Impact Assessment, Ranking, Fairness, Strategic Behaviour},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3533241,
author = {Gansky, Ben and McDonald, Sean},
title = {CounterFAccTual: How FAccT Undermines Its Organizing Principles},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533241},
doi = {10.1145/3531146.3533241},
abstract = {This essay joins recent scholarship in arguing that FAccT's fundamental framing of the potential to achieve the normative conditions for justice through bettering the design of algorithmic systems is counterproductive to achieving said justice in practice. Insofar as the FAccT community's research tends to prioritize design-stage interventions, it ignores the fact that the majority of the contextual factors that practically determine FAccT outcomes happen in the implementation and impact stages of AI/ML lifecycles.We analyze an emergent and widely-cited movement within the FAccT community for attempting to honor the centrality of contextual factors in shaping social outcomes, a set of strategies we term ‘metadata maximalism’. Symptomatic of design-centered approaches, metadata maximalism abstracts away its reliance on institutions and structures of justice that are, by every observable metric, already struggling (where not failing) to provide accessible, enforceable rights. These justice infrastructures, moreover, are currently wildly under-equipped to manage the disputes arising from digital transformation and machine learning. The political economy of AI/ML implementation provides further obstructions to realizing rights. Data and software supply chains, in tandem with intellectual property protections, introduce structural sources of opacity. Where duties of care to vulnerable persons should reign, profit incentives are given legal and regulatory primacy. Errors are inevitable and inextricable from the development of machine learning systems.In the face of these realities, FAccT programs, including metadata maximalism, tend to project their efforts in a fundamentally counter-factual universe: one in which functioning institutions and processes for due diligence in implementation and for redress of harms are working and ready to interoperate with. Unfortunately, in our world, these institutions and processes have been captured by the interests they are meant to hold accountable, intentionally hollowed-out, and/or were never designed to function in today's sociotechnical landscape. Continuing to produce (fair! accountable! transparent!) data-enabled systems that operate in high-impact areas, irrespective of this landscape's radically insufficient paths to justice, given the unavoidability of errors and/or intentional misuse in implementation, and the exhaustively-demonstrated disproportionate distribution of resulting harms onto already-marginalized communities, is a choice - a choice to be CounterFAccTual.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1982–1992},
numpages = {11},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3533242,
author = {Tschantz, Michael Carl},
title = {What is Proxy Discrimination?},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533242},
doi = {10.1145/3531146.3533242},
abstract = {The near universal condemnation of proxy discrimination hides a disagreement over what it is. This work surveys various notions of proxy and proxy discrimination found in prior work and represents them in a common framework. These notions variously turn on statistical dependencies, causal effects, and intentions. It discusses the limitations and uses of each notation and of the concept as a whole.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1993–2003},
numpages = {11},
keywords = {discrimination, proxy, conceptual analysis},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3533243,
author = {Cousins, Cyrus},
title = {Uncertainty and the Social Planner’s Problem: Why Sample Complexity Matters},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533243},
doi = {10.1145/3531146.3533243},
abstract = {Welfare measures overall utility across a population, whereas malfare measures overall disutility, and the social planner’s problem can be cast either as maximizing the former or minimizing the latter. We show novel bounds on the expectations and tail probabilities of estimators of welfare, malfare, and regret of per-group (dis)utility values, where estimates are made from a finite sample drawn from each group. In particular, we consider estimating these quantities for individual functions (e.g., allocations or classifiers) with standard probabilistic bounds, and optimizing and bounding generalization error over hypothesis classes (i.e., we quantify overfitting) using Rademacher averages. We then study algorithmic fairness through the lens of sample complexity, finding that because marginalized or minority groups are often understudied, and fewer data are therefore available, the social planner is more likely to overfit to these groups, thus even models that seem fair in training can be systematically biased against such groups. We argue that this effect can be mitigated by ensuring sufficient sample sizes for each group, and our sample complexity analysis characterizes these sample sizes. Motivated by these conclusions, we present progressive sampling algorithms to efficiently optimize various fairness objectives.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {2004–2015},
numpages = {12},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3533245,
author = {Loi, Michele and Heitz, Christoph},
title = {Is Calibration a Fairness Requirement? An Argument from the Point of View of Moral Philosophy and Decision Theory},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533245},
doi = {10.1145/3531146.3533245},
abstract = {In this paper, we provide a moral analysis of two criteria of statistical fairness debated in the machine learning literature: 1) calibration between groups and 2) equality of false positive and false negative rates between groups. In our paper, we focus on moral arguments in support of either measure. The conflict between group calibration vs. false positive and false negative rate equality is one of the core issues in the debate about group fairness definitions among practitioners. For any thorough moral analysis, the meaning of the term “fairness” has to be made explicit and defined properly. For our paper, we equate fairness with (non-)discrimination, which is a legitimate understanding in the discussion about group fairness. More specifically, we equate it with “prima facie wrongful discrimination” in the sense this is used in Prof. Lippert-Rasmussen's treatment of this definition. In this paper, we argue that a violation of group calibration may be unfair in some cases, but not unfair in others. Our argument analyzes in great detail two specific hypothetical examples of usage of predictions in decision making. The most important practical implication is that between-group calibration is defensible as a bias standard in some cases but not others; we show this by referring to examples in which the violation of between-group calibration is discriminatory, and others in which it is not. This is in line with claims already advanced in the literature, that algorithmic fairness should be defined in a way that is sensitive to context. The most important practical implication is that arguments based on examples in which fairness requires between-group calibration, or equality in the false-positive/false-negative rates, do no generalize. For it may be that group calibration is a fairness requirement in one case, but not in another.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {2026–2034},
numpages = {9},
keywords = {fairness, opportunity, equalized odds, calibration, prediction},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3534632,
author = {Sharaf, Amr and Daume III, Hal and Ni, Renkun},
title = {Promoting Fairness in Learned Models by Learning to Active Learn under Parity Constraints},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3534632},
doi = {10.1145/3531146.3534632},
abstract = {Machine learning models can have consequential effects when used to automate decisions, and disparities between groups of people in the error rates of those decisions can lead to harms suffered more by some groups than others. Past algorithmic approaches aim to enforce parity across groups given a fixed set of training data; instead, we ask: what if we can gather more data to mitigate disparities? We develop a meta-learning algorithm for parity-constrained active learning that learns a policy to decide which labels to query so as to maximize accuracy subject to parity constraints. To optimize the active learning policy, our proposed algorithm formulates the parity-constrained active learning task as a bi-level optimization problem. The inner level corresponds to training a classifier on a subset of labeled examples. The outer level corresponds to updating the selection policy choosing this subset to achieve a desired fairness and accuracy behavior on the trained classifier. To solve this constrained bi-level optimization problem, we employ the Forward-Backward Splitting optimization method. Empirically, across several parity metrics and classification tasks, our approach outperforms alternatives by a large margin.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {2149–2156},
numpages = {8},
keywords = {active learning, meta-learning},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3534633,
author = {Usunier, Nicolas and Do, Virginie and Dohmatob, Elvis},
title = {Fast Online Ranking with Fairness of Exposure},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3534633},
doi = {10.1145/3531146.3534633},
abstract = {As recommender systems become increasingly central for sorting and prioritizing the content available online, they have a growing impact on the opportunities or revenue of their items producers. For instance, they influence which recruiter a resume is recommended to, or to whom and how much a music track, video or news article is being exposed. This calls for recommendation approaches that not only maximize (a proxy of) user satisfaction, but also consider some notion of fairness in the exposure of items or groups of items. Formally, such recommendations are usually obtained by maximizing a concave objective function in the space of randomized rankings. When the total exposure of an item is defined as the sum of its exposure over users, the optimal rankings of every users become coupled, which makes the optimization process challenging. Existing approaches to find these rankings either solve the global optimization problem in a batch setting, i.e., for all users at once, which makes them inapplicable at scale, or are based on heuristics that have weak theoretical guarantees. In this paper, we propose the first efficient online algorithm to optimize concave objective functions in the space of rankings which applies to every concave and smooth objective function, such as the ones found for fairness of exposure. Based on online variants of the Frank-Wolfe algorithm, we show that our algorithm is computationally fast, generating rankings on-the-fly with computation cost dominated by the sort operation, memory efficient, and has strong theoretical guarantees. Compared to baseline policies that only maximize user-side performance, our algorithm allows to incorporate complex fairness of exposure criteria in the recommendations with negligible computational overhead. We present experiments on artificial music and movie recommendation tasks using Last.fm and MovieLens datasets which suggest that in practice, the algorithm rapidly reaches good performances on three different objectives representing different fairness of exposure criteria.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {2157–2167},
numpages = {11},
keywords = {fairness, online ranking, recommender systems},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3534635,
author = {Schw\""{o}bel, Pola and Remmers, Peter},
title = {The Long Arc of Fairness: Formalisations and Ethical Discourse},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3534635},
doi = {10.1145/3531146.3534635},
abstract = {In recent years, the idea of formalising and modelling fairness for algorithmic decision making (ADM) has advanced to a point of sophisticated specialisation. However, the relations between technical (formalised) and ethical discourse on fairness are not always clear and productive. Arguing for an alternative perspective, we review existing fairness metrics and discuss some common issues. For instance, the fairness of procedures and distributions is often formalised and discussed statically, disregarding both structural preconditions of the status quo and downstream effects of a given intervention. We then introduce dynamic fairness modelling, a more comprehensive approach that realigns formal fairness metrics with arguments from the ethical discourse. A dynamic fairness model incorporates (1) ethical goals, (2) formal metrics to quantify decision procedures and outcomes and (3) mid-term or long-term downstream effects. By contextualising these elements of fairness-related processes, dynamic fairness modelling explicates formerly latent ethical aspects and thereby provides a helpful tool to navigate trade-offs between different fairness interventions. To illustrate the framework, we discuss an example application – the current European efforts to increase the number of women on company boards, e .g.  via quota solutions – and present early technical work that fits within our framework.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {2179–2188},
numpages = {10},
keywords = {dynamic fairness modelling, algorithmic fairness, algorithmic decision making, fairness metrics, ethics of machine learning},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3534641,
author = {Ghazimatin, Azin and Kleindessner, Matthaus and Russell, Chris and Abedjan, Ziawasch and Golebiowski, Jacek},
title = {Measuring Fairness of Rankings under Noisy Sensitive Information},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3534641},
doi = {10.1145/3531146.3534641},
abstract = {Metrics commonly used to assess group fairness in ranking require the knowledge of group membership labels (e.g., whether a job applicant is male or female). Obtaining accurate group membership labels, however, may be costly, operationally difficult, or even infeasible. Where it is not possible to obtain these labels, one common solution is to use proxy labels in their place, which are typically predicted by machine learning models. Proxy labels are susceptible to systematic biases, and using them for fairness estimation can thus lead to unreliable assessments. We investigate the problem of measuring group fairness in ranking for a suite of divergence-based metrics in the presence of proxy labels. We show that under certain assumptions, fairness of a ranking can reliably be measured from the proxy labels. We formalize two assumptions and provide a theoretical analysis for each showing how the true metric values can be derived from the estimates based on proxy labels. We prove that without such assumptions fairness assessment based on proxy labels is impossible. Through extensive experiments on both synthetic and real datasets, we demonstrate the effectiveness of our proposed methods for recovering reliable fairness assessments in rankings.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {2263–2279},
numpages = {17},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3534643,
author = {Vigan\`{o}, Eleonora and Hertweck, Corinna and Heitz, Christoph and Loi, Michele},
title = {People Are Not Coins: Morally Distinct Types of Predictions Necessitate Different Fairness Constraints},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3534643},
doi = {10.1145/3531146.3534643},
abstract = {In a recent paper [1], Brian Hedden has argued that most of the group fairness constraints discussed in the machine learning literature are not necessary conditions for the fairness of predictions, and hence that there are no genuine fairness metrics. This is proven by discussing a special case of a fair prediction. In our paper, we show that Hedden's argument does not hold for the most common kind of predictions used in data science, which are about people and based on data from similar people; we call these “human-group-based practices.” We argue that there is a morally salient distinction between human-group-based practices and those that are based on data of only one person, which we call “human-individual-based practices.” Thus, what may be a necessary condition for the fairness of human-group-based practices may not be a necessary condition for the fairness of human-individual-based practices, on which Hedden's argument is based. Accordingly, the group fairness metrics discussed in the machine learning literature may still be relevant for most applications of prediction-based decision making.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {2293–2301},
numpages = {9},
keywords = {fairness metrics, fair prediction, algorithmic decision making, moral principles},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3534644,
author = {Pastaltzidis, Ioannis and Dimitriou, Nikolaos and Quezada-Tavarez, Katherine and Aidinlis, Stergios and Marquenie, Thomas and Gurzawska, Agata and Tzovaras, Dimitrios},
title = {Data Augmentation for Fairness-Aware Machine Learning: Preventing Algorithmic Bias in Law Enforcement Systems},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3534644},
doi = {10.1145/3531146.3534644},
abstract = {Researchers and practitioners in the fairness community have highlighted the ethical and legal challenges of using biased datasets in data-driven systems, with algorithmic bias being a major concern. Despite the rapidly growing body of literature on fairness in algorithmic decision-making, there remains a paucity of fairness scholarship on machine learning algorithms for the real-time detection of crime. This contribution presents an approach for fairness-aware machine learning to mitigate the algorithmic bias / discrimination issues posed by the reliance on biased data when building law enforcement technology. Our analysis is based on RWF-2000, which has served as the basis for violent activity recognition tasks in data-driven law enforcement projects. We reveal issues of overrepresentation of minority subjects in violence situations that limit the external validity of the dataset for real-time crime detection systems and propose data augmentation techniques to rebalance the dataset. The experiments on real world data show the potential to create more balanced datasets by synthetically generated samples, thus mitigating bias and discrimination concerns in law enforcement applications.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {2302–2314},
numpages = {13},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3534645,
author = {Baumann, Joachim and Hann\'{a}k, Anik\'{o} and Heitz, Christoph},
title = {Enforcing Group Fairness in Algorithmic Decision Making: Utility Maximization Under Sufficiency},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3534645},
doi = {10.1145/3531146.3534645},
abstract = {Binary decision making classifiers are not fair by default. Fairness requirements are an additional element to the decision making rationale, which is typically driven by maximizing some utility function. In that sense, algorithmic fairness can be formulated as a constrained optimization problem. This paper contributes to the discussion on how to implement fairness, focusing on the fairness concepts of positive predictive value (PPV) parity, false omission rate (FOR) parity, and sufficiency (which combines the former two). We show that group-specific threshold rules are optimal for PPV parity and FOR parity, similar to well-known results for other group fairness criteria. However, depending on the underlying population distributions and the utility function, we find that sometimes an upper-bound threshold rule for one group is optimal: utility maximization under PPV parity (or FOR parity) might thus lead to selecting the individuals with the smallest utility for one group, instead of selecting the most promising individuals. This result is counter-intuitive and in contrast to the analogous solutions for statistical parity and equality of opportunity. We also provide a solution for the optimal decision rules satisfying the fairness constraint sufficiency. We show that more complex decision rules are required and that this leads to within-group unfairness for all but one of the groups. We illustrate our findings based on simulated and real data.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {2315–2326},
numpages = {12},
keywords = {algorithmic fairness, prediction-based decision making, machine learning, sufficiency, group fairness metrics, constrained utility optimization, fairness trade-offs},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3531146.3534646,
author = {Cruz Cort\'{e}s, Efr\'{e}n and Rajtmajer, Sarah and Ghosh, Debashis},
title = {Locality of Technical Objects and the Role of Structural Interventions for Systemic Change},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3534646},
doi = {10.1145/3531146.3534646},
abstract = {Technical objects, like algorithms, exhibit causal capacities both in terms of their internal makeup and the position they occupy in relation to other objects and processes within a system. At the same time, systems encompassing technical objects interact with other systems themselves, producing a multi-scale structural composition. In the framework of fair artificial intelligence, typical causal inference interventions focus on the internal workings of technical objects (fairness constraints), and often forsake structural properties of the system. However, these interventions are often not sufficient to capture forms of discrimination and harm at a systemic level. To complement this approach we introduce the notion of locality and define structural interventions. We compare the effect of structural interventions on a system compared to local, structure-preserving interventions on technical objects. We focus on comparing interventions on generating mechanisms (representing social dynamics giving rise to discrimination) with constraining algorithms to satisfy some measure of fairness. This framework allows us to identify bias outside the algorithmic stage and propose joint interventions on social dynamics and algorithm design. We show how, for a model of financial lending, structural interventions can drive the system towards equality even when algorithmic interventions are unable to do so. This suggests that the responsibility of decision makers extends beyond ensuring that local fairness metrics are satisfied to an ecosystem that fosters equity for all.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {2327–2341},
numpages = {15},
keywords = {system dynamics, fairness, lending, sociotechnical systems, ethics, interventions},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
  
"
"@inproceedings{10.1145/3306618.3314309,
author = {Abdollahpouri, Himan},
title = {Popularity Bias in Ranking and Recommendation},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314309},
doi = {10.1145/3306618.3314309},
abstract = {Many recommender systems suffer from popularity bias: popular items are recommended frequently while less popular, niche products, are recommended rarely or not at all. However, recommending the ignored products in the ""long tail"" is critical for businesses as they are less likely to be discovered. Popularity bias is also against social justice where the entities need to have a fair chance of being served and represented. In this work, I investigate the problem of popularity bias in recommender systems and develop algorithms to address this problem.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {529–530},
numpages = {2},
keywords = {information retrieval, ranking, popularity bias, recommender systems},
location = {Honolulu, HI, USA},
series = {AIES '19}
}
  
"
"@inproceedings{10.1145/3306618.3314310,
author = {Coston, Amanda},
title = {Risk Assessments and Fairness Under Missingness and Confounding},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314310},
doi = {10.1145/3306618.3314310},
abstract = {Fairness in machine learning has become a significant area of research as risk assessments and other algorithmic decision-making systems are increasingly used in high-stakes applications such as criminal justice, consumer lending, and child welfare screening decisions. Two significant challenges to achieving fair decision-making systems are 1) access to the protected attribute may be limited and 2) the outcome may be confounded or selectively observed depending on the historical data generating process. To address the former challenge, we propose two methods for overcoming limited access to the protected attribute and empirically evaluate their success on three datasets. To address the later challenge, we develop counterfactual risk assessments that account for the effect of historical interventions on the outcome. We analyze the performance of our counterfactual risk assessments in criminal sentencing decisions in Pennsylvania. We compare our model against observational risk assessments.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {531},
numpages = {1},
keywords = {decision-making, fairness, machine learning, risk assessments},
location = {Honolulu, HI, USA},
series = {AIES '19}
}
  
"
"@inproceedings{10.1145/3306618.3314316,
author = {Sokol, Kacper},
title = {Fairness, Accountability and Transparency in Artificial Intelligence: A Case Study of Logical Predictive Models},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314316},
doi = {10.1145/3306618.3314316},
abstract = {Machine learning -- the part of artificial intelligence aimed at eliciting knowledge from data and automated decision making without explicit instructions -- is making great strides, with new algorithms being invented every day. These algorithms find myriads of applications, but their ubiquity often comes at the expense of limited interpretability, hidden biases and unexpected vulnerabilities. Whenever one of these factors is a priority, the learning algorithm of choice is often a method considered to be inherently interpretable, e.g. logical models such as decision trees. In my research I challenge this assumption and highlight (quite common) cases when the assumed interpretability fails to deliver. To restore interpretability of logical machine learning models (decision trees and their ensembles in particular) I propose to explain them with class-contrastive counterfactual statements, which are a very common type of explanation in human interactions, well-grounded in social science research. To evaluate transparency of such models I collate explainability desiderata that can be used to systematically assess and compare such methods as an addition to user studies. Given contrastive explanations, I investigate their influence on the model's security, in particular gaming and stealing the model. Finally, I evaluate model fairness, where I am interested in choosing the most fair model among all the models with equal performance.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {541–542},
numpages = {2},
keywords = {accountability, transparency, logical models, fairness},
location = {Honolulu, HI, USA},
series = {AIES '19}
}
  
"
"@inproceedings{10.1145/3375627.3377141,
author = {Neff, Gina},
title = {From Bad Users and Failed Uses to Responsible Technologies: A Call to Expand the AI Ethics Toolkit},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3377141},
doi = {10.1145/3375627.3377141},
abstract = {Recent advances in artificial intelligence applications have sparked scholarly and public attention to the challenges of the ethical design of technologies. These conversations about ethics have been targeted largely at technology designers and concerned with helping to inform building better and fairer AI tools and technologies. This approach, however, addresses only a small part of the problem of responsible use and will not be adequate for describing or redressing the problems that will arise as more types of AI technologies are more widely used.Many of the tools being developed today have potentially enormous and historic impacts on how people work, how society organises, stores and distributes information, where and how people interact with one another, and how people's work is valued and compensated. And yet, our ethical attention has looked at a fairly narrow range of questions about expanding the access to, fairness of, and accountability for existing tools. Instead, I argue that scholars should develop much broader questions of about the reconfiguration of societal power, for which AI technologies form a crucial component.This talk will argue that AI ethics needs to expand its theoretical and methodological toolkit in order to move away from prioritizing notions of good design that privilege the work of good and ethical technology designers. Instead, using approaches from feminist theory, organization studies, and science and technology, I argue for expanding how we evaluate uses of AI. This approach begins with the assumption of socially informed technological affordances, or ""imagined affordances"" [1] shaping how people understand and use technologies in practice. It also gives centrality to the power of social institutions for shaping technologies-in-practice.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {5–6},
numpages = {2},
keywords = {social sciences, theory, ai ethics, work and organizations, data work, sts, feminist theory},
location = {New York, NY, USA},
series = {AIES '20}
}
  
"
"@inproceedings{10.1145/3375627.3375808,
author = {Leben, Derek},
title = {Normative Principles for Evaluating Fairness in Machine Learning},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375808},
doi = {10.1145/3375627.3375808},
abstract = {There are many incompatible ways to measure fair outcomes for machine learning algorithms. The goal of this paper is to characterize rates of success and error across protected groups (race, gender, sexual orientation) as a distribution problem, and describe the possible solutions to this problem according to different normative principles from moral and political philosophy. These normative principles are based on various competing attributes within a distribution problem: intentions, compensation, desert, consent, and consequences. Each principle will be applied to a sample risk-assessment classifier to demonstrate the philosophical arguments underlying different sets of fairness metrics.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {86–92},
numpages = {7},
keywords = {political philosophy, discrimination, machine learning, fairness, algorithmic decision-making},
location = {New York, NY, USA},
series = {AIES '20}
}
  
"
"@inproceedings{10.1145/3375627.3375848,
author = {Huang, Lingxiao and Wei, Julia and Celis, Elisa},
title = {Towards Just, Fair and Interpretable Methods for Judicial Subset Selection},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375848},
doi = {10.1145/3375627.3375848},
abstract = {In many judicial systems -- including the United States courts of appeals, the European Court of Justice, the UK Supreme Court and the Supreme Court of Canada -- a subset of judges is selected from the entire judicial body for each case in order to hear the arguments and decide the judgment. Ideally, the subset selected is representative, i.e., the decision of the subset would match what the decision of the entire judicial body would have been had they all weighed in on the case. Further, the process should be fair in that all judges should have similar workloads, and the selection process should not allow for certain judge's opinions to be silenced or amplified via case assignments. Lastly, in order to be practical and trustworthy, the process should also be interpretable, easy to use, and (if algorithmic) computationally efficient. In this paper, we propose an algorithmic method for the judicial subset selection problem that satisfies all of the above criteria. The method satisfies fairness by design, and we prove that it has optimal representativeness asymptotically for a large range of parameters and under noisy information models about judge opinions -- something no existing methods can provably achieve. We then assess the benefits of our approach empirically by counterfactually comparing against the current practice and recent alternative algorithmic approaches using cases from the United States courts of appeals database.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {293–299},
numpages = {7},
keywords = {representative, interpretable, judicial subset selection, fair},
location = {New York, NY, USA},
series = {AIES '20}
}
  
"
"@inproceedings{10.1145/3375627.3375859,
author = {Kak, Amba},
title = {""The Global South is Everywhere, but Also Always Somewhere"": National Policy Narratives and AI Justice},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375859},
doi = {10.1145/3375627.3375859},
abstract = {There is more attention than ever on the social implications of AI. In contrast to universalized paradigms of ethics and fairness, a growing body of critical work highlights bias and discrimination in AI within the frame of social justice and human rights (""AI justice""). However, the geographical location of much of this critique in the West could be engendering its own blind spots. The global supply chain of AI (data, computational power, natural resources, labor) today replicates historical colonial inequities, and the continued subordination of Global South countries. This paper draws attention to official narratives from the Indian government and the United Nations Conference on Trade and Development (UNCTAD) advocating for the role (and place) of these regions in the AI economy. Domestically, these policies are being contested for their top-down formulation, and reflect narrow industry interests. This underscores the need to approach the political economy of AI from varying altitudes - global, national, and from the perspective of communities whose lives and livelihoods are most directly impacted in this economy. Without a deliberate effort at centering this conversation it is inevitable that mainstream discourse on AI justice will grow parallel to (and potentially undercut) demands emanating from Global South governments and communities},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {307–312},
numpages = {6},
keywords = {data flows, global south, decolonial, political economy, ai accountability},
location = {New York, NY, USA},
series = {AIES '20}
}
  
"
"@inproceedings{10.1145/3461702.3462628,
author = {Lee, Min Kyung and Nigam, Ishan and Zhang, Angie and Afriyie, Joel and Qin, Zhizhen and Gao, Sicun},
title = {Participatory Algorithmic Management: Elicitation Methods for Worker Well-Being Models},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462628},
doi = {10.1145/3461702.3462628},
abstract = {Artificial intelligence is increasingly being used to manage the workforce. Algorithmic management promises organizational efficiency, but often undermines worker well-being. How can we computationally model worker well-being so that algorithmic management can be optimized for and assessed in terms of worker well-being? Toward this goal, we propose a participatory approach for worker well-being models. We first define worker well-being models: Work preference models---preferences about work and working conditions, and managerial fairness models---beliefs about fair resource allocation among multiple workers. We then propose elicitation methods to enable workers to build their own well-being models leveraging pairwise comparisons and ranking. As a case study, we evaluate our methods in the context of algorithmic work scheduling with 25 shift workers and 3 managers. The findings show that workers expressed idiosyncratic work preference models and more uniform managerial fairness models, and the elicitation methods helped workers discover their preferences and gave them a sense of empowerment. Our work provides a method and initial evidence for enabling participatory algorithmic management for worker well-being.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {715–726},
numpages = {12},
keywords = {algorithmic management, algorithmic fairness, preference elicitation, participatory design, worker well-being},
location = {Virtual Event, USA},
series = {AIES '21}
}
  
"
"@inproceedings{10.1145/3461702.3462600,
author = {Solans, David and Fabbri, Francesco and Calsamiglia, Caterina and Castillo, Carlos and Bonchi, Francesco},
title = {Comparing Equity and Effectiveness of Different Algorithms in an Application for the Room Rental Market},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462600},
doi = {10.1145/3461702.3462600},
abstract = {Machine Learning (ML) techniques have been increasingly adopted by the real estate market in the last few years. Applications include, among many others, predicting the market value of a property or an area, advanced systems for managing marketing and ads campaigns, and recommendation systems based on user preferences. While these techniques can provide important benefits to the business owners and the users of the platforms, algorithmic biases can result in inequalities and loss of opportunities for groups of people who are already disadvantaged in their access to housing. In this work, we present a comprehensive and independent algorithmic evaluation of a recommender system for the real estate market, designed specifically for finding shared apartments in metropolitan areas. We were granted full access to the internals of the platform, including details on algorithms and usage data during a period of 2 years.We analyze the performance of the various algorithms which are deployed for the recommender system and asses their effect across different population groups.Our analysis reveals that introducing a recommender system algorithm facilitates finding an appropriate tenant or a desirable room to rent, but at the same time, it strengthen performance inequalities between groups, further reducing opportunities of finding a rental for certain minorities.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {978–988},
numpages = {11},
keywords = {performance, demographics, recommender system, fairness},
location = {Virtual Event, USA},
series = {AIES '21}
}
  
"
"@inproceedings{10.1145/3514094.3539540,
author = {Chapman, Melissa},
title = {Governing AI Applications To Monitoring and Managing Our Global Environmental Commons},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3539540},
doi = {10.1145/3514094.3539540},
abstract = {Artificial Intelligence (AI) is playing a rapidly growing role in our response to global environmental change, from how we collect and process ecological and earth system data to how we make and enforce management decisions. Importantly, AI permits the use of increasingly high-resolution data, promising more accurate monitoring of environmental change and more targeted interventions. But, while AI promises several potential contributions to addressing global environmental change, how do we ensure these technologies stand to enhance equity rather than exasperate existing environmental injustices and power asymmetries?},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {893},
numpages = {1},
keywords = {social-environmental systems, environmental policy, climate change, conservation by algorithm, biodiversity},
location = {Oxford, United Kingdom},
series = {AIES '22}
}
  
"
"@inproceedings{10.1145/3514094.3539519,
author = {Reyero Lobo, Paula},
title = {Bias in Hate Speech and Toxicity Detection},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3539519},
doi = {10.1145/3514094.3539519},
abstract = {Many Artificial Intelligence (AI) systems rely on finding patterns in large datasets, which are prone to bias and exacerbate existing segregation and inequalities of marginalised communities. Due to their socio-technical impact, bias in AI has become a pressing issue. In this work, we investigate discrimination prevention methods on the assumption that disparities of specific populations in the training samples are reproduced or even amplified in the AI system outcomes. We aim to identify the information from vulnerable groups in the training data, uncover potential inequalities in how data capture these groups and provide additional information about them to alleviate inequalities, e.g., stereotypical and generalised views that lead to learning discriminatory associations. We develop data preprocessing techniques in automated moderation (AI systems to flag or filter online abuse) due to its substantial social implications and existing challenges common to many AI applications.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {910},
numpages = {1},
keywords = {toxic speech, semantic web, bias, artificial intelligence},
location = {Oxford, United Kingdom},
series = {AIES '22}
}
  
"
"@inproceedings{10.1145/3514094.3539552,
author = {Singh, Harvineet},
title = {Fair, Robust, and Data-Efficient Machine Learning in Healthcare},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3539552},
doi = {10.1145/3514094.3539552},
abstract = {While machine learning systems have shown improvements, often, in carefully curated settings, challenges still exist to their wider deployment, especially for making consequential decisions. The research described here explores three challenges, particularly, emphasizing the interesting issues that arise at their intersection.How do we design machine learning systems to account for the systemic biases of the world, to act reliably under unseen settings, and to handle limited availability of data?Human-facing applications of machine learning such as personalized health commonly encounter these challenges, thus, these are important to address. The research has three components addressing different parts of the above central question. Here, we describe the work done on two components of the above central question and highlight the future work planned as part of the third one. We draw from methods in causal inference, algorithmic fairness, and interactive learning, and apply them to applications in health.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {914},
numpages = {1},
keywords = {robust learning, policy evaluation, active learning, algorithmic fairness},
location = {Oxford, United Kingdom},
series = {AIES '22}
}
  
"
"@inproceedings{10.1145/3465416.3483301,
author = {Nee, Julia and Macfarlane Smith, Genevieve and Sheares, Alicia and Rustagi, Ishita},
title = {Advancing Social Justice through Linguistic Justice: Strategies for Building Equity Fluent NLP Technology},
year = {2021},
isbn = {9781450385534},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3465416.3483301},
doi = {10.1145/3465416.3483301},
abstract = {Language and social reality are mutually reinforcing; as a result, natural language processing (NLP) presents a unique opportunity to shift social reality at scale, advancing social justice by promoting linguistic justice. We provide an overview of how language and bias are intertwined and implications for building NLP tools that actively advance equity and inclusion. Then, we present a framework for centering inclusion and social justice in NLP design at four overlapping layers of linguistic structure. The goal is to provide a foundation for adopting equity-centered principles in the creation of NLP tools that don’t simply mitigate social biases, but actively advance inclusion and social justice through language. This work aims to be practical and builds from a partnership between researchers at the Center for Equity, Gender, and Leadership at the UC Berkeley Haas School of Business and leaders and practitioners at a large Silicon Valley tech firm. This framework can foster equity-centered thinking to lead to greater “equity fluent” NLP tools that have the potential to advance justice more broadly.},
booktitle = {Equity and Access in Algorithms, Mechanisms, and Optimization},
articleno = {13},
numpages = {9},
keywords = {linguistic justice, equity-centered design, bias, discrimination, language ideologies},
location = {--, NY, USA},
series = {EAAMO '21}
}
  
"
"@inproceedings{10.1145/3465416.3483288,
author = {Rumpf, Adam and Kaul, Hemanshu},
title = {A Public Transit Network Optimization Model for Equitable Access to Social Services},
year = {2021},
isbn = {9781450385534},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3465416.3483288},
doi = {10.1145/3465416.3483288},
abstract = {We present a flexible public transit network design model which optimizes a social access objective while guaranteeing that the system’s costs and transit times remain within a preset margin of their current levels. The purpose of the model is to find a set of minor, immediate modifications to an existing bus network that can give more communities access to the chosen services while having a minimal impact on the current network’s operator costs and user costs. Design decisions consist of reallocation of existing resources in order to adjust line frequencies and capacities. We present a hybrid tabu search/simulated annealing algorithm for the solution of this optimization-based model. As a case study we apply the model to the problem of improving equity of access to primary health care facilities in the Chicago metropolitan area. The results of the model suggest that it is possible to achieve better primary care access equity through reassignment of existing buses and implementation of express runs, while leaving overall service levels relatively unaffected.},
booktitle = {Equity and Access in Algorithms, Mechanisms, and Optimization},
articleno = {16},
numpages = {17},
location = {--, NY, USA},
series = {EAAMO '21}
}
  
"
"
@inproceedings{pmlr-v70-mohajer17a,
	title        = {Active Learning for Top-$K$ Rank Aggregation from Noisy Comparisons},
	author       = {Soheil Mohajer and Changho Suh and Adel Elmahdy},
	booktitle    = {Proceedings of the 34th International Conference on Machine Learning},
	pages        = {2488--2497},
	year         = 2017,
	editor       = {Precup, Doina and Teh, Yee Whye},
	volume       = 70,
	series       = {Proceedings of Machine Learning Research},
	month        = {06--11 Aug},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v70/mohajer17a/mohajer17a.pdf},
	url          = {https://proceedings.mlr.press/v70/mohajer17a.html},
	abstract     = {We explore an active top-$K$ ranking problem based on pairwise comparisons that are collected possibly in a sequential manner as per our design choice. We consider two settings: (1) <em>top-$K$ sorting</em> in which the goal is to recover the top-$K$ items in order out of $n$ items; (2) <em>top-$K$ partitioning</em> where only the set of top-$K$ items is desired. Under a fairly general model which subsumes as special cases various models (e.g., Strong Stochastic Transitivity model, BTL model and uniform noise model), we characterize upper bounds on the sample size required for top-$K$ sorting as well as for top-$K$ partitioning. As a consequence, we demonstrate that active ranking can offer significant multiplicative gains in sample complexity over passive ranking. Depending on the underlying stochastic noise model, such gain varies from around $\frac{\log n}{\log \log n}$ to $\frac{ n^2 \log n }{\log \log n}$. We also present an algorithm that is applicable to both settings.},
}
"
"
"
"
@inproceedings{pmlr-v80-agrawal18b,
	title        = {Proportional Allocation: Simple, Distributed, and Diverse Matching with High Entropy},
	author       = {Agrawal, Shipra and Zadimoghaddam, Morteza and Mirrokni, Vahab},
	booktitle    = {Proceedings of the 35th International Conference on Machine Learning},
	pages        = {99--108},
	year         = 2018,
	editor       = {Dy, Jennifer and Krause, Andreas},
	volume       = 80,
	series       = {Proceedings of Machine Learning Research},
	month        = {10--15 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v80/agrawal18b/agrawal18b.pdf},
	url          = {https://proceedings.mlr.press/v80/agrawal18b.html},
	abstract     = {Inspired by many applications of bipartite matching in online advertising and machine learning, we study a simple and natural iterative proportional allocation algorithm: Maintain a priority score $\priority_a$ for each node $a\in \mathds{A}$ on one side of the bipartition, initialized as $\priority_a=1$. Iteratively allocate the nodes $i\in \impressions$ on the other side to eligible nodes in $\mathds{A}$ in proportion of their priority scores. After each round, for each node $a\in \mathds{A}$, decrease or increase the score $\priority_a$ based on whether it is over- or under- allocated. Our first result is that this simple, distributed algorithm converges to a $(1-\epsilon)$-approximate fractional $b$-matching solution in $O({\log n\over \epsilon^2} )$ rounds. We also extend the proportional allocation algorithm and convergence results to the maximum weighted matching problem, and show that the algorithm can be naturally tuned to produce maximum matching with <em>high entropy</em>. High entropy, in turn, implies additional desirable properties of this matching, e.g., it satisfies certain diversity and fairness (aka anonymity) properties that are desirable in a variety of applications in online advertising and machine learning.},
}
"
"
@inproceedings{pmlr-v80-celis18a,
	title        = {Fair and Diverse {DPP}-Based Data Summarization},
	author       = {Celis, Elisa and Keswani, Vijay and Straszak, Damian and Deshpande, Amit and Kathuria, Tarun and Vishnoi, Nisheeth},
	booktitle    = {Proceedings of the 35th International Conference on Machine Learning},
	pages        = {716--725},
	year         = 2018,
	editor       = {Dy, Jennifer and Krause, Andreas},
	volume       = 80,
	series       = {Proceedings of Machine Learning Research},
	month        = {10--15 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v80/celis18a/celis18a.pdf},
	url          = {https://proceedings.mlr.press/v80/celis18a.html},
	abstract     = {Sampling methods that choose a subset of the data proportional to its diversity in the feature space are popular for data summarization. However, recent studies have noted the occurrence of bias {–} e.g., under or over representation of a particular gender or ethnicity {–} in such data summarization methods. In this paper we initiate a study of the problem of outputting a diverse and fair summary of a given dataset. We work with a well-studied determinantal measure of diversity and corresponding distributions (DPPs) and present a framework that allows us to incorporate a general class of fairness constraints into such distributions. Designing efficient algorithms to sample from these constrained determinantal distributions, however, suffers from a complexity barrier; we present a fast sampler that is provably good when the input vectors satisfy a natural property. Our empirical results on both real-world and synthetic datasets show that the diversity of the samples produced by adding fairness constraints is not too far from the unconstrained case.},
}
"
"
@inproceedings{pmlr-v80-hashimoto18a,
	title        = {Fairness Without Demographics in Repeated Loss Minimization},
	author       = {Hashimoto, Tatsunori and Srivastava, Megha and Namkoong, Hongseok and Liang, Percy},
	booktitle    = {Proceedings of the 35th International Conference on Machine Learning},
	pages        = {1929--1938},
	year         = 2018,
	editor       = {Dy, Jennifer and Krause, Andreas},
	volume       = 80,
	series       = {Proceedings of Machine Learning Research},
	month        = {10--15 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v80/hashimoto18a/hashimoto18a.pdf},
	url          = {https://proceedings.mlr.press/v80/hashimoto18a.html},
	abstract     = {Machine learning models (e.g., speech recognizers) trained on average loss suffer from representation disparity—minority groups (e.g., non-native speakers) carry less weight in the training objective, and thus tend to suffer higher loss. Worse, as model accuracy affects user retention, a minority group can shrink over time. In this paper, we first show that the status quo of empirical risk minimization (ERM) amplifies representation disparity over time, which can even turn initially fair models unfair. To mitigate this, we develop an approach based on distributionally robust optimization (DRO), which minimizes the worst case risk over all distributions close to the empirical distribution. We prove that this approach controls the risk of the minority group at each time step, in the spirit of Rawlsian distributive justice, while remaining oblivious to the identity of the groups. We demonstrate that DRO prevents disparity amplification on examples where ERM fails, and show improvements in minority group user satisfaction in a real-world text autocomplete task.},
}
"

"
@inproceedings{pmlr-v80-liu18c,
	title        = {Delayed Impact of Fair Machine Learning},
	author       = {Liu, Lydia T. and Dean, Sarah and Rolf, Esther and Simchowitz, Max and Hardt, Moritz},
	booktitle    = {Proceedings of the 35th International Conference on Machine Learning},
	pages        = {3150--3158},
	year         = 2018,
	editor       = {Dy, Jennifer and Krause, Andreas},
	volume       = 80,
	series       = {Proceedings of Machine Learning Research},
	month        = {10--15 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v80/liu18c/liu18c.pdf},
	url          = {https://proceedings.mlr.press/v80/liu18c.html},
	abstract     = {Fairness in machine learning has predominantly been studied in static classification settings without concern for how decisions change the underlying population over time. Conventional wisdom suggests that fairness criteria promote the long-term well-being of those groups they aim to protect. We study how static fairness criteria interact with temporal indicators of well-being, such as long-term improvement, stagnation, and decline in a variable of interest. We demonstrate that even in a one-step feedback model, common fairness criteria in general do not promote improvement over time, and may in fact cause harm in cases where an unconstrained objective would not. We completely characterize the delayed impact of three standard criteria, contrasting the regimes in which these exhibit qualitatively different behavior. In addition, we find that a natural form of measurement error broadens the regime in which fairness criteria perform favorably. Our results highlight the importance of measurement and temporal modeling in the evaluation of fairness criteria, suggesting a range of new challenges and trade-offs.},
}
"
"
@inproceedings{pmlr-v80-madras18a,
	title        = {Learning Adversarially Fair and Transferable Representations},
	author       = {Madras, David and Creager, Elliot and Pitassi, Toniann and Zemel, Richard},
	booktitle    = {Proceedings of the 35th International Conference on Machine Learning},
	pages        = {3384--3393},
	year         = 2018,
	editor       = {Dy, Jennifer and Krause, Andreas},
	volume       = 80,
	series       = {Proceedings of Machine Learning Research},
	month        = {10--15 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v80/madras18a/madras18a.pdf},
	url          = {https://proceedings.mlr.press/v80/madras18a.html},
	abstract     = {In this paper, we advocate for representation learning as the key to mitigating unfair prediction outcomes downstream. Motivated by a scenario where learned representations are used by third parties with unknown objectives, we propose and explore adversarial representation learning as a natural method of ensuring those parties act fairly. We connect group fairness (demographic parity, equalized odds, and equal opportunity) to different adversarial objectives. Through worst-case theoretical guarantees and experimental validation, we show that the choice of this objective is crucial to fair prediction. Furthermore, we present the first in-depth experimental demonstration of fair transfer learning and demonstrate empirically that our learned representations admit fair predictions on new tasks while maintaining utility, an essential goal of fair representation learning.},
}
"
"
@inproceedings{pmlr-v80-olofsson18a,
	title        = {Design of Experiments for Model Discrimination Hybridising Analytical and Data-Driven Approaches},
	author       = {Olofsson, Simon and Deisenroth, Marc and Misener, Ruth},
	booktitle    = {Proceedings of the 35th International Conference on Machine Learning},
	pages        = {3908--3917},
	year         = 2018,
	editor       = {Dy, Jennifer and Krause, Andreas},
	volume       = 80,
	series       = {Proceedings of Machine Learning Research},
	month        = {10--15 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v80/olofsson18a/olofsson18a.pdf},
	url          = {https://proceedings.mlr.press/v80/olofsson18a.html},
	abstract     = {Healthcare companies must submit pharmaceutical drugs or medical device to regulatory bodies before marketing new technology. Regulatory bodies frequently require transparent and interpretable computational modelling to justify a new healthcare technology, but researchers may have several competing models for a biological system and too little data to discriminate between the models. In design of experiments for model discrimination, where the goal is to design maximally informative physical experiments in order to discriminate between rival predictive models, research has focused either on analytical approaches, which cannot manage all functions, or on data-driven approaches, which may have computational difficulties or lack interpretable marginal predictive distributions. We develop a methodology for introducing Gaussian process surrogates in lieu of the original mechanistic models. This allows us to extend existing design and model discrimination methods developed for analytical models to cases of non-analytical models.},
}
"
"
@inproceedings{pmlr-v80-yona18a,
	title        = {Probably Approximately Metric-Fair Learning},
	author       = {Yona, Gal and Rothblum, Guy},
	booktitle    = {Proceedings of the 35th International Conference on Machine Learning},
	pages        = {5680--5688},
	year         = 2018,
	editor       = {Dy, Jennifer and Krause, Andreas},
	volume       = 80,
	series       = {Proceedings of Machine Learning Research},
	month        = {10--15 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v80/yona18a/yona18a.pdf},
	url          = {https://proceedings.mlr.press/v80/yona18a.html},
	abstract     = {The seminal work of Dwork <em>et al.</em> [ITCS 2012] introduced a metric-based notion of individual fairness: given a task-specific similarity metric, their notion required that every pair of similar individuals should be treated similarly. In the context of machine learning, however, individual fairness does not generalize from a training set to the underlying population. We show that this can lead to computational intractability even for simple fair-learning tasks. With this motivation in mind, we introduce and study a relaxed notion of <em>approximate metric-fairness</em>: for a random pair of individuals sampled from the population, with all but a small probability of error, if they are similar then they should be treated similarly. We formalize the goal of achieving approximate metric-fairness simultaneously with best-possible accuracy as Probably Approximately Correct and Fair (PACF) Learning. We show that approximate metric-fairness <em>does</em> generalize, and leverage these generalization guarantees to construct polynomial-time PACF learning algorithms for the classes of linear and logistic predictors.},
}
"
"
@inproceedings{pmlr-v97-agarwal19d,
	title        = {Fair Regression: Quantitative Definitions and Reduction-Based Algorithms},
	author       = {Agarwal, Alekh and Dudik, Miroslav and Wu, Zhiwei Steven},
	booktitle    = {Proceedings of the 36th International Conference on Machine Learning},
	pages        = {120--129},
	year         = 2019,
	editor       = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	volume       = 97,
	series       = {Proceedings of Machine Learning Research},
	month        = {09--15 Jun},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v97/agarwal19d/agarwal19d.pdf},
	url          = {https://proceedings.mlr.press/v97/agarwal19d.html},
	abstract     = {In this paper, we study the prediction of a real-valued target, such as a risk score or recidivism rate, while guaranteeing a quantitative notion of fairness with respect to a protected attribute such as gender or race. We call this class of problems fair regression. We propose general schemes for fair regression under two notions of fairness: (1) statistical parity, which asks that the prediction be statistically independent of the protected attribute, and (2) bounded group loss, which asks that the prediction error restricted to any protected group remain below some pre-determined level. While we only study these two notions of fairness, our schemes are applicable to arbitrary Lipschitz-continuous losses, and so they encompass least-squares regression, logistic regression, quantile regression, and many other tasks. Our schemes only require access to standard risk minimization algorithms (such as standard classification or least-squares regression) while providing theoretical guarantees on the optimality and fairness of the obtained solutions. In addition to analyzing theoretical properties of our schemes, we empirically demonstrate their ability to uncover fairness–accuracy frontiers on several standard datasets.},
}
"
"
@inproceedings{pmlr-v97-aivodji19a,
	title        = {Fairwashing: the risk of rationalization},
	author       = {Aivodji, Ulrich and Arai, Hiromi and Fortineau, Olivier and Gambs, S{\'e}bastien and Hara, Satoshi and Tapp, Alain},
	booktitle    = {Proceedings of the 36th International Conference on Machine Learning},
	pages        = {161--170},
	year         = 2019,
	editor       = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	volume       = 97,
	series       = {Proceedings of Machine Learning Research},
	month        = {09--15 Jun},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v97/aivodji19a/aivodji19a.pdf},
	url          = {https://proceedings.mlr.press/v97/aivodji19a.html},
	abstract     = {Black-box explanation is the problem of explaining how a machine learning model – whose internal logic is hidden to the auditor and generally complex – produces its outcomes. Current approaches for solving this problem include model explanation, outcome explanation as well as model inspection. While these techniques can be beneficial by providing interpretability, they can be used in a negative manner to perform fairwashing, which we define as promoting the false perception that a machine learning model respects some ethical values. In particular, we demonstrate that it is possible to systematically rationalize decisions taken by an unfair black-box model using the model explanation as well as the outcome explanation approaches with a given fairness metric. Our solution, LaundryML, is based on a regularized rule list enumeration algorithm whose objective is to search for fair rule lists approximating an unfair black-box model. We empirically evaluate our rationalization technique on black-box models trained on real-world datasets and show that one can obtain rule lists with high fidelity to the black-box model while being considerably less unfair at the same time.},
}
"
"
@inproceedings{pmlr-v97-backurs19a,
	title        = {Scalable Fair Clustering},
	author       = {Backurs, Arturs and Indyk, Piotr and Onak, Krzysztof and Schieber, Baruch and Vakilian, Ali and Wagner, Tal},
	booktitle    = {Proceedings of the 36th International Conference on Machine Learning},
	pages        = {405--413},
	year         = 2019,
	editor       = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	volume       = 97,
	series       = {Proceedings of Machine Learning Research},
	month        = {09--15 Jun},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v97/backurs19a/backurs19a.pdf},
	url          = {https://proceedings.mlr.press/v97/backurs19a.html},
	abstract     = {We study the fair variant of the classic k-median problem introduced by (Chierichetti et al., NeurIPS 2017) in which the points are colored, and the goal is to minimize the same average distance objective as in the standard $k$-median problem while ensuring that all clusters have an “approximately equal” number of points of each color. (Chierichetti et al., NeurIPS 2017) proposed a two-phase algorithm for fair $k$-clustering. In the first step, the pointset is partitioned into subsets called fairlets that satisfy the fairness requirement and approximately preserve the k-median objective. In the second step, fairlets are merged into k clusters by one of the existing k-median algorithms. The running time of this algorithm is dominated by the first step, which takes super-quadratic time. In this paper, we present a practical approximate fairlet decomposition algorithm that runs in nearly linear time.},
}
"
"
@inproceedings{pmlr-v97-bose19a,
	title        = {Compositional Fairness Constraints for Graph Embeddings},
	author       = {Bose, Avishek and Hamilton, William},
	booktitle    = {Proceedings of the 36th International Conference on Machine Learning},
	pages        = {715--724},
	year         = 2019,
	editor       = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	volume       = 97,
	series       = {Proceedings of Machine Learning Research},
	month        = {09--15 Jun},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v97/bose19a/bose19a.pdf},
	url          = {https://proceedings.mlr.press/v97/bose19a.html},
	abstract     = {Learning high-quality node embeddings is a key building block for machine learning models that operate on graph data, such as social networks and recommender systems. However, existing graph embedding techniques are unable to cope with fairness constraints, e.g., ensuring that the learned representations do not correlate with certain attributes, such as age or gender. Here, we introduce an adversarial framework to enforce fairness constraints on graph embeddings. Our approach is <em>compositional</em>—meaning that it can flexibly accommodate different combinations of fairness constraints during inference. For instance, in the context of social recommendations, our framework would allow one user to request that their recommendations are invariant to both their age and gender, while also allowing another user to request invariance to just their age. Experiments on standard knowledge graph and recommender system benchmarks highlight the utility of our proposed framework.},
}
"
"
@inproceedings{pmlr-v97-chen19d,
	title        = {Proportionally Fair Clustering},
	author       = {Chen, Xingyu and Fain, Brandon and Lyu, Liang and Munagala, Kamesh},
	booktitle    = {Proceedings of the 36th International Conference on Machine Learning},
	pages        = {1032--1041},
	year         = 2019,
	editor       = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	volume       = 97,
	series       = {Proceedings of Machine Learning Research},
	month        = {09--15 Jun},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v97/chen19d/chen19d.pdf},
	url          = {https://proceedings.mlr.press/v97/chen19d.html},
	abstract     = {We extend the fair machine learning literature by considering the problem of proportional centroid clustering in a metric context. For clustering n points with k centers, we define fairness as proportionality to mean that any n/k points are entitled to form their own cluster if there is another center that is closer in distance for all n/k points. We seek clustering solutions to which there are no such justified complaints from any subsets of agents, without assuming any a priori notion of protected subsets. We present and analyze algorithms to efficiently compute, optimize, and audit proportional solutions. We conclude with an empirical examination of the tradeoff between proportional solutions and the k-means objective.},
}
"
"
@inproceedings{pmlr-v97-cotter19b,
	title        = {Training Well-Generalizing Classifiers for Fairness Metrics and Other Data-Dependent Constraints},
	author       = {Cotter, Andrew and Gupta, Maya and Jiang, Heinrich and Srebro, Nathan and Sridharan, Karthik and Wang, Serena and Woodworth, Blake and You, Seungil},
	booktitle    = {Proceedings of the 36th International Conference on Machine Learning},
	pages        = {1397--1405},
	year         = 2019,
	editor       = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	volume       = 97,
	series       = {Proceedings of Machine Learning Research},
	month        = {09--15 Jun},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v97/cotter19b/cotter19b.pdf},
	url          = {https://proceedings.mlr.press/v97/cotter19b.html},
	abstract     = {Classifiers can be trained with data-dependent constraints to satisfy fairness goals, reduce churn, achieve a targeted false positive rate, or other policy goals. We study the generalization performance for such constrained optimization problems, in terms of how well the constraints are satisfied at evaluation time, given that they are satisfied at training time. To improve generalization, we frame the problem as a two-player game where one player optimizes the model parameters on a training dataset, and the other player enforces the constraints on an independent validation dataset. We build on recent work in two-player constrained optimization to show that if one uses this two-dataset approach, then constraint generalization can be significantly improved. As we illustrate experimentally, this approach works not only in theory, but also in practice.},
}
"
"
@inproceedings{pmlr-v97-creager19a,
	title        = {Flexibly Fair Representation Learning by Disentanglement},
	author       = {Creager, Elliot and Madras, David and Jacobsen, Joern-Henrik and Weis, Marissa and Swersky, Kevin and Pitassi, Toniann and Zemel, Richard},
	booktitle    = {Proceedings of the 36th International Conference on Machine Learning},
	pages        = {1436--1445},
	year         = 2019,
	editor       = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	volume       = 97,
	series       = {Proceedings of Machine Learning Research},
	month        = {09--15 Jun},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v97/creager19a/creager19a.pdf},
	url          = {https://proceedings.mlr.press/v97/creager19a.html},
	abstract     = {We consider the problem of learning representations that achieve group and subgroup fairness with respect to multiple sensitive attributes. Taking inspiration from the disentangled representation learning literature, we propose an algorithm for learning compact representations of datasets that are useful for reconstruction and prediction, but are also <em>flexibly fair</em>, meaning they can be easily modified at test time to achieve subgroup demographic parity with respect to multiple sensitive attributes and their conjunctions. We show empirically that the resulting encoder—which does not require the sensitive attributes for inference—allows for the adaptation of a single representation to a variety of fair classification tasks with new target labels and subgroup definitions.},
}
"
"
@inproceedings{pmlr-v97-frosst19a,
	title        = {Analyzing and Improving Representations with the Soft Nearest Neighbor Loss},
	author       = {Frosst, Nicholas and Papernot, Nicolas and Hinton, Geoffrey},
	booktitle    = {Proceedings of the 36th International Conference on Machine Learning},
	pages        = {2012--2020},
	year         = 2019,
	editor       = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	volume       = 97,
	series       = {Proceedings of Machine Learning Research},
	month        = {09--15 Jun},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v97/frosst19a/frosst19a.pdf},
	url          = {https://proceedings.mlr.press/v97/frosst19a.html},
	abstract     = {We explore and expand the Soft Nearest Neighbor Loss to measure the entanglement of class manifolds in representation space: i.e., how close pairs of points from the same class are relative to pairs of points from different classes. We demonstrate several use cases of the loss. As an analytical tool, it provides insights into the evolution of class similarity structures during learning. Surprisingly, we find that maximizing the entanglement of representations of different classes in the hidden layers is beneficial for discrimination in the final layer, possibly because it encourages representations to identify class-independent similarity structures. Maximizing the soft nearest neighbor loss in the hidden layers leads not only to better-calibrated estimates of uncertainty on outlier data but also marginally improved generalization. Data that is not from the training distribution can be recognized by observing that in the hidden layers, it has fewer than the normal number of neighbors from the predicted class.},
}
"
"
@inproceedings{pmlr-v97-fu19a,
	title        = {Diagnosing Bottlenecks in Deep Q-learning Algorithms},
	author       = {Fu, Justin and Kumar, Aviral and Soh, Matthew and Levine, Sergey},
	booktitle    = {Proceedings of the 36th International Conference on Machine Learning},
	pages        = {2021--2030},
	year         = 2019,
	editor       = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	volume       = 97,
	series       = {Proceedings of Machine Learning Research},
	month        = {09--15 Jun},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v97/fu19a/fu19a.pdf},
	url          = {https://proceedings.mlr.press/v97/fu19a.html},
	abstract     = {Q-learning methods are a common class of algorithms used in reinforcement learning (RL). However, their behavior with function approximation, especially with neural networks, is poorly understood theoretically and empirically. In this work, we aim to experimentally investigate potential issues in Q-learning, by means of a ""unit testing"" framework where we can utilize oracles to disentangle sources of error. Specifically, we investigate questions related to function approximation, sampling error and nonstationarity, and where available, verify if trends found in oracle settings hold true with deep RL methods. We find that large neural network architectures have many benefits with regards to learning stability; offer several practical compensations for overfitting; and develop a novel sampling method based on explicitly compensating for function approximation error that yields fair improvement on high-dimensional continuous control domains.},
}
"
"
@inproceedings{pmlr-v97-gordaliza19a,
	title        = {Obtaining Fairness using Optimal Transport Theory},
	author       = {Gordaliza, Paula and Barrio, Eustasio Del and Fabrice, Gamboa and Loubes, Jean-Michel},
	booktitle    = {Proceedings of the 36th International Conference on Machine Learning},
	pages        = {2357--2365},
	year         = 2019,
	editor       = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	volume       = 97,
	series       = {Proceedings of Machine Learning Research},
	month        = {09--15 Jun},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v97/gordaliza19a/gordaliza19a.pdf},
	url          = {https://proceedings.mlr.press/v97/gordaliza19a.html},
	abstract     = {In the fair classification setup, we recast the links between fairness and predictability in terms of probability metrics. We analyze repair methods based on mapping conditional distributions to the Wasserstein barycenter. We propose a Random Repair which yields a tradeoff between minimal information loss and a certain amount of fairness.},
}
"
"
@inproceedings{pmlr-v97-heidari19a,
	title        = {On the Long-term Impact of Algorithmic Decision Policies: Effort Unfairness and Feature Segregation through Social Learning},
	author       = {Heidari, Hoda and Nanda, Vedant and Gummadi, Krishna},
	booktitle    = {Proceedings of the 36th International Conference on Machine Learning},
	pages        = {2692--2701},
	year         = 2019,
	editor       = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	volume       = 97,
	series       = {Proceedings of Machine Learning Research},
	month        = {09--15 Jun},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v97/heidari19a/heidari19a.pdf},
	url          = {https://proceedings.mlr.press/v97/heidari19a.html},
	abstract     = {Most existing notions of algorithmic fairness are one-shot: they ensure some form of allocative equality at the time of decision making, but do not account for the adverse impact of the algorithmic decisions today on the long-term welfare and prosperity of certain segments of the population. We take a broader perspective on algorithmic fairness. We propose an effort-based measure of fairness and present a data-driven framework for characterizing the long-term impact of algorithmic policies on reshaping the underlying population. Motivated by the psychological literature on social learning and the economic literature on equality of opportunity, we propose a micro-scale model of how individuals may respond to decision-making algorithms. We employ existing measures of segregation from sociology and economics to quantify the resulting macro- scale population-level change. Importantly, we observe that different models may shift the group- conditional distribution of qualifications in different directions. Our findings raise a number of important questions regarding the formalization of fairness for decision-making models.},
}
"
"
@inproceedings{pmlr-v97-huang19e,
	title        = {Stable and Fair Classification},
	author       = {Huang, Lingxiao and Vishnoi, Nisheeth},
	booktitle    = {Proceedings of the 36th International Conference on Machine Learning},
	pages        = {2879--2890},
	year         = 2019,
	editor       = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	volume       = 97,
	series       = {Proceedings of Machine Learning Research},
	month        = {09--15 Jun},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v97/huang19e/huang19e.pdf},
	url          = {https://proceedings.mlr.press/v97/huang19e.html},
	abstract     = {In a recent study, Friedler et al. pointed out that several fair classification algorithms are not stable with respect to variations in the training set – a crucial consideration in several applications. Motivated by their work, we study the problem of designing classification algorithms that are both fair and stable. We propose an extended framework based on fair classification algorithms that are formulated as optimization problems, by introducing a stability-focused regularization term. Theoretically, we prove an additional stability guarantee, that was lacking in fair classification algorithms, and also provide an accuracy guarantee for our extended framework. Our accuracy guarantee can be used to inform the selection of the regularization parameter in our framework. We assess the benefits of our approach empirically by extending several fair classification algorithms that are shown to achieve the best balance between fairness and accuracy over the \textbf{Adult} dataset. Our empirical results show that our extended framework indeed improves the stability at only a slight sacrifice in accuracy.},
}
"
"
@inproceedings{pmlr-v97-jagielski19a,
	title        = {Differentially Private Fair Learning},
	author       = {Jagielski, Matthew and Kearns, Michael and Mao, Jieming and Oprea, Alina and Roth, Aaron and -Malvajerdi, Saeed Sharifi and Ullman, Jonathan},
	booktitle    = {Proceedings of the 36th International Conference on Machine Learning},
	pages        = {3000--3008},
	year         = 2019,
	editor       = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	volume       = 97,
	series       = {Proceedings of Machine Learning Research},
	month        = {09--15 Jun},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v97/jagielski19a/jagielski19a.pdf},
	url          = {https://proceedings.mlr.press/v97/jagielski19a.html},
	abstract     = {Motivated by settings in which predictive models may be required to be non-discriminatory with respect to certain attributes (such as race), but even collecting the sensitive attribute may be forbidden or restricted, we initiate the study of fair learning under the constraint of differential privacy. Our first algorithm is a private implementation of the equalized odds post-processing approach of (Hardt et al., 2016). This algorithm is appealingly simple, but must be able to use protected group membership explicitly at test time, which can be viewed as a form of “disparate treatment”. Our second algorithm is a differentially private version of the oracle-efficient in-processing approach of (Agarwal et al., 2018) which is more complex but need not have access to protected group membership at test time. We identify new tradeoffs between fairness, accuracy, and privacy that emerge only when requiring all three properties, and show that these tradeoffs can be milder if group membership may be used at test time. We conclude with a brief experimental evaluation.},
}
"
"
@inproceedings{pmlr-v97-jay19a,
	title        = {A Deep Reinforcement Learning Perspective on Internet Congestion Control},
	author       = {Jay, Nathan and Rotman, Noga and Godfrey, Brighten and Schapira, Michael and Tamar, Aviv},
	booktitle    = {Proceedings of the 36th International Conference on Machine Learning},
	pages        = {3050--3059},
	year         = 2019,
	editor       = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	volume       = 97,
	series       = {Proceedings of Machine Learning Research},
	month        = {09--15 Jun},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v97/jay19a/jay19a.pdf},
	url          = {https://proceedings.mlr.press/v97/jay19a.html},
	abstract     = {We present and investigate a novel and timely application domain for deep reinforcement learning (RL): Internet congestion control. Congestion control is the core networking task of modulating traffic sources’ data-transmission rates to efficiently utilize network capacity, and is the subject of extensive attention in light of the advent of Internet services such as live video, virtual reality, Internet-of-Things, and more. We show that casting congestion control as RL enables training deep network policies that capture intricate patterns in data traffic and network conditions, and leverage this to outperform the state-of-the-art. We also highlight significant challenges facing real-world adoption of RL-based congestion control, including fairness, safety, and generalization, which are not trivial to address within conventional RL formalism. To facilitate further research and reproducibility of our results, we present a test suite for RL-guided congestion control based on the OpenAI Gym interface.},
}
"
"
@inproceedings{pmlr-v97-kleindessner19a,
	title        = {Fair k-Center Clustering for Data Summarization},
	author       = {Kleindessner, Matth{\""a}us and Awasthi, Pranjal and Morgenstern, Jamie},
	booktitle    = {Proceedings of the 36th International Conference on Machine Learning},
	pages        = {3448--3457},
	year         = 2019,
	editor       = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	volume       = 97,
	series       = {Proceedings of Machine Learning Research},
	month        = {09--15 Jun},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v97/kleindessner19a/kleindessner19a.pdf},
	url          = {https://proceedings.mlr.press/v97/kleindessner19a.html},
	abstract     = {In data summarization we want to choose $k$ prototypes in order to summarize a data set. We study a setting where the data set comprises several demographic groups and we are restricted to choose $k_i$ prototypes belonging to group $i$. A common approach to the problem without the fairness constraint is to optimize a centroid-based clustering objective such as $k$-center. A natural extension then is to incorporate the fairness constraint into the clustering problem. Existing algorithms for doing so run in time super-quadratic in the size of the data set, which is in contrast to the standard $k$-center problem being approximable in linear time. In this paper, we resolve this gap by providing a simple approximation algorithm for the $k$-center problem under the fairness constraint with running time linear in the size of the data set and $k$. If the number of demographic groups is small, the approximation guarantee of our algorithm only incurs a constant-factor overhead.},
}
"
"
@inproceedings{pmlr-v97-kleindessner19b,
	title        = {Guarantees for Spectral Clustering with Fairness Constraints},
	author       = {Kleindessner, Matth{\""a}us and Samadi, Samira and Awasthi, Pranjal and Morgenstern, Jamie},
	booktitle    = {Proceedings of the 36th International Conference on Machine Learning},
	pages        = {3458--3467},
	year         = 2019,
	editor       = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	volume       = 97,
	series       = {Proceedings of Machine Learning Research},
	month        = {09--15 Jun},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v97/kleindessner19b/kleindessner19b.pdf},
	url          = {https://proceedings.mlr.press/v97/kleindessner19b.html},
	abstract     = {Given the widespread popularity of spectral clustering (SC) for partitioning graph data, we study a version of constrained SC in which we try to incorporate the fairness notion proposed by Chierichetti et al. (2017). According to this notion, a clustering is fair if every demographic group is approximately proportionally represented in each cluster. To this end, we develop variants of both normalized and unnormalized constrained SC and show that they help find fairer clusterings on both synthetic and real data. We also provide a rigorous theoretical analysis of our algorithms on a natural variant of the stochastic block model, where $h$ groups have strong inter-group connectivity, but also exhibit a “natural” clustering structure which is fair. We prove that our algorithms can recover this fair clustering with high probability.},
}
"
"
@inproceedings{pmlr-v97-kusner19a,
	title        = {Making Decisions that Reduce Discriminatory Impacts},
	author       = {Kusner, Matt and Russell, Chris and Loftus, Joshua and Silva, Ricardo},
	booktitle    = {Proceedings of the 36th International Conference on Machine Learning},
	pages        = {3591--3600},
	year         = 2019,
	editor       = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	volume       = 97,
	series       = {Proceedings of Machine Learning Research},
	month        = {09--15 Jun},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v97/kusner19a/kusner19a.pdf},
	url          = {https://proceedings.mlr.press/v97/kusner19a.html},
	abstract     = {As machine learning algorithms move into real-world settings, it is crucial to ensure they are aligned with societal values. There has been much work on one aspect of this, namely the discriminatory prediction problem: How can we reduce discrimination in the predictions themselves? While an important question, solutions to this problem only apply in a restricted setting, as we have full control over the predictions. Often we care about the non-discrimination of quantities we do not have full control over. Thus, we describe another key aspect of this challenge, the discriminatory impact problem: How can we reduce discrimination arising from the real-world impact of decisions? To address this, we describe causal methods that model the relevant parts of the real-world system in which the decisions are made. Unlike previous approaches, these models not only allow us to map the causal pathway of a single decision, but also to model the effect of interference–how the impact on an individual depends on decisions made about other people. Often, the goal of decision policies is to maximize a beneficial impact overall. To reduce the discrimination of these benefits, we devise a constraint inspired by recent work in counterfactual fairness, and give an efficient procedure to solve the constrained optimization problem. We demonstrate our approach with an example: how to increase students taking college entrance exams in New York City public schools.},
}
"
"
@inproceedings{pmlr-v97-lee19c,
	title        = {Self-Attention Graph Pooling},
	author       = {Lee, Junhyun and Lee, Inyeop and Kang, Jaewoo},
	booktitle    = {Proceedings of the 36th International Conference on Machine Learning},
	pages        = {3734--3743},
	year         = 2019,
	editor       = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	volume       = 97,
	series       = {Proceedings of Machine Learning Research},
	month        = {09--15 Jun},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v97/lee19c/lee19c.pdf},
	url          = {https://proceedings.mlr.press/v97/lee19c.html},
	abstract     = {Advanced methods of applying deep learning to structured data such as graphs have been proposed in recent years. In particular, studies have focused on generalizing convolutional neural networks to graph data, which includes redefining the convolution and the downsampling (pooling) operations for graphs. The method of generalizing the convolution operation to graphs has been proven to improve performance and is widely used. However, the method of applying downsampling to graphs is still difficult to perform and has room for improvement. In this paper, we propose a graph pooling method based on self-attention. Self-attention using graph convolution allows our pooling method to consider both node features and graph topology. To ensure a fair comparison, the same training procedures and model architectures were used for the existing pooling methods and our method. The experimental results demonstrate that our method achieves superior graph classification performance on the benchmark datasets using a reasonable number of parameters.},
}
"
"
@inproceedings{pmlr-v97-liu19f,
	title        = {The Implicit Fairness Criterion of Unconstrained Learning},
	author       = {Liu, Lydia T. and Simchowitz, Max and Hardt, Moritz},
	booktitle    = {Proceedings of the 36th International Conference on Machine Learning},
	pages        = {4051--4060},
	year         = 2019,
	editor       = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	volume       = 97,
	series       = {Proceedings of Machine Learning Research},
	month        = {09--15 Jun},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v97/liu19f/liu19f.pdf},
	url          = {https://proceedings.mlr.press/v97/liu19f.html},
	abstract     = {We clarify what fairness guarantees we can and cannot expect to follow from unconstrained machine learning. Specifically, we show that in many settings, unconstrained learning on its own implies group calibration, that is, the outcome variable is conditionally independent of group membership given the score. A lower bound confirms the optimality of our upper bound. Moreover, we prove that as the excess risk of the learned score decreases, the more strongly it violates separation and independence, two other standard fairness criteria. Our results challenge the view that group calibration necessitates an active intervention, suggesting that often we ought to think of it as a byproduct of unconstrained machine learning.},
}
"
"
@inproceedings{pmlr-v97-mahabadi19a,
	title        = {Composable Core-sets for Determinant Maximization: A Simple Near-Optimal Algorithm},
	author       = {Mahabadi, Sepideh and Indyk, Piotr and Gharan, Shayan Oveis and Rezaei, Alireza},
	booktitle    = {Proceedings of the 36th International Conference on Machine Learning},
	pages        = {4254--4263},
	year         = 2019,
	editor       = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	volume       = 97,
	series       = {Proceedings of Machine Learning Research},
	month        = {09--15 Jun},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v97/mahabadi19a/mahabadi19a.pdf},
	url          = {https://proceedings.mlr.press/v97/mahabadi19a.html},
	abstract     = {“Composable core-sets” are an efficient framework for solving optimization problems in massive data models. In this work, we consider efficient construction of composable core-sets for the determinant maximization problem. This can also be cast as the MAP inference task for “determinantal point processes"", that have recently gained a lot of interest for modeling diversity and fairness. The problem was recently studied in \cite{indyk2018composable}, where they designed composable core-sets with the optimal approximation bound of $O(k)^k$. On the other hand, the more practical “Greedy"" algorithm has been previously used in similar contexts. In this work, first we provide a theoretical approximation guarantee of $C^{k^2}$ for the Greedy algorithm in the context of composable core-sets; Further, we propose to use a “Local Search"" based algorithm that while being still practical, achieves a nearly optimal approximation bound of $O(k)^{2k}$; Finally, we implement all three algorithms and show the effectiveness of our proposed algorithm on standard data sets.},
}
"
"
"
"
@inproceedings{pmlr-v97-mehrotra19a,
	title        = {Toward Controlling Discrimination in Online Ad Auctions},
	author       = {Celis, Elisa and Mehrotra, Anay and Vishnoi, Nisheeth},
	booktitle    = {Proceedings of the 36th International Conference on Machine Learning},
	pages        = {4456--4465},
	year         = 2019,
	editor       = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	volume       = 97,
	series       = {Proceedings of Machine Learning Research},
	month        = {09--15 Jun},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v97/mehrotra19a/mehrotra19a.pdf},
	url          = {https://proceedings.mlr.press/v97/mehrotra19a.html},
	abstract     = {Online advertising platforms are thriving due to the customizable audiences they offer advertisers. However, recent studies show that advertisements can be discriminatory with respect to the gender or race of the audience that sees the ad, and may inadvertently cross ethical and/or legal boundaries. To prevent this, we propose a constrained ad auction framework that maximizes the platform’s revenue conditioned on ensuring that the audience seeing an advertiser’s ad is distributed appropriately across sensitive types such as gender or race. Building upon Myerson’s classic work, we first present an optimal auction mechanism for a large class of fairness constraints. Finding the parameters of this optimal auction, however, turns out to be a non-convex problem. We show that this non-convex problem can be reformulated as a more structured non-convex problem with no saddle points or local-maxima; this allows us to develop a gradient-descent-based algorithm to solve it. Our empirical results on the A1 Yahoo! dataset demonstrate that our algorithm can obtain uniform coverage across different user types for each advertiser at a minor loss to the revenue of the platform, and a small change to the size of the audience each advertiser reaches.},
}
"
"
@inproceedings{pmlr-v97-mohri19a,
	title        = {Agnostic Federated Learning},
	author       = {Mohri, Mehryar and Sivek, Gary and Suresh, Ananda Theertha},
	booktitle    = {Proceedings of the 36th International Conference on Machine Learning},
	pages        = {4615--4625},
	year         = 2019,
	editor       = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	volume       = 97,
	series       = {Proceedings of Machine Learning Research},
	month        = {09--15 Jun},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v97/mohri19a/mohri19a.pdf},
	url          = {https://proceedings.mlr.press/v97/mohri19a.html},
	abstract     = {A key learning scenario in large-scale applications is that of federated learning, where a centralized model is trained based on data originating from a large number of clients. We argue that, with the existing training and inference, federated models can be biased towards different clients. Instead, we propose a new framework of agnostic federated learning, where the centralized model is optimized for any target distribution formed by a mixture of the client distributions. We further show that this framework naturally yields a notion of fairness. We present data-dependent Rademacher complexity guarantees for learning with this objective, which guide the definition of an algorithm for agnostic federated learning. We also give a fast stochastic optimization algorithm for solving the corresponding optimization problem, for which we prove convergence bounds, assuming a convex loss function and a convex hypothesis set. We further empirically demonstrate the benefits of our approach in several datasets. Beyond federated learning, our framework and algorithm can be of interest to other learning scenarios such as cloud computing, domain adaptation, drifting, and other contexts where the training and test distributions do not coincide.},
}
"
"
@inproceedings{pmlr-v97-nabi19a,
	title        = {Learning Optimal Fair Policies},
	author       = {Nabi, Razieh and Malinsky, Daniel and Shpitser, Ilya},
	booktitle    = {Proceedings of the 36th International Conference on Machine Learning},
	pages        = {4674--4682},
	year         = 2019,
	editor       = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	volume       = 97,
	series       = {Proceedings of Machine Learning Research},
	month        = {09--15 Jun},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v97/nabi19a/nabi19a.pdf},
	url          = {https://proceedings.mlr.press/v97/nabi19a.html},
	abstract     = {Systematic discriminatory biases present in our society influence the way data is collected and stored, the way variables are defined, and the way scientific findings are put into practice as policy. Automated decision procedures and learning algorithms applied to such data may serve to perpetuate existing injustice or unfairness in our society. In this paper, we consider how to make optimal but fair decisions, which “break the cycle of injustice” by correcting for the unfair dependence of both decisions and outcomes on sensitive features (e.g., variables that correspond to gender, race, disability, or other protected attributes). We use methods from causal inference and constrained optimization to learn optimal policies in a way that addresses multiple potential biases which afflict data analysis in sensitive contexts, extending the approach of Nabi &amp; Shpitser (2018). Our proposal comes equipped with the theoretical guarantee that the chosen fair policy will induce a joint distribution for new instances that satisfies given fairness constraints. We illustrate our approach with both synthetic data and real criminal justice data.},
}
"
"
@inproceedings{pmlr-v97-shen19c,
	title        = {Mixture Models for Diverse Machine Translation: Tricks of the Trade},
	author       = {Shen, Tianxiao and Ott, Myle and Auli, Michael and Ranzato, Marc'Aurelio},
	booktitle    = {Proceedings of the 36th International Conference on Machine Learning},
	pages        = {5719--5728},
	year         = 2019,
	editor       = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	volume       = 97,
	series       = {Proceedings of Machine Learning Research},
	month        = {09--15 Jun},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v97/shen19c/shen19c.pdf},
	url          = {https://proceedings.mlr.press/v97/shen19c.html},
	abstract     = {Mixture models trained via EM are among the simplest, most widely used and well understood latent variable models in the machine learning literature. Surprisingly, these models have been hardly explored in text generation applications such as machine translation. In principle, they provide a latent variable to control generation and produce a diverse set of hypotheses. In practice, however, mixture models are prone to degeneracies—often only one component gets trained or the latent variable is simply ignored. We find that disabling dropout noise in responsibility computation is critical to successful training. In addition, the design choices of parameterization, prior distribution, hard versus soft EM and online versus offline assignment can dramatically affect model performance. We develop an evaluation protocol to assess both quality and diversity of generations against multiple references, and provide an extensive empirical study of several mixture model variants. Our analysis shows that certain types of mixture models are more robust and offer the best trade-off between translation quality and diversity compared to variational models and diverse decoding approaches.\footnote{Code to reproduce the results in this paper is available at \url{https://github.com/pytorch/fairseq}}},
}
"
"
@inproceedings{pmlr-v97-ustun19a,
	title        = {Fairness without Harm: Decoupled Classifiers with Preference Guarantees},
	author       = {Ustun, Berk and Liu, Yang and Parkes, David},
	booktitle    = {Proceedings of the 36th International Conference on Machine Learning},
	pages        = {6373--6382},
	year         = 2019,
	editor       = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	volume       = 97,
	series       = {Proceedings of Machine Learning Research},
	month        = {09--15 Jun},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v97/ustun19a/ustun19a.pdf},
	url          = {https://proceedings.mlr.press/v97/ustun19a.html},
	abstract     = {In domains such as medicine, it can be acceptable for machine learning models to include <em>sensitive attributes</em> such as gender and ethnicity. In this work, we argue that when there is this kind of treatment disparity, then it should be in the best interest of each group. Drawing on ethical principles such as beneficence (""do the best"") and non-maleficence (""do no harm""), we show how to use sensitive attributes to train decoupled classifiers that satisfy preference guarantees. These guarantees ensure the majority of individuals in each group prefer their assigned classifier to (i) a pooled model that ignores group membership (rationality), and (ii) the model assigned to any other group (envy-freeness). We introduce a recursive procedure that adaptively selects group attributes for decoupling, and present formal conditions to ensure preference guarantees in terms of generalization error. We validate the effectiveness of the procedure on real-world datasets, showing that it improves accuracy without violating preference guarantees on test data.},
}
"
"
@inproceedings{pmlr-v97-wang19l,
	title        = {Repairing without Retraining: Avoiding Disparate Impact with Counterfactual Distributions},
	author       = {Wang, Hao and Ustun, Berk and Calmon, Flavio},
	booktitle    = {Proceedings of the 36th International Conference on Machine Learning},
	pages        = {6618--6627},
	year         = 2019,
	editor       = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	volume       = 97,
	series       = {Proceedings of Machine Learning Research},
	month        = {09--15 Jun},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v97/wang19l/wang19l.pdf},
	url          = {https://proceedings.mlr.press/v97/wang19l.html},
	abstract     = {When the performance of a machine learning model varies over groups defined by sensitive attributes (e.g., gender or ethnicity), the performance disparity can be expressed in terms of the probability distributions of the input and output variables over each group. In this paper, we exploit this fact to reduce the disparate impact of a fixed classification model over a population of interest. Given a black-box classifier, we aim to eliminate the performance gap by perturbing the distribution of input variables for the disadvantaged group. We refer to the perturbed distribution as a counterfactual distribution, and characterize its properties for common fairness criteria. We introduce a descent algorithm to learn a counterfactual distribution from data. We then discuss how the estimated distribution can be used to build a data preprocessor that can reduce disparate impact without training a new model. We validate our approach through experiments on real-world datasets, showing that it can repair different forms of disparity without a significant drop in accuracy.},
}
"
"
@inproceedings{pmlr-v97-williamson19a,
	title        = {Fairness risk measures},
	author       = {Williamson, Robert and Menon, Aditya},
	booktitle    = {Proceedings of the 36th International Conference on Machine Learning},
	pages        = {6786--6797},
	year         = 2019,
	editor       = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	volume       = 97,
	series       = {Proceedings of Machine Learning Research},
	month        = {09--15 Jun},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v97/williamson19a/williamson19a.pdf},
	url          = {https://proceedings.mlr.press/v97/williamson19a.html},
	abstract     = {Ensuring that classifiers are non-discriminatory or fair with respect to a sensitive feature (e.g., race or gender) is a topical problem. Progress in this task requires fixing a definition of fairness, and there have been several proposals in this regard over the past few years. Several of these, however, assume either binary sensitive features (thus precluding categorical or real-valued sensitive groups), or result in non-convex objectives (thus adversely affecting the optimisation landscape). In this paper, we propose a new definition of fairness that generalises some existing proposals, while allowing for generic sensitive features and resulting in a convex objective. The key idea is to enforce that the expected losses (or risks) across each subgroup induced by the sensitive feature are commensurate. We show how this relates to the rich literature on risk measures from mathematical finance. As a special case, this leads to a new convex fairness-aware objective based on minimising the conditional value at risk (CVaR).},
}
"
"
@inproceedings{pmlr-v97-zhang19i,
	title        = {Bridging Theory and Algorithm for Domain Adaptation},
	author       = {Zhang, Yuchen and Liu, Tianle and Long, Mingsheng and Jordan, Michael},
	booktitle    = {Proceedings of the 36th International Conference on Machine Learning},
	pages        = {7404--7413},
	year         = 2019,
	editor       = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	volume       = 97,
	series       = {Proceedings of Machine Learning Research},
	month        = {09--15 Jun},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v97/zhang19i/zhang19i.pdf},
	url          = {https://proceedings.mlr.press/v97/zhang19i.html},
	abstract     = {This paper addresses the problem of unsupervised domain adaption from theoretical and algorithmic perspectives. Existing domain adaptation theories naturally imply minimax optimization algorithms, which connect well with the domain adaptation methods based on adversarial learning. However, several disconnections still exist and form the gap between theory and algorithm. We extend previous theories (Mansour et al., 2009c; Ben-David et al., 2010) to multiclass classification in domain adaptation, where classifiers based on the scoring functions and margin loss are standard choices in algorithm design. We introduce Margin Disparity Discrepancy, a novel measurement with rigorous generalization bounds, tailored to the distribution comparison with the asymmetric margin loss, and to the minimax optimization for easier training. Our theory can be seamlessly transformed into an adversarial learning algorithm for domain adaptation, successfully bridging the gap between theory and algorithm. A series of empirical studies show that our algorithm achieves the state of the art accuracies on challenging domain adaptation tasks.},
}
"
"
"
"
@inproceedings{pmlr-v119-bistritz20a,
	title        = {My Fair Bandit: Distributed Learning of Max-Min Fairness with Multi-player Bandits},
	author       = {Bistritz, Ilai and Baharav, Tavor and Leshem, Amir and Bambos, Nicholas},
	booktitle    = {Proceedings of the 37th International Conference on Machine Learning},
	pages        = {930--940},
	year         = 2020,
	editor       = {III, Hal Daumé and Singh, Aarti},
	volume       = 119,
	series       = {Proceedings of Machine Learning Research},
	month        = {13--18 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v119/bistritz20a/bistritz20a.pdf},
	url          = {https://proceedings.mlr.press/v119/bistritz20a.html},
	abstract     = {Consider N cooperative but non-communicating players where each plays one out of M arms for T turns. Players have different utilities for each arm, representable as an NxM matrix. These utilities are unknown to the players. In each turn players receive noisy observations of their utility for their selected arm. However, if any other players selected the same arm that turn, they will all receive zero utility due to the conflict. No other communication or coordination between the players is possible. Our goal is to design a distributed algorithm that learns the matching between players and arms that achieves max-min fairness while minimizing the regret. We present an algorithm and prove that it is regret optimal up to a \log\log T factor. This is the first max-min fairness multi-player bandit algorithm with (near) order optimal regret.},
}
"
"
@inproceedings{pmlr-v119-bronskill20a,
	title        = {{T}ask{N}orm: Rethinking Batch Normalization for Meta-Learning},
	author       = {Bronskill, John and Gordon, Jonathan and Requeima, James and Nowozin, Sebastian and Turner, Richard},
	booktitle    = {Proceedings of the 37th International Conference on Machine Learning},
	pages        = {1153--1164},
	year         = 2020,
	editor       = {III, Hal Daumé and Singh, Aarti},
	volume       = 119,
	series       = {Proceedings of Machine Learning Research},
	month        = {13--18 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v119/bronskill20a/bronskill20a.pdf},
	url          = {https://proceedings.mlr.press/v119/bronskill20a.html},
	abstract     = {Modern meta-learning approaches for image classification rely on increasingly deep networks to achieve state-of-the-art performance, making batch normalization an essential component of meta-learning pipelines. However, the hierarchical nature of the meta-learning setting presents several challenges that can render conventional batch normalization ineffective, giving rise to the need to rethink normalization in this setting. We evaluate a range of approaches to batch normalization for meta-learning scenarios, and develop a novel approach that we call TaskNorm. Experiments on fourteen datasets demonstrate that the choice of batch normalization has a dramatic effect on both classification accuracy and training time for both gradient based- and gradient-free meta-learning approaches. Importantly, TaskNorm is found to consistently improve performance. Finally, we provide a set of best practices for normalization that will allow fair comparison of meta-learning algorithms.},
}
"
"
@inproceedings{pmlr-v119-brubach20a,
	title        = {A Pairwise Fair and Community-preserving Approach to k-Center Clustering},
	author       = {Brubach, Brian and Chakrabarti, Darshan and Dickerson, John and Khuller, Samir and Srinivasan, Aravind and Tsepenekas, Leonidas},
	booktitle    = {Proceedings of the 37th International Conference on Machine Learning},
	pages        = {1178--1189},
	year         = 2020,
	editor       = {III, Hal Daumé and Singh, Aarti},
	volume       = 119,
	series       = {Proceedings of Machine Learning Research},
	month        = {13--18 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v119/brubach20a/brubach20a.pdf},
	url          = {https://proceedings.mlr.press/v119/brubach20a.html},
	abstract     = {Clustering is a foundational problem in machine learning with numerous applications. As machine learning increases in ubiquity as a backend for automated systems, concerns about fairness arise. Much of the current literature on fairness deals with discrimination against protected classes in supervised learning (group fairness). We define a different notion of fair clustering wherein the probability that two points (or a community of points) become separated is bounded by an increasing function of their pairwise distance (or community diameter). We capture the situation where data points represent people who gain some benefit from being clustered together. Unfairness arises when certain points are deterministically separated, either arbitrarily or by someone who intends to harm them as in the case of gerrymandering election districts. In response, we formally define two new types of fairness in the clustering setting, pairwise fairness and community preservation. To explore the practicality of our fairness goals, we devise an approach for extending existing $k$-center algorithms to satisfy these fairness constraints. Analysis of this approach proves that reasonable approximations can be achieved while maintaining fairness. In experiments, we compare the effectiveness of our approach to classical $k$-center algorithms/heuristics and explore the tradeoff between optimal clustering and fairness.},
}
"
"
@inproceedings{pmlr-v119-buyl20a,
	title        = {{D}e{B}ayes: a {B}ayesian Method for Debiasing Network Embeddings},
	author       = {Buyl, Maarten and De Bie, Tijl},
	booktitle    = {Proceedings of the 37th International Conference on Machine Learning},
	pages        = {1220--1229},
	year         = 2020,
	editor       = {III, Hal Daumé and Singh, Aarti},
	volume       = 119,
	series       = {Proceedings of Machine Learning Research},
	month        = {13--18 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v119/buyl20a/buyl20a.pdf},
	url          = {https://proceedings.mlr.press/v119/buyl20a.html},
	abstract     = {As machine learning algorithms are increasingly deployed for high-impact automated decision making, ethical and increasingly also legal standards demand that they treat all individuals fairly, without discrimination based on their age, gender, race or other sensitive traits. In recent years much progress has been made on ensuring fairness and reducing bias in standard machine learning settings. Yet, for network embedding, with applications in vulnerable domains ranging from social network analysis to recommender systems, current options remain limited both in number and performance. We thus propose DeBayes: a conceptually elegant Bayesian method that is capable of learning debiased embeddings by using a biased prior. Our experiments show that these representations can then be used to perform link prediction that is significantly more fair in terms of popular metrics such as demographic parity and equalized opportunity.},
}
"
"
@inproceedings{pmlr-v119-celis20a,
	title        = {Data preprocessing to mitigate bias: A maximum entropy based approach},
	author       = {Celis, L. Elisa and Keswani, Vijay and Vishnoi, Nisheeth},
	booktitle    = {Proceedings of the 37th International Conference on Machine Learning},
	pages        = {1349--1359},
	year         = 2020,
	editor       = {III, Hal Daumé and Singh, Aarti},
	volume       = 119,
	series       = {Proceedings of Machine Learning Research},
	month        = {13--18 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v119/celis20a/celis20a.pdf},
	url          = {https://proceedings.mlr.press/v119/celis20a.html},
	abstract     = {Data containing human or social attributes may over- or under-represent groups with respect to salient social attributes such as gender or race, which can lead to biases in downstream applications. This paper presents an algorithmic framework that can be used as a data preprocessing method towards mitigating such bias. Unlike prior work, it can efficiently learn distributions over large domains, controllably adjust the representation rates of protected groups and achieve target fairness metrics such as statistical parity, yet remains close to the empirical distribution induced by the given dataset. Our approach leverages the principle of maximum entropy {–} amongst all distributions satisfying a given set of constraints, we should choose the one closest in KL-divergence to a given prior. While maximum entropy distributions can succinctly encode distributions over large domains, they can be difficult to compute. Our main contribution is an instantiation of this framework for our set of constraints and priors, which encode our bias mitigation goals, and that runs in time polynomial in the dimension of the data. Empirically, we observe that samples from the learned distribution have desired representation rates and statistical rates, and when used for training a classifier incurs only a slight loss in accuracy while maintaining fairness properties.},
}
"
"
@inproceedings{pmlr-v119-chiplunkar20a,
	title        = {How to Solve Fair k-Center in Massive Data Models},
	author       = {Chiplunkar, Ashish and Kale, Sagar and Ramamoorthy, Sivaramakrishnan Natarajan},
	booktitle    = {Proceedings of the 37th International Conference on Machine Learning},
	pages        = {1877--1886},
	year         = 2020,
	editor       = {III, Hal Daumé and Singh, Aarti},
	volume       = 119,
	series       = {Proceedings of Machine Learning Research},
	month        = {13--18 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v119/chiplunkar20a/chiplunkar20a.pdf},
	url          = {https://proceedings.mlr.press/v119/chiplunkar20a.html},
	abstract     = {Fueled by massive data, important decision making is being automated with the help of algorithms, therefore, fairness in algorithms has become an especially important research topic. In this work, we design new streaming and distributed algorithms for the fair k-center problem that models fair data summarization. The streaming and distributed models of computation have an attractive feature of being able to handle massive data sets that do not fit into main memory. Our main contributions are: (a) the first distributed algorithm; which has provably constant approximation ratio and is extremely parallelizable, and (b) a two-pass streaming algorithm with a provable approximation guarantee matching the best known algorithm (which is not a streaming algorithm). Our algorithms have the advantages of being easy to implement in practice, being fast with linear running times, having very small working memory and communication, and outperforming existing algorithms on several real and synthetic data sets. To complement our distributed algorithm, we also give a hardness result for natural distributed algorithms, which holds for even the special case of k-center.},
}
"
"
@inproceedings{pmlr-v119-choi20a,
	title        = {Fair Generative Modeling via Weak Supervision},
	author       = {Choi, Kristy and Grover, Aditya and Singh, Trisha and Shu, Rui and Ermon, Stefano},
	booktitle    = {Proceedings of the 37th International Conference on Machine Learning},
	pages        = {1887--1898},
	year         = 2020,
	editor       = {III, Hal Daumé and Singh, Aarti},
	volume       = 119,
	series       = {Proceedings of Machine Learning Research},
	month        = {13--18 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v119/choi20a/choi20a.pdf},
	url          = {https://proceedings.mlr.press/v119/choi20a.html},
	abstract     = {Real-world datasets are often biased with respect to key demographic factors such as race and gender. Due to the latent nature of the underlying factors, detecting and mitigating bias is especially challenging for unsupervised machine learning. We present a weakly supervised algorithm for overcoming dataset bias for deep generative models. Our approach requires access to an additional small, unlabeled reference dataset as the supervision signal, thus sidestepping the need for explicit labels on the underlying bias factors. Using this supplementary dataset, we detect the bias in existing datasets via a density ratio technique and learn generative models which efficiently achieve the twin goals of: 1) data efficiency by using training examples from both biased and reference datasets for learning; and 2) data generation close in distribution to the reference dataset at test time. Empirically, we demonstrate the efficacy of our approach which reduces bias w.r.t. latent factors by an average of up to 34.6% over baselines for comparable image generation using generative adversarial networks.},
}
"
"
@inproceedings{pmlr-v119-creager20a,
	title        = {Causal Modeling for Fairness In Dynamical Systems},
	author       = {Creager, Elliot and Madras, David and Pitassi, Toniann and Zemel, Richard},
	booktitle    = {Proceedings of the 37th International Conference on Machine Learning},
	pages        = {2185--2195},
	year         = 2020,
	editor       = {III, Hal Daumé and Singh, Aarti},
	volume       = 119,
	series       = {Proceedings of Machine Learning Research},
	month        = {13--18 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v119/creager20a/creager20a.pdf},
	url          = {https://proceedings.mlr.press/v119/creager20a.html},
	abstract     = {In many applications areas—lending, education, and online recommenders, for example—fairness and equity concerns emerge when a machine learning system interacts with a dynamically changing environment to produce both immediate and long-term effects for individuals and demographic groups. We discuss causal directed acyclic graphs (DAGs) as a unifying framework for the recent literature on fairness in such dynamical systems. We show that this formulation affords several new directions of inquiry to the modeler, where sound causal assumptions can be expressed and manipulated. We emphasize the importance of computing interventional quantities in the dynamical fairness setting, and show how causal assumptions enable simulation (when environment dynamics are known) and estimation by adjustment (when dynamics are unknown) of intervention on short- and long-term outcomes, at both the group and individual levels.},
}
"
"
"
"
@inproceedings{pmlr-v119-hinder20a,
	title        = {Towards Non-Parametric Drift Detection via Dynamic Adapting Window Independence Drift Detection ({DAWIDD})},
	author       = {Hinder, Fabian and Artelt, Andr{\'e} and Hammer, Barbara},
	booktitle    = {Proceedings of the 37th International Conference on Machine Learning},
	pages        = {4249--4259},
	year         = 2020,
	editor       = {III, Hal Daumé and Singh, Aarti},
	volume       = 119,
	series       = {Proceedings of Machine Learning Research},
	month        = {13--18 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v119/hinder20a/hinder20a.pdf},
	url          = {https://proceedings.mlr.press/v119/hinder20a.html},
	abstract     = {The notion of concept drift refers to the phenomenon that the distribution, which is underlying the observed data, changes over time; as a consequence machine learning models may become inaccurate and need adjustment. Many online learning schemes include drift detection to actively detect and react to observed changes. Yet, reliable drift detection constitutes a challenging problem in particular in the context of high dimensional data, varying drift characteristics, and the absence of a parametric model such as a classification scheme which reflects the drift. In this paper we present a novel concept drift detection method, Dynamic Adapting Window Independence Drift Detection (DAWIDD), which aims for non-parametric drift detection of diverse drift characteristics. For this purpose, we establish a mathematical equivalence of the presence of drift to the dependency of specific random variables in an according drift process. This allows us to rely on independence tests rather than parametric models or the classification loss, resulting in a fairly robust scheme to universally detect different types of drift, as it is also confirmed in experiments.},
}
"
"
@inproceedings{pmlr-v119-jones20a,
	title        = {Fair k-Centers via Maximum Matching},
	author       = {Jones, Matthew and Nguyen, Huy and Nguyen, Thy},
	booktitle    = {Proceedings of the 37th International Conference on Machine Learning},
	pages        = {4940--4949},
	year         = 2020,
	editor       = {III, Hal Daumé and Singh, Aarti},
	volume       = 119,
	series       = {Proceedings of Machine Learning Research},
	month        = {13--18 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v119/jones20a/jones20a.pdf},
	url          = {https://proceedings.mlr.press/v119/jones20a.html},
	abstract     = {The field of algorithms has seen a push for fairness, or the removal of inherent bias, in recent history. In data summarization, where a much smaller subset of a data set is chosen to represent the whole of the data, fairness can be introduced by guaranteeing each ""demographic group"" a specific portion of the representative subset. Specifically, this paper examines this fair variant of the k-centers problem, where a subset of the data with cardinality k is chosen to minimize distance to the rest of the data. Previous papers working on this problem presented both a 3-approximation algorithm with a super-linear runtime and a linear-time algorithm whose approximation factor is exponential in the number of demographic groups. This paper combines the best of each algorithm by presenting a linear-time algorithm with a guaranteed 3-approximation factor and provides empirical evidence of both the algorithm’s runtime and effectiveness.},
}
"
"
@inproceedings{pmlr-v119-kim20a,
	title        = {{FACT}: A Diagnostic for Group Fairness Trade-offs},
	author       = {Kim, Joon Sik and Chen, Jiahao and Talwalkar, Ameet},
	booktitle    = {Proceedings of the 37th International Conference on Machine Learning},
	pages        = {5264--5274},
	year         = 2020,
	editor       = {III, Hal Daumé and Singh, Aarti},
	volume       = 119,
	series       = {Proceedings of Machine Learning Research},
	month        = {13--18 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v119/kim20a/kim20a.pdf},
	url          = {https://proceedings.mlr.press/v119/kim20a.html},
	abstract     = {Group fairness, a class of fairness notions that measure how different groups of individuals are treated differently according to their protected attributes, has been shown to conflict with one another, often with a necessary cost in loss of model’s predictive performance. We propose a general diagnostic that enables systematic characterization of these trade-offs in group fairness. We observe that the majority of group fairness notions can be expressed via the fairness-confusion tensor, which is the confusion matrix split according to the protected attribute values. We frame several optimization problems that directly optimize both accuracy and fairness objectives over the elements of this tensor, which yield a general perspective for understanding multiple trade-offs including group fairness incompatibilities. It also suggests an alternate post-processing method for designing fair classifiers. On synthetic and real datasets, we demonstrate the use cases of our diagnostic, particularly on understanding the trade-off landscape between accuracy and fairness.},
}
"
"
@inproceedings{pmlr-v119-lin20h,
	title        = {Finite-Time Last-Iterate Convergence for Multi-Agent Learning in Games},
	author       = {Lin, Tianyi and Zhou, Zhengyuan and Mertikopoulos, Panayotis and Jordan, Michael},
	booktitle    = {Proceedings of the 37th International Conference on Machine Learning},
	pages        = {6161--6171},
	year         = 2020,
	editor       = {III, Hal Daumé and Singh, Aarti},
	volume       = 119,
	series       = {Proceedings of Machine Learning Research},
	month        = {13--18 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v119/lin20h/lin20h.pdf},
	url          = {https://proceedings.mlr.press/v119/lin20h.html},
	abstract     = {In this paper, we consider multi-agent learning via online gradient descent in a class of games called $\lambda$-cocoercive games, a fairly broad class of games that admits many Nash equilibria and that properly includes unconstrained strongly monotone games. We characterize the finite-time last-iterate convergence rate for joint OGD learning on $\lambda$-cocoercive games; further, building on this result, we develop a fully adaptive OGD learning algorithm that does not require any knowledge of problem parameter (e.g. cocoercive constant $\lambda$) and show, via a novel double-stopping time technique, that this adaptive algorithm achieves same finite-time last-iterate convergence rate as non-adaptive counterpart. Subsequently, we extend OGD learning to the noisy gradient feedback case and establish last-iterate convergence results–first qualitative almost sure convergence, then quantitative finite-time convergence rates– all under non-decreasing step-sizes. To our knowledge, we provide the first set of results that fill in several gaps of the existing multi-agent online learning literature, where three aspects–finite-time convergence rates, non-decreasing step-sizes, and fully adaptive algorithms have been unexplored before.},
}
"
"
@inproceedings{pmlr-v119-locatello20a,
	title        = {Weakly-Supervised Disentanglement Without Compromises},
	author       = {Locatello, Francesco and Poole, Ben and Raetsch, Gunnar and Sch{\""o}lkopf, Bernhard and Bachem, Olivier and Tschannen, Michael},
	booktitle    = {Proceedings of the 37th International Conference on Machine Learning},
	pages        = {6348--6359},
	year         = 2020,
	editor       = {III, Hal Daumé and Singh, Aarti},
	volume       = 119,
	series       = {Proceedings of Machine Learning Research},
	month        = {13--18 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v119/locatello20a/locatello20a.pdf},
	url          = {https://proceedings.mlr.press/v119/locatello20a.html},
	abstract     = {Intelligent agents should be able to learn useful representations by observing changes in their environment. We model such observations as pairs of non-i.i.d. images sharing at least one of the underlying factors of variation. First, we theoretically show that only knowing how many factors have changed, but not which ones, is sufficient to learn disentangled representations. Second, we provide practical algorithms that learn disentangled representations from pairs of images without requiring annotation of groups, individual factors, or the number of factors that have changed. Third, we perform a large-scale empirical study and show that such pairs of observations are sufficient to reliably learn disentangled representations on several benchmark data sets. Finally, we evaluate our learned representations and find that they are simultaneously useful on a diverse suite of tasks, including generalization under covariate shifts, fairness, and abstract reasoning. Overall, our results demonstrate that weak supervision enables learning of useful disentangled representations in realistic scenarios.},
}
"
"
@inproceedings{pmlr-v119-lohaus20a,
	title        = {Too Relaxed to Be Fair},
	author       = {Lohaus, Michael and Perrot, Michael and Luxburg, Ulrike Von},
	booktitle    = {Proceedings of the 37th International Conference on Machine Learning},
	pages        = {6360--6369},
	year         = 2020,
	editor       = {III, Hal Daumé and Singh, Aarti},
	volume       = 119,
	series       = {Proceedings of Machine Learning Research},
	month        = {13--18 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v119/lohaus20a/lohaus20a.pdf},
	url          = {https://proceedings.mlr.press/v119/lohaus20a.html},
	abstract     = {We address the problem of classification under fairness constraints. Given a notion of fairness, the goal is to learn a classifier that is not discriminatory against a group of individuals. In the literature, this problem is often formulated as a constrained optimization problem and solved using relaxations of the fairness constraints. We show that many existing relaxations are unsatisfactory: even if a model satisfies the relaxed constraint, it can be surprisingly unfair. We propose a principled framework to solve this problem. This new approach uses a strongly convex formulation and comes with theoretical guarantees on the fairness of its solution. In practice, we show that this method gives promising results on real data.},
}
"
"
@inproceedings{pmlr-v119-ma20d,
	title        = {Quadratically Regularized Subgradient Methods for Weakly Convex Optimization with Weakly Convex Constraints},
	author       = {Ma, Runchao and Lin, Qihang and Yang, Tianbao},
	booktitle    = {Proceedings of the 37th International Conference on Machine Learning},
	pages        = {6554--6564},
	year         = 2020,
	editor       = {III, Hal Daumé and Singh, Aarti},
	volume       = 119,
	series       = {Proceedings of Machine Learning Research},
	month        = {13--18 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v119/ma20d/ma20d.pdf},
	url          = {https://proceedings.mlr.press/v119/ma20d.html},
	abstract     = {Optimization models with non-convex constraints arise in many tasks in machine learning, e.g., learning with fairness constraints or Neyman-Pearson classification with non-convex loss. Although many efficient methods have been developed with theoretical convergence guarantees for non-convex unconstrained problems, it remains a challenge to design provably efficient algorithms for problems with non-convex functional constraints. This paper proposes a class of subgradient methods for constrained optimization where the objective function and the constraint functions are weakly convex and nonsmooth. Our methods solve a sequence of strongly convex subproblems, where a quadratic regularization term is added to both the objective function and each constraint function. Each subproblem can be solved by various algorithms for strongly convex optimization. Under a uniform Slater’s condition, we establish the computation complexities of our methods for finding a nearly stationary point.},
}
"
"
@inproceedings{pmlr-v119-mahabadi20a,
	title        = {Individual Fairness for k-Clustering},
	author       = {Mahabadi, Sepideh and Vakilian, Ali},
	booktitle    = {Proceedings of the 37th International Conference on Machine Learning},
	pages        = {6586--6596},
	year         = 2020,
	editor       = {III, Hal Daumé and Singh, Aarti},
	volume       = 119,
	series       = {Proceedings of Machine Learning Research},
	month        = {13--18 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v119/mahabadi20a/mahabadi20a.pdf},
	url          = {https://proceedings.mlr.press/v119/mahabadi20a.html},
	abstract     = {We give a local search based algorithm for $k$-median and $k$-means (and more generally for any $k$-clustering with $\ell_p$ norm cost function) from the perspective of individual fairness. More precisely, for a point $x$ in a point set $P$ of size $n$, let $r(x)$ be the minimum radius such that the ball of radius $r(x)$ centered at $x$ has at least $n/k$ points from $P$. Intuitively, if a set of $k$ random points are chosen from $P$ as centers, every point $x\in P$ expects to have a center within radius $r(x)$. In this work, we show how to get an approximately optimal such fair $k$-clustering: The $k$-median ($k$-means) cost of our solution is within a constant factor of the cost of an optimal fair $k$-clustering, and our solution approximately satisfies the fairness condition (also within a constant factor).},
}
"
"
@inproceedings{pmlr-v119-martinez20a,
	title        = {Minimax Pareto Fairness: A Multi Objective Perspective},
	author       = {Martinez, Natalia and Bertran, Martin and Sapiro, Guillermo},
	booktitle    = {Proceedings of the 37th International Conference on Machine Learning},
	pages        = {6755--6764},
	year         = 2020,
	editor       = {III, Hal Daumé and Singh, Aarti},
	volume       = 119,
	series       = {Proceedings of Machine Learning Research},
	month        = {13--18 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v119/martinez20a/martinez20a.pdf},
	url          = {https://proceedings.mlr.press/v119/martinez20a.html},
	abstract     = {In this work we formulate and formally characterize group fairness as a multi-objective optimization problem, where each sensitive group risk is a separate objective. We propose a fairness criterion where a classifier achieves minimax risk and is Pareto-efficient w.r.t. all groups, avoiding unnecessary harm, and can lead to the best zero-gap model if policy dictates so. We provide a simple optimization algorithm compatible with deep neural networks to satisfy these constraints. Since our method does not require test-time access to sensitive attributes, it can be applied to reduce worst-case classification errors between outcomes in unbalanced classification problems. We test the proposed methodology on real case-studies of predicting income, ICU patient mortality, skin lesions classification, and assessing credit risk, demonstrating how our framework compares favorably to other approaches.},
}
"
"
@inproceedings{pmlr-v119-mladenov20a,
	title        = {Optimizing Long-term Social Welfare in Recommender Systems: A Constrained Matching Approach},
	author       = {Mladenov, Martin and Creager, Elliot and Ben-Porat, Omer and Swersky, Kevin and Zemel, Richard and Boutilier, Craig},
	booktitle    = {Proceedings of the 37th International Conference on Machine Learning},
	pages        = {6987--6998},
	year         = 2020,
	editor       = {III, Hal Daumé and Singh, Aarti},
	volume       = 119,
	series       = {Proceedings of Machine Learning Research},
	month        = {13--18 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v119/mladenov20a/mladenov20a.pdf},
	url          = {https://proceedings.mlr.press/v119/mladenov20a.html},
	abstract     = {Most recommender systems (RS) research assumes that a user’s utility can be maximized independently of the utility of the other agents (e.g., other users, content providers). In realistic settings, this is often not true – the dynamics of an RS ecosystem couple the long-term utility of all agents. In this work, we explore settings in which content providers cannot remain viable unless they receive a certain level of user engagement. We formulate this problem as one of equilibrium selection in the induced dynamical system, and show that it can be solved as an optimal constrained matching problem. Our model ensures the system reaches an equilibrium with maximal social welfare supported by a sufficiently diverse set of viable providers. We demonstrate that even in a simple, stylized dynamical RS model, the standard myopic approach to recommendation - always matching a user to the best provider - performs poorly. We develop several scalable techniques to solve the matching problem, and also draw connections to various notions of user regret and fairness, arguing that these outcomes are fairer in a utilitarian sense.},
}
"
"
@inproceedings{pmlr-v119-mozannar20a,
	title        = {Fair Learning with Private Demographic Data},
	author       = {Mozannar, Hussein and Ohannessian, Mesrob and Srebro, Nathan},
	booktitle    = {Proceedings of the 37th International Conference on Machine Learning},
	pages        = {7066--7075},
	year         = 2020,
	editor       = {III, Hal Daumé and Singh, Aarti},
	volume       = 119,
	series       = {Proceedings of Machine Learning Research},
	month        = {13--18 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v119/mozannar20a/mozannar20a.pdf},
	url          = {https://proceedings.mlr.press/v119/mozannar20a.html},
	abstract     = {Sensitive attributes such as race are rarely available to learners in real world settings as their collection is often restricted by laws and regulations. We give a scheme that allows individuals to release their sensitive information privately while still allowing any downstream entity to learn non-discriminatory predictors. We show how to adapt non-discriminatory learners to work with privatized protected attributes giving theoretical guarantees on performance. Finally, we highlight how the methodology could apply to learning fair predictors in settings where protected attributes are only available for a subset of the data.},
}
"
"
@inproceedings{pmlr-v119-mukherjee20a,
	title        = {Two Simple Ways to Learn Individual Fairness Metrics from Data},
	author       = {Mukherjee, Debarghya and Yurochkin, Mikhail and Banerjee, Moulinath and Sun, Yuekai},
	booktitle    = {Proceedings of the 37th International Conference on Machine Learning},
	pages        = {7097--7107},
	year         = 2020,
	editor       = {III, Hal Daumé and Singh, Aarti},
	volume       = 119,
	series       = {Proceedings of Machine Learning Research},
	month        = {13--18 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v119/mukherjee20a/mukherjee20a.pdf},
	url          = {https://proceedings.mlr.press/v119/mukherjee20a.html},
	abstract     = {Individual fairness is an intuitive definition of algorithmic fairness that addresses some of the drawbacks of group fairness. Despite its benefits, it depends on a task specific fair metric that encodes our intuition of what is fair and unfair for the ML task at hand, and the lack of a widely accepted fair metric for many ML tasks is the main barrier to broader adoption of individual fairness. In this paper, we present two simple ways to learn fair metrics from a variety of data types. We show empirically that fair training with the learned metrics leads to improved fairness on three machine learning tasks susceptible to gender and racial biases. We also provide theoretical guarantees on the statistical performance of both approaches.},
}
"
"
@inproceedings{pmlr-v119-roh20a,
	title        = {{FR}-Train: A Mutual Information-Based Approach to Fair and Robust Training},
	author       = {Roh, Yuji and Lee, Kangwook and Whang, Steven and Suh, Changho},
	booktitle    = {Proceedings of the 37th International Conference on Machine Learning},
	pages        = {8147--8157},
	year         = 2020,
	editor       = {III, Hal Daumé and Singh, Aarti},
	volume       = 119,
	series       = {Proceedings of Machine Learning Research},
	month        = {13--18 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v119/roh20a/roh20a.pdf},
	url          = {https://proceedings.mlr.press/v119/roh20a.html},
	abstract     = {Trustworthy AI is a critical issue in machine learning where, in addition to training a model that is accurate, one must consider both fair and robust training in the presence of data bias and poisoning. However, the existing model fairness techniques mistakenly view poisoned data as an additional bias to be fixed, resulting in severe performance degradation. To address this problem, we propose FR-Train, which holistically performs fair and robust model training. We provide a mutual information-based interpretation of an existing adversarial training-based fairness-only method, and apply this idea to architect an additional discriminator that can identify poisoned data using a clean validation set and reduce its influence. In our experiments, FR-Train shows almost no decrease in fairness and accuracy in the presence of data poisoning by both mitigating the bias and defending against poisoning. We also demonstrate how to construct clean validation sets using crowdsourcing, and release new benchmark datasets.},
}
"
"
@inproceedings{pmlr-v119-rolf20a,
	title        = {Balancing Competing Objectives with Noisy Data: Score-Based Classifiers for Welfare-Aware Machine Learning},
	author       = {Rolf, Esther and Simchowitz, Max and Dean, Sarah and Liu, Lydia T. and Bjorkegren, Daniel and Hardt, Moritz and Blumenstock, Joshua},
	booktitle    = {Proceedings of the 37th International Conference on Machine Learning},
	pages        = {8158--8168},
	year         = 2020,
	editor       = {III, Hal Daumé and Singh, Aarti},
	volume       = 119,
	series       = {Proceedings of Machine Learning Research},
	month        = {13--18 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v119/rolf20a/rolf20a.pdf},
	url          = {https://proceedings.mlr.press/v119/rolf20a.html},
	abstract     = {While real-world decisions involve many competing objectives, algorithmic decisions are often evaluated with a single objective function. In this paper, we study algorithmic policies which explicitly trade off between a private objective (such as profit) and a public objective (such as social welfare). We analyze a natural class of policies which trace an empirical Pareto frontier based on learned scores, and focus on how such decisions can be made in noisy or data-limited regimes. Our theoretical results characterize the optimal strategies in this class, bound the Pareto errors due to inaccuracies in the scores, and show an equivalence between optimal strategies and a rich class of fairness-constrained profit-maximizing policies. We then present empirical results in two different contexts — online content recommendation and sustainable abalone fisheries — to underscore the generality of our approach to a wide range of practical decisions. Taken together, these results shed light on inherent trade-offs in using machine learning for decisions that impact social welfare.},
}
"
"
@inproceedings{pmlr-v119-sabato20a,
	title        = {Bounding the fairness and accuracy of classifiers from population statistics},
	author       = {Sabato, Sivan and Yom-Tov, Elad},
	booktitle    = {Proceedings of the 37th International Conference on Machine Learning},
	pages        = {8316--8325},
	year         = 2020,
	editor       = {III, Hal Daumé and Singh, Aarti},
	volume       = 119,
	series       = {Proceedings of Machine Learning Research},
	month        = {13--18 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v119/sabato20a/sabato20a.pdf},
	url          = {https://proceedings.mlr.press/v119/sabato20a.html},
	abstract     = {We consider the study of a classification model whose properties are impossible to estimate using a validation set, either due to the absence of such a set or because access to the classifier, even as a black-box, is impossible. Instead, only aggregate statistics on the rate of positive predictions in each of several sub-populations are available, as well as the true rates of positive labels in each of these sub-populations. We show that these aggregate statistics can be used to lower-bound the discrepancy of a classifier, which is a measure that balances inaccuracy and unfairness. To this end, we define a new measure of unfairness, equal to the fraction of the population on which the classifier behaves differently, compared to its global, ideally fair behavior, as defined by the measure of equalized odds. We propose an efficient and practical procedure for finding the best possible lower bound on the discrepancy of the classifier, given the aggregate statistics, and demonstrate in experiments the empirical tightness of this lower bound, as well as its possible uses on various types of problems, ranging from estimating the quality of voting polls to measuring the effectiveness of patient identification from internet search queries. The code and data are available at https://github.com/sivansabato/bfa.},
}
"
"
@inproceedings{pmlr-v119-saha20c,
	title        = {Measuring Non-Expert Comprehension of Machine Learning Fairness Metrics},
	author       = {Saha, Debjani and Schumann, Candice and Mcelfresh, Duncan and Dickerson, John and Mazurek, Michelle and Tschantz, Michael},
	booktitle    = {Proceedings of the 37th International Conference on Machine Learning},
	pages        = {8377--8387},
	year         = 2020,
	editor       = {III, Hal Daumé and Singh, Aarti},
	volume       = 119,
	series       = {Proceedings of Machine Learning Research},
	month        = {13--18 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v119/saha20c/saha20c.pdf},
	url          = {https://proceedings.mlr.press/v119/saha20c.html},
	abstract     = {Bias in machine learning has manifested injustice in several areas, such as medicine, hiring, and criminal justice. In response, computer scientists have developed myriad definitions of fairness to correct this bias in fielded algorithms. While some definitions are based on established legal and ethical norms, others are largely mathematical. It is unclear whether the general public agrees with these fairness definitions, and perhaps more importantly, whether they understand these definitions. We take initial steps toward bridging this gap between ML researchers and the public, by addressing the question: does a lay audience understand a basic definition of ML fairness? We develop a metric to measure comprehension of three such definitions–demographic parity, equal opportunity, and equalized odds. We evaluate this metric using an online survey, and investigate the relationship between comprehension and sentiment, demographics, and the definition itself.},
}
"
"
@inproceedings{pmlr-v119-siddique20a,
	title        = {Learning Fair Policies in Multi-Objective ({D}eep) Reinforcement Learning with Average and Discounted Rewards},
	author       = {Siddique, Umer and Weng, Paul and Zimmer, Matthieu},
	booktitle    = {Proceedings of the 37th International Conference on Machine Learning},
	pages        = {8905--8915},
	year         = 2020,
	editor       = {III, Hal Daumé and Singh, Aarti},
	volume       = 119,
	series       = {Proceedings of Machine Learning Research},
	month        = {13--18 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v119/siddique20a/siddique20a.pdf},
	url          = {https://proceedings.mlr.press/v119/siddique20a.html},
	abstract     = {As the operations of autonomous systems generally affect simultaneously several users, it is crucial that their designs account for fairness considerations. In contrast to standard (deep) reinforcement learning (RL), we investigate the problem of learning a policy that treats its users equitably. In this paper, we formulate this novel RL problem, in which an objective function, which encodes a notion of fairness that we formally define, is optimized. For this problem, we provide a theoretical discussion where we examine the case of discounted rewards and that of average rewards. During this analysis, we notably derive a new result in the standard RL setting, which is of independent interest: it states a novel bound on the approximation error with respect to the optimal average reward of that of a policy optimal for the discounted reward. Since learning with discounted rewards is generally easier, this discussion further justifies finding a fair policy for the average reward by learning a fair policy for the discounted reward. Thus, we describe how several classic deep RL algorithms can be adapted to our fair optimization problem, and we validate our approach with extensive experiments in three different domains.},
}
"
"
@inproceedings{pmlr-v119-sim20a,
	title        = {Collaborative Machine Learning with Incentive-Aware Model Rewards},
	author       = {Sim, Rachael Hwee Ling and Zhang, Yehong and Chan, Mun Choon and Low, Bryan Kian Hsiang},
	booktitle    = {Proceedings of the 37th International Conference on Machine Learning},
	pages        = {8927--8936},
	year         = 2020,
	editor       = {III, Hal Daumé and Singh, Aarti},
	volume       = 119,
	series       = {Proceedings of Machine Learning Research},
	month        = {13--18 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v119/sim20a/sim20a.pdf},
	url          = {https://proceedings.mlr.press/v119/sim20a.html},
	abstract     = {Collaborative machine learning (ML) is an appealing paradigm to build high-quality ML models by training on the aggregated data from many parties. However, these parties are only willing to share their data when given enough incentives, such as a guaranteed fair reward based on their contributions. This motivates the need for measuring a party’s contribution and designing an incentive-aware reward scheme accordingly. This paper proposes to value a party’s reward based on Shapley value and information gain on model parameters given its data. Subsequently, we give each party a model as a reward. To formally incentivize the collaboration, we define some desirable properties (e.g., fairness and stability) which are inspired by cooperative game theory but adapted for our model reward that is uniquely freely replicable. Then, we propose a novel model reward scheme to satisfy fairness and trade off between the desirable properties via an adjustable parameter. The value of each party’s model reward determined by our scheme is attained by injecting Gaussian noise to the aggregated training data with an optimized noise variance. We empirically demonstrate interesting properties of our scheme and evaluate its performance using synthetic and real-world datasets.},
}
"
"
@inproceedings{pmlr-v119-sivaprasad20a,
	title        = {Optimizer Benchmarking Needs to Account for Hyperparameter Tuning},
	author       = {Sivaprasad, Prabhu Teja and Mai, Florian and Vogels, Thijs and Jaggi, Martin and Fleuret, Fran{\c{c}}ois},
	booktitle    = {Proceedings of the 37th International Conference on Machine Learning},
	pages        = {9036--9045},
	year         = 2020,
	editor       = {III, Hal Daumé and Singh, Aarti},
	volume       = 119,
	series       = {Proceedings of Machine Learning Research},
	month        = {13--18 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v119/sivaprasad20a/sivaprasad20a.pdf},
	url          = {https://proceedings.mlr.press/v119/sivaprasad20a.html},
	abstract     = {The performance of optimizers, particularly in deep learning, depends considerably on their chosen hyperparameter configuration. The efficacy of optimizers is often studied under near-optimal problem-specific hyperparameters, and finding these settings may be prohibitively costly for practitioners. In this work, we argue that a fair assessment of optimizers’ performance must take the computational cost of hyperparameter tuning into account, i.e., how easy it is to find good hyperparameter configurations using an automatic hyperparameter search. Evaluating a variety of optimizers on an extensive set of standard datasets and architectures, our results indicate that Adam is the most practical solution, particularly in low-budget scenarios.},
}
"
"
@inproceedings{pmlr-v119-wang20t,
	title        = {Loss Function Search for Face Recognition},
	author       = {Wang, Xiaobo and Wang, Shuo and Chi, Cheng and Zhang, Shifeng and Mei, Tao},
	booktitle    = {Proceedings of the 37th International Conference on Machine Learning},
	pages        = {10029--10038},
	year         = 2020,
	editor       = {III, Hal Daumé and Singh, Aarti},
	volume       = 119,
	series       = {Proceedings of Machine Learning Research},
	month        = {13--18 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v119/wang20t/wang20t.pdf},
	url          = {https://proceedings.mlr.press/v119/wang20t.html},
	abstract     = {In face recognition, designing margin-based (\emph{e.g.}, angular, additive, additive angular margins) softmax loss functions plays an important role to learn discriminative features. However, these hand-crafted heuristic methods may be sub-optimal because they require much effort to explore the large design space. Recently, an AutoML for loss function search method AM-LFS has been derived, which leverages reinforcement learning to search loss functions during the training process. But its search space is complex and unstable that hindering its superiority. In this paper, we first analyze that the key to enhance the feature discrimination is actually \textbf{how to reduce the softmax probability}. We then design a unified formulation for the current margin-based softmax losses. Accordingly, we define a novel search space and develop a reward-guided search method to automatically obtain the best candidate. Experimental results on a variety of face recognition benchmarks have demonstrated the effectiveness of our method over the state-of-the-art alternatives.},
}
"
"
@inproceedings{pmlr-v119-zhang20m,
	title        = {Learning Structured Latent Factors from Dependent {D}ata:{A} Generative Model Framework from Information-Theoretic Perspective},
	author       = {Zhang, Ruixiang and Koyama, Masanori and Ishiguro, Katsuhiko},
	booktitle    = {Proceedings of the 37th International Conference on Machine Learning},
	pages        = {11141--11152},
	year         = 2020,
	editor       = {III, Hal Daumé and Singh, Aarti},
	volume       = 119,
	series       = {Proceedings of Machine Learning Research},
	month        = {13--18 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v119/zhang20m/zhang20m.pdf},
	url          = {https://proceedings.mlr.press/v119/zhang20m.html},
	abstract     = {Learning controllable and generalizable representation of multivariate data with desired structural properties remains a fundamental problem in machine learning. In this paper, we present a novel framework for learning generative models with various underlying structures in the latent space. Learning controllable and generalizable representation of multivariate data with desired structural properties remains a fundamental problem in machine learning. In this paper, we present a novel framework for learning generative models with various underlying structures in the latent space. We represent the inductive bias in the form of mask variables to model the dependency structure in the graphical model and extend the theory of multivariate information bottleneck (Friedman et al., 2001) to enforce it. Our model provides a principled approach to learn a set of semantically meaningful latent factors that reflect various types of desired structures like capturing correlation or encoding invariance, while also offering the flexibility to automatically estimate the dependency structure from data. We show that our framework unifies many existing generative models and can be applied to a variety of tasks, including multimodal data modeling, algorithmic fairness, and out-of-distribution generalization.},
}
"
"
@inproceedings{pmlr-v119-zhao20e,
	title        = {Individual Calibration with Randomized Forecasting},
	author       = {Zhao, Shengjia and Ma, Tengyu and Ermon, Stefano},
	booktitle    = {Proceedings of the 37th International Conference on Machine Learning},
	pages        = {11387--11397},
	year         = 2020,
	editor       = {III, Hal Daumé and Singh, Aarti},
	volume       = 119,
	series       = {Proceedings of Machine Learning Research},
	month        = {13--18 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v119/zhao20e/zhao20e.pdf},
	url          = {https://proceedings.mlr.press/v119/zhao20e.html},
	abstract     = {Machine learning applications often require calibrated predictions, e.g. a 90% credible interval should contain the true outcome 90% of the times. However, typical definitions of calibration only require this to hold on average, and offer no guarantees on predictions made on individual samples. Thus, predictions can be systematically over or under confident on certain subgroups, leading to issues of fairness and potential vulnerabilities. We show that calibration for individual samples is possible in the regression setup if and only if the predictions are randomized, i.e. outputting randomized credible intervals. Randomization removes systematic bias by trading off bias with variance. We design a training objective to enforce individual calibration and use it to train randomized regression functions. The resulting models are more calibrated for arbitrarily chosen subgroups of the data, and can achieve higher utility in decision making against adversaries that exploit miscalibrated predictions.},
}
"
"
@inproceedings{pmlr-v139-balseiro21a,
	title        = {Regularized Online Allocation Problems: Fairness and Beyond},
	author       = {Balseiro, Santiago and Lu, Haihao and Mirrokni, Vahab},
	booktitle    = {Proceedings of the 38th International Conference on Machine Learning},
	pages        = {630--639},
	year         = 2021,
	editor       = {Meila, Marina and Zhang, Tong},
	volume       = 139,
	series       = {Proceedings of Machine Learning Research},
	month        = {18--24 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v139/balseiro21a/balseiro21a.pdf},
	url          = {https://proceedings.mlr.press/v139/balseiro21a.html},
	abstract     = {Online allocation problems with resource constraints have a rich history in computer science and operations research. In this paper, we introduce the regularized online allocation problem, a variant that includes a non-linear regularizer acting on the total resource consumption. In this problem, requests repeatedly arrive over time and, for each request, a decision maker needs to take an action that generates a reward and consumes resources. The objective is to simultaneously maximize total rewards and the value of the regularizer subject to the resource constraints. Our primary motivation is the online allocation of internet advertisements wherein firms seek to maximize additive objectives such as the revenue or efficiency of the allocation. By introducing a regularizer, firms can account for the fairness of the allocation or, alternatively, punish under-delivery of advertisements—two common desiderata in internet advertising markets. We design an algorithm when arrivals are drawn independently from a distribution that is unknown to the decision maker. Our algorithm is simple, fast, and attains the optimal order of sub-linear regret compared to the optimal allocation with the benefit of hindsight. Numerical experiments confirm the effectiveness of the proposed algorithm and of the regularizers in an internet advertising application.},
}
"
"
@inproceedings{pmlr-v139-biggs21a,
	title        = {Model Distillation for Revenue Optimization: Interpretable Personalized Pricing},
	author       = {Biggs, Max and Sun, Wei and Ettl, Markus},
	booktitle    = {Proceedings of the 38th International Conference on Machine Learning},
	pages        = {946--956},
	year         = 2021,
	editor       = {Meila, Marina and Zhang, Tong},
	volume       = 139,
	series       = {Proceedings of Machine Learning Research},
	month        = {18--24 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v139/biggs21a/biggs21a.pdf},
	url          = {https://proceedings.mlr.press/v139/biggs21a.html},
	abstract     = {Data-driven pricing strategies are becoming increasingly common, where customers are offered a personalized price based on features that are predictive of their valuation of a product. It is desirable for this pricing policy to be simple and interpretable, so it can be verified, checked for fairness, and easily implemented. However, efforts to incorporate machine learning into a pricing framework often lead to complex pricing policies that are not interpretable, resulting in slow adoption in practice. We present a novel, customized, prescriptive tree-based algorithm that distills knowledge from a complex black-box machine learning algorithm, segments customers with similar valuations and prescribes prices in such a way that maximizes revenue while maintaining interpretability. We quantify the regret of a resulting policy and demonstrate its efficacy in applications with both synthetic and real-world datasets.},
}
"
"
@inproceedings{pmlr-v139-celis21a,
	title        = {Fair Classification with Noisy Protected Attributes: A Framework with Provable Guarantees},
	author       = {Celis, L. Elisa and Huang, Lingxiao and Keswani, Vijay and Vishnoi, Nisheeth K.},
	booktitle    = {Proceedings of the 38th International Conference on Machine Learning},
	pages        = {1349--1361},
	year         = 2021,
	editor       = {Meila, Marina and Zhang, Tong},
	volume       = 139,
	series       = {Proceedings of Machine Learning Research},
	month        = {18--24 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v139/celis21a/celis21a.pdf},
	url          = {https://proceedings.mlr.press/v139/celis21a.html},
	abstract     = {We present an optimization framework for learning a fair classifier in the presence of noisy perturbations in the protected attributes. Compared to prior work, our framework can be employed with a very general class of linear and linear-fractional fairness constraints, can handle multiple, non-binary protected attributes, and outputs a classifier that comes with provable guarantees on both accuracy and fairness. Empirically, we show that our framework can be used to attain either statistical rate or false positive rate fairness guarantees with a minimal loss in accuracy, even when the noise is large, in two real-world datasets.},
}
"
"
@inproceedings{pmlr-v139-chi21a,
	title        = {Understanding and Mitigating Accuracy Disparity in Regression},
	author       = {Chi, Jianfeng and Tian, Yuan and Gordon, Geoffrey J. and Zhao, Han},
	booktitle    = {Proceedings of the 38th International Conference on Machine Learning},
	pages        = {1866--1876},
	year         = 2021,
	editor       = {Meila, Marina and Zhang, Tong},
	volume       = 139,
	series       = {Proceedings of Machine Learning Research},
	month        = {18--24 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v139/chi21a/chi21a.pdf},
	url          = {https://proceedings.mlr.press/v139/chi21a.html},
	abstract     = {With the widespread deployment of large-scale prediction systems in high-stakes domains, e.g., face recognition, criminal justice, etc., disparity on prediction accuracy between different demographic subgroups has called for fundamental understanding on the source of such disparity and algorithmic intervention to mitigate it. In this paper, we study the accuracy disparity problem in regression. To begin with, we first propose an error decomposition theorem, which decomposes the accuracy disparity into the distance between marginal label distributions and the distance between conditional representations, to help explain why such accuracy disparity appears in practice. Motivated by this error decomposition and the general idea of distribution alignment with statistical distances, we then propose an algorithm to reduce this disparity, and analyze its game-theoretic optima of the proposed objective functions. To corroborate our theoretical findings, we also conduct experiments on five benchmark datasets. The experimental results suggest that our proposed algorithms can effectively mitigate accuracy disparity while maintaining the predictive power of the regression models.},
}
"
"
@inproceedings{pmlr-v139-correa21a,
	title        = {Fairness and Bias in Online Selection},
	author       = {Correa, Jose and Cristi, Andres and Duetting, Paul and Norouzi-Fard, Ashkan},
	booktitle    = {Proceedings of the 38th International Conference on Machine Learning},
	pages        = {2112--2121},
	year         = 2021,
	editor       = {Meila, Marina and Zhang, Tong},
	volume       = 139,
	series       = {Proceedings of Machine Learning Research},
	month        = {18--24 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v139/correa21a/correa21a.pdf},
	url          = {https://proceedings.mlr.press/v139/correa21a.html},
	abstract     = {There is growing awareness and concern about fairness in machine learning and algorithm design. This is particularly true in online selection problems where decisions are often biased, for example, when assessing credit risks or hiring staff. We address the issues of fairness and bias in online selection by introducing multi-color versions of the classic secretary and prophet problem. Interestingly, existing algorithms for these problems are either very unfair or very inefficient, so we develop optimal fair algorithms for these new problems and provide tight bounds on their competitiveness. We validate our theoretical findings on real-world data.},
}
"
"
"
"
@inproceedings{pmlr-v139-creager21a,
	title        = {Environment Inference for Invariant Learning},
	author       = {Creager, Elliot and Jacobsen, Joern-Henrik and Zemel, Richard},
	booktitle    = {Proceedings of the 38th International Conference on Machine Learning},
	pages        = {2189--2200},
	year         = 2021,
	editor       = {Meila, Marina and Zhang, Tong},
	volume       = 139,
	series       = {Proceedings of Machine Learning Research},
	month        = {18--24 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v139/creager21a/creager21a.pdf},
	url          = {https://proceedings.mlr.press/v139/creager21a.html},
	abstract     = {Learning models that gracefully handle distribution shifts is central to research on domain generalization, robust optimization, and fairness. A promising formulation is domain-invariant learning, which identifies the key issue of learning which features are domain-specific versus domain-invariant. An important assumption in this area is that the training examples are partitioned into “domains” or “environments”. Our focus is on the more common setting where such partitions are not provided. We propose EIIL, a general framework for domain-invariant learning that incorporates Environment Inference to directly infer partitions that are maximally informative for downstream Invariant Learning. We show that EIIL outperforms invariant learning methods on the CMNIST benchmark without using environment labels, and significantly outperforms ERM on worst-group performance in the Waterbirds dataset. Finally, we establish connections between EIIL and algorithmic fairness, which enables EIIL to improve accuracy and calibration in a fair prediction problem.},
}
"
"
@inproceedings{pmlr-v139-devos21a,
	title        = {Versatile Verification of Tree Ensembles},
	author       = {Devos, Laurens and Meert, Wannes and Davis, Jesse},
	booktitle    = {Proceedings of the 38th International Conference on Machine Learning},
	pages        = {2654--2664},
	year         = 2021,
	editor       = {Meila, Marina and Zhang, Tong},
	volume       = 139,
	series       = {Proceedings of Machine Learning Research},
	month        = {18--24 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v139/devos21a/devos21a.pdf},
	url          = {https://proceedings.mlr.press/v139/devos21a.html},
	abstract     = {Machine learned models often must abide by certain requirements (e.g., fairness or legal). This has spurred interested in developing approaches that can provably verify whether a model satisfies certain properties. This paper introduces a generic algorithm called Veritas that enables tackling multiple different verification tasks for tree ensemble models like random forests (RFs) and gradient boosted decision trees (GBDTs). This generality contrasts with previous work, which has focused exclusively on either adversarial example generation or robustness checking. Veritas formulates the verification task as a generic optimization problem and introduces a novel search space representation. Veritas offers two key advantages. First, it provides anytime lower and upper bounds when the optimization problem cannot be solved exactly. In contrast, many existing methods have focused on exact solutions and are thus limited by the verification problem being NP-complete. Second, Veritas produces full (bounded suboptimal) solutions that can be used to generate concrete examples. We experimentally show that our method produces state-of-the-art robustness estimates, especially when executed with strict time constraints. This is exceedingly important when checking the robustness of large datasets. Additionally, we show that Veritas enables tackling more real-world verification scenarios.},
}
"
"
@inproceedings{pmlr-v139-ermolov21a,
	title        = {Whitening for Self-Supervised Representation Learning},
	author       = {Ermolov, Aleksandr and Siarohin, Aliaksandr and Sangineto, Enver and Sebe, Nicu},
	booktitle    = {Proceedings of the 38th International Conference on Machine Learning},
	pages        = {3015--3024},
	year         = 2021,
	editor       = {Meila, Marina and Zhang, Tong},
	volume       = 139,
	series       = {Proceedings of Machine Learning Research},
	month        = {18--24 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v139/ermolov21a/ermolov21a.pdf},
	url          = {https://proceedings.mlr.press/v139/ermolov21a.html},
	abstract     = {Most of the current self-supervised representation learning (SSL) methods are based on the contrastive loss and the instance-discrimination task, where augmented versions of the same image instance (""positives"") are contrasted with instances extracted from other images (""negatives""). For the learning to be effective, many negatives should be compared with a positive pair, which is computationally demanding. In this paper, we propose a different direction and a new loss function for SSL, which is based on the whitening of the latent-space features. The whitening operation has a ""scattering"" effect on the batch samples, avoiding degenerate solutions where all the sample representations collapse to a single point. Our solution does not require asymmetric networks and it is conceptually simple. Moreover, since negatives are not needed, we can extract multiple positive pairs from the same image instance. The source code of the method and of all the experiments is available at: https://github.com/htdt/self-supervised.},
}
"
"
@inproceedings{pmlr-v139-gorantla21a,
	title        = {On the Problem of Underranking in Group-Fair Ranking},
	author       = {Gorantla, Sruthi and Deshpande, Amit and Louis, Anand},
	booktitle    = {Proceedings of the 38th International Conference on Machine Learning},
	pages        = {3777--3787},
	year         = 2021,
	editor       = {Meila, Marina and Zhang, Tong},
	volume       = 139,
	series       = {Proceedings of Machine Learning Research},
	month        = {18--24 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v139/gorantla21a/gorantla21a.pdf},
	url          = {https://proceedings.mlr.press/v139/gorantla21a.html},
	abstract     = {Bias in ranking systems, especially among the top ranks, can worsen social and economic inequalities, polarize opinions, and reinforce stereotypes. On the other hand, a bias correction for minority groups can cause more harm if perceived as favoring group-fair outcomes over meritocracy. Most group-fair ranking algorithms post-process a given ranking and output a group-fair ranking. In this paper, we formulate the problem of underranking in group-fair rankings based on how close the group-fair rank of each item is to its original rank, and prove a lower bound on the trade-off achievable for simultaneous underranking and group fairness in ranking. We give a fair ranking algorithm that takes any given ranking and outputs another ranking with simultaneous underranking and group fairness guarantees comparable to the lower bound we prove. Our experimental results confirm the theoretical trade-off between underranking and group fairness, and also show that our algorithm achieves the best of both when compared to the state-of-the-art baselines.},
}
"
"
@inproceedings{pmlr-v139-guo21b,
	title        = {Adversarial Policy Learning in Two-player Competitive Games},
	author       = {Guo, Wenbo and Wu, Xian and Huang, Sui and Xing, Xinyu},
	booktitle    = {Proceedings of the 38th International Conference on Machine Learning},
	pages        = {3910--3919},
	year         = 2021,
	editor       = {Meila, Marina and Zhang, Tong},
	volume       = 139,
	series       = {Proceedings of Machine Learning Research},
	month        = {18--24 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v139/guo21b/guo21b.pdf},
	url          = {https://proceedings.mlr.press/v139/guo21b.html},
	abstract     = {In a two-player deep reinforcement learning task, recent work shows an attacker could learn an adversarial policy that triggers a target agent to perform poorly and even react in an undesired way. However, its efficacy heavily relies upon the zero-sum assumption made in the two-player game. In this work, we propose a new adversarial learning algorithm. It addresses the problem by resetting the optimization goal in the learning process and designing a new surrogate optimization function. Our experiments show that our method significantly improves adversarial agents’ exploitability compared with the state-of-art attack. Besides, we also discover that our method could augment an agent with the ability to abuse the target game’s unfairness. Finally, we show that agents adversarially re-trained against our adversarial agents could obtain stronger adversary-resistance.},
}
"
"
@inproceedings{pmlr-v139-hiranandani21a,
	title        = {Optimizing Black-box Metrics with Iterative Example Weighting},
	author       = {Hiranandani, Gaurush and Mathur, Jatin and Narasimhan, Harikrishna and Fard, Mahdi Milani and Koyejo, Sanmi},
	booktitle    = {Proceedings of the 38th International Conference on Machine Learning},
	pages        = {4239--4249},
	year         = 2021,
	editor       = {Meila, Marina and Zhang, Tong},
	volume       = 139,
	series       = {Proceedings of Machine Learning Research},
	month        = {18--24 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v139/hiranandani21a/hiranandani21a.pdf},
	url          = {https://proceedings.mlr.press/v139/hiranandani21a.html},
	abstract     = {We consider learning to optimize a classification metric defined by a black-box function of the confusion matrix. Such black-box learning settings are ubiquitous, for example, when the learner only has query access to the metric of interest, or in noisy-label and domain adaptation applications where the learner must evaluate the metric via performance evaluation using a small validation sample. Our approach is to adaptively learn example weights on the training dataset such that the resulting weighted objective best approximates the metric on the validation sample. We show how to model and estimate the example weights and use them to iteratively post-shift a pre-trained class probability estimator to construct a classifier. We also analyze the resulting procedure’s statistical properties. Experiments on various label noise, domain shift, and fair classification setups confirm that our proposal compares favorably to the state-of-the-art baselines for each application.},
}
"
"
@inproceedings{pmlr-v139-jalal21b,
	title        = {Fairness for Image Generation with Uncertain Sensitive Attributes},
	author       = {Jalal, Ajil and Karmalkar, Sushrut and Hoffmann, Jessica and Dimakis, Alex and Price, Eric},
	booktitle    = {Proceedings of the 38th International Conference on Machine Learning},
	pages        = {4721--4732},
	year         = 2021,
	editor       = {Meila, Marina and Zhang, Tong},
	volume       = 139,
	series       = {Proceedings of Machine Learning Research},
	month        = {18--24 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v139/jalal21b/jalal21b.pdf},
	url          = {https://proceedings.mlr.press/v139/jalal21b.html},
	abstract     = {This work tackles the issue of fairness in the context of generative procedures, such as image super-resolution, which entail different definitions from the standard classification setting. Moreover, while traditional group fairness definitions are typically defined with respect to specified protected groups – camouflaging the fact that these groupings are artificial and carry historical and political motivations – we emphasize that there are no ground truth identities. For instance, should South and East Asians be viewed as a single group or separate groups? Should we consider one race as a whole or further split by gender? Choosing which groups are valid and who belongs in them is an impossible dilemma and being “fair” with respect to Asians may require being “unfair” with respect to South Asians. This motivates the introduction of definitions that allow algorithms to be \emph{oblivious} to the relevant groupings. We define several intuitive notions of group fairness and study their incompatibilities and trade-offs. We show that the natural extension of demographic parity is strongly dependent on the grouping, and \emph{impossible} to achieve obliviously. On the other hand, the conceptually new definition we introduce, Conditional Proportional Representation, can be achieved obliviously through Posterior Sampling. Our experiments validate our theoretical results and achieve fair image reconstruction using state-of-the-art generative models.},
}
"
"
@inproceedings{pmlr-v139-laturnus21a,
	title        = {MorphVAE: Generating Neural Morphologies from 3D-Walks using a Variational Autoencoder with Spherical Latent Space},
	author       = {Laturnus, Sophie C. and Berens, Philipp},
	booktitle    = {Proceedings of the 38th International Conference on Machine Learning},
	pages        = {6021--6031},
	year         = 2021,
	editor       = {Meila, Marina and Zhang, Tong},
	volume       = 139,
	series       = {Proceedings of Machine Learning Research},
	month        = {18--24 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v139/laturnus21a/laturnus21a.pdf},
	url          = {https://proceedings.mlr.press/v139/laturnus21a.html},
	abstract     = {For the past century, the anatomy of a neuron has been considered one of its defining features: The shape of a neuron’s dendrites and axon fundamentally determines what other neurons it can connect to. These neurites have been described using mathematical tools e.g. in the context of cell type classification, but generative models of these structures have only rarely been proposed and are often computationally inefficient. Here we propose MorphVAE, a sequence-to-sequence variational autoencoder with spherical latent space as a generative model for neural morphologies. The model operates on walks within the tree structure of a neuron and can incorporate expert annotations on a subset of the data using semi-supervised learning. We develop our model on artificially generated toy data and evaluate its performance on dendrites of excitatory cells and axons of inhibitory cells of mouse motor cortex (M1) and dendrites of retinal ganglion cells. We show that the learned latent feature space allows for better cell type discrimination than other commonly used features. By sampling new walks from the latent space we can easily construct new morphologies with a specified degree of similarity to their reference neuron, providing an efficient generative model for neural morphologies.},
}
"
"
@inproceedings{pmlr-v139-lee21b,
	title        = {Fair Selective Classification Via Sufficiency},
	author       = {Lee, Joshua K and Bu, Yuheng and Rajan, Deepta and Sattigeri, Prasanna and Panda, Rameswar and Das, Subhro and Wornell, Gregory W},
	booktitle    = {Proceedings of the 38th International Conference on Machine Learning},
	pages        = {6076--6086},
	year         = 2021,
	editor       = {Meila, Marina and Zhang, Tong},
	volume       = 139,
	series       = {Proceedings of Machine Learning Research},
	month        = {18--24 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v139/lee21b/lee21b.pdf},
	url          = {https://proceedings.mlr.press/v139/lee21b.html},
	abstract     = {Selective classification is a powerful tool for decision-making in scenarios where mistakes are costly but abstentions are allowed. In general, by allowing a classifier to abstain, one can improve the performance of a model at the cost of reducing coverage and classifying fewer samples. However, recent work has shown, in some cases, that selective classification can magnify disparities between groups, and has illustrated this phenomenon on multiple real-world datasets. We prove that the sufficiency criterion can be used to mitigate these disparities by ensuring that selective classification increases performance on all groups, and introduce a method for mitigating the disparity in precision across the entire coverage scale based on this criterion. We then provide an upper bound on the conditional mutual information between the class label and sensitive attribute, conditioned on the learned features, which can be used as a regularizer to achieve fairer selective classification. The effectiveness of the method is demonstrated on the Adult, CelebA, Civil Comments, and CheXpert datasets.},
}
"
"
@inproceedings{pmlr-v139-li21f,
	title        = {Theory of Spectral Method for Union of Subspaces-Based Random Geometry Graph},
	author       = {Li, Gen and Gu, Yuantao},
	booktitle    = {Proceedings of the 38th International Conference on Machine Learning},
	pages        = {6337--6345},
	year         = 2021,
	editor       = {Meila, Marina and Zhang, Tong},
	volume       = 139,
	series       = {Proceedings of Machine Learning Research},
	month        = {18--24 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v139/li21f/li21f.pdf},
	url          = {https://proceedings.mlr.press/v139/li21f.html},
	abstract     = {Spectral method is a commonly used scheme to cluster data points lying close to Union of Subspaces, a task known as Subspace Clustering. The typical usage is to construct a Random Geometry Graph first and then apply spectral method to the graph to obtain clustering result. The latter step has been coined the name Spectral Clustering. As far as we know, in spite of the significance of both steps in spectral-method-based Subspace Clustering, all existing theoretical results focus on the first step of constructing the graph, but ignore the final step to correct false connections through spectral clustering. This paper establishes a theory to show the power of this method for the first time, in which we demonstrate the mechanism of spectral clustering by analyzing a simplified algorithm under the widely used semi-random model. Based on this theory, we prove the efficiency of Subspace Clustering in fairly broad conditions. The insights and analysis techniques developed in this paper might also have implications for other random graph problems.},
}
"
"
@inproceedings{pmlr-v139-li21h,
	title        = {Ditto: Fair and Robust Federated Learning Through Personalization},
	author       = {Li, Tian and Hu, Shengyuan and Beirami, Ahmad and Smith, Virginia},
	booktitle    = {Proceedings of the 38th International Conference on Machine Learning},
	pages        = {6357--6368},
	year         = 2021,
	editor       = {Meila, Marina and Zhang, Tong},
	volume       = 139,
	series       = {Proceedings of Machine Learning Research},
	month        = {18--24 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v139/li21h/li21h.pdf},
	url          = {https://proceedings.mlr.press/v139/li21h.html},
	abstract     = {Fairness and robustness are two important concerns for federated learning systems. In this work, we identify that robustness to data and model poisoning attacks and fairness, measured as the uniformity of performance across devices, are competing constraints in statistically heterogeneous networks. To address these constraints, we propose employing a simple, general framework for personalized federated learning, Ditto, that can inherently provide fairness and robustness benefits, and develop a scalable solver for it. Theoretically, we analyze the ability of Ditto to achieve fairness and robustness simultaneously on a class of linear problems. Empirically, across a suite of federated datasets, we show that Ditto not only achieves competitive performance relative to recent personalization methods, but also enables more accurate, robust, and fair models relative to state-of-the-art fair or robust baselines.},
}
"
@inproceedings{pmlr-v139-li21j,
	title        = {Approximate Group Fairness for Clustering},
	author       = {Li, Bo and Li, Lijun and Sun, Ankang and Wang, Chenhao and Wang, Yingfan},
	booktitle    = {Proceedings of the 38th International Conference on Machine Learning},
	pages        = {6381--6391},
	year         = 2021,
	editor       = {Meila, Marina and Zhang, Tong},
	volume       = 139,
	series       = {Proceedings of Machine Learning Research},
	month        = {18--24 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v139/li21j/li21j.pdf},
	url          = {https://proceedings.mlr.press/v139/li21j.html},
	abstract     = {We incorporate group fairness into the algorithmic centroid clustering problem, where $k$ centers are to be located to serve $n$ agents distributed in a metric space. We refine the notion of proportional fairness proposed in [Chen et al., ICML 2019] as {\em core fairness}. A $k$-clustering is in the core if no coalition containing at least $n/k$ agents can strictly decrease their total distance by deviating to a new center together. Our solution concept is motivated by the situation where agents are able to coordinate and utilities are transferable. A string of existence, hardness and approximability results is provided. Particularly, we propose two dimensions to relax core requirements: one is on the degree of distance improvement, and the other is on the size of deviating coalition. For both relaxations and their combination, we study the extent to which relaxed core fairness can be satisfied in metric spaces including line, tree and general metric space, and design approximation algorithms accordingly. We also conduct experiments on synthetic and real-world data to examine the performance of our algorithms.},
}
"
"
@inproceedings{pmlr-v139-li21z,
	title        = {A Second look at Exponential and Cosine Step Sizes: Simplicity, Adaptivity, and Performance},
	author       = {Li, Xiaoyu and Zhuang, Zhenxun and Orabona, Francesco},
	booktitle    = {Proceedings of the 38th International Conference on Machine Learning},
	pages        = {6553--6564},
	year         = 2021,
	editor       = {Meila, Marina and Zhang, Tong},
	volume       = 139,
	series       = {Proceedings of Machine Learning Research},
	month        = {18--24 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v139/li21z/li21z.pdf},
	url          = {https://proceedings.mlr.press/v139/li21z.html},
	abstract     = {Stochastic Gradient Descent (SGD) is a popular tool in training large-scale machine learning models. Its performance, however, is highly variable, depending crucially on the choice of the step sizes. Accordingly, a variety of strategies for tuning the step sizes have been proposed, ranging from coordinate-wise approaches (a.k.a. “adaptive” step sizes) to sophisticated heuristics to change the step size in each iteration. In this paper, we study two step size schedules whose power has been repeatedly confirmed in practice: the exponential and the cosine step sizes. For the first time, we provide theoretical support for them proving convergence rates for smooth non-convex functions, with and without the Polyak-Ł{}ojasiewicz (PL) condition. Moreover, we show the surprising property that these two strategies are \emph{adaptive} to the noise level in the stochastic gradients of PL functions. That is, contrary to polynomial step sizes, they achieve almost optimal performance without needing to know the noise level nor tuning their hyperparameters based on it. Finally, we conduct a fair and comprehensive empirical evaluation of real-world datasets with deep learning architectures. Results show that, even if only requiring at most two hyperparameters to tune, these two strategies best or match the performance of various finely-tuned state-of-the-art strategies.},
}
"
"
@inproceedings{pmlr-v139-liang21a,
	title        = {Towards Understanding and Mitigating Social Biases in Language Models},
	author       = {Liang, Paul Pu and Wu, Chiyu and Morency, Louis-Philippe and Salakhutdinov, Ruslan},
	booktitle    = {Proceedings of the 38th International Conference on Machine Learning},
	pages        = {6565--6576},
	year         = 2021,
	editor       = {Meila, Marina and Zhang, Tong},
	volume       = 139,
	series       = {Proceedings of Machine Learning Research},
	month        = {18--24 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v139/liang21a/liang21a.pdf},
	url          = {https://proceedings.mlr.press/v139/liang21a.html},
	abstract     = {As machine learning methods are deployed in real-world settings such as healthcare, legal systems, and social science, it is crucial to recognize how they shape social biases and stereotypes in these sensitive decision-making processes. Among such real-world deployments are large-scale pretrained language models (LMs) that can be potentially dangerous in manifesting undesirable representational biases - harmful biases resulting from stereotyping that propagate negative generalizations involving gender, race, religion, and other social constructs. As a step towards improving the fairness of LMs, we carefully define several sources of representational biases before proposing new benchmarks and metrics to measure them. With these tools, we propose steps towards mitigating social biases during text generation. Our empirical results and human evaluation demonstrate effectiveness in mitigating bias while retaining crucial contextual information for high-fidelity text generation, thereby pushing forward the performance-fairness Pareto frontier.},
}
"
"
@inproceedings{pmlr-v139-martinez21a,
	title        = {Blind Pareto Fairness and Subgroup Robustness},
	author       = {Martinez, Natalia L and Bertran, Martin A and Papadaki, Afroditi and Rodrigues, Miguel and Sapiro, Guillermo},
	booktitle    = {Proceedings of the 38th International Conference on Machine Learning},
	pages        = {7492--7501},
	year         = 2021,
	editor       = {Meila, Marina and Zhang, Tong},
	volume       = 139,
	series       = {Proceedings of Machine Learning Research},
	month        = {18--24 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v139/martinez21a/martinez21a.pdf},
	url          = {https://proceedings.mlr.press/v139/martinez21a.html},
	abstract     = {Much of the work in the field of group fairness addresses disparities between predefined groups based on protected features such as gender, age, and race, which need to be available at train, and often also at test, time. These approaches are static and retrospective, since algorithms designed to protect groups identified a priori cannot anticipate and protect the needs of different at-risk groups in the future. In this work we analyze the space of solutions for worst-case fairness beyond demographics, and propose Blind Pareto Fairness (BPF), a method that leverages no-regret dynamics to recover a fair minimax classifier that reduces worst-case risk of any potential subgroup of sufficient size, and guarantees that the remaining population receives the best possible level of service. BPF addresses fairness beyond demographics, that is, it does not rely on predefined notions of at-risk groups, neither at train nor at test time. Our experimental results show that the proposed framework improves worst-case risk in multiple standard datasets, while simultaneously providing better levels of service for the remaining population. The code is available at github.com/natalialmg/BlindParetoFairness},
}
@inproceedings{lum2022biasing,
	title        = {De-biasing “bias” measurement},
	author       = {Lum, Kristian and Zhang, Yunfeng and Bower, Amanda},
	booktitle    = {2022 ACM Conference on Fairness, Accountability, and Transparency},
	pages        = {379--389},
	year         = 2022,
}
@inproceedings{cai2022adaptive,
	title        = {Adaptive sampling strategies to construct equitable training datasets},
	author       = {Cai, William and Encarnacion, Ro and Chern, Bobbie and Corbett-Davies, Sam and Bogen, Miranda and Bergman, Stevie and Goel, Sharad},
	booktitle    = {2022 ACM Conference on Fairness, Accountability, and Transparency},
	pages        = {1467--1478},
	year         = 2022,
}
"
"
@inproceedings{pmlr-v139-nguyen21g,
	title        = {Tight Bounds on the Smallest Eigenvalue of the Neural Tangent Kernel for Deep ReLU Networks},
	author       = {Nguyen, Quynh and Mondelli, Marco and Montufar, Guido F},
	booktitle    = {Proceedings of the 38th International Conference on Machine Learning},
	pages        = {8119--8129},
	year         = 2021,
	editor       = {Meila, Marina and Zhang, Tong},
	volume       = 139,
	series       = {Proceedings of Machine Learning Research},
	month        = {18--24 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v139/nguyen21g/nguyen21g.pdf},
	url          = {https://proceedings.mlr.press/v139/nguyen21g.html},
	abstract     = {A recent line of work has analyzed the theoretical properties of deep neural networks via the Neural Tangent Kernel (NTK). In particular, the smallest eigenvalue of the NTK has been related to the memorization capacity, the global convergence of gradient descent algorithms and the generalization of deep nets. However, existing results either provide bounds in the two-layer setting or assume that the spectrum of the NTK matrices is bounded away from 0 for multi-layer networks. In this paper, we provide tight bounds on the smallest eigenvalue of NTK matrices for deep ReLU nets, both in the limiting case of infinite widths and for finite widths. In the finite-width setting, the network architectures we consider are fairly general: we require the existence of a wide layer with roughly order of $N$ neurons, $N$ being the number of data samples; and the scaling of the remaining layer widths is arbitrary (up to logarithmic factors). To obtain our results, we analyze various quantities of independent interest: we give lower bounds on the smallest singular value of hidden feature matrices, and upper bounds on the Lipschitz constant of input-output feature maps.},
}
"
"
@inproceedings{pmlr-v139-qiu21c,
	title        = {Optimization Planning for 3D ConvNets},
	author       = {Qiu, Zhaofan and Yao, Ting and Ngo, Chong-Wah and Mei, Tao},
	booktitle    = {Proceedings of the 38th International Conference on Machine Learning},
	pages        = {8726--8736},
	year         = 2021,
	editor       = {Meila, Marina and Zhang, Tong},
	volume       = 139,
	series       = {Proceedings of Machine Learning Research},
	month        = {18--24 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v139/qiu21c/qiu21c.pdf},
	url          = {https://proceedings.mlr.press/v139/qiu21c.html},
	abstract     = {It is not trivial to optimally learn a 3D Convolutional Neural Networks (3D ConvNets) due to high complexity and various options of the training scheme. The most common hand-tuning process starts from learning 3D ConvNets using short video clips and then is followed by learning long-term temporal dependency using lengthy clips, while gradually decaying the learning rate from high to low as training progresses. The fact that such process comes along with several heuristic settings motivates the study to seek an optimal ""path"" to automate the entire training. In this paper, we decompose the path into a series of training ""states"" and specify the hyper-parameters, e.g., learning rate and the length of input clips, in each state. The estimation of the knee point on the performance-epoch curve triggers the transition from one state to another. We perform dynamic programming over all the candidate states to plan the optimal permutation of states, i.e., optimization path. Furthermore, we devise a new 3D ConvNets with a unique design of dual-head classifier to improve spatial and temporal discrimination. Extensive experiments on seven public video recognition benchmarks demonstrate the advantages of our proposal. With the optimization planning, our 3D ConvNets achieves superior results when comparing to the state-of-the-art recognition methods. More remarkably, we obtain the top-1 accuracy of 80.5% and 82.7% on Kinetics-400 and Kinetics-600 datasets, respectively.},
}
"
"
@inproceedings{pmlr-v139-rothblum21a,
	title        = {Multi-group Agnostic PAC Learnability},
	author       = {Rothblum, Guy N and Yona, Gal},
	booktitle    = {Proceedings of the 38th International Conference on Machine Learning},
	pages        = {9107--9115},
	year         = 2021,
	editor       = {Meila, Marina and Zhang, Tong},
	volume       = 139,
	series       = {Proceedings of Machine Learning Research},
	month        = {18--24 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v139/rothblum21a/rothblum21a.pdf},
	url          = {https://proceedings.mlr.press/v139/rothblum21a.html},
	abstract     = {An agnostic PAC learning algorithm finds a predictor that is competitive with the best predictor in a benchmark hypothesis class, where competitiveness is measured with respect to a given loss function. However, its predictions might be quite sub-optimal for structured subgroups of individuals, such as protected demographic groups. Motivated by such fairness concerns, we study “multi-group agnostic PAC learnability”: fixing a measure of loss, a benchmark class $\H$ and a (potentially) rich collection of subgroups $\G$, the objective is to learn a single predictor such that the loss experienced by every group $g \in \G$ is not much larger than the best possible loss for this group within $\H$. Under natural conditions, we provide a characterization of the loss functions for which such a predictor is guaranteed to exist. For any such loss function we construct a learning algorithm whose sample complexity is logarithmic in the size of the collection $\G$. Our results unify and extend previous positive and negative results from the multi-group fairness literature, which applied for specific loss functions.},
}
"
"
@inproceedings{pmlr-v139-schwarzschild21a,
	title        = {Just How Toxic is Data Poisoning? A Unified Benchmark for Backdoor and Data Poisoning Attacks},
	author       = {Schwarzschild, Avi and Goldblum, Micah and Gupta, Arjun and Dickerson, John P and Goldstein, Tom},
	booktitle    = {Proceedings of the 38th International Conference on Machine Learning},
	pages        = {9389--9398},
	year         = 2021,
	editor       = {Meila, Marina and Zhang, Tong},
	volume       = 139,
	series       = {Proceedings of Machine Learning Research},
	month        = {18--24 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v139/schwarzschild21a/schwarzschild21a.pdf},
	url          = {https://proceedings.mlr.press/v139/schwarzschild21a.html},
	abstract     = {Data poisoning and backdoor attacks manipulate training data in order to cause models to fail during inference. A recent survey of industry practitioners found that data poisoning is the number one concern among threats ranging from model stealing to adversarial attacks. However, it remains unclear exactly how dangerous poisoning methods are and which ones are more effective considering that these methods, even ones with identical objectives, have not been tested in consistent or realistic settings. We observe that data poisoning and backdoor attacks are highly sensitive to variations in the testing setup. Moreover, we find that existing methods may not generalize to realistic settings. While these existing works serve as valuable prototypes for data poisoning, we apply rigorous tests to determine the extent to which we should fear them. In order to promote fair comparison in future work, we develop standardized benchmarks for data poisoning and backdoor attacks.},
}
"
"
@inproceedings{pmlr-v139-si21a,
	title        = {Testing Group Fairness via Optimal Transport Projections},
	author       = {Si, Nian and Murthy, Karthyek and Blanchet, Jose and Nguyen, Viet Anh},
	booktitle    = {Proceedings of the 38th International Conference on Machine Learning},
	pages        = {9649--9659},
	year         = 2021,
	editor       = {Meila, Marina and Zhang, Tong},
	volume       = 139,
	series       = {Proceedings of Machine Learning Research},
	month        = {18--24 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v139/si21a/si21a.pdf},
	url          = {https://proceedings.mlr.press/v139/si21a.html},
	abstract     = {We have developed a statistical testing framework to detect if a given machine learning classifier fails to satisfy a wide range of group fairness notions. Our test is a flexible, interpretable, and statistically rigorous tool for auditing whether exhibited biases are intrinsic to the algorithm or simply due to the randomness in the data. The statistical challenges, which may arise from multiple impact criteria that define group fairness and which are discontinuous on model parameters, are conveniently tackled by projecting the empirical measure to the set of group-fair probability models using optimal transport. This statistic is efficiently computed using linear programming, and its asymptotic distribution is explicitly obtained. The proposed framework can also be used to test for composite fairness hypotheses and fairness with multiple sensitive attributes. The optimal transport testing formulation improves interpretability by characterizing the minimal covariate perturbations that eliminate the bias observed in the audit.},
}
"
"
@inproceedings{pmlr-v139-sim21b,
	title        = {Collaborative Bayesian Optimization with Fair Regret},
	author       = {Sim, Rachael Hwee Ling and Zhang, Yehong and Low, Bryan Kian Hsiang and Jaillet, Patrick},
	booktitle    = {Proceedings of the 38th International Conference on Machine Learning},
	pages        = {9691--9701},
	year         = 2021,
	editor       = {Meila, Marina and Zhang, Tong},
	volume       = 139,
	series       = {Proceedings of Machine Learning Research},
	month        = {18--24 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v139/sim21b/sim21b.pdf},
	url          = {https://proceedings.mlr.press/v139/sim21b.html},
	abstract     = {Bayesian optimization (BO) is a popular tool for optimizing complex and costly-to-evaluate black-box objective functions. To further reduce the number of function evaluations, any party performing BO may be interested to collaborate with others to optimize the same objective function concurrently. To do this, existing BO algorithms have considered optimizing a batch of input queries in parallel and provided theoretical bounds on their cumulative regret reflecting inefficiency. However, when the objective function values are correlated with real-world rewards (e.g., money), parties may be hesitant to collaborate if they risk incurring larger cumulative regret (i.e., smaller real-world reward) than others. This paper shows that fairness and efficiency are both necessary for the collaborative BO setting. Inspired by social welfare concepts from economics, we propose a new notion of regret capturing these properties and a collaborative BO algorithm whose convergence rate can be theoretically guaranteed by bounding the new regret, both of which share an adjustable parameter for trading off between fairness vs. efficiency. We empirically demonstrate the benefits (e.g., increased fairness) of our algorithm using synthetic and real-world datasets.},
}
"
"
@inproceedings{pmlr-v139-terjek21a,
	title        = {Moreau-Yosida $f$-divergences},
	author       = {Terj{\'e}k, D{\'a}vid},
	booktitle    = {Proceedings of the 38th International Conference on Machine Learning},
	pages        = {10214--10224},
	year         = 2021,
	editor       = {Meila, Marina and Zhang, Tong},
	volume       = 139,
	series       = {Proceedings of Machine Learning Research},
	month        = {18--24 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v139/terjek21a/terjek21a.pdf},
	url          = {https://proceedings.mlr.press/v139/terjek21a.html},
	abstract     = {Variational representations of $f$-divergences are central to many machine learning algorithms, with Lipschitz constrained variants recently gaining attention. Inspired by this, we define the Moreau-Yosida approximation of $f$-divergences with respect to the Wasserstein-$1$ metric. The corresponding variational formulas provide a generalization of a number of recent results, novel special cases of interest and a relaxation of the hard Lipschitz constraint. Additionally, we prove that the so-called tight variational representation of $f$-divergences can be to be taken over the quotient space of Lipschitz functions, and give a characterization of functions achieving the supremum in the variational representation. On the practical side, we propose an algorithm to calculate the tight convex conjugate of $f$-divergences compatible with automatic differentiation frameworks. As an application of our results, we propose the Moreau-Yosida $f$-GAN, providing an implementation of the variational formulas for the Kullback-Leibler, reverse Kullback-Leibler, $\chi^2$, reverse $\chi^2$, squared Hellinger, Jensen-Shannon, Jeffreys, triangular discrimination and total variation divergences as GANs trained on CIFAR-10, leading to competitive results and a simple solution to the problem of uniqueness of the optimal critic.},
}
"
"
@inproceedings{pmlr-v139-trauble21a,
	title        = {On Disentangled Representations Learned from Correlated Data},
	author       = {Tr{\""a}uble, Frederik and Creager, Elliot and Kilbertus, Niki and Locatello, Francesco and Dittadi, Andrea and Goyal, Anirudh and Sch{\""o}lkopf, Bernhard and Bauer, Stefan},
	booktitle    = {Proceedings of the 38th International Conference on Machine Learning},
	pages        = {10401--10412},
	year         = 2021,
	editor       = {Meila, Marina and Zhang, Tong},
	volume       = 139,
	series       = {Proceedings of Machine Learning Research},
	month        = {18--24 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v139/trauble21a/trauble21a.pdf},
	url          = {https://proceedings.mlr.press/v139/trauble21a.html},
	abstract     = {The focus of disentanglement approaches has been on identifying independent factors of variation in data. However, the causal variables underlying real-world observations are often not statistically independent. In this work, we bridge the gap to real-world scenarios by analyzing the behavior of the most prominent disentanglement approaches on correlated data in a large-scale empirical study (including 4260 models). We show and quantify that systematically induced correlations in the dataset are being learned and reflected in the latent representations, which has implications for downstream applications of disentanglement such as fairness. We also demonstrate how to resolve these latent correlations, either using weak supervision during training or by post-hoc correcting a pre-trained model with a small number of labels.},
}
"
"
@inproceedings{pmlr-v139-wang21b,
	title        = {Fairness of Exposure in Stochastic Bandits},
	author       = {Wang, Lequn and Bai, Yiwei and Sun, Wen and Joachims, Thorsten},
	booktitle    = {Proceedings of the 38th International Conference on Machine Learning},
	pages        = {10686--10696},
	year         = 2021,
	editor       = {Meila, Marina and Zhang, Tong},
	volume       = 139,
	series       = {Proceedings of Machine Learning Research},
	month        = {18--24 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v139/wang21b/wang21b.pdf},
	url          = {https://proceedings.mlr.press/v139/wang21b.html},
	abstract     = {Contextual bandit algorithms have become widely used for recommendation in online systems (e.g. marketplaces, music streaming, news), where they now wield substantial influence on which items get shown to users. This raises questions of fairness to the items — and to the sellers, artists, and writers that benefit from this exposure. We argue that the conventional bandit formulation can lead to an undesirable and unfair winner-takes-all allocation of exposure. To remedy this problem, we propose a new bandit objective that guarantees merit-based fairness of exposure to the items while optimizing utility to the users. We formulate fairness regret and reward regret in this setting and present algorithms for both stochastic multi-armed bandits and stochastic linear bandits. We prove that the algorithms achieve sublinear fairness regret and reward regret. Beyond the theoretical analysis, we also provide empirical evidence that these algorithms can allocate exposure to different arms effectively.},
}
"
"
@inproceedings{pmlr-v139-wang21t,
	title        = {Directional Bias Amplification},
	author       = {Wang, Angelina and Russakovsky, Olga},
	booktitle    = {Proceedings of the 38th International Conference on Machine Learning},
	pages        = {10882--10893},
	year         = 2021,
	editor       = {Meila, Marina and Zhang, Tong},
	volume       = 139,
	series       = {Proceedings of Machine Learning Research},
	month        = {18--24 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v139/wang21t/wang21t.pdf},
	url          = {https://proceedings.mlr.press/v139/wang21t.html},
	abstract     = {Mitigating bias in machine learning systems requires refining our understanding of bias propagation pathways: from societal structures to large-scale data to trained models to impact on society. In this work, we focus on one aspect of the problem, namely bias amplification: the tendency of models to amplify the biases present in the data they are trained on. A metric for measuring bias amplification was introduced in the seminal work by Zhao et al. (2017); however, as we demonstrate, this metric suffers from a number of shortcomings including conflating different types of bias amplification and failing to account for varying base rates of protected attributes. We introduce and analyze a new, decoupled metric for measuring bias amplification, $BiasAmp_{\rightarrow}$ (Directional Bias Amplification). We thoroughly analyze and discuss both the technical assumptions and normative implications of this metric. We provide suggestions about its measurement by cautioning against predicting sensitive attributes, encouraging the use of confidence intervals due to fluctuations in the fairness of models across runs, and discussing the limitations of what this metric captures. Throughout this paper, we work to provide an interrogative look at the technical measurement of bias amplification, guided by our normative ideas of what we want it to encompass. Code is located at https://github.com/princetonvisualai/directional-bias-amp.},
}

@article{wang2022against,
  title={Against Predictive Optimization: On the Legitimacy of Decision-Making Algorithms that Optimize Predictive Accuracy},
  author={Wang, Angelina and Kapoor, Sayash and Barocas, Solon and Narayanan, Arvind},
  journal={Available at SSRN},
  year={2022}
}

@inproceedings{pmlr-v139-xu21b,
	title        = {To be Robust or to be Fair: Towards Fairness in Adversarial Training},
	author       = {Xu, Han and Liu, Xiaorui and Li, Yaxin and Jain, Anil and Tang, Jiliang},
	booktitle    = {Proceedings of the 38th International Conference on Machine Learning},
	pages        = {11492--11501},
	year         = 2021,
	editor       = {Meila, Marina and Zhang, Tong},
	volume       = 139,
	series       = {Proceedings of Machine Learning Research},
	month        = {18--24 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v139/xu21b/xu21b.pdf},
	url          = {https://proceedings.mlr.press/v139/xu21b.html},
	abstract     = {Adversarial training algorithms have been proved to be reliable to improve machine learning models’ robustness against adversarial examples. However, we find that adversarial training algorithms tend to introduce severe disparity of accuracy and robustness between different groups of data. For instance, PGD adversarially trained ResNet18 model on CIFAR-10 has 93% clean accuracy and 67% PGD l_infty-8 adversarial accuracy on the class ”automobile” but only 65% and 17% on class ”cat”. This phenomenon happens in balanced datasets and does not exist in naturally trained models when only using clean samples. In this work, we empirically and theoretically show that this phenomenon can generally happen under adversarial training algorithms which minimize DNN models’ robust errors. Motivated by these findings, we propose a Fair-Robust-Learning (FRL) framework to mitigate this unfairness problem when doing adversarial defenses and experimental results validate the effectiveness of FRL.},
}
"
"
@inproceedings{pmlr-v139-yang21i,
	title        = {Accelerating Safe Reinforcement Learning with Constraint-mismatched Baseline Policies},
	author       = {Yang, Tsung-Yen and Rosca, Justinian and Narasimhan, Karthik and Ramadge, Peter J},
	booktitle    = {Proceedings of the 38th International Conference on Machine Learning},
	pages        = {11795--11807},
	year         = 2021,
	editor       = {Meila, Marina and Zhang, Tong},
	volume       = 139,
	series       = {Proceedings of Machine Learning Research},
	month        = {18--24 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v139/yang21i/yang21i.pdf},
	url          = {https://proceedings.mlr.press/v139/yang21i.html},
	abstract     = {We consider the problem of reinforcement learning when provided with (1) a baseline control policy and (2) a set of constraints that the learner must satisfy. The baseline policy can arise from demonstration data or a teacher agent and may provide useful cues for learning, but it might also be sub-optimal for the task at hand, and is not guaranteed to satisfy the specified constraints, which might encode safety, fairness or other application-specific requirements. In order to safely learn from baseline policies, we propose an iterative policy optimization algorithm that alternates between maximizing expected return on the task, minimizing distance to the baseline policy, and projecting the policy onto the constraint-satisfying set. We analyze our algorithm theoretically and provide a finite-time convergence guarantee. In our experiments on five different control tasks, our algorithm consistently outperforms several state-of-the-art baselines, achieving 10 times fewer constraint violations and 40% higher reward on average.},
}
"
"
@inproceedings{pmlr-v139-zimmer21a,
	title        = {Learning Fair Policies in Decentralized Cooperative Multi-Agent Reinforcement Learning},
	author       = {Zimmer, Matthieu and Glanois, Claire and Siddique, Umer and Weng, Paul},
	booktitle    = {Proceedings of the 38th International Conference on Machine Learning},
	pages        = {12967--12978},
	year         = 2021,
	editor       = {Meila, Marina and Zhang, Tong},
	volume       = 139,
	series       = {Proceedings of Machine Learning Research},
	month        = {18--24 Jul},
	publisher    = {PMLR},
	pdf          = {http://proceedings.mlr.press/v139/zimmer21a/zimmer21a.pdf},
	url          = {https://proceedings.mlr.press/v139/zimmer21a.html},
	abstract     = {We consider the problem of learning fair policies in (deep) cooperative multi-agent reinforcement learning (MARL). We formalize it in a principled way as the problem of optimizing a welfare function that explicitly encodes two important aspects of fairness: efficiency and equity. We provide a theoretical analysis of the convergence of policy gradient for this problem. As a solution method, we propose a novel neural network architecture, which is composed of two sub-networks specifically designed for taking into account these two aspects of fairness. In experiments, we demonstrate the importance of the two sub-networks for fair optimization. Our overall approach is general as it can accommodate any (sub)differentiable welfare function. Therefore, it is compatible with various notions of fairness that have been proposed in the literature (e.g., lexicographic maximin, generalized Gini social welfare function, proportional fairness). Our method is generic and can be implemented in various MARL settings: centralized training and decentralized execution, or fully decentralized. Finally, we experimentally validate our approach in various domains and show that it can perform much better than previous methods, both in terms of efficiency and equity.},
}
"
"
@inproceedings{pmlr-v162-abernethy22a,
	title        = {Active Sampling for Min-Max Fairness},
	author       = {Abernethy, Jacob D and Awasthi, Pranjal and Kleindessner, Matth{\""a}us and Morgenstern, Jamie and Russell, Chris and Zhang, Jie},
	booktitle    = {Proceedings of the 39th International Conference on Machine Learning},
	pages        = {53--65},
	year         = 2022,
	editor       = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
	volume       = 162,
	series       = {Proceedings of Machine Learning Research},
	month        = {17--23 Jul},
	publisher    = {PMLR},
	pdf          = {https://proceedings.mlr.press/v162/abernethy22a/abernethy22a.pdf},
	url          = {https://proceedings.mlr.press/v162/abernethy22a.html},
	abstract     = {We propose simple active sampling and reweighting strategies for optimizing min-max fairness that can be applied to any classification or regression model learned via loss minimization. The key intuition behind our approach is to use at each timestep a datapoint from the group that is worst off under the current model for updating the model. The ease of implementation and the generality of our robust formulation make it an attractive option for improving model performance on disadvantaged groups. For convex learning problems, such as linear or logistic regression, we provide a fine-grained analysis, proving the rate of convergence to a min-max fair solution.},
}
"
"
@inproceedings{pmlr-v162-agussurja22a,
	title        = {On the Convergence of the Shapley Value in Parametric {B}ayesian Learning Games},
	author       = {Agussurja, Lucas and Xu, Xinyi and Low, Bryan Kian Hsiang},
	booktitle    = {Proceedings of the 39th International Conference on Machine Learning},
	pages        = {180--196},
	year         = 2022,
	editor       = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
	volume       = 162,
	series       = {Proceedings of Machine Learning Research},
	month        = {17--23 Jul},
	publisher    = {PMLR},
	pdf          = {https://proceedings.mlr.press/v162/agussurja22a/agussurja22a.pdf},
	url          = {https://proceedings.mlr.press/v162/agussurja22a.html},
	abstract     = {Measuring contributions is a classical problem in cooperative game theory where the Shapley value is the most well-known solution concept. In this paper, we establish the convergence property of the Shapley value in parametric Bayesian learning games where players perform a Bayesian inference using their combined data, and the posterior-prior KL divergence is used as the characteristic function. We show that for any two players, under some regularity conditions, their difference in Shapley value converges in probability to the difference in Shapley value of a limiting game whose characteristic function is proportional to the log-determinant of the joint Fisher information. As an application, we present an online collaborative learning framework that is asymptotically Shapley-fair. Our result enables this to be achieved without any costly computations of posterior-prior KL divergences. Only a consistent estimator of the Fisher information is needed. The effectiveness of our framework is demonstrated with experiments using real-world data.},
}
"
"
@inproceedings{pmlr-v162-ahmadi22a,
	title        = {Individual Preference Stability for Clustering},
	author       = {Ahmadi, Saba and Awasthi, Pranjal and Khuller, Samir and Kleindessner, Matth{\""a}us and Morgenstern, Jamie and Sukprasert, Pattara and Vakilian, Ali},
	booktitle    = {Proceedings of the 39th International Conference on Machine Learning},
	pages        = {197--246},
	year         = 2022,
	editor       = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
	volume       = 162,
	series       = {Proceedings of Machine Learning Research},
	month        = {17--23 Jul},
	publisher    = {PMLR},
	pdf          = {https://proceedings.mlr.press/v162/ahmadi22a/ahmadi22a.pdf},
	url          = {https://proceedings.mlr.press/v162/ahmadi22a.html},
	abstract     = {In this paper, we propose a natural notion of individual preference (IP) stability for clustering, which asks that every data point, on average, is closer to the points in its own cluster than to the points in any other cluster. Our notion can be motivated from several perspectives, including game theory and algorithmic fairness. We study several questions related to our proposed notion. We first show that deciding whether a given data set allows for an IP-stable clustering in general is NP-hard. As a result, we explore the design of efficient algorithms for finding IP-stable clusterings in some restricted metric spaces. We present a polytime algorithm to find a clustering satisfying exact IP-stability on the real line, and an efficient algorithm to find an IP-stable 2-clustering for a tree metric. We also consider relaxing the stability constraint, i.e., every data point should not be too far from its own cluster compared to any other cluster. For this case, we provide polytime algorithms with different guarantees. We evaluate some of our algorithms and several standard clustering approaches on real data sets.},
}
"
"
@inproceedings{pmlr-v162-almanza22a,
	title        = {{RUM}s from Head-to-Head Contests},
	author       = {Almanza, Matteo and Chierichetti, Flavio and Kumar, Ravi and Panconesi, Alessandro and Tomkins, Andrew},
	booktitle    = {Proceedings of the 39th International Conference on Machine Learning},
	pages        = {452--467},
	year         = 2022,
	editor       = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
	volume       = 162,
	series       = {Proceedings of Machine Learning Research},
	month        = {17--23 Jul},
	publisher    = {PMLR},
	pdf          = {https://proceedings.mlr.press/v162/almanza22a/almanza22a.pdf},
	url          = {https://proceedings.mlr.press/v162/almanza22a.html},
	abstract     = {Random utility models (RUMs) encode the likelihood that a particular item will be selected from a slate of competing items. RUMs are well-studied objects in both discrete choice theory and, more recently, in the machine learning community, as they encode a fairly broad notion of rational user behavior. In this paper, we focus on slates of size two representing head-to-head contests. Given a tournament matrix $M$ such that $M_{i,j}$ is the probability that item $j$ will be selected from $\{i, j\}$, we consider the problem of finding the RUM that most closely reproduces $M$. For this problem we obtain a polynomial-time algorithm returning a RUM that approximately minimizes the average error over the pairs. Our experiments show that RUMs can <em>perfectly</em> represent many of the tournament matrices that have been considered in the literature; in fact, the maximum average error induced by RUMs on the matrices we considered is negligible ($\approx 0.001$). We also show that RUMs are competitive, on prediction tasks, with previous approaches.},
}
"
"
@inproceedings{pmlr-v162-angelidakis22a,
	title        = {Fair and Fast k-Center Clustering for Data Summarization},
	author       = {Angelidakis, Haris and Kurpisz, Adam and Sering, Leon and Zenklusen, Rico},
	booktitle    = {Proceedings of the 39th International Conference on Machine Learning},
	pages        = {669--702},
	year         = 2022,
	editor       = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
	volume       = 162,
	series       = {Proceedings of Machine Learning Research},
	month        = {17--23 Jul},
	publisher    = {PMLR},
	pdf          = {https://proceedings.mlr.press/v162/angelidakis22a/angelidakis22a.pdf},
	url          = {https://proceedings.mlr.press/v162/angelidakis22a.html},
	abstract     = {We consider two key issues faced by many clustering methods when used for data summarization, namely (a) an unfair representation of ""demographic groups” and (b) distorted summarizations, where data points in the summary represent subsets of the original data of vastly different sizes. Previous work made important steps towards handling separately each of these two issues in the context of the fundamental k-Center clustering objective through the study of fast algorithms for natural models that address them. We show that it is possible to effectively address both (a) and (b) simultaneously by presenting a clustering procedure that works for a canonical combined model and (i) is fast, both in theory and practice, (ii) exhibits a worst-case constant-factor guarantee, and (iii) gives promising computational results showing that there can be significant benefits in addressing both issues together instead of sequentially.},
}
"
"
@inproceedings{pmlr-v162-bartan22a,
	title        = {Neural {F}isher Discriminant Analysis: Optimal Neural Network Embeddings in Polynomial Time},
	author       = {Bartan, Burak and Pilanci, Mert},
	booktitle    = {Proceedings of the 39th International Conference on Machine Learning},
	pages        = {1647--1663},
	year         = 2022,
	editor       = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
	volume       = 162,
	series       = {Proceedings of Machine Learning Research},
	month        = {17--23 Jul},
	publisher    = {PMLR},
	pdf          = {https://proceedings.mlr.press/v162/bartan22a/bartan22a.pdf},
	url          = {https://proceedings.mlr.press/v162/bartan22a.html},
	abstract     = {Fisher’s Linear Discriminant Analysis (FLDA) is a statistical analysis method that linearly embeds data points to a lower dimensional space to maximize a discrimination criterion such that the variance between classes is maximized while the variance within classes is minimized. We introduce a natural extension of FLDA that employs neural networks, called Neural Fisher Discriminant Analysis (NFDA). This method finds the optimal two-layer neural network that embeds data points to optimize the same discrimination criterion. We use tools from convex optimization to transform the optimal neural network embedding problem into a convex problem. The resulting problem is easy to interpret and solve to global optimality. We evaluate the method’s performance on synthetic and real datasets.},
}
"
"
@inproceedings{pmlr-v162-bechavod22a,
	title        = {Information Discrepancy in Strategic Learning},
	author       = {Bechavod, Yahav and Podimata, Chara and Wu, Steven and Ziani, Juba},
	booktitle    = {Proceedings of the 39th International Conference on Machine Learning},
	pages        = {1691--1715},
	year         = 2022,
	editor       = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
	volume       = 162,
	series       = {Proceedings of Machine Learning Research},
	month        = {17--23 Jul},
	publisher    = {PMLR},
	pdf          = {https://proceedings.mlr.press/v162/bechavod22a/bechavod22a.pdf},
	url          = {https://proceedings.mlr.press/v162/bechavod22a.html},
	abstract     = {We initiate the study of the effects of non-transparency in decision rules on individuals’ ability to improve in strategic learning settings. Inspired by real-life settings, such as loan approvals and college admissions, we remove the assumption typically made in the strategic learning literature, that the decision rule is fully known to individuals, and focus instead on settings where it is inaccessible. In their lack of knowledge, individuals try to infer this rule by learning from their peers (e.g., friends and acquaintances who previously applied for a loan), naturally forming groups in the population, each with possibly different type and level of information regarding the decision rule. We show that, in equilibrium, the principal’s decision rule optimizing welfare across sub-populations may cause a strong negative externality: the true quality of some of the groups can actually deteriorate. On the positive side, we show that, in many natural cases, optimal improvement can be guaranteed simultaneously for all sub-populations. We further introduce a measure we term information overlap proxy, and demonstrate its usefulness in characterizing the disparity in improvements across sub-populations. Finally, we identify a natural condition under which improvement can be guaranteed for all sub-populations while maintaining high predictive accuracy. We complement our theoretical analysis with experiments on real-world datasets.},
}
"
"
@inproceedings{pmlr-v162-bitterwolf22a,
	title        = {Breaking Down Out-of-Distribution Detection: Many Methods Based on {OOD} Training Data Estimate a Combination of the Same Core Quantities},
	author       = {Bitterwolf, Julian and Meinke, Alexander and Augustin, Maximilian and Hein, Matthias},
	booktitle    = {Proceedings of the 39th International Conference on Machine Learning},
	pages        = {2041--2074},
	year         = 2022,
	editor       = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
	volume       = 162,
	series       = {Proceedings of Machine Learning Research},
	month        = {17--23 Jul},
	publisher    = {PMLR},
	pdf          = {https://proceedings.mlr.press/v162/bitterwolf22a/bitterwolf22a.pdf},
	url          = {https://proceedings.mlr.press/v162/bitterwolf22a.html},
	abstract     = {It is an important problem in trustworthy machine learning to recognize out-of-distribution (OOD) inputs which are inputs unrelated to the in-distribution task. Many out-of-distribution detection methods have been suggested in recent years. The goal of this paper is to recognize common objectives as well as to identify the implicit scoring functions of different OOD detection methods. We focus on the sub-class of methods that use surrogate OOD data during training in order to learn an OOD detection score that generalizes to new unseen out-distributions at test time. We show that binary discrimination between in- and (different) out-distributions is equivalent to several distinct formulations of the OOD detection problem. When trained in a shared fashion with a standard classifier, this binary discriminator reaches an OOD detection performance similar to that of Outlier Exposure. Moreover, we show that the confidence loss which is used by Outlier Exposure has an implicit scoring function which differs in a non-trivial fashion from the theoretically optimal scoring function in the case where training and test out-distribution are the same, which again is similar to the one used when training an Energy-Based OOD detector or when adding a background class. In practice, when trained in exactly the same way, all these methods perform similarly.},
}
"
"
@inproceedings{pmlr-v162-chai22a,
	title        = {Fairness with Adaptive Weights},
	author       = {Chai, Junyi and Wang, Xiaoqian},
	booktitle    = {Proceedings of the 39th International Conference on Machine Learning},
	pages        = {2853--2866},
	year         = 2022,
	editor       = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
	volume       = 162,
	series       = {Proceedings of Machine Learning Research},
	month        = {17--23 Jul},
	publisher    = {PMLR},
	pdf          = {https://proceedings.mlr.press/v162/chai22a/chai22a.pdf},
	url          = {https://proceedings.mlr.press/v162/chai22a.html},
	abstract     = {Fairness is now an important issue in machine learning. There are arising concerns that automated decision-making systems reflect real-world biases. Although a wide range of fairness-related methods have been proposed in recent years, the under-representation problem has been less studied. Due to the uneven distribution of samples from different populations, machine learning models tend to be biased against minority groups when trained by minimizing the average empirical risk across all samples. In this paper, we propose a novel adaptive reweighing method to address representation bias. The goal of our method is to achieve group-level balance among different demographic groups by learning adaptive weights for each sample. Our approach emphasizes more on error-prone samples in prediction and enhances adequate representation of minority groups for fairness. We derive a closed-form solution for adaptive weight assignment and propose an efficient algorithm with theoretical convergence guarantees. We theoretically analyze the fairness of our model and empirically verify that our method strikes a balance between fairness and accuracy. In experiments, our method achieves comparable or better performance than state-of-the-art methods in both classification and regression tasks. Furthermore, our method exhibits robustness to label noise on various benchmark datasets.},
}
"
"
@inproceedings{pmlr-v162-conti22a,
	title        = {Mitigating Gender Bias in Face Recognition using the von Mises-{F}isher Mixture Model},
	author       = {Conti, Jean-R{\'e}my and Noiry, Nathan and Clemencon, Stephan and Despiegel, Vincent and Gentric, St{\'e}phane},
	booktitle    = {Proceedings of the 39th International Conference on Machine Learning},
	pages        = {4344--4369},
	year         = 2022,
	editor       = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
	volume       = 162,
	series       = {Proceedings of Machine Learning Research},
	month        = {17--23 Jul},
	publisher    = {PMLR},
	pdf          = {https://proceedings.mlr.press/v162/conti22a/conti22a.pdf},
	url          = {https://proceedings.mlr.press/v162/conti22a.html},
	abstract     = {In spite of the high performance and reliability of deep learning algorithms in a wide range of everyday applications, many investigations tend to show that a lot of models exhibit biases, discriminating against specific subgroups of the population (e.g. gender, ethnicity). This urges the practitioner to develop fair systems with a uniform/comparable performance across sensitive groups. In this work, we investigate the gender bias of deep Face Recognition networks. In order to measure this bias, we introduce two new metrics, BFAR and BFRR, that better reflect the inherent deployment needs of Face Recognition systems. Motivated by geometric considerations, we mitigate gender bias through a new post-processing methodology which transforms the deep embeddings of a pre-trained model to give more representation power to discriminated subgroups. It consists in training a shallow neural network by minimizing a Fair von Mises-Fisher loss whose hyperparameters account for the intra-class variance of each gender. Interestingly, we empirically observe that these hyperparameters are correlated with our fairness metrics. In fact, extensive numerical experiments on a variety of datasets show that a careful selection significantly reduces gender bias.},
}
"
"
@inproceedings{pmlr-v162-dasgupta22b,
	title        = {Distinguishing rule and exemplar-based generalization in learning systems},
	author       = {Dasgupta, Ishita and Grant, Erin and Griffiths, Tom},
	booktitle    = {Proceedings of the 39th International Conference on Machine Learning},
	pages        = {4816--4830},
	year         = 2022,
	editor       = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
	volume       = 162,
	series       = {Proceedings of Machine Learning Research},
	month        = {17--23 Jul},
	publisher    = {PMLR},
	pdf          = {https://proceedings.mlr.press/v162/dasgupta22b/dasgupta22b.pdf},
	url          = {https://proceedings.mlr.press/v162/dasgupta22b.html},
	abstract     = {Machine learning systems often do not share the same inductive biases as humans and, as a result, extrapolate or generalize in ways that are inconsistent with our expectations. The trade-off between exemplar- and rule-based generalization has been studied extensively in cognitive psychology; in this work, we present a protocol inspired by these experimental approaches to probe the inductive biases that control this trade-off in category-learning systems such as artificial neural networks. We isolate two such inductive biases: feature-level bias (differences in which features are more readily learned) and exemplar-vs-rule bias (differences in how these learned features are used for generalization of category labels). We find that standard neural network models are feature-biased and have a propensity towards exemplar-based extrapolation; we discuss the implications of these findings for machine-learning research on data augmentation, fairness, and systematic generalization.},
}
"
"
@inproceedings{pmlr-v162-do22a,
	title        = {Fair Generalized Linear Models with a Convex Penalty},
	author       = {Do, Hyungrok and Putzel, Preston and Martin, Axel S and Smyth, Padhraic and Zhong, Judy},
	booktitle    = {Proceedings of the 39th International Conference on Machine Learning},
	pages        = {5286--5308},
	year         = 2022,
	editor       = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
	volume       = 162,
	series       = {Proceedings of Machine Learning Research},
	month        = {17--23 Jul},
	publisher    = {PMLR},
	pdf          = {https://proceedings.mlr.press/v162/do22a/do22a.pdf},
	url          = {https://proceedings.mlr.press/v162/do22a.html},
	abstract     = {Despite recent advances in algorithmic fairness, methodologies for achieving fairness with generalized linear models (GLMs) have yet to be explored in general, despite GLMs being widely used in practice. In this paper we introduce two fairness criteria for GLMs based on equalizing expected outcomes or log-likelihoods. We prove that for GLMs both criteria can be achieved via a convex penalty term based solely on the linear components of the GLM, thus permitting efficient optimization. We also derive theoretical properties for the resulting fair GLM estimator. To empirically demonstrate the efficacy of the proposed fair GLM, we compare it with other well-known fair prediction methods on an extensive set of benchmark datasets for binary classification and regression. In addition, we demonstrate that the fair GLM can generate fair predictions for a range of response variables, other than binary and continuous outcomes.},
}
"
"
@inproceedings{pmlr-v162-foster22a,
	title        = {Contrastive Mixture of Posteriors for Counterfactual Inference, Data Integration and Fairness},
	author       = {Foster, Adam and Vezer, Arpi and Glastonbury, Craig A. and Creed, Paidi and Abujudeh, Samer and Sim, Aaron},
	booktitle    = {Proceedings of the 39th International Conference on Machine Learning},
	pages        = {6578--6621},
	year         = 2022,
	editor       = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
	volume       = 162,
	series       = {Proceedings of Machine Learning Research},
	month        = {17--23 Jul},
	publisher    = {PMLR},
	pdf          = {https://proceedings.mlr.press/v162/foster22a/foster22a.pdf},
	url          = {https://proceedings.mlr.press/v162/foster22a.html},
	abstract     = {Learning meaningful representations of data that can address challenges such as batch effect correction and counterfactual inference is a central problem in many domains including computational biology. Adopting a Conditional VAE framework, we show that marginal independence between the representation and a condition variable plays a key role in both of these challenges. We propose the Contrastive Mixture of Posteriors (CoMP) method that uses a novel misalignment penalty defined in terms of mixtures of the variational posteriors to enforce this independence in latent space. We show that CoMP has attractive theoretical properties compared to previous approaches, and we prove counterfactual identifiability of CoMP under additional assumptions. We demonstrate state-of-the-art performance on a set of challenging tasks including aligning human tumour samples with cancer cell-lines, predicting transcriptome-level perturbation responses, and batch correction on single-cell RNA sequencing data. We also find parallels to fair representation learning and demonstrate that CoMP is competitive on a common task in the field.},
}
"
"
@inproceedings{pmlr-v162-ganev22a,
	title        = {Robin Hood and Matthew Effects: Differential Privacy Has Disparate Impact on Synthetic Data},
	author       = {Ganev, Georgi and Oprisanu, Bristena and De Cristofaro, Emiliano},
	booktitle    = {Proceedings of the 39th International Conference on Machine Learning},
	pages        = {6944--6959},
	year         = 2022,
	editor       = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
	volume       = 162,
	series       = {Proceedings of Machine Learning Research},
	month        = {17--23 Jul},
	publisher    = {PMLR},
	pdf          = {https://proceedings.mlr.press/v162/ganev22a/ganev22a.pdf},
	url          = {https://proceedings.mlr.press/v162/ganev22a.html},
	abstract     = {Generative models trained with Differential Privacy (DP) can be used to generate synthetic data while minimizing privacy risks. We analyze the impact of DP on these models vis-a-vis underrepresented classes/subgroups of data, specifically, studying: 1) the size of classes/subgroups in the synthetic data, and 2) the accuracy of classification tasks run on them. We also evaluate the effect of various levels of imbalance and privacy budgets. Our analysis uses three state-of-the-art DP models (PrivBayes, DP-WGAN, and PATE-GAN) and shows that DP yields opposite size distributions in the generated synthetic data. It affects the gap between the majority and minority classes/subgroups; in some cases by reducing it (a ""Robin Hood"" effect) and, in others, by increasing it (a ""Matthew"" effect). Either way, this leads to (similar) disparate impacts on the accuracy of classification tasks on the synthetic data, affecting disproportionately more the underrepresented subparts of the data. Consequently, when training models on synthetic data, one might incur the risk of treating different subpopulations unevenly, leading to unreliable or unfair conclusions.},
}
"
"
@inproceedings{pmlr-v162-hang22a,
	title        = {Dual Perspective of Label-Specific Feature Learning for Multi-Label Classification},
	author       = {Hang, Jun-Yi and Zhang, Min-Ling},
	booktitle    = {Proceedings of the 39th International Conference on Machine Learning},
	pages        = {8375--8386},
	year         = 2022,
	editor       = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
	volume       = 162,
	series       = {Proceedings of Machine Learning Research},
	month        = {17--23 Jul},
	publisher    = {PMLR},
	pdf          = {https://proceedings.mlr.press/v162/hang22a/hang22a.pdf},
	url          = {https://proceedings.mlr.press/v162/hang22a.html},
	abstract     = {Label-specific features serve as an effective strategy to facilitate multi-label classification, which account for the distinct discriminative properties of each class label via tailoring its own features. Existing approaches implement this strategy in a quite straightforward way, i.e. finding the most pertinent and discriminative features for each class label and directly inducing classifiers on constructed label-specific features. In this paper, we propose a dual perspective for label-specific feature learning, where label-specific discriminative properties are considered by identifying each label’s own non-informative features and making the discrimination process immutable to variations of these features. To instantiate it, we present a perturbation-based approach DELA to provide classifiers with label-specific immutability on simultaneously identified non-informative features, which is optimized towards a probabilistically-relaxed expected risk minimization problem. Comprehensive experiments on 10 benchmark data sets show that our approach outperforms the state-of-the-art counterparts.},
}
"
"
@inproceedings{pmlr-v162-harris22a,
	title        = {Strategic Instrumental Variable Regression: Recovering Causal Relationships From Strategic Responses},
	author       = {Harris, Keegan and Ngo, Dung Daniel T and Stapleton, Logan and Heidari, Hoda and Wu, Steven},
	booktitle    = {Proceedings of the 39th International Conference on Machine Learning},
	pages        = {8502--8522},
	year         = 2022,
	editor       = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
	volume       = 162,
	series       = {Proceedings of Machine Learning Research},
	month        = {17--23 Jul},
	publisher    = {PMLR},
	pdf          = {https://proceedings.mlr.press/v162/harris22a/harris22a.pdf},
	url          = {https://proceedings.mlr.press/v162/harris22a.html},
	abstract     = {In settings where Machine Learning (ML) algorithms automate or inform consequential decisions about people, individual decision subjects are often incentivized to strategically modify their observable attributes to receive more favorable predictions. As a result, the distribution the assessment rule is trained on may differ from the one it operates on in deployment. While such distribution shifts, in general, can hinder accurate predictions, our work identifies a unique opportunity associated with shifts due to strategic responses: We show that we can use strategic responses effectively to recover causal relationships between the observable features and outcomes we wish to predict, even under the presence of unobserved confounding variables. Specifically, our work establishes a novel connection between strategic responses to ML models and instrumental variable (IV) regression by observing that the sequence of deployed models can be viewed as an instrument that affects agents’ observable features but does not directly influence their outcomes. We show that our causal recovery method can be utilized to improve decision-making across several important criteria: individual fairness, agent outcomes, and predictive risk. In particular, we show that if decision subjects differ in their ability to modify non-causal attributes, any decision rule deviating from the causal coefficients can lead to (potentially unbounded) individual-level unfairness. .},
}
"
"
@inproceedings{pmlr-v162-jin22g,
	title        = {Input-agnostic Certified Group Fairness via {G}aussian Parameter Smoothing},
	author       = {Jin, Jiayin and Zhang, Zeru and Zhou, Yang and Wu, Lingfei},
	booktitle    = {Proceedings of the 39th International Conference on Machine Learning},
	pages        = {10340--10361},
	year         = 2022,
	editor       = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
	volume       = 162,
	series       = {Proceedings of Machine Learning Research},
	month        = {17--23 Jul},
	publisher    = {PMLR},
	pdf          = {https://proceedings.mlr.press/v162/jin22g/jin22g.pdf},
	url          = {https://proceedings.mlr.press/v162/jin22g.html},
	abstract     = {Only recently, researchers attempt to provide classification algorithms with provable group fairness guarantees. Most of these algorithms suffer from harassment caused by the requirement that the training and deployment data follow the same distribution. This paper proposes an input-agnostic certified group fairness algorithm, FairSmooth, for improving the fairness of classification models while maintaining the remarkable prediction accuracy. A Gaussian parameter smoothing method is developed to transform base classifiers into their smooth versions. An optimal individual smooth classifier is learnt for each group with only the data regarding the group and an overall smooth classifier for all groups is generated by averaging the parameters of all the individual smooth ones. By leveraging the theory of nonlinear functional analysis, the smooth classifiers are reformulated as output functions of a Nemytskii operator. Theoretical analysis is conducted to derive that the Nemytskii operator is smooth and induces a Frechet differentiable smooth manifold. We theoretically demonstrate that the smooth manifold has a global Lipschitz constant that is independent of the domain of the input data, which derives the input-agnostic certified group fairness.},
}
"
"
@inproceedings{pmlr-v162-kancheti22a,
	title        = {Matching Learned Causal Effects of Neural Networks with Domain Priors},
	author       = {Kancheti, Sai Srinivas and Reddy, Abbavaram Gowtham and Balasubramanian, Vineeth N and Sharma, Amit},
	booktitle    = {Proceedings of the 39th International Conference on Machine Learning},
	pages        = {10676--10696},
	year         = 2022,
	editor       = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
	volume       = 162,
	series       = {Proceedings of Machine Learning Research},
	month        = {17--23 Jul},
	publisher    = {PMLR},
	pdf          = {https://proceedings.mlr.press/v162/kancheti22a/kancheti22a.pdf},
	url          = {https://proceedings.mlr.press/v162/kancheti22a.html},
	abstract     = {A trained neural network can be interpreted as a structural causal model (SCM) that provides the effect of changing input variables on the model’s output. However, if training data contains both causal and correlational relationships, a model that optimizes prediction accuracy may not necessarily learn the true causal relationships between input and output variables. On the other hand, expert users often have prior knowledge of the causal relationship between certain input variables and output from domain knowledge. Therefore, we propose a regularization method that aligns the learned causal effects of a neural network with domain priors, including both direct and total causal effects. We show that this approach can generalize to different kinds of domain priors, including monotonicity of causal effect of an input variable on output or zero causal effect of a variable on output for purposes of fairness. Our experiments on twelve benchmark datasets show its utility in regularizing a neural network model to maintain desired causal effects, without compromising on accuracy. Importantly, we also show that a model thus trained is robust and gets improved accuracy on noisy inputs.},
}
"
"
@inproceedings{pmlr-v162-khajehnejad22a,
	title        = {Neural Network Poisson Models for Behavioural and Neural Spike Train Data},
	author       = {Khajehnejad, Moein and Habibollahi, Forough and Nock, Richard and Arabzadeh, Ehsan and Dayan, Peter and Dezfouli, Amir},
	booktitle    = {Proceedings of the 39th International Conference on Machine Learning},
	pages        = {10974--10996},
	year         = 2022,
	editor       = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
	volume       = 162,
	series       = {Proceedings of Machine Learning Research},
	month        = {17--23 Jul},
	publisher    = {PMLR},
	pdf          = {https://proceedings.mlr.press/v162/khajehnejad22a/khajehnejad22a.pdf},
	url          = {https://proceedings.mlr.press/v162/khajehnejad22a.html},
	abstract     = {One of the most important and challenging application areas for complex machine learning methods is to predict, characterize and model rich, multi-dimensional, neural data. Recent advances in neural recording techniques have made it possible to monitor the activity of a large number of neurons across different brain regions as animals perform behavioural tasks. This poses the critical challenge of establishing links between neural activity at a microscopic scale, which might for instance represent sensory input, and at a macroscopic scale, which then generates behaviour. Predominant modeling methods apply rather disjoint techniques to these scales; by contrast, we suggest an end-to-end model which exploits recent developments of flexible, but tractable, neural network point-process models to characterize dependencies between stimuli, actions, and neural data. We apply this model to a public dataset collected using Neuropixel probes in mice performing a visually-guided behavioural task as well as a synthetic dataset produced from a hierarchical network model with reciprocally connected sensory and integration circuits intended to characterize animal behaviour in a fixed-duration motion discrimination task. We show that our model outperforms previous approaches and contributes novel insights into the relationships between neural activity and behaviour.},
}
"
"
@inproceedings{pmlr-v162-kim22b,
	title        = {Learning fair representation with a parametric integral probability metric},
	author       = {Kim, Dongha and Kim, Kunwoong and Kong, Insung and Ohn, Ilsang and Kim, Yongdai},
	booktitle    = {Proceedings of the 39th International Conference on Machine Learning},
	pages        = {11074--11101},
	year         = 2022,
	editor       = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
	volume       = 162,
	series       = {Proceedings of Machine Learning Research},
	month        = {17--23 Jul},
	publisher    = {PMLR},
	pdf          = {https://proceedings.mlr.press/v162/kim22b/kim22b.pdf},
	url          = {https://proceedings.mlr.press/v162/kim22b.html},
	abstract     = {As they have a vital effect on social decision-making, AI algorithms should be not only accurate but also fair. Among various algorithms for fairness AI, learning fair representation (LFR), whose goal is to find a fair representation with respect to sensitive variables such as gender and race, has received much attention. For LFR, the adversarial training scheme is popularly employed as is done in the generative adversarial network type algorithms. The choice of a discriminator, however, is done heuristically without justification. In this paper, we propose a new adversarial training scheme for LFR, where the integral probability metric (IPM) with a specific parametric family of discriminators is used. The most notable result of the proposed LFR algorithm is its theoretical guarantee about the fairness of the final prediction model, which has not been considered yet. That is, we derive theoretical relations between the fairness of representation and the fairness of the prediction model built on the top of the representation (i.e., using the representation as the input). Moreover, by numerical experiments, we show that our proposed LFR algorithm is computationally lighter and more stable, and the final prediction model is competitive or superior to other LFR algorithms using more complex discriminators.},
}
"
"
@inproceedings{pmlr-v162-levanon22a,
	title        = {Generalized Strategic Classification and the Case of Aligned Incentives},
	author       = {Levanon, Sagi and Rosenfeld, Nir},
	booktitle    = {Proceedings of the 39th International Conference on Machine Learning},
	pages        = {12593--12618},
	year         = 2022,
	editor       = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
	volume       = 162,
	series       = {Proceedings of Machine Learning Research},
	month        = {17--23 Jul},
	publisher    = {PMLR},
	pdf          = {https://proceedings.mlr.press/v162/levanon22a/levanon22a.pdf},
	url          = {https://proceedings.mlr.press/v162/levanon22a.html},
	abstract     = {Strategic classification studies learning in settings where self-interested users can strategically modify their features to obtain favorable predictive outcomes. A key working assumption, however, is that “favorable” always means “positive”; this may be appropriate in some applications (e.g., loan approval), but reduces to a fairly narrow view of what user interests can be. In this work we argue for a broader perspective on what accounts for strategic user behavior, and propose and study a flexible model of generalized strategic classification. Our generalized model subsumes most current models but includes other novel settings; among these, we identify and target one intriguing sub-class of problems in which the interests of users and the system are aligned. This setting reveals a surprising fact: that standard max-margin losses are ill-suited for strategic inputs. Returning to our fully generalized model, we propose a novel max-margin framework for strategic learning that is practical and effective, and which we analyze theoretically. We conclude with a set of experiments that empirically demonstrate the utility of our approach.},
}
"
"
@inproceedings{pmlr-v162-li22p,
	title        = {Achieving Fairness at No Utility Cost via Data Reweighing with Influence},
	author       = {Li, Peizhao and Liu, Hongfu},
	booktitle    = {Proceedings of the 39th International Conference on Machine Learning},
	pages        = {12917--12930},
	year         = 2022,
	editor       = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
	volume       = 162,
	series       = {Proceedings of Machine Learning Research},
	month        = {17--23 Jul},
	publisher    = {PMLR},
	pdf          = {https://proceedings.mlr.press/v162/li22p/li22p.pdf},
	url          = {https://proceedings.mlr.press/v162/li22p.html},
	abstract     = {With the fast development of algorithmic governance, fairness has become a compulsory property for machine learning models to suppress unintentional discrimination. In this paper, we focus on the pre-processing aspect for achieving fairness, and propose a data reweighing approach that only adjusts the weight for samples in the training phase. Different from most previous reweighing methods which usually assign a uniform weight for each (sub)group, we granularly model the influence of each training sample with regard to fairness-related quantity and predictive utility, and compute individual weights based on influence under the constraints from both fairness and utility. Experimental results reveal that previous methods achieve fairness at a non-negligible cost of utility, while as a significant advantage, our approach can empirically release the tradeoff and obtain cost-free fairness for equal opportunity. We demonstrate the cost-free fairness through vanilla classifiers and standard training processes, compared to baseline methods on multiple real-world tabular datasets. Code available at https://github.com/brandeis-machine-learning/influence-fairness.},
}
"
"
@inproceedings{pmlr-v162-li22v,
	title        = {Let Invariant Rationale Discovery Inspire Graph Contrastive Learning},
	author       = {Li, Sihang and Wang, Xiang and Zhang, An and Wu, Yingxin and He, Xiangnan and Chua, Tat-Seng},
	booktitle    = {Proceedings of the 39th International Conference on Machine Learning},
	pages        = {13052--13065},
	year         = 2022,
	editor       = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
	volume       = 162,
	series       = {Proceedings of Machine Learning Research},
	month        = {17--23 Jul},
	publisher    = {PMLR},
	pdf          = {https://proceedings.mlr.press/v162/li22v/li22v.pdf},
	url          = {https://proceedings.mlr.press/v162/li22v.html},
	abstract     = {Leading graph contrastive learning (GCL) methods perform graph augmentations in two fashions: (1) randomly corrupting the anchor graph, which could cause the loss of semantic information, or (2) using domain knowledge to maintain salient features, which undermines the generalization to other domains. Taking an invariance look at GCL, we argue that a high-performing augmentation should preserve the salient semantics of anchor graphs regarding instance-discrimination. To this end, we relate GCL with invariant rationale discovery, and propose a new framework, Rationale-aware Graph Contrastive Learning (RGCL). Specifically, without supervision signals, RGCL uses a rationale generator to reveal salient features about graph instance-discrimination as the rationale, and then creates rationale-aware views for contrastive learning. This rationale-aware pre-training scheme endows the backbone model with the powerful representation ability, further facilitating the fine-tuning on downstream tasks. On MNIST-Superpixel and MUTAG datasets, visual inspections on the discovered rationales showcase that the rationale generator successfully captures the salient features (\ie distinguishing semantic nodes in graphs). On biochemical molecule and social network benchmark datasets, the state-of-the-art performance of RGCL demonstrates the effectiveness of rationale-aware views for contrastive learning. Our codes are available at https://github.com/lsh0520/RGCL.},
}
"
"
@inproceedings{pmlr-v162-lu22e,
	title        = {Multi-slots Online Matching with High Entropy},
	author       = {Lu, Xingyu and Wu, Qintong and Zhong, Wenliang},
	booktitle    = {Proceedings of the 39th International Conference on Machine Learning},
	pages        = {14412--14428},
	year         = 2022,
	editor       = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
	volume       = 162,
	series       = {Proceedings of Machine Learning Research},
	month        = {17--23 Jul},
	publisher    = {PMLR},
	pdf          = {https://proceedings.mlr.press/v162/lu22e/lu22e.pdf},
	url          = {https://proceedings.mlr.press/v162/lu22e.html},
	abstract     = {Online matching with diversity and fairness pursuit, a common building block in the recommendation and advertising, can be modeled as constrained convex programming with high entropy. While most existing approaches are based on the “single slot” assumption (i.e., assigning one item per iteration), they cannot be directly applied to cases with multiple slots, e.g., stock-aware top-N recommendation and advertising at multiple places. Particularly, the gradient computation and resource allocation are both challenging under this setting due to the absence of a closed-form solution. To overcome these obstacles, we develop a novel algorithm named Online subGradient descent for Multi-slots Allocation (OG-MA). It uses an efficient pooling algorithm to compute closed-form of the gradient then performs a roulette swapping for allocation, yielding a sub-linear regret with linear cost per iteration. Extensive experiments on synthetic and industrial data sets demonstrate that OG-MA is a fast and promising method for multi-slots online matching.},
}
"
"
@inproceedings{pmlr-v162-ma22b,
	title        = {Quantification and Analysis of Layer-wise and Pixel-wise Information Discarding},
	author       = {Ma, Haotian and Zhang, Hao and Zhou, Fan and Zhang, Yinqing and Zhang, Quanshi},
	booktitle    = {Proceedings of the 39th International Conference on Machine Learning},
	pages        = {14664--14698},
	year         = 2022,
	editor       = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
	volume       = 162,
	series       = {Proceedings of Machine Learning Research},
	month        = {17--23 Jul},
	publisher    = {PMLR},
	pdf          = {https://proceedings.mlr.press/v162/ma22b/ma22b.pdf},
	url          = {https://proceedings.mlr.press/v162/ma22b.html},
	abstract     = {This paper presents a method to explain how the information of each input variable is gradually discarded during the forward propagation in a deep neural network (DNN), which provides new perspectives to explain DNNs. We define two types of entropy-based metrics, i.e. (1) the discarding of pixel-wise information used in the forward propagation, and (2) the uncertainty of the input reconstruction, to measure input information contained by a specific layer from two perspectives. Unlike previous attribution metrics, the proposed metrics ensure the fairness of comparisons between different layers of different DNNs. We can use these metrics to analyze the efficiency of information processing in DNNs, which exhibits strong connections to the performance of DNNs. We analyze information discarding in a pixel-wise manner, which is different from the information bottleneck theory measuring feature information w.r.t. the sample distribution. Experiments have shown the effectiveness of our metrics in analyzing classic DNNs and explaining existing deep-learning techniques. The code is available at https://github.com/haotianSustc/deepinfo.},
}
"
"
@inproceedings{pmlr-v162-marcu22a,
	title        = {On the Effects of Artificial Data Modification},
	author       = {Marcu, Antonia and Prugel-Bennett, Adam},
	booktitle    = {Proceedings of the 39th International Conference on Machine Learning},
	pages        = {15050--15069},
	year         = 2022,
	editor       = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
	volume       = 162,
	series       = {Proceedings of Machine Learning Research},
	month        = {17--23 Jul},
	publisher    = {PMLR},
	pdf          = {https://proceedings.mlr.press/v162/marcu22a/marcu22a.pdf},
	url          = {https://proceedings.mlr.press/v162/marcu22a.html},
	abstract     = {Data distortion is commonly applied in vision models during both training (e.g methods like MixUp and CutMix) and evaluation (e.g. shape-texture bias and robustness). This data modification can introduce artificial information. It is often assumed that the resulting artefacts are detrimental to training, whilst being negligible when analysing models. We investigate these assumptions and conclude that in some cases they are unfounded and lead to incorrect results. Specifically, we show current shape bias identification methods and occlusion robustness measures are biased and propose a fairer alternative for the latter. Subsequently, through a series of experiments we seek to correct and strengthen the community’s perception of how augmenting affects learning of vision models. Based on our empirical results we argue that the impact of the artefacts must be understood and exploited rather than eliminated.},
}
"
"
@inproceedings{pmlr-v162-marfoq22a,
	title        = {Personalized Federated Learning through Local Memorization},
	author       = {Marfoq, Othmane and Neglia, Giovanni and Vidal, Richard and Kameni, Laetitia},
	booktitle    = {Proceedings of the 39th International Conference on Machine Learning},
	pages        = {15070--15092},
	year         = 2022,
	editor       = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
	volume       = 162,
	series       = {Proceedings of Machine Learning Research},
	month        = {17--23 Jul},
	publisher    = {PMLR},
	pdf          = {https://proceedings.mlr.press/v162/marfoq22a/marfoq22a.pdf},
	url          = {https://proceedings.mlr.press/v162/marfoq22a.html},
	abstract     = {Federated learning allows clients to collaboratively learn statistical models while keeping their data local. Federated learning was originally used to train a unique global model to be served to all clients, but this approach might be sub-optimal when clients’ local data distributions are heterogeneous. In order to tackle this limitation, recent personalized federated learning methods train a separate model for each client while still leveraging the knowledge available at other clients. In this work, we exploit the ability of deep neural networks to extract high quality vectorial representations (embeddings) from non-tabular data, e.g., images and text, to propose a personalization mechanism based on local memorization. Personalization is obtained by interpolating a collectively trained global model with a local $k$-nearest neighbors (kNN) model based on the shared representation provided by the global model. We provide generalization bounds for the proposed approach in the case of binary classification, and we show on a suite of federated datasets that this approach achieves significantly higher accuracy and fairness than state-of-the-art methods.},
}
"
"
@inproceedings{pmlr-v162-nilforoshan22a,
	title        = {Causal Conceptions of Fairness and their Consequences},
	author       = {Nilforoshan, Hamed and Gaebler, Johann D and Shroff, Ravi and Goel, Sharad},
	booktitle    = {Proceedings of the 39th International Conference on Machine Learning},
	pages        = {16848--16887},
	year         = 2022,
	editor       = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
	volume       = 162,
	series       = {Proceedings of Machine Learning Research},
	month        = {17--23 Jul},
	publisher    = {PMLR},
	pdf          = {https://proceedings.mlr.press/v162/nilforoshan22a/nilforoshan22a.pdf},
	url          = {https://proceedings.mlr.press/v162/nilforoshan22a.html},
	abstract     = {Recent work highlights the role of causality in designing equitable decision-making algorithms. It is not immediately clear, however, how existing causal conceptions of fairness relate to one another, or what the consequences are of using these definitions as design principles. Here, we first assemble and categorize popular causal definitions of algorithmic fairness into two broad families: (1) those that constrain the effects of decisions on counterfactual disparities; and (2) those that constrain the effects of legally protected characteristics, like race and gender, on decisions. We then show, analytically and empirically, that both families of definitions <em>almost always</em>—in a measure theoretic sense—result in strongly Pareto dominated decision policies, meaning there is an alternative, unconstrained policy favored by every stakeholder with preferences drawn from a large, natural class. For example, in the case of college admissions decisions, policies constrained to satisfy causal fairness definitions would be disfavored by every stakeholder with neutral or positive preferences for both academic preparedness and diversity. Indeed, under a prominent definition of causal fairness, we prove the resulting policies require admitting all students with the same probability, regardless of academic qualifications or group membership. Our results highlight formal limitations and potential adverse consequences of common mathematical notions of causal fairness.},
}
"
"
@inproceedings{pmlr-v162-pichler22a,
	title        = {A Differential Entropy Estimator for Training Neural Networks},
	author       = {Pichler, Georg and Colombo, Pierre Jean A. and Boudiaf, Malik and Koliander, G{\""u}nther and Piantanida, Pablo},
	booktitle    = {Proceedings of the 39th International Conference on Machine Learning},
	pages        = {17691--17715},
	year         = 2022,
	editor       = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
	volume       = 162,
	series       = {Proceedings of Machine Learning Research},
	month        = {17--23 Jul},
	publisher    = {PMLR},
	pdf          = {https://proceedings.mlr.press/v162/pichler22a/pichler22a.pdf},
	url          = {https://proceedings.mlr.press/v162/pichler22a.html},
	abstract     = {Mutual Information (MI) has been widely used as a loss regularizer for training neural networks. This has been particularly effective when learn disentangled or compressed representations of high dimensional data. However, differential entropy (DE), another fundamental measure of information, has not found widespread use in neural network training. Although DE offers a potentially wider range of applications than MI, off-the-shelf DE estimators are either non differentiable, computationally intractable or fail to adapt to changes in the underlying distribution. These drawbacks prevent them from being used as regularizers in neural networks training. To address shortcomings in previously proposed estimators for DE, here we introduce KNIFE, a fully parameterized, differentiable kernel-based estimator of DE. The flexibility of our approach also allows us to construct KNIFE-based estimators for conditional (on either discrete or continuous variables) DE, as well as MI. We empirically validate our method on high-dimensional synthetic data and further apply it to guide the training of neural networks for real-world tasks. Our experiments on a large variety of tasks, including visual domain adaptation, textual fair classification, and textual fine-tuning demonstrate the effectiveness of KNIFE-based estimation. Code can be found at https://github.com/g-pichler/knife.},
}
"
"
@inproceedings{pmlr-v162-shah22a,
	title        = {Selective Regression under Fairness Criteria},
	author       = {Shah, Abhin and Bu, Yuheng and Lee, Joshua K and Das, Subhro and Panda, Rameswar and Sattigeri, Prasanna and Wornell, Gregory W},
	booktitle    = {Proceedings of the 39th International Conference on Machine Learning},
	pages        = {19598--19615},
	year         = 2022,
	editor       = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
	volume       = 162,
	series       = {Proceedings of Machine Learning Research},
	month        = {17--23 Jul},
	publisher    = {PMLR},
	pdf          = {https://proceedings.mlr.press/v162/shah22a/shah22a.pdf},
	url          = {https://proceedings.mlr.press/v162/shah22a.html},
	abstract     = {Selective regression allows abstention from prediction if the confidence to make an accurate prediction is not sufficient. In general, by allowing a reject option, one expects the performance of a regression model to increase at the cost of reducing coverage (i.e., by predicting on fewer samples). However, as we show, in some cases, the performance of a minority subgroup can decrease while we reduce the coverage, and thus selective regression can magnify disparities between different sensitive subgroups. Motivated by these disparities, we propose new fairness criteria for selective regression requiring the performance of every subgroup to improve with a decrease in coverage. We prove that if a feature representation satisfies the <em>sufficiency</em> criterion or is <em>calibrated for mean and variance</em>, then the proposed fairness criteria is met. Further, we introduce two approaches to mitigate the performance disparity across subgroups: (a) by regularizing an upper bound of conditional mutual information under a Gaussian assumption and (b) by regularizing a contrastive loss for conditional mean and conditional variance prediction. The effectiveness of these approaches is demonstrated on synthetic and real-world datasets.},
}
"
"
@inproceedings{pmlr-v162-shen22b,
	title        = {Metric-Fair Active Learning},
	author       = {Shen, Jie and Cui, Nan and Wang, Jing},
	booktitle    = {Proceedings of the 39th International Conference on Machine Learning},
	pages        = {19809--19826},
	year         = 2022,
	editor       = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
	volume       = 162,
	series       = {Proceedings of Machine Learning Research},
	month        = {17--23 Jul},
	publisher    = {PMLR},
	pdf          = {https://proceedings.mlr.press/v162/shen22b/shen22b.pdf},
	url          = {https://proceedings.mlr.press/v162/shen22b.html},
	abstract     = {Active learning has become a prevalent technique for designing label-efficient algorithms, where the central principle is to only query and fit “informative” labeled instances. It is, however, known that an active learning algorithm may incur unfairness due to such instance selection procedure. In this paper, we henceforth study metric-fair active learning of homogeneous halfspaces, and show that under the distribution-dependent PAC learning model, fairness and label efficiency can be achieved simultaneously. We further propose two extensions of our main results: 1) we show that it is possible to make the algorithm robust to the adversarial noise&nbsp;–&nbsp;one of the most challenging noise models in learning theory; and 2) it is possible to significantly improve the label complexity when the underlying halfspace is sparse.},
}
"
"
@inproceedings{pmlr-v162-shui22a,
	title        = {Fair Representation Learning through Implicit Path Alignment},
	author       = {Shui, Changjian and Chen, Qi and Li, Jiaqi and Wang, Boyu and Gagn{\'e}, Christian},
	booktitle    = {Proceedings of the 39th International Conference on Machine Learning},
	pages        = {20156--20175},
	year         = 2022,
	editor       = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
	volume       = 162,
	series       = {Proceedings of Machine Learning Research},
	month        = {17--23 Jul},
	publisher    = {PMLR},
	pdf          = {https://proceedings.mlr.press/v162/shui22a/shui22a.pdf},
	url          = {https://proceedings.mlr.press/v162/shui22a.html},
	abstract     = {We consider a fair representation learning perspective, where optimal predictors, on top of the data representation, are ensured to be invariant with respect to different sub-groups. Specifically, we formulate this intuition as a bi-level optimization, where the representation is learned in the outer-loop, and invariant optimal group predictors are updated in the inner-loop. Moreover, the proposed bi-level objective is demonstrated to fulfill the sufficiency rule, which is desirable in various practical scenarios but was not commonly studied in the fair learning. Besides, to avoid the high computational and memory cost of differentiating in the inner-loop of bi-level objective, we propose an implicit path alignment algorithm, which only relies on the solution of inner optimization and the implicit differentiation rather than the exact optimization path. We further analyze the error gap of the implicit approach and empirically validate the proposed method in both classification and regression settings. Experimental results show the consistently better trade-off in prediction performance and fairness measurement.},
}
"
"
@inproceedings{pmlr-v162-sukeni-k22a,
	title        = {Intriguing Properties of Input-Dependent Randomized Smoothing},
	author       = {S{\'u}ken\'{\i}k, Peter and Kuvshinov, Aleksei and G{\""u}nnemann, Stephan},
	booktitle    = {Proceedings of the 39th International Conference on Machine Learning},
	pages        = {20697--20743},
	year         = 2022,
	editor       = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
	volume       = 162,
	series       = {Proceedings of Machine Learning Research},
	month        = {17--23 Jul},
	publisher    = {PMLR},
	pdf          = {https://proceedings.mlr.press/v162/sukeni-k22a/sukeni-k22a.pdf},
	url          = {https://proceedings.mlr.press/v162/sukeni-k22a.html},
	abstract     = {Randomized smoothing is currently considered the state-of-the-art method to obtain certifiably robust classifiers. Despite its remarkable performance, the method is associated with various serious problems such as “certified accuracy waterfalls”, certification vs. accuracy trade-off, or even fairness issues. Input-dependent smoothing approaches have been proposed with intention of overcoming these flaws. However, we demonstrate that these methods lack formal guarantees and so the resulting certificates are not justified. We show that in general, the input-dependent smoothing suffers from the curse of dimensionality, forcing the variance function to have low semi-elasticity. On the other hand, we provide a theoretical and practical framework that enables the usage of input-dependent smoothing even in the presence of the curse of dimensionality, under strict restrictions. We present one concrete design of the smoothing variance function and test it on CIFAR10 and MNIST. Our design mitigates some of the problems of classical smoothing and is formally underlined, yet further improvement of the design is still necessary.},
}
"
"
@inproceedings{pmlr-v162-tosh22a,
	title        = {Simple and near-optimal algorithms for hidden stratification and multi-group learning},
	author       = {Tosh, Christopher J and Hsu, Daniel},
	booktitle    = {Proceedings of the 39th International Conference on Machine Learning},
	pages        = {21633--21657},
	year         = 2022,
	editor       = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
	volume       = 162,
	series       = {Proceedings of Machine Learning Research},
	month        = {17--23 Jul},
	publisher    = {PMLR},
	pdf          = {https://proceedings.mlr.press/v162/tosh22a/tosh22a.pdf},
	url          = {https://proceedings.mlr.press/v162/tosh22a.html},
	abstract     = {Multi-group agnostic learning is a formal learning criterion that is concerned with the conditional risks of predictors within subgroups of a population. The criterion addresses recent practical concerns such as subgroup fairness and hidden stratification. This paper studies the structure of solutions to the multi-group learning problem, and provides simple and near-optimal algorithms for the learning problem.},
}
"
"
@inproceedings{pmlr-v162-tucker22a,
	title        = {Prototype Based Classification from Hierarchy to Fairness},
	author       = {Tucker, Mycal and Shah, Julie A.},
	booktitle    = {Proceedings of the 39th International Conference on Machine Learning},
	pages        = {21884--21900},
	year         = 2022,
	editor       = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
	volume       = 162,
	series       = {Proceedings of Machine Learning Research},
	month        = {17--23 Jul},
	publisher    = {PMLR},
	pdf          = {https://proceedings.mlr.press/v162/tucker22a/tucker22a.pdf},
	url          = {https://proceedings.mlr.press/v162/tucker22a.html},
	abstract     = {Artificial neural nets can represent and classify many types of high-dimensional data but are often tailored to particular applications – e.g., for “fair” or “hierarchical” classification. Once an architecture has been selected, it is often difficult for humans to adjust models for a new task; for example, a hierarchical classifier cannot be easily transformed into a fair classifier that shields a protected field. Our contribution in this work is a new neural network architecture, the concept subspace network (CSN), which generalizes existing specialized classifiers to produce a unified model capable of learning a spectrum of multi-concept relationships. We demonstrate that CSNs reproduce state-of-the-art results in fair classification when enforcing concept independence, may be transformed into hierarchical classifiers, or may even reconcile fairness and hierarchy within a single classifier. The CSN is inspired by and matches the performance of existing prototype-based classifiers that promote interpretability.},
}
"
"
@inproceedings{pmlr-v162-wang22ac,
	title        = {Understanding Instance-Level Impact of Fairness Constraints},
	author       = {Wang, Jialu and Wang, Xin Eric and Liu, Yang},
	booktitle    = {Proceedings of the 39th International Conference on Machine Learning},
	pages        = {23114--23130},
	year         = 2022,
	editor       = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
	volume       = 162,
	series       = {Proceedings of Machine Learning Research},
	month        = {17--23 Jul},
	publisher    = {PMLR},
	pdf          = {https://proceedings.mlr.press/v162/wang22ac/wang22ac.pdf},
	url          = {https://proceedings.mlr.press/v162/wang22ac.html},
	abstract     = {A variety of fairness constraints have been proposed in the literature to mitigate group-level statistical bias. Their impacts have been largely evaluated for different groups of populations corresponding to a set of sensitive attributes, such as race or gender. Nonetheless, the community has not observed sufficient explorations for how imposing fairness constraints fare at an instance level. Building on the concept of influence function, a measure that characterizes the impact of a training example on the target model and its predictive performance, this work studies the influence of training examples when fairness constraints are imposed. We find out that under certain assumptions, the influence function with respect to fairness constraints can be decomposed into a kernelized combination of training examples. One promising application of the proposed fairness influence function is to identify suspicious training examples that may cause model discrimination by ranking their influence scores. We demonstrate with extensive experiments that training on a subset of weighty data examples leads to lower fairness violations with a trade-off of accuracy.},
}
"
"
@inproceedings{pmlr-v162-wu22a,
	title        = {Metric-Fair Classifier Derandomization},
	author       = {Wu, Jimmy and Chen, Yatong and Liu, Yang},
	booktitle    = {Proceedings of the 39th International Conference on Machine Learning},
	pages        = {23999--24016},
	year         = 2022,
	editor       = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
	volume       = 162,
	series       = {Proceedings of Machine Learning Research},
	month        = {17--23 Jul},
	publisher    = {PMLR},
	pdf          = {https://proceedings.mlr.press/v162/wu22a/wu22a.pdf},
	url          = {https://proceedings.mlr.press/v162/wu22a.html},
	abstract     = {We study the problem of <em>classifier derandomization</em> in machine learning: given a stochastic binary classifier $f: X \to [0,1]$, sample a deterministic classifier $\hat{f}: X \to \{0,1\}$ that approximates the output of $f$ in aggregate over any data distribution. Recent work revealed how to efficiently derandomize a stochastic classifier with strong output approximation guarantees, but at the cost of individual fairness — that is, if $f$ treated similar inputs similarly, $\hat{f}$ did not. In this paper, we initiate a systematic study of classifier derandomization with metric fairness guarantees. We show that the prior derandomization approach is almost maximally metric-unfair, and that a simple “random threshold” derandomization achieves optimal fairness preservation but with weaker output approximation. We then devise a derandomization procedure that provides an appealing tradeoff between these two: if $f$ is $\alpha$-metric fair according to a metric $d$ with a locality-sensitive hash (LSH) family, then our derandomized $\hat{f}$ is, with high probability, $O(\alpha)$-metric fair and a close approximation of $f$. We also prove generic results applicable to all (fair and unfair) classifier derandomization procedures, including a bias-variance decomposition and reductions between various notions of metric fairness.},
}
"
"
@inproceedings{pmlr-v162-yan22c,
	title        = {Active fairness auditing},
	author       = {Yan, Tom and Zhang, Chicheng},
	booktitle    = {Proceedings of the 39th International Conference on Machine Learning},
	pages        = {24929--24962},
	year         = 2022,
	editor       = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
	volume       = 162,
	series       = {Proceedings of Machine Learning Research},
	month        = {17--23 Jul},
	publisher    = {PMLR},
	pdf          = {https://proceedings.mlr.press/v162/yan22c/yan22c.pdf},
	url          = {https://proceedings.mlr.press/v162/yan22c.html},
	abstract     = {The fast spreading adoption of machine learning (ML) by companies across industries poses significant regulatory challenges. One such challenge is scalability: how can regulatory bodies efficiently <em>audit</em> these ML models, ensuring that they are fair? In this paper, we initiate the study of query-based auditing algorithms that can estimate the demographic parity of ML models in a query-efficient manner. We propose an optimal deterministic algorithm, as well as a practical randomized, oracle-efficient algorithm with comparable guarantees. Furthermore, we make inroads into understanding the optimal query complexity of randomized active fairness estimation algorithms. Our first exploration of active fairness estimation aims to put AI governance on firmer theoretical foundations.},
}
"
"
@inproceedings{pmlr-v162-zaffran22a,
	title        = {Adaptive Conformal Predictions for Time Series},
	author       = {Zaffran, Margaux and Feron, Olivier and Goude, Yannig and Josse, Julie and Dieuleveut, Aymeric},
	booktitle    = {Proceedings of the 39th International Conference on Machine Learning},
	pages        = {25834--25866},
	year         = 2022,
	editor       = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
	volume       = 162,
	series       = {Proceedings of Machine Learning Research},
	month        = {17--23 Jul},
	publisher    = {PMLR},
	pdf          = {https://proceedings.mlr.press/v162/zaffran22a/zaffran22a.pdf},
	url          = {https://proceedings.mlr.press/v162/zaffran22a.html},
	abstract     = {Uncertainty quantification of predictive models is crucial in decision-making problems. Conformal prediction is a general and theoretically sound answer. However, it requires exchangeable data, excluding time series. While recent works tackled this issue, we argue that Adaptive Conformal Inference (ACI, Gibbs &amp; Cand{è}s, 2021), developed for distribution-shift time series, is a good procedure for time series with general dependency. We theoretically analyse the impact of the learning rate on its efficiency in the exchangeable and auto-regressive case. We propose a parameter-free method, AgACI, that adaptively builds upon ACI based on online expert aggregation. We lead extensive fair simulations against competing methods that advocate for ACI’s use in time series. We conduct a real case study: electricity price forecasting. The proposed aggregation algorithm provides efficient prediction intervals for day-ahead forecasting. All the code and data to reproduce the experiments are made available on GitHub.},
}
"
"
@inproceedings{pmlr-v162-zhang22l,
	title        = {Fairness Interventions as ({D}is){I}ncentives for Strategic Manipulation},
	author       = {Zhang, Xueru and Khalili, Mohammad Mahdi and Jin, Kun and Naghizadeh, Parinaz and Liu, Mingyan},
	booktitle    = {Proceedings of the 39th International Conference on Machine Learning},
	pages        = {26239--26264},
	year         = 2022,
	editor       = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
	volume       = 162,
	series       = {Proceedings of Machine Learning Research},
	month        = {17--23 Jul},
	publisher    = {PMLR},
	pdf          = {https://proceedings.mlr.press/v162/zhang22l/zhang22l.pdf},
	url          = {https://proceedings.mlr.press/v162/zhang22l.html},
	abstract     = {Although machine learning (ML) algorithms are widely used to make decisions about individuals in various domains, concerns have arisen that (1) these algorithms are vulnerable to strategic manipulation and ""gaming the algorithm""; and (2) ML decisions may exhibit bias against certain social groups. Existing works have largely examined these as two separate issues, e.g., by focusing on building ML algorithms robust to strategic manipulation, or on training a fair ML algorithm. In this study, we set out to understand the impact they each have on the other, and examine how to characterize fair policies in the presence of strategic behavior. The strategic interaction between a decision maker and individuals (as decision takers) is modeled as a two-stage (Stackelberg) game; when designing an algorithm, the former anticipates the latter may manipulate their features in order to receive more favorable decisions. We analytically characterize the equilibrium strategies of both, and examine how the algorithms and their resulting fairness properties are affected when the decision maker is strategic (anticipates manipulation), as well as the impact of fairness interventions on equilibrium strategies. In particular, we identify conditions under which anticipation of strategic behavior may mitigate/exacerbate unfairness, and conditions under which fairness interventions can serve as (dis)incentives for strategic manipulation.},
}
"
@inproceedings{leungchi2020,
	author       = {Leung, Weiwen and Zhang, Zheng and Jibuti, Daviti and Zhao, Jinhao and Klein, Maximilian and Pierce, Casey and Robert, Lionel and Zhu, Haiyi},
	title        = {Race, Gender and Beauty: The Effect of Information Provision on Online Hiring Biases},
	year         = 2020,
	isbn         = 9781450367080,
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	url          = {https://doi.org/10.1145/3313831.3376874},
	doi          = {10.1145/3313831.3376874},
	abstract     = {We conduct a study of hiring bias on a simulation platform where we ask Amazon MTurk participants to make hiring decisions for a mathematically intensive task. Our findings suggest hiring biases against Black workers and less attractive workers, and preferences towards Asian workers, female workers and more attractive workers. We also show that certain UI designs, including provision of candidates' information at the individual level and reducing the number of choices, can significantly reduce discrimination. However, provision of candidate's information at the subgroup level can increase discrimination. The results have practical implications for designing better online freelance marketplaces.},
	booktitle    = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
	pages        = {1–11},
	numpages     = 11,
	keywords     = {discrimination, gig economy, hiring},
	location     = {Honolulu, HI, USA},
	series       = {CHI '20},
}
@inproceedings{anikchi21,
	author       = {Anik, Ariful Islam and Bunt, Andrea},
	title        = {Data-Centric Explanations: Explaining Training Data of Machine Learning Systems to Promote Transparency},
	year         = 2021,
	isbn         = 9781450380966,
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	url          = {https://doi.org/10.1145/3411764.3445736},
	doi          = {10.1145/3411764.3445736},
	abstract     = {Training datasets fundamentally impact the performance of machine learning (ML) systems. Any biases introduced during training (implicit or explicit) are often reflected in the system's behaviors leading to questions about fairness and loss of trust in the system. Yet, information on training data is rarely communicated to stakeholders. In this work, we explore the concept of data-centric explanations for ML systems that describe the training data to end-users. Through a formative study, we investigate the potential utility of such an approach, including the information about training data that participants find most compelling. In a second study, we investigate reactions to our explanations across four different system scenarios. Our results suggest that data-centric explanations have the potential to impact how users judge the trustworthiness of a system and to assist users in assessing fairness. We discuss the implications of our findings for designing explanations to support users’ perceptions of ML systems.},
	booktitle    = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
	articleno    = 75,
	numpages     = 13,
	keywords     = {Training Data, Fairness, Trust, Transparency, Explanations, Machine Learning Systems, User Expertise},
	location     = {Yokohama, Japan},
	series       = {CHI '21},
}
@inproceedings{chengchi21,
	author       = {Cheng, Hao-Fei and Stapleton, Logan and Wang, Ruiqi and Bullock, Paige and Chouldechova, Alexandra and Wu, Zhiwei Steven Steven and Zhu, Haiyi},
	title        = {Soliciting Stakeholders’ Fairness Notions in Child Maltreatment Predictive Systems},
	year         = 2021,
	isbn         = 9781450380966,
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	url          = {https://doi.org/10.1145/3411764.3445308},
	doi          = {10.1145/3411764.3445308},
	abstract     = {Recent work in fair machine learning has proposed dozens of technical definitions of algorithmic fairness and methods for enforcing these definitions. However, we still lack an understanding of how to develop machine learning systems with fairness criteria that reflect relevant stakeholders’ nuanced viewpoints in real-world contexts. To address this gap, we propose a framework for eliciting stakeholders’ subjective fairness notions. Combining a user interface that allows stakeholders to examine the data and the algorithm’s predictions with an interview protocol to probe stakeholders’ thoughts while they are interacting with the interface, we can identify stakeholders’ fairness beliefs and principles. We conduct a user study to evaluate our framework in the setting of a child maltreatment predictive system. Our evaluations show that the framework allows stakeholders to comprehensively convey their fairness viewpoints. We also discuss how our results can inform the design of predictive systems.},
	booktitle    = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
	articleno    = 390,
	numpages     = 17,
	keywords     = {machine learning, child welfare, human-centered AI, algorithm-assisted decision-making, algorithmic fairness},
	location     = {Yokohama, Japan},
	series       = {CHI '21},
}
@inproceedings{robertsonchi21,
	author       = {Robertson, Samantha and Nguyen, Tonya and Salehi, Niloufar},
	title        = {Modeling Assumptions Clash with the Real World: Transparency, Equity, and Community Challenges for Student Assignment Algorithms},
	year         = 2021,
	isbn         = 9781450380966,
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	url          = {https://doi.org/10.1145/3411764.3445748},
	doi          = {10.1145/3411764.3445748},
	abstract     = {Across the United States, a growing number of school districts are turning to matching algorithms to assign students to public schools. The designers of these algorithms aimed to promote values such as transparency, equity, and community in the process. However, school districts have encountered practical challenges in their deployment. In fact, San Francisco Unified School District voted to stop using and completely redesign their student assignment algorithm because it was frustrating for families and it was not promoting educational equity in practice. We analyze this system using a Value Sensitive Design approach and find that one reason values are not met in practice is that the system relies on modeling assumptions about families’ priorities, constraints, and goals that clash with the real world. These assumptions overlook the complex barriers to ideal participation that many families face, particularly because of socioeconomic inequalities. We argue that direct, ongoing engagement with stakeholders is central to aligning algorithmic values with real world conditions. In doing so we must broaden how we evaluate algorithms while recognizing the limitations of purely algorithmic solutions in addressing complex socio-political problems.},
	booktitle    = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
	articleno    = 589,
	numpages     = 14,
	keywords     = {mechanism design, value sensitive design, student assignment},
	location     = {Yokohama, Japan},
	series       = {CHI '21},
}
@inproceedings{christoforouchi21,
	author       = {Christoforou, Evgenia and Barlas, Pinar and Otterbacher, Jahna},
	title        = {It’s About Time: A View of Crowdsourced Data Before and During the Pandemic},
	year         = 2021,
	isbn         = 9781450380966,
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	url          = {https://doi.org/10.1145/3411764.3445317},
	doi          = {10.1145/3411764.3445317},
	abstract     = {Data attained through crowdsourcing have an essential role in the development of computer vision algorithms. Crowdsourced data might include reporting biases, since crowdworkers usually describe what is “worth saying” in addition to images’ content. We explore how the unprecedented events of 2020, including the unrest surrounding racial discrimination, and the COVID-19 pandemic, might be reflected in responses to an open-ended annotation task on people images, originally executed in 2018 and replicated in 2020. Analyzing themes of Identity and Health conveyed in workers’ tags, we find evidence that supports the potential for temporal sensitivity in crowdsourced data. The 2020 data exhibit more race-marking of images depicting non-Whites, as well as an increase in tags describing Weight. We relate our findings to the emerging research on crowdworkers’ moods. Furthermore, we discuss the implications of (and suggestions for) designing tasks on proprietary platforms, having demonstrated the possibility for additional, unexpected variation in crowdsourced data due to significant events.},
	booktitle    = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
	articleno    = 649,
	numpages     = 14,
	keywords     = {reporting bias, data reproducibility, temporal sensitivity, image annotation, crowdsourcing},
	location     = {Yokohama, Japan},
	series       = {CHI '21},
}
@inproceedings{zhangchi22,
	author       = {Zhang, Angie and Boltz, Alexander and Wang, Chun Wei and Lee, Min Kyung},
	title        = {Algorithmic Management Reimagined For Workers and By Workers: Centering Worker Well-Being in Gig Work},
	year         = 2022,
	isbn         = 9781450391573,
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	url          = {https://doi.org/10.1145/3491102.3501866},
	doi          = {10.1145/3491102.3501866},
	abstract     = {Prior research has studied the detrimental impact of algorithmic management on gig workers and strategies that workers devise in response. However, little work has investigated alternative platform designs to promote worker well-being, particularly from workers’ own perspectives. We use a participatory design approach wherein workers explore their algorithmic imaginaries to co-design interventions that center their lived experiences, preferences, and well-being in algorithmic management. Our interview and participatory design sessions highlight how various design dimensions of algorithmic management, including information asymmetries and unfair, manipulative incentives, hurt worker well-being. Workers generate designs to address these issues while considering competing interests of the platforms, customers, and themselves, such as information translucency, incentives co-configured by workers and platforms, worker-centered data-driven insights for well-being, and collective driver data sharing. Our work offers a case study that responds to a call for designing worker-centered digital work and contributes to emerging literature on algorithmic work.},
	booktitle    = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
	articleno    = 14,
	numpages     = 20,
	keywords     = {worker well-being, Algorithmic management, participatory design, gig work, worker-centered work design},
	location     = {New Orleans, LA, USA},
	series       = {CHI '22},
}
@inproceedings{parkchi22,
	author       = {Park, Hyanghee and Ahn, Daehwan and Hosanagar, Kartik and Lee, Joonhwan},
	title        = {Designing Fair AI in Human Resource Management: Understanding Tensions Surrounding Algorithmic Evaluation and Envisioning Stakeholder-Centered Solutions},
	year         = 2022,
	isbn         = 9781450391573,
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	url          = {https://doi.org/10.1145/3491102.3517672},
	doi          = {10.1145/3491102.3517672},
	abstract     = {Enterprises have recently adopted AI to human resource management (HRM) to evaluate employees’ work performance evaluation. However, in such an HRM context where multiple stakeholders are complexly intertwined with different incentives, it is problematic to design AI reflecting one stakeholder group's needs (e.g., enterprises, HR managers). Our research aims to investigate what tensions surrounding AI in HRM exist among stakeholders and explore design solutions to balance the tensions. By conducting stakeholder-centered participatory workshops with diverse stakeholders (including employees, employers/HR teams, and AI/business experts), we identified five major tensions: 1)&nbsp;divergent perspectives on fairness, 2)&nbsp;the accuracy of AI, 3)&nbsp;the transparency of the algorithm and its decision process, 4)&nbsp;the interpretability of algorithmic decisions, and 5)&nbsp;the trade-off between productivity and inhumanity. We present stakeholder-centered design ideas for solutions to mitigate these tensions and further discuss how to promote harmony among various stakeholders at the workplace.},
	booktitle    = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
	articleno    = 51,
	numpages     = 22,
	keywords     = {Fair and responsible AI, Human resource management, Transparency, Algorithmic management, Future of work, Interpretability, Explainable AI (XAI), Artificial intelligence (AI), Stakeholder-centered design, Human Intervention},
	location     = {New Orleans, LA, USA},
	series       = {CHI '22},
}
@inproceedings{machi22,
	author       = {Ma, Zilin and Gajos, Krzysztof Z.},
	title        = {Not Just a Preference: Reducing Biased Decision-Making on Dating Websites},
	year         = 2022,
	isbn         = 9781450391573,
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	url          = {https://doi.org/10.1145/3491102.3517587},
	doi          = {10.1145/3491102.3517587},
	abstract     = {As dating websites are becoming an essential part of how people meet intimate and romantic partners, it is vital to design these systems to be resistant to, or at least do not amplify, bias and discrimination. Instead, the results of our online experiment with a simulated dating website, demonstrate that popular dating website design choices, such as the user of the swipe interface (swiping in one direction to indicate a like and in the other direction to express a dislike) and match scores, resulted in people racially biases choices even when they explicitly claimed not to have considered race in their decision-making. This bias was significantly reduced when the order of information presentation was reversed such that people first saw substantive profile information related to their explicitly-stated preferences before seeing the profile name and photo. These results indicate that currently-popular design choices amplify people’s implicit biases in their choices of potential romantic partners, but the effects of the implicit biases can be reduced by carefully redesigning the dating website interfaces.},
	booktitle    = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
	articleno    = 203,
	numpages     = 14,
	keywords     = {dating websites, implicit bias},
	location     = {New Orleans, LA, USA},
	series       = {CHI '22},
}
@inproceedings{watkinscscw20,
	author       = {Watkins, Elizabeth Anne},
	title        = {Took a Pic and Got Declined, Vexed and Perplexed: Facial Recognition in Algorithmic Management},
	year         = 2020,
	isbn         = 9781450380591,
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	url          = {https://doi.org/10.1145/3406865.3418383},
	doi          = {10.1145/3406865.3418383},
	abstract     = {The rise of biometric security changes how users make decisions about their privacy. As passwords give way to faces and fingerprints, the algorithmic nature of these processes creates new cognitive labor for users. When biometrics are used in spaces of algorithmic management, workers must negotiate tradeoffs between security, privacy, fairness, and their livelihood. A mixed-methods, human-centered research design paired with theory frameworks from algorithmic management, usable security, and algorithmic fairness illuminates how workers navigate facial recognition at the level of local practice. As AI/ML technologies for management and security become increasingly interwoven, the implications of this research are significant.},
	booktitle    = {Conference Companion Publication of the 2020 on Computer Supported Cooperative Work and Social Computing},
	pages        = {177–182},
	numpages     = 6,
	keywords     = {recognition technology, gig work, empirical studies in hci, facial recognition, human and societal aspects of security and privacy, algorithmic management, biometrics, usability in security and privacy, human computer interaction},
	location     = {Virtual Event, USA},
	series       = {CSCW '20 Companion},
}
@inproceedings{hettiachchicscw21,
	author       = {Hettiachchi, Danula and Sanderson, Mark and Goncalves, Jorge and Hosio, Simo and Kazai, Gabriella and Lease, Matthew and Schaekermann, Mike and Yilmaz, Emine},
	title        = {Investigating and Mitigating Biases in Crowdsourced Data},
	year         = 2021,
	isbn         = 9781450384797,
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	url          = {https://doi.org/10.1145/3462204.3481729},
	doi          = {10.1145/3462204.3481729},
	abstract     = {It is common practice for machine learning systems to rely on crowdsourced label data for training and evaluation. It is also well-known that biases present in the label data can induce biases in the trained models. Biases may be introduced by the mechanisms used for deciding what data should/could be labelled or by the mechanisms employed to obtain the labels. Various approaches have been proposed to detect and correct biases once the label dataset has been constructed. However, proactively reducing biases during the data labelling phase and ensuring data fairness could be more economical compared to post-processing bias mitigation approaches. In this workshop, we aim to foster discussion on ongoing research around biases in crowdsourced data and to identify future research directions to detect, quantify and mitigate biases before, during and after the labelling process such that both task requesters and crowd workers can benefit. We will explore how specific crowdsourcing workflows, worker attributes, and work practices contribute to biases in the labelled data; how to quantify and mitigate biases as part of the labelling process; and how such mitigation approaches may impact workers and the crowdsourcing ecosystem. The outcome of the workshop will include a collaborative publication of a research agenda to improve or develop novel methods relating to crowdsourcing tools, processes and work practices to address biases in crowdsourced data. We also plan to run a Crowd Bias Challenge prior to the workshop, where participants will be asked to collect labels for a given dataset while minimising potential biases.},
	booktitle    = {Companion Publication of the 2021 Conference on Computer Supported Cooperative Work and Social Computing},
	pages        = {331–334},
	numpages     = 4,
	keywords     = {biases, crowdsourcing, data quality},
	location     = {Virtual Event, USA},
	series       = {CSCW '21},
}
@inproceedings{navarrocscw22,
	author       = {Navarro, Maria Conchita A. and Shaer, Orit},
	title        = {Re-Imagining Systems in the Realm of Immigration in Higher Education through Participatory Design},
	year         = 2022,
	isbn         = 9781450391900,
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	url          = {https://doi.org/10.1145/3500868.3559457},
	doi          = {10.1145/3500868.3559457},
	abstract     = {Students identifying with communities within the realm of immigration, including immigrants, refugees, international students, and undocumented individuals, often face difficulties navigating life in elite institutions of higher education. HCI scholars who have worked with migrant communities have called for future explorations on the role of participatory approaches in helping bring different stakeholders together to design for better technologies and socio-technical systems supporting their needs. In this paper we present preliminary findings from a participatory design study at an elite liberal-arts college in the U.S., exploring the role of an equity-centered hackathon-style event in bringing together different student communities on campus and collaboratively re-imagining today’s technologies and systems. We discuss possible best practices for designing inclusive and safe hackathon-style events for communities in the realm of immigration.},
	booktitle    = {Companion Publication of the 2022 Conference on Computer Supported Cooperative Work and Social Computing},
	pages        = {76–79},
	numpages     = 4,
	keywords     = {intersectional HCI, migrant communities, equity, feminist HCI, hackathons, higher education, immigration, participatory design},
	location     = {Virtual Event, Taiwan},
	series       = {CSCW'22 Companion},
}
@inproceedings{nikkhahcscw22,
	author       = {Nikkhah, Sarah and Rode, Akash Uday and Mittal, Priyanjali and Kulkarni, Neha K. and Nadkarni, Salonee and Mueller, Emily L. and Miller, Andrew D.},
	title        = {"I Feel like I Need to Split Myself in Half": Using Role Theory to Design for Parents as Caregiving Teams in the Children's Hospital},
	year         = 2022,
	isbn         = 9781450391900,
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	url          = {https://doi.org/10.1145/3500868.3559466},
	doi          = {10.1145/3500868.3559466},
	abstract     = {When their child is hospitalized, parents take on new caregiving roles, in addition to their existing home and work-related responsibilities.&nbsp;Previous CSCW research has shown how technologies can support caregiving, but more research is needed to systematically understand how technology could support parents and other family caregivers as they adopt new coordination roles in their collaborations with each other. This paper reports findings from an interview study with parents of children hospitalized for cancer treatment. We used the Role Theory framework from the social sciences to show how parents adopt and enact caregiving roles during hospitalization and the challenges they experience as they adapt to this stressful situation. We show how parents experience 'role strain' as they attempt to divide caregiving work and introduce the concept of 'inter-caregiver information disparity.' We propose design opportunities for caregiving coordination technologies to better support caregiving roles in multi-caregiver teams.},
	booktitle    = {Companion Publication of the 2022 Conference on Computer Supported Cooperative Work and Social Computing},
	pages        = {115–120},
	numpages     = 6,
	location     = {Virtual Event, Taiwan},
	series       = {CSCW'22 Companion},
}
@inproceedings{farnadiaies18,
	author       = {Farnadi, Golnoosh and Babaki, Behrouz and Getoor, Lise},
	title        = {Fairness in Relational Domains},
	year         = 2018,
	isbn         = 9781450360128,
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	url          = {https://doi.org/10.1145/3278721.3278733},
	doi          = {10.1145/3278721.3278733},
	abstract     = {AI and machine learning tools are being used with increasing frequency for decision making in domains that affect peoples' lives such as employment, education, policing and loan approval. These uses raise concerns about biases of algorithmic discrimination and have motivated the development of fairness-aware machine learning. However, existing fairness approaches are based solely on attributes of individuals. In many cases, discrimination is much more complex, and taking into account the social, organizational, and other connections between individuals is important. We introduce new notions of fairness that are able to capture the relational structure in a domain. We use first-order logic to provide a flexible and expressive language for specifying complex relational patterns of discrimination. Furthermore, we extend an existing statistical relational learning framework, probabilistic soft logic (PSL), to incorporate our definition of relational fairness. We refer to this fairness-aware framework FairPSL. FairPSL makes use of the logical definitions of fairnesss but also supports a probabilistic interpretation. In particular, we show how to perform maximum a posteriori(MAP) inference by exploiting probabilistic dependencies within the domain while avoiding violation of fairness guarantees. Preliminary empirical evaluation shows that we are able to make both accurate and fair decisions.},
	booktitle    = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
	pages        = {108–114},
	numpages     = 7,
	keywords     = {statistical relational learning, fairness, probabilistic soft logic},
	location     = {New Orleans, LA, USA},
	series       = {AIES '18},
}
@inproceedings{goelaies18,
	author       = {Goel, Naman and Yaghini, Mohammad and Faltings, Boi},
	title        = {Non-Discriminatory Machine Learning through Convex Fairness Criteria},
	year         = 2018,
	isbn         = 9781450360128,
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	url          = {https://doi.org/10.1145/3278721.3278722},
	doi          = {10.1145/3278721.3278722},
	abstract     = {We introduce a novel technique to achieve non-discrimination in machine learning without sacrificing convexity and probabilistic interpretation. We also propose a new notion of fairness for machine learning called the weighted proportional fairness and show that our technique satisfies this subjective fairness criterion.},
	booktitle    = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
	pages        = 116,
	numpages     = 1,
	keywords     = {machine learning, non-discrimination, proportional fairness},
	location     = {New Orleans, LA, USA},
	series       = {AIES '18},
}
@inproceedings{raffaies18,
	author       = {Raff, Edward and Sylvester, Jared and Mills, Steven},
	title        = {Fair Forests: Regularized Tree Induction to Minimize Model Bias},
	year         = 2018,
	isbn         = 9781450360128,
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	url          = {https://doi.org/10.1145/3278721.3278742},
	doi          = {10.1145/3278721.3278742},
	abstract     = {The potential lack of fairness in the outputs of machine learning algorithms has recently gained attention both within the research community as well as in society more broadly. Surprisingly, there is no prior work developing tree-induction algorithms for building fair decision trees or fair random forests. These methods have widespread popularity as they are one of the few to be simultaneously interpretable, non-linear, and easy-to-use. In this paper we develop, to our knowledge, the first technique for the induction of fair decision trees.We show that our "Fair Forest" retains the benefits of the tree-based approach, while providing both greater accuracy and fairness than other alternatives, for both "group fairness'' and "individual fairness.'' We also introduce new measures for fairness which are able to handle multinomial and continues attributes as well as regression problems, as opposed to binary attributes and labels only. Finally, we demonstrate a new, more robust evaluation procedure for algorithms that considers the dataset in its entirety rather than only a specific protected attribute.},
	booktitle    = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
	pages        = {243–250},
	numpages     = 8,
	keywords     = {feature importance, fairness, random forest},
	location     = {New Orleans, LA, USA},
	series       = {AIES '18},
}
@inproceedings{camperoaies19,
	author       = {Noriega-Campero, Alejandro and Bakker, Michiel A. and Garcia-Bulle, Bernardo and Pentland, Alex 'Sandy'},
	title        = {Active Fairness in Algorithmic Decision Making},
	year         = 2019,
	isbn         = 9781450363242,
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	url          = {https://doi.org/10.1145/3306618.3314277},
	doi          = {10.1145/3306618.3314277},
	abstract     = {Society increasingly relies on machine learning models for automated decision making. Yet, efficiency gains from automation have come paired with concern for algorithmic discrimination that can systematize inequality. Recent work has proposed optimal post-processing methods that randomize classification decisions for a fraction of individuals, in order to achieve fairness measures related to parity in errors and calibration. These methods, however, have raised concern due to the information inefficiency, intra-group unfairness, and Pareto sub-optimality they entail. The present work proposes an alternativeactive framework for fair classification, where, in deployment, a decision-maker adaptively acquires information according to the needs of different groups or individuals, towards balancing disparities in classification performance. We propose two such methods, where information collection is adapted to group- and individual-level needs respectively. We show on real-world datasets that these can achieve: 1) calibration and single error parity (e.g.,equal opportunity ); and 2) parity in both false positive and false negative rates (i.e.,equal odds ). Moreover, we show that by leveraging their additional degree of freedom,active approaches can substantially outperform randomization-based classifiers previously considered optimal, while avoiding limitations such as intra-group unfairness.},
	booktitle    = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
	pages        = {77–83},
	numpages     = 7,
	keywords     = {algorithmic fairness, active feature acquisition, adaptive inquiry},
	location     = {Honolulu, HI, USA},
	series       = {AIES '19},
}
@inproceedings{aliaies19,
	author       = {Ali, Junaid and Zafar, Muhammad Bilal and Singla, Adish and Gummadi, Krishna P.},
	title        = {Loss-Aversively Fair Classification},
	year         = 2019,
	isbn         = 9781450363242,
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	url          = {https://doi.org/10.1145/3306618.3314266},
	doi          = {10.1145/3306618.3314266},
	abstract     = {The use of algorithmic (learning-based) decision making in scenarios that affect human lives has motivated a number of recent studies to investigate such decision making systems for potential unfairness, such as discrimination against subjects based on their sensitive features like gender or race. However, when judging the fairness of a newly designed decision making system, these studies have overlooked an important influence on people's perceptions of fairness, which is how the new algorithm changes the status quo, i.e., decisions of the existing decision making system. Motivated by extensive literature in behavioral economics and behavioral psychology (prospect theory), we propose a notion of fair updates that we refer to as loss-averse updates. Loss-averse updates constrain the updates to yield improved (more beneficial) outcomes to subjects compared to the status quo. We propose tractable proxy measures that would allow this notion to be incorporated in the training of a variety of linear and non-linear classifiers. We show how our proxy measures can be combined with existing measures for training nondiscriminatory classifiers.Our evaluation using synthetic and real-world datasets demonstrates that the proposed proxy measures are effective for their desired tasks.},
	booktitle    = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
	pages        = {211–218},
	numpages     = 8,
	keywords     = {algorithmic fairness, loss-averse fairness, fairness in machine learning, fair updates},
	location     = {Honolulu, HI, USA},
	series       = {AIES '19},
}
@inproceedings{gargaies19,
	author       = {Garg, Sahaj and Perot, Vincent and Limtiaco, Nicole and Taly, Ankur and Chi, Ed H. and Beutel, Alex},
	title        = {Counterfactual Fairness in Text Classification through Robustness},
	year         = 2019,
	isbn         = 9781450363242,
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	url          = {https://doi.org/10.1145/3306618.3317950},
	doi          = {10.1145/3306618.3317950},
	abstract     = {In this paper, we study counterfactual fairness in text classification, which asks the question: How would the prediction change if the sensitive attribute referenced in the example were different? Toxicity classifiers demonstrate a counterfactual fairness issue by predicting that "Some people are gay" is toxic while "Some people are straight" is nontoxic. We offer a metric, counterfactual token fairness (CTF), for measuring this particular form of fairness in text classifiers, and describe its relationship with group fairness. Further, we offer three approaches, blindness, counterfactual augmentation, and counterfactual logit pairing (CLP), for optimizing counterfactual token fairness during training, bridging the robustness and fairness literature. Empirically, we find that blindness and CLP address counterfactual token fairness. The methods do not harm classifier performance, and have varying tradeoffs with group fairness. These approaches, both for measurement and optimization, provide a new path forward for addressing fairness concerns in text classification.},
	booktitle    = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
	pages        = {219–226},
	numpages     = 8,
	keywords     = {counterfactual fairness, text classification, robustness, fairness},
	location     = {Honolulu, HI, USA},
	series       = {AIES '19},
}
@inproceedings{onetoaies19,
	author       = {Oneto, Luca and Doninini, Michele and Elders, Amon and Pontil, Massimiliano},
	title        = {Taking Advantage of Multitask Learning for Fair Classification},
	year         = 2019,
	isbn         = 9781450363242,
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	url          = {https://doi.org/10.1145/3306618.3314255},
	doi          = {10.1145/3306618.3314255},
	abstract     = {A central goal of algorithmic fairness is to reduce bias in automated decision making. An unavoidable tension exists between accuracy gains obtained by using sensitive information as part of a statistical model, and any commitment to protect these characteristics. Often, due to biases present in the data, using the sensitive information in the functional form of a classifier improves classification accuracy. In this paper we show how it is possible to get the best of both worlds: optimize model accuracy and fairness without explicitly using the sensitive feature in the functional form of the model, thereby treating different individuals equally. Our method is based on two key ideas. On the one hand, we propose to use Multitask Learning (MTL), enhanced with fairness constraints, to jointly learn group specific classifiers that leverage information between sensitive groups. On the other hand, since learning group specific models might not be permitted, we propose to first predict the sensitive features by any learning method and then to use the predicted sensitive feature to train MTL with fairness constraints. This enables us to tackle fairness with a three-pronged approach, that is, by increasing accuracy on each group, enforcing measures of fairness during training, and protecting sensitive information during testing. Experimental results on two real datasets support our proposal, showing substantial improvements in both accuracy and fairness.},
	booktitle    = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
	pages        = {227–237},
	numpages     = 11,
	keywords     = {fairness, classification, multitask learning},
	location     = {Honolulu, HI, USA},
	series       = {AIES '19},
}
@inproceedings{kimaies19,
	author       = {Kim, Michael P. and Ghorbani, Amirata and Zou, James},
	title        = {Multiaccuracy: Black-Box Post-Processing for Fairness in Classification},
	year         = 2019,
	isbn         = 9781450363242,
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	url          = {https://doi.org/10.1145/3306618.3314287},
	doi          = {10.1145/3306618.3314287},
	abstract     = {Prediction systems are successfully deployed in applications ranging from disease diagnosis, to predicting credit worthiness, to image recognition. Even when the overall accuracy is high, these systems may exhibit systematic biases that harm specific subpopulations; such biases may arise inadvertently due to underrepresentation in the data used to train a machine-learning model, or as the result of intentional malicious discrimination. We develop a rigorous framework of *multiaccuracy* auditing and post-processing to ensure accurate predictions across *identifiable subgroups*. Our algorithm, MULTIACCURACY-BOOST, works in any setting where we have black-box access to a predictor and a relatively small set of labeled data for auditing; importantly, this black-box framework allows for improved fairness and accountability of predictions, even when the predictor is minimally transparent. We prove that MULTIACCURACY-BOOST converges efficiently and show that if the initial model is accurate on an identifiable subgroup, then the post-processed model will be also. We experimentally demonstrate the effectiveness of the approach to improve the accuracy among minority subgroups in diverse applications (image classification, finance, population health). Interestingly, MULTIACCURACY-BOOST can improve subpopulation accuracy (e.g. for "black women") even when the sensitive features (e.g. "race", "gender") are not given to the algorithm explicitly.},
	booktitle    = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
	pages        = {247–254},
	numpages     = 8,
	keywords     = {discrimination, fairness, post-processing, machine learning},
	location     = {Honolulu, HI, USA},
	series       = {AIES '19},
}
@inproceedings{beutelaies19,
	author       = {Beutel, Alex and Chen, Jilin and Doshi, Tulsee and Qian, Hai and Woodruff, Allison and Luu, Christine and Kreitmann, Pierre and Bischof, Jonathan and Chi, Ed H.},
	title        = {Putting Fairness Principles into Practice: Challenges, Metrics, and Improvements},
	year         = 2019,
	isbn         = 9781450363242,
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	url          = {https://doi.org/10.1145/3306618.3314234},
	doi          = {10.1145/3306618.3314234},
	abstract     = {As more researchers have become aware of and passionate about algorithmic fairness, there has been an explosion in papers laying out new metrics, suggesting algorithms to address issues, and calling attention to issues in existing applications of machine learning. This research has greatly expanded our understanding of the concerns and challenges in deploying machine learning, but there has been much less work in seeing how the rubber meets the road. In this paper we provide a case-study on the application of fairness in machine learning research to a production classification system, and offer new insights in how to measure and address algorithmic fairness issues. We discuss open questions in implementing equality of opportunity and describe our fairness metric, conditional equality, that takes into account distributional differences. Further, we provide a new approach to improve on the fairness metric during model training and demonstrate its efficacy in improving performance for a real-world product.},
	booktitle    = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
	pages        = {453–459},
	numpages     = 7,
	keywords     = {equality of opportunity, fairness, classification},
	location     = {Honolulu, HI, USA},
	series       = {AIES '19},
}
@inproceedings{michalskyaies19,
	author       = {Michalsky, Filip},
	title        = {Fairness Criteria for Face Recognition Applications},
	year         = 2019,
	isbn         = 9781450363242,
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	url          = {https://doi.org/10.1145/3306618.3314308},
	doi          = {10.1145/3306618.3314308},
	abstract     = {Nowadays, machine learning algorithms play an important role in our daily lives and it is important to ensure their fairness and transparency. A number of methodologies for evaluating machine learning fairness have been introduced in the literature. In this research we propose a systematic confidence evaluation approach to measure fairness discrepancies of our deep learning architecture for image recognition using UTKFace database.},
	booktitle    = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
	pages        = {527–528},
	numpages     = 2,
	keywords     = {neural networks, machine learning, datasets, fairness, face recognition},
	location     = {Honolulu, HI, USA},
	series       = {AIES '19},
}
@inproceedings{sharmaaies20,
	author       = {Sharma, Shubham and Henderson, Jette and Ghosh, Joydeep},
	title        = {CERTIFAI: A Common Framework to Provide Explanations and Analyse the Fairness and Robustness of Black-Box Models},
	year         = 2020,
	isbn         = 9781450371100,
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	url          = {https://doi.org/10.1145/3375627.3375812},
	doi          = {10.1145/3375627.3375812},
	abstract     = {Concerns within the machine learning community and external pressures from regulators over the vulnerabilities of machine learning algorithms have spurred on the fields of explainability, robustness, and fairness. Often, issues in explainability, robustness, and fairness are confined to their specific sub-fields and few tools exist for model developers to use to simultaneously build their modeling pipelines in a transparent, accountable, and fair way. This can lead to a bottleneck on the model developer's side as they must juggle multiple methods to evaluate their algorithms. In this paper, we present a single framework for analyzing the robustness, fairness, and explainability of a classifier. The framework, which is based on the generation of counterfactual explanations through a custom genetic algorithm, is flexible, model-agnostic, and does not require access to model internals. The framework allows the user to calculate robustness and fairness scores for individual models and generate explanations for individual predictions which provide a means for actionable recourse (changes to an input to help get a desired outcome). This is the first time that a unified tool has been developed to address three key issues pertaining towards building a responsible artificial intelligence system.},
	booktitle    = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
	pages        = {166–172},
	numpages     = 7,
	keywords     = {explainability, robust-ness, responsible artificial intelligence, machine learning, fairness},
	location     = {New York, NY, USA},
	series       = {AIES '20},
}
@inproceedings{chenaies20,
	author       = {Chen, Violet (Xinying) and Hooker, J. N.},
	title        = {A Just Approach Balancing Rawlsian Leximax Fairness and Utilitarianism},
	year         = 2020,
	isbn         = 9781450371100,
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	url          = {https://doi.org/10.1145/3375627.3375844},
	doi          = {10.1145/3375627.3375844},
	abstract     = {Numerous AI-assisted resource allocation decisions need to balance the conflicting goals of fairness and efficiency. Our paper studies the challenging task of defining and modeling a proper fairness-efficiency trade off. We define fairness with Rawlsian leximax fairness, which views the lexicographic maximum among all feasible outcomes as the most equitable; and define efficiency with Utilitarianism, which seeks to maximize the sum of utilities received by entities regardless of individual differences. Motivated by a justice-driven trade off principle: prioritize fairness to benefit the less advantaged unless too much efficiency is sacrificed, we propose a sequential optimization procedure to balance leximax fairness and utilitarianism in decision-making. Each iteration of our approach maximizes a social welfare function, and we provide a practical mixed integer/linear programming (MILP) formulation for each maximization problem. We illustrate our method on a budget allocation example. Compared with existing approaches of balancing equity and efficiency, our method is more interpretable in terms of parameter selection, and incorporates a strong equity criterion with a thoroughly balanced perspective.},
	booktitle    = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
	pages        = {221–227},
	numpages     = 7,
	keywords     = {utilitarianism, trade off, fairness, distributive justice},
	location     = {New York, NY, USA},
	series       = {AIES '20},
}
@inproceedings{heaies20,
	author       = {He, Yuzi and Burghardt, Keith and Lerman, Kristina},
	title        = {A Geometric Solution to Fair Representations},
	year         = 2020,
	isbn         = 9781450371100,
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	url          = {https://doi.org/10.1145/3375627.3375864},
	doi          = {10.1145/3375627.3375864},
	abstract     = {To reduce human error and prejudice, many high-stakes decisions have been turned over to machine algorithms. However, recent research suggests that this does not remove discrimination, and can perpetuate harmful stereotypes. While algorithms have been developed to improve fairness, they typically face at least one of three shortcomings: they are not interpretable, their prediction quality deteriorates quickly compared to unbiased equivalents, and the methodology cannot easily extend other algorithms they are not easily transferable across models (e.g., methods to reduce bias in random forests cannot be extended to neural networks) . To address these shortcomings, we propose a geometric method that removes correlations between data and any number of protected variables. Further, we can control the strength of debiasing through an adjustable parameter to address the trade-off between prediction quality and fairness. The resulting features are interpretable and can be used with many popular models, such as linear regression, random forest, and multilayer perceptrons. The resulting predictions are found to be more accurate and fair compared to several state-of-the-art fair AI algorithms across a variety of benchmark datasets. Our work shows that debiasing data is a simple and effective solution toward improving fairness.},
	booktitle    = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
	pages        = {279–285},
	numpages     = 7,
	keywords     = {fair ai, fair classification, projection, sensitive information, debiased features, geometric method, orthogonal space, interpretable method},
	location     = {New York, NY, USA},
	series       = {AIES '20},
}
@inproceedings{dianaaies21,
	author       = {Diana, Emily and Gill, Wesley and Kearns, Michael and Kenthapadi, Krishnaram and Roth, Aaron},
	title        = {Minimax Group Fairness: Algorithms and Experiments},
	year         = 2021,
	isbn         = 9781450384735,
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	url          = {https://doi.org/10.1145/3461702.3462523},
	doi          = {10.1145/3461702.3462523},
	abstract     = {We consider a recently introduced framework in which fairness is measured by worst-case outcomes across groups, rather than by the more standard differences between group outcomes. In this framework we provide provably convergent oracle-efficient learning algorithms (or equivalently, reductions to non-fair learning) for minimax group fairness. Here the goal is that of minimizing the maximum loss across all groups, rather than equalizing group losses. Our algorithms apply to both regression and classification settings and support both overall error and false positive or false negative rates as the fairness measure of interest. They also support relaxations of the fairness constraints, thus permitting study of the tradeoff between overall accuracy and minimax fairness. We compare the experimental behavior and performance of our algorithms across a variety of fairness-sensitive data sets and show empirical cases in which minimax fairness is strictly and strongly preferable to equal outcome notions.},
	booktitle    = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
	pages        = {66–76},
	numpages     = 11,
	keywords     = {fair machine learning, minimax fairness, game theory},
	location     = {Virtual Event, USA},
	series       = {AIES '21},
}
@inproceedings{liuaies21,
	author       = {Liu, David and Shafi, Zohair and Fleisher, William and Eliassi-Rad, Tina and Alfeld, Scott},
	title        = {RAWLSNET: Altering Bayesian Networks to Encode Rawlsian Fair Equality of Opportunity},
	year         = 2021,
	isbn         = 9781450384735,
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	url          = {https://doi.org/10.1145/3461702.3462618},
	doi          = {10.1145/3461702.3462618},
	abstract     = {We present RAWLSNET, a system for altering Bayesian Network (BN) models to satisfy the Rawlsian principle of fair equality of opportunity (FEO). RAWLSNET's BN models generate aspirational data distributions: data generated to reflect an ideally fair, FEO-satisfying society. FEO states that everyone with the same talent and willingness to use it should have the same chance of achieving advantageous social positions (e.g., employment), regardless of their background circumstances (e.g., socioeconomic status). Satisfying FEO requires alterations to social structures such as school assignments. Our paper describes RAWLSNET, a method which takes as input a BN representation of an FEO application and alters the BN's parameters so as to satisfy FEO when possible, and minimize deviation from FEO otherwise. We also offer guidance for applying RAWLSNET, including on recognizing proper applications of FEO. We demonstrate the use of RAWLSNET with publicly available data sets. RAWLSNET's altered BNs offer the novel capability of generating aspirational data for FEO-relevant tasks. Aspirational data are free from biases of real-world data, and thus are useful for recognizing and detecting sources of unfairness in machine learning algorithms besides biased data.},
	booktitle    = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
	pages        = {745–755},
	numpages     = 11,
	keywords     = {rawlsian fair equality of opportunity, aspirational data, Bayesian networks},
	location     = {Virtual Event, USA},
	series       = {AIES '21},
}
@inproceedings{mhasawadeaies21,
	author       = {Mhasawade, Vishwali and Chunara, Rumi},
	title        = {Causal Multi-Level Fairness},
	year         = 2021,
	isbn         = 9781450384735,
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	url          = {https://doi.org/10.1145/3461702.3462587},
	doi          = {10.1145/3461702.3462587},
	abstract     = {Algorithmic systems are known to impact marginalized groups severely, and more so, if all sources of bias are not considered. While work in algorithmic fairness to-date has primarily focused on addressing discrimination due to individually linked attributes, social science research elucidates how some properties we link to individuals can be conceptualized as having causes at macro (e.g. structural) levels, and it may be important to be fair to attributes at multiple levels. For example, instead of simply considering race as a causal, protected attribute of an individual, the cause may be distilled as perceived racial discrimination an individual experiences, which in turn can be affected by neighborhood-level factors. This multi-level conceptualization is relevant to questions of fairness, as it may not only be important to take into account if the individual belonged to another demographic group, but also if the individual received advantaged treatment at the macro-level. In this paper, we formalize the problem of multi-level fairness using tools from causal inference in a manner that allows one to assess and account for effects of sensitive attributes at multiple levels. We show importance of the problem by illustrating residual unfairness if macro-level sensitive attributes are not accounted for, or included without accounting for their multi-level nature. Further, in the context of a real-world task of predicting income based on macro and individual-level attributes, we demonstrate an approach for mitigating unfairness, a result of multi-level sensitive attributes.},
	booktitle    = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
	pages        = {784–794},
	numpages     = 11,
	keywords     = {racial justice, fairness, social sciences},
	location     = {Virtual Event, USA},
	series       = {AIES '21},
}
@inproceedings{shahguptaaies21,
	author       = {Shah, Kulin and Gupta, Pooja and Deshpande, Amit and Bhattacharyya, Chiranjib},
	title        = {Rawlsian Fair Adaptation of Deep Learning Classifiers},
	year         = 2021,
	isbn         = 9781450384735,
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	url          = {https://doi.org/10.1145/3461702.3462592},
	doi          = {10.1145/3461702.3462592},
	abstract     = {Group-fairness in classification aims for equality of a predictive utility across different sensitive sub-populations, e.g., race or gender. Equality or near-equality constraints in group-fairness often worsen not only the aggregate utility but also the utility for the least advantaged sub-population. In this paper, we apply the principles of Pareto-efficiency and least-difference to the utility being accuracy, as an illustrative example, and arrive at the Rawls classifier that minimizes the error rate on the worst-off sensitive sub-population. Our mathematical characterization shows that the Rawls classifier uniformly applies a threshold to an ideal score of features, in the spirit of fair equality of opportunity. In practice, such a score or a feature representation is often computed by a black-box model that has been useful but unfair. Our second contribution is practical Rawlsian fair adaptation of any given black-box deep learning model, without changing the score or feature representation it computes. Given any score function or feature representation and only its second-order statistics on the sensitive sub-populations, we seek a threshold classifier on the given score or a linear threshold classifier on the given feature representation that achieves the Rawls error rate restricted to this hypothesis class. Our technical contribution is to formulate the above problems using ambiguous chance constraints, and to provide efficient algorithms for Rawlsian fair adaptation, along with provable upper bounds on the Rawls error rate. Our empirical results show significant improvement over state-of-the-art group-fair algorithms, even without retraining for fairness.},
	booktitle    = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
	pages        = {936–945},
	numpages     = 10,
	keywords     = {fairness for deep learning classifiers, fair adaptation, Rawlsian fairness},
	location     = {Virtual Event, USA},
	series       = {AIES '21},
}
@inproceedings{sharmaaies21,
	author       = {Sharma, Shubham and Gee, Alan H. and Paydarfar, David and Ghosh, Joydeep},
	title        = {FaiR-N: Fair and Robust Neural Networks for Structured Data},
	year         = 2021,
	isbn         = 9781450384735,
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	url          = {https://doi.org/10.1145/3461702.3462559},
	doi          = {10.1145/3461702.3462559},
	abstract     = {Fairness and robustness in machine learning are crucial when individuals are subject to automated decisions made by models in high-stake domains. To promote ethical artificial intelligence, fairness metrics that rely on comparing model error rates across subpopulations have been widely investigated for the detection and mitigation of bias. However, fairness measures that rely on comparing the ability to achieve recourse have been relatively unexplored. In this paper, we present a novel formulation for training neural networks that considers the distance of data observations to the decision boundary such that the new objective: (1) reduces the disparity in the average ability of recourse between individuals in each protected group, and (2) increases the average distance of data points to the boundary to promote adversarial robustness. We demonstrate that models trained with this new objective are more fair and adversarially robust neural networks, with similar accuracies, when compared to models without it. We also investigate a trade-off between the recourse-based fairness and robustness objectives. Moreover, we qualitatively motivate and empirically show that reducing recourse disparity across protected groups also improves fairness measures that rely on error rates. To the best of our knowledge, this is the first time that recourse disparity across groups are considered to train fairer neural networks.},
	booktitle    = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
	pages        = {946–955},
	numpages     = 10,
	keywords     = {robustness, ethical artificial intelligence, fairness, neural networks},
	location     = {Virtual Event, USA},
	series       = {AIES '21},
}
@inproceedings{mutluaies22,
	author       = {Mutlu, Ece \c{C}i\u{g}dem and Yousefi, Niloofar and Ozmen Garibay, Ozlem},
	title        = {Contrastive Counterfactual Fairness in Algorithmic Decision-Making},
	year         = 2022,
	isbn         = 9781450392471,
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	url          = {https://doi.org/10.1145/3514094.3534143},
	doi          = {10.1145/3514094.3534143},
	abstract     = {The widespread use of artificial intelligence algorithms and their role in decision-making with consequential decisions for human subjects has resulted in a growing interest in designing AI algorithms accounting for fairness considerations. There have been attempts to account for fairness of AI algorithms without compromising their accuracy to improve poorly designed algorithms that disregard sensitive attributes (e.g., age, race, and gender) at the peril of introducing or increasing bias against specific groups. Although many studies have examined the optimal trade-off between fairness and accuracy, it remains a challenge to understand the sources of unfairness in decision-making and mitigate it effectively. To tackle this problem, researchers have proposed fair causal learning approaches which assist us in modeling cause and effect knowledge structures, discovering bias sources, and refining AI algorithms to make them more transparent and explainable. In this study, we formalize probabilistic interpretations of both contrastive and counterfactual causality as essential features in order to encourage users' trust and to expand the applicability of such automated systems. We use this formalism to define a novel fairness criterion that we call contrastive counterfactual fairness. This paper introduces, to the best of our knowledge, the first probabilistic fairness-aware data augmentation approach that is based on contrastive counterfactual causality. We tested our approach on two well-known fairness-related datasets, UCI Adult and German Credit, and concluded that our proposed method has a promising ability to capture and mitigate unfairness in AI deployment. This model-agnostic approach can be used with any AI model because it is applied in pre-processing.},
	booktitle    = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
	pages        = {499–507},
	numpages     = 9,
	keywords     = {counterfactual fairness, fair classification, contrastive fairness, fair causal learning, fair data augmentation},
	location     = {Oxford, United Kingdom},
	series       = {AIES '22},
}
@inproceedings{halevyeaamo21,
	author       = {Halevy, Matan and Harris, Camille and Bruckman, Amy and Yang, Diyi and Howard, Ayanna},
	title        = {Mitigating Racial Biases in Toxic Language Detection with an Equity-Based Ensemble Framework},
	year         = 2021,
	isbn         = 9781450385534,
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	url          = {https://doi.org/10.1145/3465416.3483299},
	doi          = {10.1145/3465416.3483299},
	abstract     = {Recent research has demonstrated how racial biases against users who write African American English exists in popular toxic language datasets. While previous work has focused on a single fairness criteria, we propose to use additional descriptive fairness metrics to better understand the source of these biases. We demonstrate that different benchmark classifiers, as well as two in-process bias-remediation techniques, propagate racial biases even in a larger corpus. We then propose a novel ensemble-framework that uses a specialized classifier that is fine-tuned to the African American English dialect. We show that our proposed framework substantially reduces the racial biases that the model learns from these datasets. We demonstrate how the ensemble framework improves fairness metrics across all sample datasets with minimal impact on the classification performance, and provide empirical evidence for its ability to unlearn the annotation biases towards authors who use African American English. ** Please note that this work may contain examples of offensive words and phrases.},
	booktitle    = {Equity and Access in Algorithms, Mechanisms, and Optimization},
	articleno    = 7,
	numpages     = 11,
	keywords     = {moderation, hate speech detection, bias mitigation, AI fairness},
	location     = {--, NY, USA},
	series       = {EAAMO '21},
}
@inproceedings{thomaseaamo21,
	author       = {Thomas, Oliver and Zilka, Miri and Weller, Adrian and Quadrianto, Novi},
	title        = {An Algorithmic Framework for Positive Action},
	year         = 2021,
	isbn         = 9781450385534,
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	url          = {https://doi.org/10.1145/3465416.3483303},
	doi          = {10.1145/3465416.3483303},
	abstract     = {Positive action is defined within anti-discrimination legislation as voluntary, legal action taken to address an imbalance of opportunity affecting individuals belonging to under-represented groups. Within this theme, we propose a novel algorithmic fairness framework to advance equal representation while respecting anti-discrimination legislation and equal-treatment rights. We use a counterfactual fairness approach to assign one of three outcomes to each candidate: accept; reject; or flagged as a positive action candidate.},
	booktitle    = {Equity and Access in Algorithms, Mechanisms, and Optimization},
	articleno    = 18,
	numpages     = 13,
	keywords     = {Fair Machine Learning, Causal Inference, Auditing},
	location     = {--, NY, USA},
	series       = {EAAMO '21},
}
@inproceedings{currenteaamo22,
	author       = {Current, Sean and He, Yuntian and Gurukar, Saket and Parthasarathy, Srinivasan},
	title        = {FairEGM: Fair Link Prediction and Recommendation via Emulated Graph Modification},
	year         = 2022,
	isbn         = 9781450394772,
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	url          = {https://doi.org/10.1145/3551624.3555287},
	doi          = {10.1145/3551624.3555287},
	abstract     = {As machine learning becomes more widely adopted across domains, it is critical that researchers and ML engineers think about the inherent biases in the data that may be perpetuated by the model. Recently, many studies have shown that such biases are also imbibed in Graph Neural Network (GNN) models if the input graph is biased, potentially to the disadvantage of underserved and underrepresented communities. In this work, we aim to mitigate the bias learned by GNNs by jointly optimizing two different loss functions: one for the task of link prediction and one for the task of demographic parity. We further implement three different techniques inspired by graph modification approaches: the Global Fairness Optimization (GFO), Constrained Fairness Optimization (CFO), and Fair Edge Weighting (FEW) models. These techniques mimic the effects of changing underlying graph structures within the GNN and offer a greater degree of interpretability over more integrated neural network methods. Our proposed models emulate microscopic or macroscopic edits to the input graph while training GNNs and learn node embeddings that are both accurate and fair under the context of link recommendations. We demonstrate the effectiveness of our approach on four real world datasets and show that we can improve the recommendation fairness by several factors at negligible cost to link prediction accuracy.},
	booktitle    = {Equity and Access in Algorithms, Mechanisms, and Optimization},
	articleno    = 3,
	numpages     = 14,
	keywords     = {graph fairness, graph neural networks, group fairness, graph representation learning, graph convolution neural networks, link prediction, link recommendation, demographic parity},
	location     = {Arlington, VA, USA},
	series       = {EAAMO '22},
}
@inproceedings{fisheaamo22,
	author       = {Fish, Benjamin and Stark, Luke},
	title        = {It’s Not Fairness, and It’s Not Fair: The Failure of Distributional Equality and the Promise of Relational Equality in Complete-Information Hiring Games},
	year         = 2022,
	isbn         = 9781450394772,
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	url          = {https://doi.org/10.1145/3551624.3555296},
	doi          = {10.1145/3551624.3555296},
	abstract     = {Existing efforts to formulate computational definitions of fairness have largely focused on distributional notions of equality, defined through how resources or decisions are divided. Yet existing discrimination is often the result of unequal social relations, rather than simply an unequal distribution of resources. We show how optimizing for existing computational definitions of fairness fails to prevent unequal social relations by providing an example of a self-confirming equilibrium in a simple hiring market that is relationally unequal but satisfies existing distributional notions of fairness. We introduce a notion of blatant relational unfairness for complete-information games, and discuss how this definition helps initiate a new approach to incorporating relational equality into computational systems.},
	booktitle    = {Equity and Access in Algorithms, Mechanisms, and Optimization},
	articleno    = 11,
	numpages     = 15,
	location     = {Arlington, VA, USA},
	series       = {EAAMO '22},
}
@inproceedings{russellneurips17,
	author       = {Russell, Chris and Kusner, Matt J and Loftus, Joshua and Silva, Ricardo},
	booktitle    = {Advances in Neural Information Processing Systems},
	editor       = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
	pages        = {},
	publisher    = {Curran Associates, Inc.},
	title        = {When Worlds Collide: Integrating Different Counterfactual Assumptions in Fairness},
	url          = {https://proceedings.neurips.cc/paper\%5Ffiles/paper/2017/file/1271a7029c9df08643b631b02cf9e116-Paper.pdf},
	volume       = 30,
	year         = 2017,
}
@inproceedings{quadriantoneurips17,
	author       = {Quadrianto, Novi and Sharmanska, Viktoriia},
	booktitle    = {Advances in Neural Information Processing Systems},
	editor       = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
	pages        = {},
	publisher    = {Curran Associates, Inc.},
	title        = {Recycling Privileged Learning and Distribution Matching for Fairness},
	url          = {https://proceedings.neurips.cc/paper\%5Ffiles/paper/2017/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf},
	volume       = 30,
	year         = 2017,
}
@inproceedings{louppeneurips17,
	author       = {Louppe, Gilles and Kagan, Michael and Cranmer, Kyle},
	booktitle    = {Advances in Neural Information Processing Systems},
	editor       = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
	pages        = {},
	publisher    = {Curran Associates, Inc.},
	title        = {Learning to Pivot with Adversarial Networks},
	url          = {https://proceedings.neurips.cc/paper\%5Ffiles/paper/2017/file/48ab2f9b45957ab574cf005eb8a76760-Paper.pdf},
	volume       = 30,
	year         = 2017,
}
@inproceedings{zafarneurips17,
	author       = {Zafar, Muhammad Bilal and Valera, Isabel and Rodriguez, Manuel and Gummadi, Krishna and Weller, Adrian},
	booktitle    = {Advances in Neural Information Processing Systems},
	editor       = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
	pages        = {},
	publisher    = {Curran Associates, Inc.},
	title        = {From Parity to Preference-based Notions of Fairness in Classification},
	url          = {https://proceedings.neurips.cc/paper\%5Ffiles/paper/2017/file/82161242827b703e6acf9c726942a1e4-Paper.pdf},
	volume       = 30,
	year         = 2017,
}
@inproceedings{heidarineurips18,
	author       = {Heidari, Hoda and Ferrari, Claudio and Gummadi, Krishna and Krause, Andreas},
	booktitle    = {Advances in Neural Information Processing Systems},
	editor       = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
	pages        = {},
	publisher    = {Curran Associates, Inc.},
	title        = {Fairness Behind a Veil of Ignorance: A Welfare Analysis for Automated Decision Making},
	url          = {https://proceedings.neurips.cc/paper\%5Ffiles/paper/2018/file/be3159ad04564bfb90db9e32851ebf9c-Paper.pdf},
	volume       = 31,
	year         = 2018,
}
@inproceedings{kimneurips18,
	author       = {Kim, Michael and Reingold, Omer and Rothblum, Guy},
	booktitle    = {Advances in Neural Information Processing Systems},
	editor       = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
	pages        = {},
	publisher    = {Curran Associates, Inc.},
	title        = {Fairness Through Computationally-Bounded Awareness},
	url          = {https://proceedings.neurips.cc/paper\%5Ffiles/paper/2018/file/c8dfece5cc68249206e4690fc4737a8d-Paper.pdf},
	volume       = 31,
	year         = 2018,
}
@inproceedings{zhangneurips18,
	author       = {Zhang, Junzhe and Bareinboim, Elias},
	booktitle    = {Advances in Neural Information Processing Systems},
	editor       = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
	pages        = {},
	publisher    = {Curran Associates, Inc.},
	title        = {Equality of Opportunity in Classification: A Causal Approach},
	url          = {https://proceedings.neurips.cc/paper\%5Ffiles/paper/2018/file/ff1418e8cc993fe8abcfe3ce2003e5c5-Paper.pdf},
	volume       = 31,
	year         = 2018,
}
@inproceedings{sharifmalvajerdineurips19,
	author       = {Sharifi-Malvajerdi, Saeed and Kearns, Michael and Roth, Aaron},
	booktitle    = {Advances in Neural Information Processing Systems},
	editor       = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
	pages        = {},
	publisher    = {Curran Associates, Inc.},
	title        = {Average Individual Fairness: Algorithms, Generalization and Experiments},
	url          = {https://proceedings.neurips.cc/paper\%5Ffiles/paper/2019/file/0e1feae55e360ff05fef58199b3fa521-Paper.pdf},
	volume       = 32,
	year         = 2019,
}
@inproceedings{kallusneurips19,
	author       = {Kallus, Nathan and Zhou, Angela},
	booktitle    = {Advances in Neural Information Processing Systems},
	editor       = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
	pages        = {},
	publisher    = {Curran Associates, Inc.},
	title        = {The Fairness of Risk Scores Beyond Classification: Bipartite Ranking and the XAUC Metric},
	url          = {https://proceedings.neurips.cc/paper\%5Ffiles/paper/2019/file/73e0f7487b8e5297182c5a711d20bf26-Paper.pdf},
	volume       = 32,
	year         = 2019,
}
@inproceedings{harpeledneurips19,
	author       = {Har-Peled, Sariel and Mahabadi, Sepideh},
	booktitle    = {Advances in Neural Information Processing Systems},
	editor       = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
	pages        = {},
	publisher    = {Curran Associates, Inc.},
	title        = {Near Neighbor: Who is the Fairest of Them All?},
	url          = {https://proceedings.neurips.cc/paper\%5Ffiles/paper/2019/file/742141ceda6b8f6786609d31c8ef129f-Paper.pdf},
	volume       = 32,
	year         = 2019,
}
@inproceedings{chzhenneurips19,
	author       = {Chzhen, Evgenii and Denis, Christophe and Hebiri, Mohamed and Oneto, Luca and Pontil, Massimiliano},
	booktitle    = {Advances in Neural Information Processing Systems},
	editor       = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
	pages        = {},
	publisher    = {Curran Associates, Inc.},
	title        = {Leveraging Labeled and Unlabeled Data for Consistent Fair Binary Classification},
	url          = {https://proceedings.neurips.cc/paper\%5Ffiles/paper/2019/file/ba51e6158bcaf80fd0d834950251e693-Paper.pdf},
	volume       = 32,
	year         = 2019,
}
@inproceedings{liuneurips19,
	author       = {Leqi, Liu and Prasad, Adarsh and Ravikumar, Pradeep K},
	booktitle    = {Advances in Neural Information Processing Systems},
	editor       = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
	pages        = {},
	publisher    = {Curran Associates, Inc.},
	title        = {On Human-Aligned Risk Minimization},
	url          = {https://proceedings.neurips.cc/paper\%5Ffiles/paper/2019/file/cd6b73b67c77edeaff94e24b961119dd-Paper.pdf},
	volume       = 32,
	year         = 2019,
}
@inproceedings{balcanneurips19,
	author       = {Balcan, Maria-Florina F and Dick, Travis and Noothigattu, Ritesh and Procaccia, Ariel D},
	booktitle    = {Advances in Neural Information Processing Systems},
	editor       = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
	pages        = {},
	publisher    = {Curran Associates, Inc.},
	title        = {Envy-Free Classification},
	url          = {https://proceedings.neurips.cc/paper\%5Ffiles/paper/2019/file/e94550c93cd70fe748e6982b3439ad3b-Paper.pdf},
	volume       = 32,
	year         = 2019,
}
@inproceedings{romanoneurips20,
	author       = {Romano, Yaniv and Bates, Stephen and Candes, Emmanuel},
	booktitle    = {Advances in Neural Information Processing Systems},
	editor       = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
	pages        = {361--371},
	publisher    = {Curran Associates, Inc.},
	title        = {Achieving Equalized Odds by Resampling Sensitive Attributes},
	url          = {https://proceedings.neurips.cc/paper\%5Ffiles/paper/2020/file/03593ce517feac573fdaafa6dcedef61-Paper.pdf},
	volume       = 33,
	year         = 2020,
}
@inproceedings{lahotineurips20,
	author       = {Lahoti, Preethi and Beutel, Alex and Chen, Jilin and Lee, Kang and Prost, Flavien and Thain, Nithum and Wang, Xuezhi and Chi, Ed},
	booktitle    = {Advances in Neural Information Processing Systems},
	editor       = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
	pages        = {728--740},
	publisher    = {Curran Associates, Inc.},
	title        = {Fairness without Demographics through Adversarially Reweighted Learning},
	url          = {https://proceedings.neurips.cc/paper\%5Ffiles/paper/2020/file/07fc15c9d169ee48573edd749d25945d-Paper.pdf},
	volume       = 33,
	year         = 2020,
}
@inproceedings{yangneurips20,
	author       = {Yang, Forest and Cisse, Mouhamadou and Koyejo, Sanmi},
	booktitle    = {Advances in Neural Information Processing Systems},
	editor       = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
	pages        = {4067--4078},
	publisher    = {Curran Associates, Inc.},
	title        = {Fairness with Overlapping Groups; a Probabilistic Perspective},
	url          = {https://proceedings.neurips.cc/paper\%5Ffiles/paper/2020/file/29c0605a3bab4229e46723f89cf59d83-Paper.pdf},
	volume       = 33,
	year         = 2020,
}
@inproceedings{chzhenneurips20,
	author       = {Chzhen, Evgenii and Denis, Christophe and Hebiri, Mohamed and Oneto, Luca and Pontil, Massimiliano},
	booktitle    = {Advances in Neural Information Processing Systems},
	editor       = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
	pages        = {7321--7331},
	publisher    = {Curran Associates, Inc.},
	title        = {Fair regression with Wasserstein barycenters},
	url          = {https://proceedings.neurips.cc/paper\%5Ffiles/paper/2020/file/51cdbd2611e844ece5d80878eb770436-Paper.pdf},
	volume       = 33,
	year         = 2020,
}
@inproceedings{chenneurips20,
	author       = {Chen, Sitan and Koehler, Frederic and Moitra, Ankur and Yau, Morris},
	booktitle    = {Advances in Neural Information Processing Systems},
	editor       = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
	pages        = {8391--8403},
	publisher    = {Curran Associates, Inc.},
	title        = {Classification Under Misspecification: Halfspaces, Generalized Linear Models, and Evolvability},
	url          = {https://proceedings.neurips.cc/paper\%5Ffiles/paper/2020/file/5f8b73c0d4b1bf60dd7173b660b87c29-Paper.pdf},
	volume       = 33,
	year         = 2020,
}
@inproceedings{yanglorchneurips20,
	author       = {Yang, Wanqian and Lorch, Lars and Graule, Moritz and Lakkaraju, Himabindu and Doshi-Velez, Finale},
	booktitle    = {Advances in Neural Information Processing Systems},
	editor       = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
	pages        = {12721--12731},
	publisher    = {Curran Associates, Inc.},
	title        = {Incorporating Interpretable Output Constraints in Bayesian Neural Networks},
	url          = {https://proceedings.neurips.cc/paper\%5Ffiles/paper/2020/file/95c7dfc5538e1ce71301cf92a9a96bd0-Paper.pdf},
	volume       = 33,
	year         = 2020,
}
@inproceedings{choneurips20,
	author       = {Cho, Jaewoong and Hwang, Gyeongjo and Suh, Changho},
	booktitle    = {Advances in Neural Information Processing Systems},
	editor       = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
	pages        = {15088--15099},
	publisher    = {Curran Associates, Inc.},
	title        = {A Fair Classifier Using Kernel Density Estimation},
	url          = {https://proceedings.neurips.cc/paper\%5Ffiles/paper/2020/file/ac3870fcad1cfc367825cda0101eee62-Paper.pdf},
	volume       = 33,
	year         = 2020,
}
@inproceedings{chzhendenisneurips20,
	author       = {Chzhen, Evgenii and Denis, Christophe and Hebiri, Mohamed and Oneto, Luca and Pontil, Massimiliano},
	booktitle    = {Advances in Neural Information Processing Systems},
	editor       = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
	pages        = {19137--19148},
	publisher    = {Curran Associates, Inc.},
	title        = {Fair regression via plug-in estimator and recalibration with statistical guarantees},
	url          = {https://proceedings.neurips.cc/paper\%5Ffiles/paper/2020/file/ddd808772c035aed516d42ad3559be5f-Paper.pdf},
	volume       = 33,
	year         = 2020,
}
@inproceedings{smithneurips20,
	author       = {Smith, Gavin and Mansilla, Roberto and Goulding, James},
	booktitle    = {Advances in Neural Information Processing Systems},
	editor       = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
	pages        = {22305--22315},
	publisher    = {Curran Associates, Inc.},
	title        = {Model Class Reliance for Random Forests},
	url          = {https://proceedings.neurips.cc/paper\%5Ffiles/paper/2020/file/fd512441a1a791770a6fa573d688bff5-Paper.pdf},
	volume       = 33,
	year         = 2020,
}
@inproceedings{rohneurips21,
	author       = {Roh, Yuji and Lee, Kangwook and Whang, Steven and Suh, Changho},
	booktitle    = {Advances in Neural Information Processing Systems},
	editor       = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
	pages        = {815--827},
	publisher    = {Curran Associates, Inc.},
	title        = {Sample Selection for Fair and Robust Training},
	url          = {https://proceedings.neurips.cc/paper\%5Ffiles/paper/2021/file/07563a3fe3bbe7e3ba84431ad9d055af-Paper.pdf},
	volume       = 34,
	year         = 2021,
}
@inproceedings{huangneurips21,
	author       = {Huang, Feihu and Wu, Xidong and Huang, Heng},
	booktitle    = {Advances in Neural Information Processing Systems},
	editor       = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
	pages        = {10431--10443},
	publisher    = {Curran Associates, Inc.},
	title        = {Efficient Mirror Descent Ascent Methods for Nonsmooth Minimax Problems},
	url          = {https://proceedings.neurips.cc/paper\%5Ffiles/paper/2021/file/56503192b14190d3826780d47c0d3bf3-Paper.pdf},
	volume       = 34,
	year         = 2021,
}
@inproceedings{shekharneurips21,
	author       = {Shekhar, Shubhanshu and Fields, Greg and Ghavamzadeh, Mohammad and Javidi, Tara},
	booktitle    = {Advances in Neural Information Processing Systems},
	editor       = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
	pages        = {24535--24544},
	publisher    = {Curran Associates, Inc.},
	title        = {Adaptive Sampling for Minimax Fair Classification},
	url          = {https://proceedings.neurips.cc/paper\%5Ffiles/paper/2021/file/cd7c230fc5deb01ff5f7b1be1acef9cf-Paper.pdf},
	volume       = 34,
	year         = 2021,
}
@inproceedings{petersenneurips21,
	author       = {Petersen, Felix and Mukherjee, Debarghya and Sun, Yuekai and Yurochkin, Mikhail},
	booktitle    = {Advances in Neural Information Processing Systems},
	editor       = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
	pages        = {25944--25955},
	publisher    = {Curran Associates, Inc.},
	title        = {Post-processing for Individual Fairness},
	url          = {https://proceedings.neurips.cc/paper\%5Ffiles/paper/2021/file/d9fea4ca7e4a74c318ec27c1deb0796c-Paper.pdf},
	volume       = 34,
	year         = 2021,
}
@inproceedings{bendekgeyneurips21,
	author       = {Bendekgey, Henry C and Sudderth, Erik},
	booktitle    = {Advances in Neural Information Processing Systems},
	editor       = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
	pages        = {30023--30036},
	publisher    = {Curran Associates, Inc.},
	title        = {Scalable and Stable Surrogates for Flexible Classifiers with Fairness Constraints},
	url          = {https://proceedings.neurips.cc/paper\%5Ffiles/paper/2021/file/fc2e6a440b94f64831840137698021e1-Paper.pdf},
	volume       = 34,
	year         = 2021,
}

@inproceedings{heringtonaies2020,
author = {Herington, Jonathan},
title = {Measuring Fairness in an Unfair World},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375854},
doi = {10.1145/3375627.3375854},
abstract = {Computer scientists have made great strides in characterizing different measures of algorithmic fairness, and showing that certain measures of fairness cannot be jointly satisfied. In this paper, I argue that the three most popular families of measures - unconditional independence, target-conditional independence and classification-conditional independence - make assumptions that are unsustainable in the context of an unjust world. I begin by introducing the measures and the implicit idealizations they make about the underlying causal structure of the contexts in which they are deployed. I then discuss how these idealizations fall apart in the context of historical injustice, ongoing unmodeled oppression, and the permissibility of using sensitive attributes to rectify injustice. In the final section, I suggest an alternative framework for measuring fairness in the context of existing injustice: distributive fairness.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {286–292},
numpages = {7},
keywords = {causal inference, distributive justice, discrimination, fairness, algorithmic decision-making},
location = {New York, NY, USA},
series = {AIES '20}
}
@inproceedings{cousinsneurips21,
 author = {Cousins, Cyrus},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {16610--16621},
 publisher = {Curran Associates, Inc.},
 title = {An Axiomatic Theory of Provably-Fair Welfare-Centric Machine Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/8b0bb3eff8c1e5bf7f206125959921d7-Paper.pdf},
 volume = {34},
 year = {2021}
}
@inproceedings{NEURIPS2019_d69768b3,
 author = {Metevier, Blossom and Giguere, Stephen and Brockman, Sarah and Kobren, Ari and Brun, Yuriy and Brunskill, Emma and Thomas, Philip S.},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Offline Contextual Bandits with High Probability Fairness Guarantees},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/d69768b3da745b77e82cdbddcc8bac98-Paper.pdf},
 volume = {32},
 year = {2019}
}
@inproceedings{narasimhanneurips20,
 author = {Narasimhan, Harikrishna and Cotter, Andrew and Zhou, Yichen and Wang, Serena and Guo, Wenshuo},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {8693--8703},
 publisher = {Curran Associates, Inc.},
 title = {Approximate Heavily-Constrained Learning with Lagrange Multiplier Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/62db9e3397c76207a687c360e0243317-Paper.pdf},
 volume = {33},
 year = {2020}
}
@inproceedings{bechavodneurips20,
 author = {Bechavod, Yahav and Jung, Christopher and Wu, Steven Z.},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {11214--11225},
 publisher = {Curran Associates, Inc.},
 title = {Metric-Free Individual Fairness in Online Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/80b618ebcac7aa97a6dac2ba65cb7e36-Paper.pdf},
 volume = {33},
 year = {2020}
}
@inproceedings{caycineurips20,
 author = {Cayci, Semih and Gupta, Swati and Eryilmaz, Atilla},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {13750--13761},
 publisher = {Curran Associates, Inc.},
 title = {Group-Fair Online Allocation in Continuous Time},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/9ec0cfdc84044494e10582436e013e64-Paper.pdf},
 volume = {33},
 year = {2020}
}
@inproceedings{doneurips21,
 author = {Do, Virginie and Corbett-Davies, Sam and Atif, Jamal and Usunier, Nicolas},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {8596--8608},
 publisher = {Curran Associates, Inc.},
 title = {Two-sided fairness in rankings via Lorenz dominance},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/48259990138bc03361556fb3f94c5d45-Paper.pdf},
 volume = {34},
 year = {2021}
}
@inproceedings{singhneurips21,
 author = {Singh, Ashudeep and Kempe, David and Joachims, Thorsten},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {11896--11908},
 publisher = {Curran Associates, Inc.},
 title = {Fairness in Ranking under Uncertainty},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/63c3ddcc7b23daa1e42dc41f9a44a873-Paper.pdf},
 volume = {34},
 year = {2021}
}
@inproceedings{xuneurips21,
 author = {Xu, Xinyi and Lyu, Lingjuan and Ma, Xingjun and Miao, Chenglin and Foo, Chuan Sheng and Low, Bryan Kian Hsiang},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {16104--16117},
 publisher = {Curran Associates, Inc.},
 title = {Gradient Driven Rewards to Guarantee Fairness in Collaborative Machine Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/8682cc30db9c025ecd3fee433f8ab54c-Paper.pdf},
 volume = {34},
 year = {2021}
}
@inproceedings{barikneurips21,
 author = {Barik, Adarsh and Honorio, Jean},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {23245--23257},
 publisher = {Curran Associates, Inc.},
 title = {Fair Sparse Regression with Clustering: An Invex Relaxation for a Combinatorial Problem},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/c39b9a47811f1eaf3244a63ae8c22734-Paper.pdf},
 volume = {34},
 year = {2021}
}




@inproceedings{elzaynfat19,
author = {Elzayn, Hadi and Jabbari, Shahin and Jung, Christopher and Kearns, Michael and Neel, Seth and Roth, Aaron and Schutzman, Zachary},
title = {Fair Algorithms for Learning in Allocation Problems},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287571},
doi = {10.1145/3287560.3287571},
abstract = {Settings such as lending and policing can be modeled by a centralized agent allocating a scarce resource (e.g. loans or police officers) amongst several groups, in order to maximize some objective (e.g. loans given that are repaid, or criminals that are apprehended). Often in such problems fairness is also a concern. One natural notion of fairness, based on general principles of equality of opportunity, asks that conditional on an individual being a candidate for the resource in question, the probability of actually receiving it is approximately independent of the individual's group. For example, in lending this would mean that equally creditworthy individuals in different racial groups have roughly equal chances of receiving a loan. In policing it would mean that two individuals committing the same crime in different districts would have roughly equal chances of being arrested.In this paper, we formalize this general notion of fairness for allocation problems and investigate its algorithmic consequences. Our main technical results include an efficient learning algorithm that converges to an optimal fair allocation even when the allocator does not know the frequency of candidates (i.e. creditworthy individuals or criminals) in each group. This algorithm operates in a censored feedback model in which only the number of candidates who received the resource in a given allocation can be observed, rather than the true number of candidates in each group. This models the fact that we do not learn the creditworthiness of individuals we do not give loans to and do not learn about crimes committed if the police presence in a district is low.As an application of our framework and algorithm, we consider the predictive policing problem, in which the resource being allocated to each group is the number of police officers assigned to each district. The learning algorithm is trained on arrest data gathered from its own deployments on previous days, resulting in a potential feedback loop that our algorithm provably overcomes. In this case, the fairness constraint asks that the probability that an individual who has committed a crime is arrested should be independent of the district in which they live. We investigate the performance of our learning algorithm on the Philadelphia Crime Incidents dataset.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {170–179},
numpages = {10},
keywords = {algorithmic fairness, resource allocation, censored feedback, online learning},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}

@inproceedings{canettifat19,
author = {Canetti, Ran and Cohen, Aloni and Dikkala, Nishanth and Ramnarayan, Govind and Scheffler, Sarah and Smith, Adam},
title = {From Soft Classifiers to Hard Decisions: How Fair Can We Be?},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287561},
doi = {10.1145/3287560.3287561},
abstract = {A popular methodology for building binary decision-making classifiers in the presence of imperfect information is to first construct a calibrated non-binary "scoring" classifier, and then to post-process this score to obtain a binary decision. We study various fairness (or, error-balance) properties of this methodology, when the non-binary scores are calibrated over all protected groups, and with a variety of post-processing algorithms. Specifically, we show:First, there does not exist a general way to post-process a calibrated classifier to equalize protected groups' positive or negative predictive value (PPV or NPV). For certain "nice" calibrated classifiers, either PPV or NPV can be equalized when the post-processor uses different thresholds across protected groups. Still, when the post-processing consists of a single global threshold across all groups, natural fairness properties, such as equalizing PPV in a nontrivial way, do not hold even for "nice" classifiers.Second, when the post-processing stage is allowed to defer on some decisions (that is, to avoid making a decision by handing off some examples to a separate process), then for the non-deferred decisions, the resulting classifier can be made to equalize PPV, NPV, false positive rate (FPR) and false negative rate (FNR) across the protected groups. This suggests a way to partially evade the impossibility results of Chouldechova and Kleinberg et al., which preclude equalizing all of these measures simultaneously. We also present different deferring strategies and show how they affect the fairness properties of the overall system.We evaluate our post-processing techniques using the COMPAS data set from 2016.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {309–318},
numpages = {10},
keywords = {post-processing, classification, algorithmic fairness},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}

@inproceedings{ilventofat20,
author = {Ilvento, Christina and Jagadeesan, Meena and Chawla, Shuchi},
title = {Multi-Category Fairness in Sponsored Search Auctions},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372848},
doi = {10.1145/3351095.3372848},
abstract = {Fairness in advertising is a topic of particular concern motivated by theoretical and empirical observations in both the computer science and economics literature. We examine the problem of fairness in advertising for general purpose platforms that service advertisers from many different categories. First, we propose inter-category and intra-category fairness desiderata that take inspiration from individual fairness and envy-freeness. Second, we investigate the "platform utility" (a proxy for the quality of allocation) achievable by mechanisms satisfying these desiderata. More specifically, we compare the utility of fair mechanisms against the unfair optimum, and show by construction that our fairness desiderata are compatible with utility. Our mechanisms also enjoy nice implementation properties including metric-obliviousness, which allows the platform to produce fair allocations without needing to know the specifics of the fairness requirements.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {348–358},
numpages = {11},
keywords = {utility, advertisement auctions, envy-freeness, algorithmic fairness, individual fairness},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{kimfat20,
author = {Kim, Michael P. and Korolova, Aleksandra and Rothblum, Guy N. and Yona, Gal},
title = {Preference-Informed Fairness},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3373155},
doi = {10.1145/3351095.3373155},
abstract = {In this work, we study notions of fairness in decision-making systems when individuals have diverse preferences over the possible outcomes of the decisions. Our starting point is the seminal work of Dwork et al. [ITCS 2012] which introduced a notion of individual fairness (IF): given a task-specific similarity metric, every pair of individuals who are similarly qualified according to the metric should receive similar outcomes. We show that when individuals have diverse preferences over outcomes, requiring IF may unintentionally lead to less-preferred outcomes for the very individuals that IF aims to protect (e.g. a protected minority group). A natural alternative to IF is the classic notion of fair division, envy-freeness (EF): no individual should prefer another individual's outcome over their own. Although EF allows for solutions where all individuals receive a highly-preferred outcome, EF may also be overly-restrictive for the decision-maker. For instance, if many individuals agree on the best outcome, then if any individual receives this outcome, they all must receive it, regardless of each individual's underlying qualifications for the outcome.We introduce and study a new notion of preference-informed individual fairness (PIIF) that is a relaxation of both individual fairness and envy-freeness. At a high-level, PIIF requires that outcomes satisfy IF-style constraints, but allows for deviations provided they are in line with individuals' preferences. We show that PIIF can permit outcomes that are more favorable to individuals than any IF solution, while providing considerably more flexibility to the decision-maker than EF. In addition, we show how to efficiently optimize any convex objective over the outcomes subject to PIIF for a rich class of individual preferences. Finally, we demonstrate the broad applicability of the PIIF framework by extending our definitions and algorithms to the multiple-task targeted advertising setting introduced by Dwork and Ilvento [ITCS 2019].},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {546},
numpages = {1},
keywords = {algorithmic fairness},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{costonfat20,
author = {Coston, Amanda and Mishler, Alan and Kennedy, Edward H. and Chouldechova, Alexandra},
title = {Counterfactual Risk Assessments, Evaluation, and Fairness},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372851},
doi = {10.1145/3351095.3372851},
abstract = {Algorithmic risk assessments are increasingly used to help humans make decisions in high-stakes settings, such as medicine, criminal justice and education. In each of these cases, the purpose of the risk assessment tool is to inform actions, such as medical treatments or release conditions, often with the aim of reducing the likelihood of an adverse event such as hospital readmission or recidivism. Problematically, most tools are trained and evaluated on historical data in which the outcomes observed depend on the historical decision-making policy. These tools thus reflect risk under the historical policy, rather than under the different decision options that the tool is intended to inform. Even when tools are constructed to predict risk under a specific decision, they are often improperly evaluated as predictors of the target outcome.Focusing on the evaluation task, in this paper we define counterfactual analogues of common predictive performance and algorithmic fairness metrics that we argue are better suited for the decision-making context. We introduce a new method for estimating the proposed metrics using doubly robust estimation. We provide theoretical results that show that only under strong conditions can fairness according to the standard metric and the counterfactual metric simultaneously hold. Consequently, fairness-promoting methods that target parity in a standard fairness metric may---and as we show empirically, do---induce greater imbalance in the counterfactual analogue. We provide empirical comparisons on both synthetic data and a real world child welfare dataset to demonstrate how the proposed method improves upon standard practice.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {582–593},
numpages = {12},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{cohenfacct21,
author = {Cohen, Maxime C. and Elmachtoub, Adam N. and Lei, Xiao},
title = {Price Discrimination with Fairness Constraints},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445864},
doi = {10.1145/3442188.3445864},
abstract = {Price discrimination - offering different prices to different customers - has become common practice. While it allows sellers to increase their profits, it also raises several concerns in terms of fairness. This topic has received extensive attention from media, industry, and regulatory agencies. In this paper, we consider the problem of setting prices for different groups under fairness constraints.In this paper, we propose a formal framework for pricing with fairness, including several definitions of fairness and their potential impact on consumers, sellers, and society at large. In a first step towards the ambitious agenda of designing pricing strategies that are fair, we consider the simplest scenario of a single-product seller facing consumers who can be partitioned into two groups based on a single, binary feature observable to the seller. For each group, we assume that the seller knows the valuation distribution and the population size. The seller's goal is to maximize profit by optimally selecting a price for each group, subject to a fairness constraint which may be self-imposed or explicitly enforced by laws and regulations.We first propose four definitions: fairness in price, demand, consumer surplus, and no-purchase valuation. With our model and definitions in place, we first show that satisfying all four fairness goals simultaneously is impossible unless the mean valuations are the same for both groups. In fact, even achieving two fairness measures simultaneously cannot be done in basic settings. We then consider the impact of imposing each fairness criterion separately, and identify conditions under which the consumer surplus and the social welfare increase or decrease. Under linear or exponential demand, we show that imposing a small amount of fairness in price or no-purchase valuation increases social welfare, whereas fairness in demand or surplus reduces social welfare. We fully characterize the impact of imposing different types of fairness for linear demand. We discover that imposing too much price fairness may result in a lower social welfare relative to imposing no price fairness. Imposing demand and surplus fairness always decreases social welfare. However, imposing no-purchase valuation fairness always increases social welfare. We also extend our results to the cases when there are multiple groups or there is an unprotected feature.Finally, we computationally show that most of our findings continue to hold for three common nonlinear demand models. Our results and insights provide a first step in understanding the impact of imposing fairness in the context of pricing.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {2},
numpages = {1},
keywords = {price discrimination, social welfare, fairness, personalization},
location = {Virtual Event, Canada},
series = {FAccT '21}
}

@inproceedings{mishlerfacct21,
author = {Mishler, Alan and Kennedy, Edward H. and Chouldechova, Alexandra},
title = {Fairness in Risk Assessment Instruments: Post-Processing to Achieve Counterfactual Equalized Odds},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445902},
doi = {10.1145/3442188.3445902},
abstract = {In domains such as criminal justice, medicine, and social welfare, decision makers increasingly have access to algorithmic Risk Assessment Instruments (RAIs). RAIs estimate the risk of an adverse outcome such as recidivism or child neglect, potentially informing high-stakes decisions such as whether to release a defendant on bail or initiate a child welfare investigation. It is important to ensure that RAIs are fair, so that the benefits and harms of such decisions are equitably distributed.The most widely used algorithmic fairness criteria are formulated with respect to observable outcomes, such as whether a person actually recidivates, but these criteria are misleading when applied to RAIs. Since RAIs are intended to inform interventions that can reduce risk, the prediction itself affects the downstream outcome. Recent work has argued that fairness criteria for RAIs should instead utilize potential outcomes, i.e. the outcomes that would occur in the absence of an appropriate intervention [11]. However, no methods currently exist to satisfy such fairness criteria.In this paper, we target one such criterion, counterfactual equalized odds. We develop a post-processed predictor that is estimated via doubly robust estimators, extending and adapting previous postprocessing approaches [16] to the counterfactual setting. We also provide doubly robust estimators of the risk and fairness properties of arbitrary fixed post-processed predictors. Our predictor converges to an optimal fair predictor at fast rates. We illustrate properties of our method and show that it performs well on both simulated and real data.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {386–400},
numpages = {15},
keywords = {risk assessment, fairness, post-processing, counterfactual},
location = {Virtual Event, Canada},
series = {FAccT '21}
}

@inproceedings{wangfacct22,
author = {Wang, Xiuling and Wang, Wendy Hui},
title = {Providing Item-Side Individual Fairness for Deep Recommender Systems},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533079},
doi = {10.1145/3531146.3533079},
abstract = {Recent advent of deep learning techniques have reinforced the development of new recommender systems. Although these systems have been demonstrated as efficient and effective, the issue of item popularity bias in these recommender systems has raised serious concerns. While most of the existing works focus on group fairness at item side, individual fairness at item side is left largely unexplored. To address this issue, in this paper, first, we define a new notion of individual fairness from the perspective of items, namely (α, β)-fairness, to deal with item popularity bias in recommendations. In particular, (α, β)-fairness requires that similar items should receive similar coverage in the recommendations, where α and β control item similarity and coverage similarity respectively, and both item and coverage similarity metrics are defined as task specific for deep recommender systems. Next, we design two bias mitigation methods, namely embedding-based re-ranking (ER) and greedy substitution (GS), for deep recommender systems. ER is an in-processing mitigation method that equips (α, β)-fairness as a constraint to the objective function of the recommendation algorithm, while GS is a post-processing approach that accepts the biased recommendations as the input, and substitutes high-coverage items with low-coverage ones in the recommendations to satisfy (α, β)-fairness. We evaluate the performance of both mitigation algorithms on two real-world datasets and a set of state-of-the-art deep recommender systems. Our results demonstrate that both ER and GS outperform the existing minimum-coverage (MC) mitigation solutions [Koutsopoulos and Halkidi 2018; Patro et&nbsp;al. 2020] in terms of both fairness and accuracy of recommendations. Furthermore, ER delivers the best trade-off between fairness and recommendation accuracy among a set of alternative mitigation methods, including GS, the hybrid of ER and GS, and the existing MC solutions.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {117–127},
numpages = {11},
keywords = {Individual fairness, algorithmic fairness in machine learning, deep recommender systems},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}

@inproceedings{zhangmarilynfacct22,
author = {Zhang, Marilyn},
title = {Affirmative Algorithms: Relational Equality as Algorithmic Fairness},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533115},
doi = {10.1145/3531146.3533115},
abstract = {Many statistical fairness notions have been proposed for algorithmic decision-making systems, and especially public safety pretrial risk assessment (PSPRA) algorithms such as COMPAS. Most fairness notions equalize something between groups, whether it is false positive rates or accuracy. In fact, I demonstrate that most prominent notions have their basis in equalizing some form of accuracy. However, statistical fairness metrics often do not capture the substantive point of equality. I argue that equal accuracy is not only difficult to measure but also unsatisfactory for ensuring equal justice. In response, I introduce philosopher Elizabeth Anderson’s theory of relational equality as a fruitful alternative framework: to relate as equals, people need access to certain basic capabilities. I show that relational equality requires Affirmative PSPRA algorithms that lower risk scores for Black defendants. This is because fairness based on relational equality means considering the impact of PSPRA algorithms’ decisions on access to basic capabilities. This impact is racially asymmetric in an unjust society. I make three main contributions: (1) I illustrate the shortcomings of statistical fairness notions in their reliance on equalizing some form of accuracy; (2) I present the first comprehensive ethical defense of Affirmative PSPRA algorithms, based on fairness in terms of relational equality instead; and (3) I show that equalizing accuracy is neither sufficient nor necessary for fairness based on relational equality. Overall, this work serves narrowly as a reason to re-evaluate algorithmic fairness for PSPRA algorithms, and serves broadly as an example of how discussions of algorithmic fairness can benefit from egalitarian philosophical frameworks.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {495–507},
numpages = {13},
keywords = {affirmative algorithms, relational equality, fairness, philosophy, pretrial risk assessments, criminal justice, algorithmic fairness},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}

@inproceedings{mishlerfadefacct22,
author = {Mishler, Alan and Kennedy, Edward H.},
title = {FADE: FAir Double Ensemble Learning for Observable and Counterfactual Outcomes},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533167},
doi = {10.1145/3531146.3533167},
abstract = {Methods for building fair predictors often involve tradeoffs between fairness and accuracy and between different fairness criteria. Recent work seeks to characterize these tradeoffs in specific problem settings, but these methods often do not accommodate users who wish to improve the fairness of an existing benchmark model without sacrificing accuracy, or vice versa. These results are also typically restricted to observable accuracy and fairness criteria. We develop a flexible framework for fair ensemble learning that allows users to efficiently explore the fairness-accuracy space or to improve the fairness or accuracy of a benchmark model. Our framework can simultaneously target multiple observable or counterfactual fairness criteria, and it enables users to combine a large number of previously trained and newly trained predictors. We provide theoretical guarantees that our estimators converge at fast rates. We apply our method on both simulated and real data, with respect to both observable and counterfactual accuracy and fairness criteria. We show that, surprisingly, multiple unfairness measures can sometimes be minimized simultaneously with little impact on accuracy, relative to unconstrained predictors or existing benchmark models.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1053},
numpages = {1},
keywords = {semiparametric, fairness, ensemble learning, counterfactual},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}

@inproceedings{wufacct22,
author = {Wu, Ziwei and He, Jingrui},
title = {Fairness-Aware Model-Agnostic Positive and Unlabeled Learning},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533225},
doi = {10.1145/3531146.3533225},
abstract = {With the increasing application of machine learning in high-stake decision-making problems, potential algorithmic bias towards people from certain social groups poses negative impacts on individuals and our society at large. In the real-world scenario, many such problems involve positive and unlabeled data such as medical diagnosis, criminal risk assessment and recommender systems. For instance, in medical diagnosis, only the diagnosed diseases will be recorded (positive) while others will not (unlabeled). Despite the large amount of existing work on fairness-aware machine learning in the (semi-)supervised and unsupervised settings, the fairness issue is largely under-explored in the aforementioned Positive and Unlabeled Learning (PUL) context, where it is usually more severe. In this paper, to alleviate this tension, we propose a fairness-aware PUL method named FairPUL. In particular, for binary classification over individuals from two populations, we aim to achieve similar true positive rates and false positive rates in both populations as our fairness metric. Based on the analysis of the optimal fair classifier for PUL, we design a model-agnostic post-processing framework, leveraging both the positive examples and unlabeled ones. Our framework is proven to be statistically consistent in terms of both the classification error and the fairness metric. Experiments on the synthetic and real-world data sets demonstrate that our framework outperforms state-of-the-art in both PUL and fair classification.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1698–1708},
numpages = {11},
keywords = {Machine Learning, Fairness, Positive and Unlabeled Learning},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}


@inproceedings{grabowiczfacct22,
author = {Grabowicz, Przemyslaw A. and Perello, Nicholas and Mishra, Aarshee},
title = {Marrying Fairness and Explainability in Supervised Learning},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533236},
doi = {10.1145/3531146.3533236},
abstract = {Machine learning algorithms that aid human decision-making may inadvertently discriminate against certain protected groups. Therefore, we formalize direct discrimination as a direct causal effect of the protected attributes on the decisions, while induced discrimination as a change in the causal influence of non-protected features associated with the protected attributes. The measurements of marginal direct effect (MDE) and SHapley Additive exPlanations (SHAP) reveal that state-of-the-art fair learning methods can induce discrimination via association or reverse discrimination in synthetic and real-world datasets. To inhibit discrimination in algorithmic systems, we propose to nullify the influence of the protected attribute on the output of the system, while preserving the influence of remaining features. We introduce and study post-processing methods achieving such objectives, finding that they yield relatively high model accuracy, prevent direct discrimination, and diminishes various disparity measures, e.g., demographic disparity.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1905–1916},
numpages = {12},
keywords = {discrimination, algorithmic fairness, supervised learning, machine learning, explainability},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}

@inproceedings{usunierfacct22,
author = {Usunier, Nicolas and Do, Virginie and Dohmatob, Elvis},
title = {Fast Online Ranking with Fairness of Exposure},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3534633},
doi = {10.1145/3531146.3534633},
abstract = {As recommender systems become increasingly central for sorting and prioritizing the content available online, they have a growing impact on the opportunities or revenue of their items producers. For instance, they influence which recruiter a resume is recommended to, or to whom and how much a music track, video or news article is being exposed. This calls for recommendation approaches that not only maximize (a proxy of) user satisfaction, but also consider some notion of fairness in the exposure of items or groups of items. Formally, such recommendations are usually obtained by maximizing a concave objective function in the space of randomized rankings. When the total exposure of an item is defined as the sum of its exposure over users, the optimal rankings of every users become coupled, which makes the optimization process challenging. Existing approaches to find these rankings either solve the global optimization problem in a batch setting, i.e., for all users at once, which makes them inapplicable at scale, or are based on heuristics that have weak theoretical guarantees. In this paper, we propose the first efficient online algorithm to optimize concave objective functions in the space of rankings which applies to every concave and smooth objective function, such as the ones found for fairness of exposure. Based on online variants of the Frank-Wolfe algorithm, we show that our algorithm is computationally fast, generating rankings on-the-fly with computation cost dominated by the sort operation, memory efficient, and has strong theoretical guarantees. Compared to baseline policies that only maximize user-side performance, our algorithm allows to incorporate complex fairness of exposure criteria in the recommendations with negligible computational overhead. We present experiments on artificial music and movie recommendation tasks using Last.fm and MovieLens datasets which suggest that in practice, the algorithm rapidly reaches good performances on three different objectives representing different fairness of exposure criteria.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {2157–2167},
numpages = {11},
keywords = {fairness, recommender systems, online ranking},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}

